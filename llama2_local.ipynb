{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93430d03074848f9be8f3dfbf081195f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62b42fb170914ae2859ad9f4a4f6c6e3",
              "IPY_MODEL_626c82c514f9496392d669ca94bea5d8",
              "IPY_MODEL_2554ac96ec0d42148bb9f24ebb9645d1"
            ],
            "layout": "IPY_MODEL_87566fff38c6452895dd5abcc647cfa4"
          }
        },
        "62b42fb170914ae2859ad9f4a4f6c6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb94b6b01ae340199cc06b463ebad2dc",
            "placeholder": "​",
            "style": "IPY_MODEL_6a45500f62ef470083a72d86fe92ef39",
            "value": "modules.json: 100%"
          }
        },
        "626c82c514f9496392d669ca94bea5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21e6397c5fbd4f20a42c4f8a5a7d192f",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ab82e8bfaf414582f958a4bd955d34",
            "value": 349
          }
        },
        "2554ac96ec0d42148bb9f24ebb9645d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf2ad19d1881458aa8629f103041dab3",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4486839e824aba97d0a3a6b12814a2",
            "value": " 349/349 [00:00&lt;00:00, 25.3kB/s]"
          }
        },
        "87566fff38c6452895dd5abcc647cfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb94b6b01ae340199cc06b463ebad2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a45500f62ef470083a72d86fe92ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21e6397c5fbd4f20a42c4f8a5a7d192f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91ab82e8bfaf414582f958a4bd955d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf2ad19d1881458aa8629f103041dab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4486839e824aba97d0a3a6b12814a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81798aafdc5446c9a81ccba217e5cbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_869905f73cc04e6a95a1102619acfe62",
              "IPY_MODEL_88c59c06c4db46d79457c6acacdd2b67",
              "IPY_MODEL_f1d9d31b4fb6402989cdb06c6431bd5c"
            ],
            "layout": "IPY_MODEL_51cc8b4855544cfe9ccdab35d0c5aa49"
          }
        },
        "869905f73cc04e6a95a1102619acfe62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23dff6590f3f4d79880617116b644408",
            "placeholder": "​",
            "style": "IPY_MODEL_732b53f33a5249fbb25fab1b570d9f00",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "88c59c06c4db46d79457c6acacdd2b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c84fdfb4794aa29020424bab02d985",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_105f22e4261248babb0933fb55d5cf3a",
            "value": 124
          }
        },
        "f1d9d31b4fb6402989cdb06c6431bd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d7e3bab8c748f39cfa6a2395a8e11d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6ac444c177f46a5905e3bcb1d83b896",
            "value": " 124/124 [00:00&lt;00:00, 8.16kB/s]"
          }
        },
        "51cc8b4855544cfe9ccdab35d0c5aa49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23dff6590f3f4d79880617116b644408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732b53f33a5249fbb25fab1b570d9f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c84fdfb4794aa29020424bab02d985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105f22e4261248babb0933fb55d5cf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d7e3bab8c748f39cfa6a2395a8e11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ac444c177f46a5905e3bcb1d83b896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21ee623c821941469252d801a8b9e242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1c42cfeaafb4adf82a8bdac5f3bec67",
              "IPY_MODEL_977bdb51c44a4af3ac4d2a8c2c37a621",
              "IPY_MODEL_ac3caf7f36114ab4b2c55f6ceff8a646"
            ],
            "layout": "IPY_MODEL_0c5101dff33948a7a7d32b7669885415"
          }
        },
        "d1c42cfeaafb4adf82a8bdac5f3bec67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57efdc2cc6b4d859571d788aa5a5031",
            "placeholder": "​",
            "style": "IPY_MODEL_78773eac7a284985a949fc2d8e0f80fe",
            "value": "README.md: 100%"
          }
        },
        "977bdb51c44a4af3ac4d2a8c2c37a621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b925d85e6a4c609dc1cdf12953e3ad",
            "max": 94551,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f93e097171f4360b43c7d2d0168192b",
            "value": 94551
          }
        },
        "ac3caf7f36114ab4b2c55f6ceff8a646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124484d2c3f94b03bf88325927389fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_c8a0304957cb4ef0b0d2aebe8e601499",
            "value": " 94.6k/94.6k [00:00&lt;00:00, 4.51MB/s]"
          }
        },
        "0c5101dff33948a7a7d32b7669885415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57efdc2cc6b4d859571d788aa5a5031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78773eac7a284985a949fc2d8e0f80fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0b925d85e6a4c609dc1cdf12953e3ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f93e097171f4360b43c7d2d0168192b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "124484d2c3f94b03bf88325927389fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a0304957cb4ef0b0d2aebe8e601499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e93fe32181541608b224ca076d254ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa16c3ee65734f318d9167c7d4bad5fd",
              "IPY_MODEL_daae94809af34de68863abe6bc1b3861",
              "IPY_MODEL_f2654ed2e10b4ecb806780c7d276bebc"
            ],
            "layout": "IPY_MODEL_bd07fda5a94040aca5de54fe7be1a40c"
          }
        },
        "fa16c3ee65734f318d9167c7d4bad5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a96a66dc8340759c2d2e9ae80af146",
            "placeholder": "​",
            "style": "IPY_MODEL_23870c64bb704dc4bb5f9e622540bbcb",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "daae94809af34de68863abe6bc1b3861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f26d76dff0498a85ce9449c1fa3ac2",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78d8010e91694efba40983108d4d4d01",
            "value": 52
          }
        },
        "f2654ed2e10b4ecb806780c7d276bebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abff5acd0df3474894d62c5830feeb43",
            "placeholder": "​",
            "style": "IPY_MODEL_572fdf4ab6b144d1b64662d0921eacdd",
            "value": " 52.0/52.0 [00:00&lt;00:00, 1.80kB/s]"
          }
        },
        "bd07fda5a94040aca5de54fe7be1a40c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a96a66dc8340759c2d2e9ae80af146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23870c64bb704dc4bb5f9e622540bbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53f26d76dff0498a85ce9449c1fa3ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d8010e91694efba40983108d4d4d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abff5acd0df3474894d62c5830feeb43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "572fdf4ab6b144d1b64662d0921eacdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77a80f6756b74e14ae07b916887b52ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_095868cbfffe4b37bffc59f8fffb513b",
              "IPY_MODEL_640a73d653304920bd798840d4e94cf6",
              "IPY_MODEL_7732feb6ed1d4810887172d7b9bad19a"
            ],
            "layout": "IPY_MODEL_0d6886c4e2e2429bbe1207416d4ffdff"
          }
        },
        "095868cbfffe4b37bffc59f8fffb513b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b92a12eb800142be830c7c022991bed8",
            "placeholder": "​",
            "style": "IPY_MODEL_f075fb177b1e4674bfe0bdfe1577dd4e",
            "value": "config.json: 100%"
          }
        },
        "640a73d653304920bd798840d4e94cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811d66bcb7ea4a2faa3dd3ca14523574",
            "max": 777,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbdf24d99a17435b855bfffba205f4c9",
            "value": 777
          }
        },
        "7732feb6ed1d4810887172d7b9bad19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27485e2356ab457f9615c17f0b77739e",
            "placeholder": "​",
            "style": "IPY_MODEL_b2bdf4a06ddf486189c3449821f098f8",
            "value": " 777/777 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "0d6886c4e2e2429bbe1207416d4ffdff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b92a12eb800142be830c7c022991bed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f075fb177b1e4674bfe0bdfe1577dd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "811d66bcb7ea4a2faa3dd3ca14523574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbdf24d99a17435b855bfffba205f4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27485e2356ab457f9615c17f0b77739e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2bdf4a06ddf486189c3449821f098f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f8f9930ff924375bb0fa1a914c3d1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c37eb1dcda34405a9d02c4d97be5b8a",
              "IPY_MODEL_7980ac43adc84617b6502dac14dfb6a0",
              "IPY_MODEL_8048db3ade8042088c3c070a070a2da3"
            ],
            "layout": "IPY_MODEL_869e9d4e023b46e1a951861256fde0f4"
          }
        },
        "0c37eb1dcda34405a9d02c4d97be5b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc284e609c9d465b94ffb34486934893",
            "placeholder": "​",
            "style": "IPY_MODEL_4344137941984d5ab1146992e1281c39",
            "value": "model.safetensors: 100%"
          }
        },
        "7980ac43adc84617b6502dac14dfb6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e72256a4ec7e43eba1915ac337731e45",
            "max": 437955512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6f85b9c30fe41858bbb4bea46fa157f",
            "value": 437955512
          }
        },
        "8048db3ade8042088c3c070a070a2da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa50e22ef3a145759c0c0b6398ba42d3",
            "placeholder": "​",
            "style": "IPY_MODEL_e0e50bf991bd4c23bdc413c43aff25a9",
            "value": " 438M/438M [00:01&lt;00:00, 230MB/s]"
          }
        },
        "869e9d4e023b46e1a951861256fde0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc284e609c9d465b94ffb34486934893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4344137941984d5ab1146992e1281c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e72256a4ec7e43eba1915ac337731e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6f85b9c30fe41858bbb4bea46fa157f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa50e22ef3a145759c0c0b6398ba42d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e50bf991bd4c23bdc413c43aff25a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d8ab9fd8fe4b2b99a4c52ebd6e37d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c843bfc26c3140948ebc37cd37aa7090",
              "IPY_MODEL_d8d6b112e5ac4508a77b895223803a2a",
              "IPY_MODEL_a87ff9a2048e4702a752df52ee44218a"
            ],
            "layout": "IPY_MODEL_a312ec5369f741bd86003f70ac4fc2e3"
          }
        },
        "c843bfc26c3140948ebc37cd37aa7090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9944dcf95974813b4baebd0c849449a",
            "placeholder": "​",
            "style": "IPY_MODEL_3dfd9dfcb3f14741a3e6bdefd139c53c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d8d6b112e5ac4508a77b895223803a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62fb7cd8de814860bdf17ddc4997b94b",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2c7945d5f9847cd96a6f9fdae6e8e17",
            "value": 366
          }
        },
        "a87ff9a2048e4702a752df52ee44218a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b754465b341047dda868558d3e8d3680",
            "placeholder": "​",
            "style": "IPY_MODEL_70552717858d4c48b4bf3be7a9133493",
            "value": " 366/366 [00:00&lt;00:00, 13.8kB/s]"
          }
        },
        "a312ec5369f741bd86003f70ac4fc2e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9944dcf95974813b4baebd0c849449a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dfd9dfcb3f14741a3e6bdefd139c53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62fb7cd8de814860bdf17ddc4997b94b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c7945d5f9847cd96a6f9fdae6e8e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b754465b341047dda868558d3e8d3680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70552717858d4c48b4bf3be7a9133493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "decb1228973f4e4aac39407fcfbeda7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9c1206d4f1c4c56af7b577b823f5be5",
              "IPY_MODEL_7b5752009d744116a1df6dff67fc2eba",
              "IPY_MODEL_73d2f6d48cba4f15be11b43be9a1b856"
            ],
            "layout": "IPY_MODEL_96af099bde9f4c168fca55dabc1a4aef"
          }
        },
        "e9c1206d4f1c4c56af7b577b823f5be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5bc6fb2e8a847979437a4ee626ea288",
            "placeholder": "​",
            "style": "IPY_MODEL_53e800eebeb34d6d8ab9ea15e48ad797",
            "value": "vocab.txt: 100%"
          }
        },
        "7b5752009d744116a1df6dff67fc2eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d34688c12ee44d18ac264e7ecb44814",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3807d177472643ad9a87d9d97a89aa6a",
            "value": 231508
          }
        },
        "73d2f6d48cba4f15be11b43be9a1b856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caee1c76a4cf4111b2f15b4574ff7c5b",
            "placeholder": "​",
            "style": "IPY_MODEL_43dd87c094394419a819322ca7a0cc53",
            "value": " 232k/232k [00:00&lt;00:00, 14.1MB/s]"
          }
        },
        "96af099bde9f4c168fca55dabc1a4aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5bc6fb2e8a847979437a4ee626ea288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e800eebeb34d6d8ab9ea15e48ad797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d34688c12ee44d18ac264e7ecb44814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3807d177472643ad9a87d9d97a89aa6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caee1c76a4cf4111b2f15b4574ff7c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43dd87c094394419a819322ca7a0cc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8508d7293fa2453489672dc86b05b263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef46865e2c6e42ab95abb8c97cc2afda",
              "IPY_MODEL_6219b1fc995a4cc882004b8eefbdcfc9",
              "IPY_MODEL_4659a04e843344c5a85bca07d1bc63a2"
            ],
            "layout": "IPY_MODEL_19ab72d5c1fd484b9366b1db6c627f9a"
          }
        },
        "ef46865e2c6e42ab95abb8c97cc2afda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_001da0fe539649a6b493c186f2398ca0",
            "placeholder": "​",
            "style": "IPY_MODEL_2847ade045d646b68f3443614cf2c561",
            "value": "tokenizer.json: 100%"
          }
        },
        "6219b1fc995a4cc882004b8eefbdcfc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280fb007a922464b808d899ef161d18a",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c85334bac9a4424b9a49a4c14ad6ce4",
            "value": 711396
          }
        },
        "4659a04e843344c5a85bca07d1bc63a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a2273763d4452792a90833cedcfd8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ce5ae08b1e47db8b2877795ea98775",
            "value": " 711k/711k [00:00&lt;00:00, 1.65MB/s]"
          }
        },
        "19ab72d5c1fd484b9366b1db6c627f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "001da0fe539649a6b493c186f2398ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2847ade045d646b68f3443614cf2c561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "280fb007a922464b808d899ef161d18a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c85334bac9a4424b9a49a4c14ad6ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7a2273763d4452792a90833cedcfd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ce5ae08b1e47db8b2877795ea98775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b6c79a04a534cbda8e08c9addae87a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55b67f32d66248b7b653d6a852b7fd57",
              "IPY_MODEL_61fd8f296be44fab813115bd98149ad2",
              "IPY_MODEL_06911b70ce9e4f6ab27bc6fc30be7d9b"
            ],
            "layout": "IPY_MODEL_05aed2e7af1d4676ac698f4c77851ce6"
          }
        },
        "55b67f32d66248b7b653d6a852b7fd57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2df6310a008942f0aed7ebc32cfe4fe4",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf83aab0bf241e58ba697a3b94a2f77",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "61fd8f296be44fab813115bd98149ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969cd24ed58c44068a102ee3174f8520",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48cbf5daf5f94f6d9566099f6cdb15e7",
            "value": 125
          }
        },
        "06911b70ce9e4f6ab27bc6fc30be7d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3458d407754b2abca9d7e251eaa910",
            "placeholder": "​",
            "style": "IPY_MODEL_0b7d78634c854863bbc1f3300937eb8f",
            "value": " 125/125 [00:00&lt;00:00, 9.04kB/s]"
          }
        },
        "05aed2e7af1d4676ac698f4c77851ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df6310a008942f0aed7ebc32cfe4fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf83aab0bf241e58ba697a3b94a2f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "969cd24ed58c44068a102ee3174f8520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48cbf5daf5f94f6d9566099f6cdb15e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f3458d407754b2abca9d7e251eaa910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7d78634c854863bbc1f3300937eb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c3a0a19fcdf4717854bef0ce42a2bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2234a70ca03046b9870eb728d1abb9c3",
              "IPY_MODEL_e98123229a26444c90c41698e66bef06",
              "IPY_MODEL_c01a887766df4d55b82b908b46f782f9"
            ],
            "layout": "IPY_MODEL_d4db20c626814310ab69c24984f1d210"
          }
        },
        "2234a70ca03046b9870eb728d1abb9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae24fc9e31643a1acc2b241d3ded9b1",
            "placeholder": "​",
            "style": "IPY_MODEL_3a0ab32383864d149213f013d4da7e4e",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "e98123229a26444c90c41698e66bef06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c05eef26806841c7adb2286be32a3387",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75ac11c089b44fdf9f2bac2b947d230f",
            "value": 190
          }
        },
        "c01a887766df4d55b82b908b46f782f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d3f1cafccf246759c4366b37a8c717a",
            "placeholder": "​",
            "style": "IPY_MODEL_3771c43120fa4839ab9c050a670b8f4f",
            "value": " 190/190 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "d4db20c626814310ab69c24984f1d210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae24fc9e31643a1acc2b241d3ded9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0ab32383864d149213f013d4da7e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c05eef26806841c7adb2286be32a3387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ac11c089b44fdf9f2bac2b947d230f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d3f1cafccf246759c4366b37a8c717a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3771c43120fa4839ab9c050a670b8f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwrPlEsuZVSM",
        "outputId": "0b76f484-966d-42ef-d119-0a401c0745bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing the Packages :"
      ],
      "metadata": {
        "id": "pbcXsFMytl_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchain_community\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q accelerate\n",
        "!pip install -q ctransformers\n",
        "!pip install -q pypdf\n",
        "!pip install -q faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYr5KdWkZV1j",
        "outputId": "4c4086f8-130e-474a-8ee0-b42a971b13b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOPurlzMSYGr",
        "outputId": "b9c76d74-f165-4670-9c22-91805a7e2cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.120)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPzjjwjitrOS",
        "outputId": "3a5faa3f-03a7-467e-f40f-6a7918734d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv9MWXd9xT2S",
        "outputId": "a3b12b73-3851-406a-9677-68a4294e9164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.120)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VAFiGEbCZ72F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing LLM"
      ],
      "metadata": {
        "id": "SbQuBmWWxFUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import CTransformers"
      ],
      "metadata": {
        "id": "KzvD9uzcaI_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/llama-2-7b-chat.ggmlv3.q4_0.bin\""
      ],
      "metadata": {
        "id": "HNx_A7Mdan01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CTransformers(\n",
        "    model = model_path,\n",
        "    model_type = \"llama\",\n",
        "    config = {'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000}\n",
        ")"
      ],
      "metadata": {
        "id": "1AVb04wXpP26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating embeddings"
      ],
      "metadata": {
        "id": "rg8OB96iyOvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "vZkFzwzRpxA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5', cache_folder='.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "93430d03074848f9be8f3dfbf081195f",
            "62b42fb170914ae2859ad9f4a4f6c6e3",
            "626c82c514f9496392d669ca94bea5d8",
            "2554ac96ec0d42148bb9f24ebb9645d1",
            "87566fff38c6452895dd5abcc647cfa4",
            "bb94b6b01ae340199cc06b463ebad2dc",
            "6a45500f62ef470083a72d86fe92ef39",
            "21e6397c5fbd4f20a42c4f8a5a7d192f",
            "91ab82e8bfaf414582f958a4bd955d34",
            "bf2ad19d1881458aa8629f103041dab3",
            "7c4486839e824aba97d0a3a6b12814a2",
            "81798aafdc5446c9a81ccba217e5cbe7",
            "869905f73cc04e6a95a1102619acfe62",
            "88c59c06c4db46d79457c6acacdd2b67",
            "f1d9d31b4fb6402989cdb06c6431bd5c",
            "51cc8b4855544cfe9ccdab35d0c5aa49",
            "23dff6590f3f4d79880617116b644408",
            "732b53f33a5249fbb25fab1b570d9f00",
            "a1c84fdfb4794aa29020424bab02d985",
            "105f22e4261248babb0933fb55d5cf3a",
            "06d7e3bab8c748f39cfa6a2395a8e11d",
            "b6ac444c177f46a5905e3bcb1d83b896",
            "21ee623c821941469252d801a8b9e242",
            "d1c42cfeaafb4adf82a8bdac5f3bec67",
            "977bdb51c44a4af3ac4d2a8c2c37a621",
            "ac3caf7f36114ab4b2c55f6ceff8a646",
            "0c5101dff33948a7a7d32b7669885415",
            "a57efdc2cc6b4d859571d788aa5a5031",
            "78773eac7a284985a949fc2d8e0f80fe",
            "f0b925d85e6a4c609dc1cdf12953e3ad",
            "6f93e097171f4360b43c7d2d0168192b",
            "124484d2c3f94b03bf88325927389fb8",
            "c8a0304957cb4ef0b0d2aebe8e601499",
            "3e93fe32181541608b224ca076d254ab",
            "fa16c3ee65734f318d9167c7d4bad5fd",
            "daae94809af34de68863abe6bc1b3861",
            "f2654ed2e10b4ecb806780c7d276bebc",
            "bd07fda5a94040aca5de54fe7be1a40c",
            "a5a96a66dc8340759c2d2e9ae80af146",
            "23870c64bb704dc4bb5f9e622540bbcb",
            "53f26d76dff0498a85ce9449c1fa3ac2",
            "78d8010e91694efba40983108d4d4d01",
            "abff5acd0df3474894d62c5830feeb43",
            "572fdf4ab6b144d1b64662d0921eacdd",
            "77a80f6756b74e14ae07b916887b52ab",
            "095868cbfffe4b37bffc59f8fffb513b",
            "640a73d653304920bd798840d4e94cf6",
            "7732feb6ed1d4810887172d7b9bad19a",
            "0d6886c4e2e2429bbe1207416d4ffdff",
            "b92a12eb800142be830c7c022991bed8",
            "f075fb177b1e4674bfe0bdfe1577dd4e",
            "811d66bcb7ea4a2faa3dd3ca14523574",
            "fbdf24d99a17435b855bfffba205f4c9",
            "27485e2356ab457f9615c17f0b77739e",
            "b2bdf4a06ddf486189c3449821f098f8",
            "0f8f9930ff924375bb0fa1a914c3d1c6",
            "0c37eb1dcda34405a9d02c4d97be5b8a",
            "7980ac43adc84617b6502dac14dfb6a0",
            "8048db3ade8042088c3c070a070a2da3",
            "869e9d4e023b46e1a951861256fde0f4",
            "fc284e609c9d465b94ffb34486934893",
            "4344137941984d5ab1146992e1281c39",
            "e72256a4ec7e43eba1915ac337731e45",
            "d6f85b9c30fe41858bbb4bea46fa157f",
            "fa50e22ef3a145759c0c0b6398ba42d3",
            "e0e50bf991bd4c23bdc413c43aff25a9",
            "17d8ab9fd8fe4b2b99a4c52ebd6e37d6",
            "c843bfc26c3140948ebc37cd37aa7090",
            "d8d6b112e5ac4508a77b895223803a2a",
            "a87ff9a2048e4702a752df52ee44218a",
            "a312ec5369f741bd86003f70ac4fc2e3",
            "f9944dcf95974813b4baebd0c849449a",
            "3dfd9dfcb3f14741a3e6bdefd139c53c",
            "62fb7cd8de814860bdf17ddc4997b94b",
            "f2c7945d5f9847cd96a6f9fdae6e8e17",
            "b754465b341047dda868558d3e8d3680",
            "70552717858d4c48b4bf3be7a9133493",
            "decb1228973f4e4aac39407fcfbeda7a",
            "e9c1206d4f1c4c56af7b577b823f5be5",
            "7b5752009d744116a1df6dff67fc2eba",
            "73d2f6d48cba4f15be11b43be9a1b856",
            "96af099bde9f4c168fca55dabc1a4aef",
            "a5bc6fb2e8a847979437a4ee626ea288",
            "53e800eebeb34d6d8ab9ea15e48ad797",
            "0d34688c12ee44d18ac264e7ecb44814",
            "3807d177472643ad9a87d9d97a89aa6a",
            "caee1c76a4cf4111b2f15b4574ff7c5b",
            "43dd87c094394419a819322ca7a0cc53",
            "8508d7293fa2453489672dc86b05b263",
            "ef46865e2c6e42ab95abb8c97cc2afda",
            "6219b1fc995a4cc882004b8eefbdcfc9",
            "4659a04e843344c5a85bca07d1bc63a2",
            "19ab72d5c1fd484b9366b1db6c627f9a",
            "001da0fe539649a6b493c186f2398ca0",
            "2847ade045d646b68f3443614cf2c561",
            "280fb007a922464b808d899ef161d18a",
            "3c85334bac9a4424b9a49a4c14ad6ce4",
            "d7a2273763d4452792a90833cedcfd8c",
            "c4ce5ae08b1e47db8b2877795ea98775",
            "8b6c79a04a534cbda8e08c9addae87a2",
            "55b67f32d66248b7b653d6a852b7fd57",
            "61fd8f296be44fab813115bd98149ad2",
            "06911b70ce9e4f6ab27bc6fc30be7d9b",
            "05aed2e7af1d4676ac698f4c77851ce6",
            "2df6310a008942f0aed7ebc32cfe4fe4",
            "6bf83aab0bf241e58ba697a3b94a2f77",
            "969cd24ed58c44068a102ee3174f8520",
            "48cbf5daf5f94f6d9566099f6cdb15e7",
            "4f3458d407754b2abca9d7e251eaa910",
            "0b7d78634c854863bbc1f3300937eb8f",
            "8c3a0a19fcdf4717854bef0ce42a2bee",
            "2234a70ca03046b9870eb728d1abb9c3",
            "e98123229a26444c90c41698e66bef06",
            "c01a887766df4d55b82b908b46f782f9",
            "d4db20c626814310ab69c24984f1d210",
            "3ae24fc9e31643a1acc2b241d3ded9b1",
            "3a0ab32383864d149213f013d4da7e4e",
            "c05eef26806841c7adb2286be32a3387",
            "75ac11c089b44fdf9f2bac2b947d230f",
            "8d3f1cafccf246759c4366b37a8c717a",
            "3771c43120fa4839ab9c050a670b8f4f"
          ]
        },
        "id": "UtYdmp2lp6wg",
        "outputId": "2906d861-ada9-428c-e525-4bd44574a119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93430d03074848f9be8f3dfbf081195f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81798aafdc5446c9a81ccba217e5cbe7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21ee623c821941469252d801a8b9e242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e93fe32181541608b224ca076d254ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77a80f6756b74e14ae07b916887b52ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8f9930ff924375bb0fa1a914c3d1c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17d8ab9fd8fe4b2b99a4c52ebd6e37d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "decb1228973f4e4aac39407fcfbeda7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8508d7293fa2453489672dc86b05b263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b6c79a04a534cbda8e08c9addae87a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c3a0a19fcdf4717854bef0ce42a2bee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "enRyGnKjqPev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_path = \"/content/models--BAAI--bge-base-en-v1.5/snapshots/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a\""
      ],
      "metadata": {
        "id": "vwga5cPqqVTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name = embedding_model_path,\n",
        "                                   model_kwargs = {'device':'cuda'},\n",
        "                                  encode_kwargs = {'normalize_embeddings':True}\n",
        "                                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxt3v3WKuomM",
        "outputId": "6155575a-644b-417a-c0d8-c424efba4ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-41e16a778cc7>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name = embedding_model_path,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the PDF"
      ],
      "metadata": {
        "id": "SBLYcyNcybZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ca786XMzu_F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_reader = PyPDFLoader('/content/drive/MyDrive/DeepLearningBook.pdf')"
      ],
      "metadata": {
        "id": "rL6tdo-Dwbpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = pdf_reader.load()"
      ],
      "metadata": {
        "id": "UDoE3CJswjRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxzuSsACwpWG",
        "outputId": "3d328df2-0876-4088-b006-e036b4a79c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 0}, page_content='DeepLearningIanGoodfellowYoshuaBengioAaronCourville'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 1}, page_content='ContentsWebsiteviiAcknowledgmentsviiiNotationxi1Introduction11.1WhoShouldReadThisBook?. . . . . .. . . . . . . . .. . . . .81.2HistoricalTrendsinDeepLearning. . . . . . . . .. . . . . . . .11IAppliedMathandMachineLearningBasics292LinearAlgebra312.1Scalars,Vectors,MatricesandTensors. . . . . . . . .. . . . . .312.2MultiplyingMatricesandVectors. . . . . . . . .. . . . . . . ..342.3IdentityandInverseMatrices. . . . . . . . .. . . . . . . .. . .362.4LinearDependenceandSpan. . . . . . . . .. . . . . . . .. . .372.5Norms. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .392.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . .402.7Eigendecomposition. . . . . . . . . .. . . . . . . .. . . . . . . .422.8SingularValueDecomposition. . . . . . . .. . . . . . . .. . . .442.9TheMoore-PenrosePseudoinverse. . . . . . . . .. . . . . . . ..452.10TheTraceOperator. . . . . . . . .. . . . . . . .. . . . . . . .462.11TheDeterminant. .. . . . . . . .. . . . . . . .. . . . . . . . .472.12Example:PrincipalComponentsAnalysis. . . . . . . . .. . . .483ProbabilityandInformationTheory533.1WhyProbability?. . . . .. . . . . . . . .. . . . . . . .. . . . .54i'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 2}, page_content='CONTENTS3.2RandomVariables. . . . .. . . . . . . .. . . . . . . . .. . . .563.3ProbabilityDistributions. . . . . . . . .. . . . . . . .. . . . . .563.4MarginalProbability. . . . . . . . .. . . . . . . . .. . . . . . .583.5ConditionalProbability. .. . . . . . . .. . . . . . . .. . . . .593.6TheChainRuleofConditionalProbabilities. . . . . . . . .. . .593.7IndependenceandConditionalIndependence. . . . . . . . .. . .603.8Expectation,VarianceandCovariance. . . . . . . . . .. . . . .603.9CommonProbabilityDistributions. . . . . . . . . . . . . . .. .623.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . .. .673.11Bayes’Rule. . . . . . . . . .. . . . . . . .. . . . . . . .. . . .703.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . .713.13InformationTheory. . . . . . . . . .. . . . . . . .. . . . . . . .723.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .754NumericalComputation804.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . . .804.2PoorConditioning. . . . . . . . .. . . . . . . .. . . . . . . . .824.3Gradient-BasedOptimization. . . . . . .. . . . . . . .. . . . .824.4ConstrainedOptimization. . . . . . . . . . . . .. . . . . . . ..934.5Example:LinearLeastSquares. . . . . . .. . . . . . . . .. . .965MachineLearningBasics985.1LearningAlgorithms. . . . . . . . . . .. . . . . . . .. . . . . .995.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . .. . . . .1105.3HyperparametersandValidationSets.. . . . . . . .. . . . . . .1205.4Estimators,BiasandVariance. . . . . .. . . . . . . .. . . . . .1225.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . .. . .1315.6BayesianStatistics. . . . . . . . . . .. . . . . . . .. . . . . . .1355.7SupervisedLearningAlgorithms. . .. . . . . . . .. . . . . . . .1395.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . .. .1455.9StochasticGradientDescent. . . .. . . . . . . . .. . . . . . . .1505.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . ..1525.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . .. .154IIDeepNetworks:ModernPractices1656DeepFeedforwardNetworks1676.1Example:LearningXOR.. . . . . . . . .. . . . . . . .. . . . .1706.2Gradient-BasedLearning.. . . . . . . .. . . . . . . .. . . . . .176ii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 3}, page_content='CONTENTS6.3HiddenUnits. . . . . .. . . . . . . .. . . . . . . . .. . . . . .1906.4ArchitectureDesign. . . . . . . . .. . . . . . . .. . . . . . . ..1966.5Back-PropagationandOtherDiﬀerentiationAlgorithms. . . . .2036.6HistoricalNotes. . . . . . .. . . . . . . .. . . . . . . . .. . . .2247RegularizationforDeepLearning2287.1ParameterNormPenalties. . . . .. . . . . . . . .. . . . . . . .2307.2NormPenaltiesasConstrainedOptimization. . . . . . . .. . . .2377.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .2397.4DatasetAugmentation. . . . . . . . . .. . . . . . . . .. . . . .2407.5NoiseRobustness. . . . . . . . .. . . . . . . .. . . . . . . .. .2427.6Semi-SupervisedLearning. . . . . . . . . . . . . . . .. . . . . .2437.7Multi-TaskLearning. . . . . . . . . . . . . .. . . . . . . . .. .2447.8EarlyStopping. . . . . . . . .. . . . . . . .. . . . . . . .. . .2467.9ParameterTyingandParameterSharing . . . . . . . . . . . . . .2537.10SparseRepresentations. . . . . . . . .. . . . . . . .. . . . . . .2547.11BaggingandOtherEnsembleMethods.. . . . . . . . .. . . . .2567.12Dropout. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .2587.13AdversarialTraining. . . . . . . .. . . . . . . . .. . . . . . . .2687.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer2708OptimizationforTrainingDeepModels2748.1HowLearningDiﬀersfromPureOptimization. . . . . . . . . . .2758.2ChallengesinNeuralNetworkOptimization. . . . .. . . . . . .2828.3BasicAlgorithms. . . . . . . . . . . . .. . . . . . . .. . . . . .2948.4ParameterInitializationStrategies.. . . . . . . . .. . . . . . .3018.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .3068.6ApproximateSecond-OrderMethods. . . .. . . . . . . . .. . .3108.7OptimizationStrategiesandMeta-Algorithms. . . . .. . . . . .3179ConvolutionalNetworks3309.1TheConvolutionOperation. . . . . . . . . . . . . . . .. . . . .3319.2Motivation. .. . . . . . . .. . . . . . . . .. . . . . . . .. . . .3359.3Pooling. . . . . . . . . . . . .. . . . . . . .. . . . . . . . .. . .3399.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .3459.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . .3479.6StructuredOutputs.. . . . . . . .. . . . . . . . .. . . . . . . .3589.7DataTypes. . . . . .. . . . . . . .. . . . . . . .. . . . . . . .3609.8EﬃcientConvolutionAlgorithms. . . . . . . .. . . . . . . .. .3629.9RandomorUnsupervisedFeatures. . . . . . . .. . . . . . . ..363iii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 4}, page_content='CONTENTS9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks. . . . . ..3649.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .37110 SequenceModeling:RecurrentandRecursiveNets37310.1UnfoldingComputationalGraphs. . . . . . . . . . . . .. . . . .37510.2RecurrentNeuralNetworks. . .. . . . . . . . .. . . . . . . ..37810.3BidirectionalRNNs . . . . . . . . . . . . . .. . . . . . . . .. . .39510.4Encoder-DecoderSequence-to-SequenceArchitectures. . . . . ..39610.5DeepRecurrentNetworks. . . . . . . .. . . . . . . . .. . . . .39810.6RecursiveNeuralNetworks. . . . .. . . . . . . . .. . . . . . . .40010.7TheChallengeofLong-TermDependencies. . . . . . . . . .. . .40210.8EchoStateNetworks. . . . . . . . . .. . . . . . . . .. . . . . .40510.9LeakyUnitsandOtherStrategiesforMultipleTimeScales. . ..40810.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .41010.11 OptimizationforLong-TermDependencies. . . . . . . .. . . . .41410.12 ExplicitMemory. . . . . . . . . .. . . . . . . . .. . . . . . . .41811 PracticalMethodology42311.1PerformanceMetrics. . . . . . . . . .. . . . . . . .. . . . . . .42411.2DefaultBaselineModels. . . . . . . .. . . . . . . .. . . . . . .42711.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . .42811.4SelectingHyperparameters. . . . . . . . .. . . . . . . .. . . . .42911.5DebuggingStrategies. . . . .. . . . . . . .. . . . . . . . .. . .43811.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . .44212 Applications44512.1LargeScaleDeepLearning.. . . . . . . .. . . . . . . . .. . . .44512.2ComputerVision. . . . . . . . .. . . . . . . .. . . . . . . .. .45412.3SpeechRecognition . . . . . .. . . . . . . .. . . . . . . . .. . .46012.4NaturalLanguageProcessing. . .. . . . . . . .. . . . . . . ..46312.5OtherApplications. . . . . . . . .. . . . . . . .. . . . . . . ..479IIIDeepLearningResearch48813 LinearFactorModels49113.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . .49213.2IndependentComponentAnalysis(ICA). . . . . . . . . . . .. .49313.3SlowFeatureAnalysis. . . . . .. . . . . . . . .. . . . . . . ..49513.4SparseCoding. . . . . .. . . . . . . .. . . . . . . . .. . . . . .498iv'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 5}, page_content='CONTENTS13.5ManifoldInterpretationofPCA. . . . . . . . . . . . . . . . .. .50114 Autoencoders50414.1UndercompleteAutoencoders. . . . . . . . . .. . . . . . . .. .50514.2RegularizedAutoencoders. . . . . . . . .. . . . . . . .. . . . .50614.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .51014.4StochasticEncodersandDecoders. . . . . . . . . . .. . . . . . .51114.5DenoisingAutoencoders. .. . . . . . . .. . . . . . . . .. . . .51214.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . .51714.7ContractiveAutoencoders.. . . . . . . .. . . . . . . .. . . . .52314.8PredictiveSparseDecomposition. . . . . . . .. . . . . . . . ..52514.9ApplicationsofAutoencoders. . . . .. . . . . . . . .. . . . . .52615 RepresentationLearning52815.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .53015.2TransferLearningandDomainAdaptation. . . .. . . . . . . ..53815.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .54315.4DistributedRepresentation. . . . . . . . . . . .. . . . . . . . ..54815.5ExponentialGainsfromDepth. . . . . . . . . .. . . . . . . ..55515.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .55616 StructuredProbabilisticModelsforDeepLearning56016.1TheChallengeofUnstructuredModeling.. . . . . . . .. . . . .56116.2UsingGraphstoDescribeModelStructure. .. . . . . . . .. . .56516.3SamplingfromGraphicalModels. . .. . . . . . . .. . . . . . .58216.4AdvantagesofStructuredModeling .. . . . . . . . .. . . . . . .58416.5LearningaboutDependencies. . . .. . . . . . . .. . . . . . . .58416.6InferenceandApproximateInference. . . . . . . . .. . . . . . .58516.7TheDeepLearningApproachtoStructuredProbabilisticModels58617 MonteCarloMethods59217.1SamplingandMonteCarloMethods. . . . . . . .. . . . . . . .59217.2ImportanceSampling. . . . . . . . . . . .. . . . . . . .. . . . .59417.3MarkovChainMonteCarloMethods. . . . .. . . . . . . .. . .59717.4GibbsSampling . . . . . . .. . . . . . . . .. . . . . . . .. . . .60117.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..60118 ConfrontingthePartitionFunction60718.1TheLog-LikelihoodGradient.. . . . . . . .. . . . . . . . .. .60818.2StochasticMaximumLikelihoodandContrastiveDivergence. . .609v'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 6}, page_content='CONTENTS18.3Pseudolikelihood. . . . . . . . . . .. . . . . . . . .. . . . . . .61718.4ScoreMatchingandRatioMatching. . . . . . . .. . . . . . . .61918.5DenoisingScoreMatching. . . . . . . . .. . . . . . . .. . . . .62118.6Noise-ContrastiveEstimation. . . . .. . . . . . . .. . . . . . .62218.7EstimatingthePartitionFunction. . . . . . . . . . .. . . . . . .62519 ApproximateInference63319.1InferenceasOptimization.. . . . . . . . .. . . . . . . .. . . .63519.2ExpectationMaximization. .. . . . . . . .. . . . . . . . .. . .63619.3MAPInferenceandSparseCoding.. . . . . . . . .. . . . . . .63719.4VariationalInferenceandLearning. . . . . . . . . . . . . . .. .64019.5LearnedApproximateInference. . .. . . . . . . . .. . . . . . .65320 DeepGenerativeModels65620.1BoltzmannMachines. . . . . . . . . . .. . . . . . . . .. . . . .65620.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . .. . .65820.3DeepBeliefNetworks.. . . . . . . .. . . . . . . .. . . . . . . .66220.4DeepBoltzmannMachines. . . . . . . . . .. . . . . . . .. . . .66520.5BoltzmannMachinesforReal-ValuedData. . . . . . . . .. . . .67820.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . ..68520.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .68720.8OtherBoltzmannMachines. . . . .. . . . . . . .. . . . . . . .68820.9Back-PropagationthroughRandomOperations. . . . . .. . . .68920.10 DirectedGenerativeNets. . . . . . . . . . . .. . . . . . . . .. .69420.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . .. .71220.12 GenerativeStochasticNetworks. . .. . . . . . . .. . . . . . . .71620.13 OtherGenerationSchemes. . . . . . . . . . . . .. . . . . . . . .71720.14 EvaluatingGenerativeModels . . . . . . . . . . . . .. . . . . . .71920.15 Conclusion. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . .721Bibliography723Index780\\nvi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 7}, page_content='Websitewww.deeplearningbook.orgThisbookisaccompaniedbytheabovewebsite.Thewebsiteprovidesavarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsofmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\\nvii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 8}, page_content='AcknowledgmentsThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.Wewouldliketothankthosewhocommentedonourproposalforthebookandhelpedplanitscontentsandorganization:GuillaumeAlain,KyunghyunCho,ÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomasRohée.Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthebookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,GuillaumeAlain,IonAndroutsopoulos,FredBertsch,OlexaBilaniuk,UfukCanBiçici,MatkoBošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLucCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,LaurentDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,Nando deFreitas,Çağlar Gülçehre, Jurgen VanGael,JavierAlonso García,JonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,RudolfMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,RomanNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,KariPulli,RousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,HalisSak,CésarSalgado,GrigorySapunov,YoshinoriSasaki,MikeSchuster,JulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,DavidSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,AnTran,ShubhenduTrivedi,AlexeyUmnov,VincentVanhoucke,MarcoVisentini-Scarzanella,DavidWarde-Farley,DustinWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackonindividualchapters:•Notation:ZhangYuanhang.•Chapter,:YusufAkgul,SebastienBratieres,SamiraEbrahimi,1IntroductionCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescuviii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 9}, page_content='CONTENTSandAlfredoSolano.•Chapter,:AmjadAlmahairi,NikolaBanić,KevinBennett,2LinearAlgebraPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,SergeyOreshkov, IstvánPetrás,DennisPrangle, ThomasRohée, ColbyToland,MassimilianoTomassoli,AlessandroVitaleandBobWelland.•Chapter,:JohnPhilipAnderson,Kai3ProbabilityandInformationTheoryArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,AnttiRasmus,AlexeySurkovandVolkerTresp.•Chapter,:TranLamAn,IanFischer,andHu4NumericalComputationYuhuang.•Chapter, :DzmitryBahdanau, Nikhil Garg,5MachineLearning BasicsMakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,Kee-BongSong,ZhengSunandAndyWu.•Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,ElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKruegerandAdityaKumarPraharaj.•Chapter,:KshitijLauria,InkyuLee,7RegularizationforDeepLearningSunilMohanandJoshuaSalisbury.•Chapter,8OptimizationforTrainingDeepModels:MarcelAckermann,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,KlausStroblandMartinVita.•Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-stantinDivilov,EricJensen,AsifullahKhan,MehdiMirza,AlexPaino,EddiePierce,MarjorieSayer,RyanStoutandWentaoWu.•Chapter,10SequenceModeling:RecurrentandRecursiveNets:GökçenEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,DmitriySerdyuk,DongyuShiandKaiyuYang.•Chapter,:DanielBeckstein.11PracticalMethodology•Chapter,:GeorgeDahlandRibanaRoscher.12Applications•Chapter,:KunalGhosh.15RepresentationLearningix'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 10}, page_content='CONTENTS•Chapter,: MinhLê16StructuredProbabilisticModelsforDeepLearningandAntonVarfolom.•Chapter,18ConfrontingthePartitionFunction:SamBowman.•Chapter,:YujiaBao.19ApproximateInference•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,WenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.•Bibliography:LukasMichelbacherandLeslieN.Smith.Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresordatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptionsthroughoutthetext.WewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedtomakethewebversionofthebook,andforoﬀeringsupporttoimprovethequalityoftheresultingHTML.We would liketothank Ian’swifeDaniela FloriGoodfellowforpatientlysupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.WewouldliketothanktheGoogleBrainteamforprovidinganintellectualenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthisbookandreceivefeedbackandguidancefromcolleagues.WewouldespeciallyliketothankIan’sformermanager,GregCorrado,andhiscurrentmanager,SamyBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀreyHintonforencouragementwhenwritingwasdiﬃcult.\\nx'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 11}, page_content='NotationThissectionprovidesaconcisereferencedescribingthenotationusedthroughoutthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematicalconcepts,thisnotationreferencemayseemintimidating.However,donotdespair,wedescribemostoftheseideasinchapters2-4.NumbersandArraysaAscalar(integerorreal)aAvectorAAmatrixAAtensorInIdentitymatrixwithrowsandcolumnsnnIIdentitymatrixwithdimensionalityimpliedbycontexte()iStandardbasisvector[0,...,0,1,0,...,0]witha1atpositionidiag()aAsquare,diagonalmatrixwithdiagonalentriesgivenbyaaAscalarrandomvariableaAvector-valuedrandomvariableAAmatrix-valuedrandomvariablexi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 12}, page_content='CONTENTSSetsandGraphsAAsetRThesetofrealnumbers{}01,Thesetcontaining0and1{}01,,...,nThesetofallintegersbetweenand0n[]a,bTherealintervalincludingandab(]a,bTherealintervalexcludingbutincludingabAB\\\\Setsubtraction,i.e., thesetcontainingtheele-mentsofthatarenotinABGAgraphPaG(xi)TheparentsofxiinGIndexingaiElementiofvectora,withindexingstartingat1a−iAllelementsofvectorexceptforelementaiAi,jElementofmatrixi,jAAi,:RowofmatrixiAA:,iColumnofmatrixiAAi,j,kElementofa3-Dtensor()i,j,kAA::,,i2-Dsliceofa3-DtensoraiElementoftherandomvectoriaLinearAlgebraOperationsA\\ue03eTransposeofmatrixAA+Moore-PenrosepseudoinverseofAAB\\ue00cElement-wise(Hadamard)productofandABdet()ADeterminantofAxii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 13}, page_content='CONTENTSCalculusdydxDerivativeofwithrespecttoyx∂y∂xPartialderivativeofwithrespecttoyx∇xyGradientofwithrespecttoyx∇XyMatrixderivativesofwithrespecttoyX∇XyTensorcontainingderivativesofywithrespecttoX∂f∂xJacobianmatrixJ∈Rmn×off: Rn→Rm∇2xfff()(xorH)()xTheHessianmatrixofatinputpointx\\ue05afd()xxDeﬁniteintegralovertheentiredomainofx\\ue05aSfd()xxDeﬁniteintegralwithrespecttooverthesetxSProbabilityandInformationTheoryabTherandomvariablesaandbareindependent⊥abcTheyareareconditionallyindependentgivenc⊥|P()aAprobabilitydistributionoveradiscretevariablep()aAprobabilitydistributionoveracontinuousvari-able,oroveravariablewhosetypehasnotbeenspeciﬁedaRandomvariableahasdistribution∼PPEx∼P[()]()()()fxorEfxExpectationoffxwithrespecttoPxVar(())fxVarianceofunderxfx()P()Cov(()())fx,gxCovarianceofandunderxfx()gx()P()H()xShannonentropyoftherandomvariablexDKL()PQ\\ue06bKullback-LeiblerdivergenceofPandQN(;)xµ,ΣGaussiandistributionoverxwithmeanµandcovarianceΣxiii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 14}, page_content='CONTENTSFunctionsff: AB→ThefunctionwithdomainandrangeABfgfg◦Compositionofthefunctionsandf(;)xθAfunctionofxparametrizedbyθ.Sometimeswejustwritef(x)andignoretheargumentθtolightennotation.logxxNaturallogarithmofσx()Logisticsigmoid,11+exp()−xζxx()log(1+exp(Softplus,))||||xpLpnormofx||||xL2normofxx+Positivepartof,i.e.,xmax(0),x1conditionis1iftheconditionistrue,0otherwiseSometimesweuseafunctionfwhoseargumentisascalar,butapplyittoavector,matrix,ortensor:f(x),f(X),orf(X).Thismeanstoapplyftothearrayelement-wise.Forexample,ifC=σ(X),thenCi,j,k=σ(Xi,j,k)forallvalidvaluesof,and.ijkDatasetsanddistributionspdataThedatageneratingdistributionˆpdataTheempiricaldistributiondeﬁnedbythetrainingsetXAsetoftrainingexamplesx()iThe-thexample(input)fromadatasetiy()iory()iThetargetassociatedwithx()iforsupervisedlearn-ingXThemn×matrixwithinputexamplex()iinrowXi,:xiv'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 15}, page_content='Chapter1IntroductionInventorshavelongdreamedofcreatingmachinesthatthink.ThisdesiredatesbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,andGalatea,Talos,andPandoramayallberegardedasartiﬁciallife(,OvidandMartin2004Sparkes1996Tandy1997;,;,).Whenprogrammablecomputerswereﬁrstconceived,peoplewonderedwhethertheymightbecomeintelligent,overahundredyearsbeforeonewasbuilt(Lovelace,1842).Today,artiﬁcialintelligence(AI)isathrivingﬁeldwithmanypracticalapplicationsandactiveresearchtopics.Welooktointelligentsoftwaretoautomateroutinelabor, understandspeechorimages, makediagnosesinmedicineandsupportbasicscientiﬁcresearch.Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolvedproblemsthatareintellectuallydiﬃcultforhumanbeingsbutrelativelystraight-forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-ematicalrules. Thetruechallengetoartiﬁcialintelligenceprovedtobesolvingthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribeformally—problemsthatwesolveintuitively,thatfeelautomatic,likerecognizingspokenwordsorfacesinimages.Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionistoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofahierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimplerconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneedforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputerneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconceptsbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese1'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 16}, page_content='CHAPTER1.INTRODUCTIONconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.Forthisreason,wecallthisapproachtoAIdeeplearning.ManyoftheearlysuccessesofAItookplaceinrelativelysterileandformalenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabouttheworld. Forexample,IBM’sDeepBluechess-playingsystemdefeatedworldchampionGarryKasparovin1997(,).ChessisofcourseaverysimpleHsu2002world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmoveinonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis atremendousaccomplishment, butthechallengeisnotduetothediﬃcultyofdescribingthesetofchesspiecesandallowablemovestothecomputer.Chesscanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easilyprovidedaheadoftimebytheprogrammer.Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmentalundertakingsforahumanbeingareamongtheeasiestforacomputer.Computershavelongbeenabletodefeateventhebesthumanchessplayer,butareonlyrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjectsorspeech.Aperson’severydayliferequiresanimmenseamountofknowledgeabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andthereforediﬃculttoarticulateinaformalway.Computersneedtocapturethissameknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesinartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabouttheworldinformallanguages.Acomputercanreasonaboutstatementsintheseformallanguagesautomaticallyusinglogicalinferencerules.Thisisknownastheknowledgebaseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasledtoamajorsuccess.OneofthemostfamoussuchprojectsisCyc(,LenatandGuha1989).CycisaninferenceengineandadatabaseofstatementsinalanguagecalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisanunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexitytoaccuratelydescribetheworld.Forexample,CycfailedtounderstandastoryaboutapersonnamedFredshavinginthemorning(,).ItsinferenceLinde1992enginedetectedaninconsistencyinthestory: itknewthatpeopledonothaveelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedtheentity“FredWhileShaving”containedelectricalparts.ItthereforeaskedwhetherFredwasstillapersonwhilehewasshaving.Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggestthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextractingpatternsfromrawdata.Thiscapabilityisknownasmachinelearning.Theintroduction2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 17}, page_content='CHAPTER1.INTRODUCTIONofmachinelearningallowedcomputerstotackleproblemsinvolvingknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimplemachinelearningalgorithmcalledlogisticregressioncandeterminewhethertorecommendcesareandelivery(Mor-Yosef1990etal.,).Asimplemachinelearningalgorithmcalledcanseparatelegitimatee-mailfromspame-mail.naiveBayesTheperformanceofthesesimplemachinelearningalgorithmsdependsheavilyontherepresentationofthedatatheyaregiven.Forexample,whenlogisticregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexaminethepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevantinformation,suchasthepresenceorabsenceofauterinescar.Eachpieceofinformationincludedintherepresentationofthepatientisknownasafeature.Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswithvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesaredeﬁnedinanyway. IflogisticregressionwasgivenanMRIscanofthepatient,ratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeusefulpredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithanycomplicationsthatmightoccurduringdelivery.Thisdependenceonrepresentationsisageneralphenomenonthatappearsthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-tionssuchassearchingacollectionofdatacanproceedexponentiallyfasterifthecollectionisstructuredandindexedintelligently. PeoplecaneasilyperformarithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuchmoretime-consuming.Itisnotsurprisingthatthechoiceofrepresentationhasanenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimplevisualexample,seeFig..1.1Manyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetoffeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachinelearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfromsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrongclueastowhetherthespeakerisaman,woman,orchild.However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.Forexample,supposethatwewouldliketowriteaprogramtodetectcarsinphotographs.Weknowthatcarshavewheels,sowemightliketousethepresenceofawheelasafeature. Unfortunately,itisdiﬃculttodescribeexactlywhatawheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebutitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀthemetalpartsofthewheel,thefenderofthecaroranobjectintheforegroundobscuringpartofthewheel,andsoon.3'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 18}, page_content='CHAPTER1.INTRODUCTION\\n\\ue078\\ue079\\ue043\\ue061\\ue072\\ue074\\ue065\\ue073\\ue069\\ue061\\ue06e\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073\\n\\ue072\\ue0b5\\ue050\\ue06f\\ue06c\\ue061\\ue072\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073\\nFigure1.1:Exampleof diﬀerentrepresentations:supposewewanttoseparate twocategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplotontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpletosolvewithaverticalline.(FigureproducedincollaborationwithDavidWarde-Farley)Onesolutiontothisproblemistousemachinelearningtodiscovernotonlythemappingfromrepresentationtooutputbutalsotherepresentationitself.Thisapproachisknownasrepresentationlearning.Learnedrepresentationsoftenresultinmuchbetterperformance thancanbeobtained withhand-designedrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,withminimalhumanintervention.Arepresentationlearningalgorithmcandiscoveragoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhourstomonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealofhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.Thequintessentialexampleofarepresentationlearningalgorithmistheau-toencoderencoder.Anautoencoderisthecombinationofanfunctionthatconvertstheinputdataintoadiﬀerentrepresentation,andadecoderfunctionthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencodersaretrainedtopreserveasmuchinformationaspossiblewhenaninputisrunthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenewrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimtoachievediﬀerentkindsofproperties.Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusuallytoseparatethethatexplaintheobserveddata.Inthiscontext,factorsofvariationweusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;thefactorsareusuallynotcombinedbymultiplication.Suchfactorsareoftennotquantities4'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 19}, page_content='CHAPTER1.INTRODUCTIONthataredirectlyobserved.Instead,theymayexisteitherasunobservedobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifyingexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofasconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’sage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzinganimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,andtheangleandbrightnessofthesun.Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplicationsisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweareabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclosetoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.Mostapplicationsrequireustothefactorsofvariationanddiscardthedisentangleonesthatwedonotcareabout.Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeaturesfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,canbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingofthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvetheoriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.Deeplearningsolvesthiscentralprobleminrepresentationlearningbyintroduc-ingrepresentationsthatareexpressedintermsofother,simplerrepresentations.Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-cepts.Fig.showshowadeeplearningsystemcanrepresenttheconceptofan1.2imageofapersonbycombiningsimplerconcepts,suchascornersandcontours,whichareinturndeﬁnedintermsofedges.Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeepnetworkormultilayerperceptron(MLP).Amultilayerperceptronisjustamathe-maticalfunctionmappingsomesetofinputvaluestooutputvalues.Thefunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeachapplicationofadiﬀerentmathematicalfunctionasprovidinganewrepresentationoftheinput.Theideaoflearningtherightrepresentationforthedataprovidesoneperspec-tiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthecomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentationcanbethoughtofasthestateofthecomputer’smemoryafterexecutinganothersetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemoreinstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselaterinstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis5'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 20}, page_content='CHAPTER1.INTRODUCTION\\nVisible layer(input pixels)1st hidden layer(edges)2nd hidden layer(corners andcontours)3rd hidden layer(object parts)CARPERSONANIMALOutput(object identity)\\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstandthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollectionofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisverycomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoaseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.Theinputispresentedatthe,sonamedbecauseitcontainsthevariablesthatwevisiblelayerareabletoobserve.Thenaseriesofextractsincreasinglyabstractfeatureshiddenlayersfromtheimage. Theselayersarecalled“hidden”becausetheirvaluesarenotgiveninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplainingtherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekindoffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasilyidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthiddenlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersandextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhiddenlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayercandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursandcorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscanbeusedtorecognizetheobjectspresentintheimage.ImagesreproducedwithpermissionfromZeilerandFergus2014().6'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 21}, page_content='CHAPTER1.INTRODUCTION\\nx1x1σ\\nw1w1×x2x2w2w2×+ElementSet+×σx xw wElementSetLogisticRegressionLogisticRegressionFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhereeachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputtooutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,σ(wTx),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationandlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepththree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.viewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarilyencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostoresstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditionalcomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,butithelpsthemodeltoorganizeitsprocessing.Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewisbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluatethearchitecture.Wecanthinkofthisasthelengthofthelongestpaththroughaﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgivenitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengthsdependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmaybedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionsweallowtobeusedasindividualstepsintheﬂowchart.Fig.illustrateshowthischoice1.3oflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofamodelasbeingnotthedepthofthecomputationalgraphbutthedepthofthegraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepthoftheﬂowchartofthecomputationsneededtocomputetherepresentationof7'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 22}, page_content='CHAPTER1.INTRODUCTIONeachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁnedgiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystemobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobablypresentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers—alayerforeyesandalayerforfaces—butthegraphofcomputationsincludes2nlayersifwereﬁneourestimateofeachconceptgiventheothertimes.nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthecomputationalgraph,orthedepthoftheprobabilisticmodelinggraph—ismostrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelementsfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthedepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthofacomputerprogram. Noristhereaconsensusabouthowmuchdepthamodelrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthestudyofmodelsthateitherinvolveagreateramountofcompositionoflearnedfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputersystemstoimprovewithexperienceanddata. Accordingtotheauthorsofthisbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthatcanoperateincomplicated,real-worldenvironments.Deeplearningisaparticularkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningtorepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedinrelationtosimplerconcepts,andmoreabstractrepresentationscomputedintermsoflessabstractones.Fig.illustratestherelationshipbetweenthesediﬀerent1.4AIdisciplines.Fig.givesahigh-levelschematicofhoweachworks.1.51.1WhoShouldReadThisBook?Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomaintargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents(undergraduateorgraduate)learningaboutmachinelearning,includingthosewhoarebeginningacareerindeeplearningandartiﬁcialintelligenceresearch. Theothertargetaudienceissoftwareengineerswhodonothaveamachinelearningorstatisticsbackground,butwanttorapidlyacquireoneandbeginusingdeeplearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulinmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,8'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 23}, page_content='CHAPTER1.INTRODUCTION\\nAIMachine learningRepresentation learningDeep learningExample:KnowledgebasesExample:LogisticregressionExample:ShallowautoencodersExample:MLPs\\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproachestoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\\n9'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 24}, page_content='CHAPTER1.INTRODUCTION\\nInputHand-designed programOutput\\nInputHand-designed featuresMapping from featuresOutput\\nInputFeaturesMapping from featuresOutput\\nInputSimple featuresMapping from featuresOutput\\nAdditional layers of more abstract features\\nRule-basedsystemsClassicmachinelearningRepresentationlearningDeeplearningFigure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeachotherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareabletolearnfromdata.10'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 25}, page_content='CHAPTER1.INTRODUCTIONnaturallanguageprocessing,robotics,bioinformaticsandchemistry,videogames,searchengines,onlineadvertisingandﬁnance.Thisbookhasbeenorganizedintothreepartsinordertobestaccommodateavarietyofreaders.PartintroducesbasicmathematicaltoolsandmachinelearningIconcepts.PartdescribesthemostestablisheddeeplearningalgorithmsthatareIIessentiallysolvedtechnologies.PartdescribesmorespeculativeideasthatareIIIwidelybelievedtobeimportantforfutureresearchindeeplearning.Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterestsorbackground.Readersfamiliarwithlinearalgebra,probability,andfundamentalmachinelearningconceptscanskipPart,forexample,whilereaderswhojustwantItoimplementaworkingsystemneednotreadbeyondPart.TohelpchoosewhichIIchapterstoread,Fig.providesaﬂowchartshowingthehigh-levelorganization1.6ofthebook.Wedoassumethatallreaderscomefromacomputersciencebackground.Weassumefamiliaritywithprogramming,abasicunderstandingofcomputationalperformanceissues,complexitytheory,introductorylevelcalculusandsomeoftheterminologyofgraphtheory.1.2HistoricalTrendsinDeepLearningItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthanprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:•Deeplearninghashadalongandrichhistory,buthasgonebymanynamesreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedinpopularity.•Deeplearninghasbecomemoreusefulastheamountofavailabletrainingdatahasincreased.•Deeplearningmodelshavegrowninsizeovertimeascomputerhardwareandsoftwareinfrastructurefordeeplearninghasimproved.•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasingaccuracyovertime.11'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 26}, page_content='CHAPTER1.INTRODUCTION1. IntroductionPart I: Applied Math and Machine Learning Basics2. Linear Algebra3. Probability and Information Theory4. Numerical Computation5. Machine Learning BasicsPart II: Deep Networks: Modern Practices6. Deep Feedforward Networks7. Regularization8. Optimization9.  CNNs10.  RNNs11. Practical Methodology12. ApplicationsPart III: Deep Learning Research13. Linear Factor Models14. Autoencoders15. Representation Learning16. Structured Probabilistic Models17. Monte Carlo Methods18. Partition Function19. Inference20. Deep Generative ModelsFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanotherindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.12'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 27}, page_content='CHAPTER1.INTRODUCTION1.2.1TheManyNamesandChangingFortunesofNeuralNet-worksWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasanexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabookaboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deeplearningonlyappearstobenew,becauseitwasrelativelyunpopularforseveralyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmanydiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeldhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchersanddiﬀerentperspectives.Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.However,somebasiccontextisusefulforunderstandingdeeplearning.Broadlyspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deeplearn-ingknownascyberneticsconnectionisminthe1940s–1960s,deeplearningknownasinthe1980s–1990s,andthecurrentresurgenceunderthenamedeeplearningbeginningin2006.ThisisquantitativelyillustratedinFig..1.7Someoftheearliestlearningalgorithmswerecognizetodaywereintendedtobecomputationalmodelsofbiologicallearning,i.e.modelsofhowlearninghappensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeeplearninghasgonebyisartiﬁcialneuralnetworks(ANNs).Thecorrespondingperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspiredbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).Whilethekindsofneuralnetworksusedformachinelearninghavesometimesbeenusedtounderstandbrainfunction(,),theyareHintonandShallice1991generallynotdesignedtoberealisticmodelsofbiologicalfunction. Theneuralperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthatthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,andaconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthecomputationalprinciplesbehindthebrainandduplicateitsfunctionality.Anotherperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandtheprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshedlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolveengineeringapplications.Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspectiveonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneralprincipleoflearningmultiplelevelsofcomposition,whichcanbeappliedinmachinelearningframeworksthatarenotnecessarilyneurallyinspired.13'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 28}, page_content='CHAPTER1.INTRODUCTION\\n1940195019601970198019902000Year0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrasecybernetics(connectionism+neuralnetworks)\\nFigure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnetsresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).Theﬁrstwavestartedwithcyberneticsinthe1940s–1960s,withthedevelopmentoftheoriesofbiologicallearning(,;,)andimplementationsofMcCullochandPitts1943Hebb1949theﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingleneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,withback-propagation(,)totrainaneuralnetworkwithoneortwoRumelhartetal.1986ahiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hintonetal.etal.etal.,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook2007aformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthanthecorrespondingscientiﬁcactivityoccurred.\\n14'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 29}, page_content='CHAPTER1.INTRODUCTIONTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodelsmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedtotakeasetofninputvaluesx1,...,xnandassociatethemwithanoutputy.Thesemodelswouldlearnasetofweightsw1,...,wnandcomputetheiroutputf(xw,)=x1w1+···+xnwn.Thisﬁrstwaveofneuralnetworksresearchwasknownascybernetics,asillustratedinFig..1.7TheMcCulloch-PittsNeuron(,)wasanearlymodelMcCullochandPitts1943ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesofinputsbytestingwhetherf(xw,)ispositiveornegative.Ofcourse,forthemodeltocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobesetcorrectly. Theseweightscouldbesetbythehumanoperator. Inthe1950s,theperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.Theadaptivelinearelement(ADALINE),whichdatesfromaboutthesametime,simplyreturnedthevalueoff(x)itselftopredictarealnumber(WidrowandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofmachinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADA-LINEwasaspecialcaseofanalgorithmcalledstochasticgradientdescent.Slightlymodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominanttrainingalgorithmsfordeeplearningmodelstoday.Modelsbasedonthef(xw,)usedbytheperceptronandADALINEarecalledlinearmodels.Thesemodelsremainsomeofthemostwidelyusedmachinelearningmodels,thoughinmanycasestheyaretrainedindiﬀerentwaysthantheoriginalmodelsweretrained.Linearmodelshavemanylimitations.Mostfamously,theycannotlearntheXORfunction,wheref([0,1],w)=1andf([1,0],w)=1butf([1,1],w)=0andf([0,0],w)=0.Criticswhoobservedtheseﬂawsinlinearmodelscausedabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.Today,neuroscienceisregardedasanimportantsourceofinspirationfordeeplearningresearchers,butitisnolongerthepredominantguidefortheﬁeld.Themainreasonforthediminishedrole ofneuroscienceindeeplearningresearchtodayisthatwesimplydonothaveenoughinformationaboutthebraintouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsusedbythebrain,wewouldneedtobeabletomonitortheactivityof(attheveryleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenotabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand15'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 30}, page_content='CHAPTER1.INTRODUCTIONwell-studiedpartsofthebrain(,).OlshausenandField2005Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithmcansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewiredtosendvisualsignalstothatarea(VonMelchner2000etal.,).Thissuggeststhatmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthediﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearningresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudyingnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearningresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaofhavingmanycomputationalunitsthatbecomeintelligentonlyviatheirinteractionswitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspiredbythestructureofthemammalianvisualsystemandlaterbecamethebasisforthemodernconvolutionalnetwork(,),aswewillseeinSec..LeCunetal.1998b9.10Mostneuralnetworkstodayarebasedonamodelneuroncalledtherectiﬁedlinearunit.TheoriginalCognitron(Fukushima1975,)introducedamorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrainfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrommanyviewpoints,withNairandHinton2010Glorot2011a()andetal.()citingneuroscienceasaninﬂuence,andJarrett2009etal.()citingmoreengineering-orientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,itneednotbetakenasarigidguide.Weknowthatactualneuronscomputeverydiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealismhasnotyetledtoanimprovementinmachinelearningperformance.Also,whileneurosciencehassuccessfullyinspiredseveralneuralnetworkarchitectures,wedonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuchguidanceforthelearningalgorithmsweusetotrainthesearchitectures.Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.WhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasaninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernelmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempttosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,especiallyappliedmathfundamentalslikelinearalgebra,probability,informationtheory,andnumericaloptimization.Whilesomedeeplearningresearchersciteneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith16'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 31}, page_content='CHAPTER1.INTRODUCTIONneuroscienceatall.Itis worth notingthat theeﬀorttounderstandhowthe brainworksonan algorithmic level is alive andwell.This endeavor isprimarily knownas“computationalneuroscience”andisaseparateﬁeldofstudyfromdeeplearning.Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.Theﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystemsthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldofcomputationalneuroscienceisprimarilyconcernedwithbuildingmoreaccuratemodelsofhowthebrainactuallyworks.Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreatpartviaamovementcalledconnectionismparalleldistributedprocessingor(Rumelhartetal.etal.,;1986cMcClelland,).Connectionismaroseinthecontextof1995cognitivescience.Cognitivescienceisaninterdisciplinaryapproachtounderstand-ingthemind,combiningmultiplediﬀerentlevelsofanalysis.Duringtheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsofhowthebraincouldactuallyimplementthemusingneurons.Theconnectionistsbegantostudymodelsofcognitionthatcouldactuallybegroundedinneuralimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingbacktotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949Thecentralideainconnectionismisthatalargenumberofsimplecomputationalunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsightappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsincomputationalmodels.Severalkeyconceptsaroseduringtheconnectionismmovementofthe1980sthatremaincentraltotoday’sdeeplearning.Oneoftheseconceptsisthatofdistributedrepresentation(,).Hintonetal.1986Thisistheideathateachinputtoasystemshouldberepresentedbymanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmanypossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognizecars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Onewayofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunitthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,redbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuronmustindependentlylearntheconceptofcolorandobjectidentity.Onewaytoimproveonthissituationistouseadistributedrepresentation,withthreeneuronsdescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequiresonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisabletolearnaboutredness17'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 32}, page_content='CHAPTER1.INTRODUCTIONfromimagesofcars,trucksandbirds,notonlyfromimagesofonespeciﬁccategoryofobjects.Theconceptofdistributedrepresentationiscentraltothisbook,andwillbedescribedingreaterdetailinChapter.15Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc-cessfuluseofback-propagationtotraindeepneuralnetworkswithinternalrepre-sentationsandthepopularizationoftheback-propagationalgorithm(Rumelhartetal.,;,).Thisalgorithmhaswaxedandwanedinpopularity1986aLeCun1987butasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.Duringthe1990s,researchersmadeimportantadvancesinmodelingsequenceswithneuralnetworks.()and()identiﬁedsomeHochreiter1991Bengioetal.1994ofthefundamentalmathematicaldiﬃcultiesinmodelinglongsequences,describedinSec..10.7HochreiterandSchmidhuber1997()introducedthelongshort-termmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTMiswidelyusedformanysequencemodelingtasks,includingmanynaturallanguageprocessingtasksatGoogle.Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁlltheseunreasonableexpectations,investorsweredisappointed.Simultaneously,otherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boseretal.1992CortesandVapnik1995Schölkopf1999Jor-;,;etal.,)andgraphicalmodels(dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactorsledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformanceonsometasks(,;,).TheCanadianInstituteLeCunetal.1998bBengioetal.2001forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchaliveviaitsNeuralComputationandAdaptivePerception(NCAP)researchinitiative.ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHintonatUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYannLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehadamulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhumanandcomputervision.Atthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃculttotrain. Wenowknowthatalgorithmsthathaveexistedsincethe1980sworkquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythatthesealgorithmsweretoocomputationallycostlytoallowmuchexperimentationwiththehardwareavailableatthetime.Thethirdwaveofneuralnetworksresearchbeganwithabreakthroughin18'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 33}, page_content='CHAPTER1.INTRODUCTION2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbeliefnetworkcould be eﬃcientlytrainedusinga strategycalled greedylayer-wisepretraining(Hinton2006etal.,),whichwillbedescribedinmoredetailinSec.15.1.TheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategycouldbeusedtotrainmanyotherkindsofdeepnetworks(,Bengioetal.2007Ranzato2007a;etal.,)andsystematicallyhelpedtoimprovegeneralizationontestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthetermdeeplearningtoemphasizethatresearcherswerenowabletotraindeeperneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthetheoreticalimportanceofdepth(,;,BengioandLeCun2007DelalleauandBengio2011Pascanu2014aMontufar2014;etal.,;etal.,).Atthistime,deepneuralnetworksoutperformedcompetingAIsystemsbasedonothermachinelearningtechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularityofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeeplearningresearchhaschangeddramaticallywithinthetimeofthiswave.Thethirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandtheabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereismoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeepmodelstoleveragelargelabeleddatasets.1.2.2IncreasingDatasetSizesOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasacrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswereconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercialapplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthanatechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistruethatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.Fortunately,theamountofskillrequiredreducesastheamountoftrainingdataincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextaskstodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoyproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshaveundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themostimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswiththeresourcestheyneedtosucceed.Fig.showshowthesizeofbenchmark1.8datasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasingdigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasinglynetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem19'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 34}, page_content='CHAPTER1.INTRODUCTIONintoadatasetappropriateformachinelearningapplications.Theageof“BigData”hasmademachinelearningmucheasierbecausethekeyburdenofstatisticalestimation—generalizingwelltonewdataafterobservingonlyasmallamountofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumbisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptableperformancewitharound5,000labeledexamplespercategory,andwillmatchorexceedhumanperformancewhentrainedwithadatasetcontainingatleast10millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisisanimportantresearcharea,focusinginparticularonhowwecantakeadvantageoflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervisedlearning.1.2.3IncreasingModelSizesAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoyingcomparativelylittlesuccesssincethe1980sisthatwehavethecomputationalresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.Biologicalneuronsarenotespeciallydenselyconnected.AsseeninFig.,1.10ourmachinelearningmodelshavehadanumberofconnectionsperneuronthatwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishinglysmalluntilquiterecently,asshowninFig..Sincetheintroductionofhidden1.11units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Thisgrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailabilityoflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmorecomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologiesallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberofneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmayrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiologicalneuralnetworksmaybeevenlargerthanthisplotportrays.Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewerneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-lems.Eventoday’snetworks,whichweconsiderquitelargefromacomputationalsystemspointofview,aresmallerthanthenervoussystemofevenrelativelyprimitivevertebrateanimalslikefrogs.Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,20'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 35}, page_content='CHAPTER1.INTRODUCTION\\n\\ue031\\ue039\\ue030\\ue030\\ue031\\ue039\\ue035\\ue030\\ue031\\ue039\\ue038\\ue035\\ue032\\ue030\\ue030\\ue030\\ue032\\ue030\\ue031\\ue035\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue031\\ue030\\ue036\\ue031\\ue030\\ue037\\ue031\\ue030\\ue038\\ue031\\ue030\\ue039\\ue044\\ue061\\ue074\\ue061\\ue073\\ue065\\ue074\\ue020\\ue073\\ue069\\ue07a\\ue065\\ue020\\ue028\\ue06e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue029\\ue049\\ue072\\ue069\\ue073\\ue04d\\ue04e\\ue049\\ue053\\ue054\\ue050\\ue075\\ue062\\ue06c\\ue069\\ue063\\ue020\\ue053\\ue056\\ue048\\ue04e\\ue049\\ue06d\\ue061\\ue067\\ue065\\ue04e\\ue065\\ue074\\ue043\\ue049\\ue046\\ue041\\ue052\\ue02d\\ue031\\ue030\\ue049\\ue06d\\ue061\\ue067\\ue065\\ue04e\\ue065\\ue074\\ue031\\ue030\\ue06b\\ue049\\ue04c\\ue053\\ue056\\ue052\\ue043\\ue020\\ue032\\ue030\\ue031\\ue034\\ue053\\ue070\\ue06f\\ue072\\ue074\\ue073\\ue02d\\ue031\\ue04d\\n\\ue052\\ue06f\\ue074\\ue061\\ue074\\ue065\\ue064\\ue020\\ue054\\ue020\\ue076\\ue073\\ue020\\ue043\\ue054\\ue020\\ue076\\ue073\\ue020\\ue047\\ue020\\ue076\\ue073\\ue020\\ue046\\ue043\\ue072\\ue069\\ue06d\\ue069\\ue06e\\ue061\\ue06c\\ue073\\ue043\\ue061\\ue06e\\ue061\\ue064\\ue069\\ue061\\ue06e\\ue020\\ue048\\ue061\\ue06e\\ue073\\ue061\\ue072\\ue064\\ue057\\ue04d\\ue054\\ue049\\ue06e\\ue063\\ue072\\ue065\\ue061\\ue073\\ue069\\ue06e\\ue067\\ue020\\ue064\\ue061\\ue074\\ue061\\ue073\\ue065\\ue074\\ue020\\ue073\\ue069\\ue07a\\ue065\\ue020\\ue06f\\ue076\\ue065\\ue072\\ue020\\ue074\\ue069\\ue06d\\ue065\\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticiansstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson1900Gosset1908Anderson1935Fisher1936;,;,;,).Inthe1950sthrough1980s,thepioneersofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,suchaslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostanddemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(WidrowandHoﬀ1960Rumelhart1986b,;etal.,).Inthe1980sand1990s,machinelearningbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtensofthousandsofexamplessuchastheMNISTdataset(showninFig.)ofscansof1.9handwrittennumbers(,).Intheﬁrstdecadeofthe2000s, moreLeCunetal.1998bsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(KrizhevskyandHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughouttheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousandstotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset(,Netzeretal.2011),variousversionsoftheImageNetdataset(,,;Dengetal.20092010aRussakovskyetal.etal.,),andtheSports-1Mdataset(2014aKarpathy,).Atthetopofthe2014graph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructedfromtheCanadianHansard(,)andtheWMT2014EnglishtoFrenchBrownetal.1990dataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.21'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 36}, page_content='CHAPTER1.INTRODUCTION\\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNationalInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewithmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigitsandassociatedlabelsdescribingwhichdigit0-9iscontainedineachimage.Thissimpleclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearningresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.GeoﬀreyHintonhasdescribeditas“thedrosophilaofmachinelearning,”meaningthatitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratoryconditions,muchasbiologistsoftenstudyfruitﬂies.22'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 37}, page_content='CHAPTER1.INTRODUCTIONtheadventofgeneralpurposeGPUs(describedinSec.),fasternetwork12.1.2connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneofthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerallyexpectedtocontinuewellintothefuture.1.2.4IncreasingAccuracy,ComplexityandReal-WorldImpactSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovideaccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeenappliedwithsuccesstobroaderandbroadersetsofapplications.Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightlycropped,extremelysmallimages(,).SincethentherehasRumelhartetal.1986abeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modernobjectrecognitionnetworksprocessrichhigh-resolutionphotographsanddonothavearequirementthatthephotobecroppedneartheobjecttoberecognized(,).Similarly,theearliestnetworkscouldonlyrecognizeKrizhevskyetal.2012twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindofobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerentcategoriesofobjects. ThelargestcontestinobjectrecognitionistheImageNetLarge-ScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramaticmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetworkwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthestate-of-the-arttop-5errorratefrom26.1%to15.3%(,),Krizhevskyetal.2012meaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategoriesforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthislistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsareconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesindeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,asshowninFig..1.12Deeplearninghasalsohadadramaticimpactonspeechrecognition.Afterimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnatedstartinginabout2000.Theintroductionofdeeplearning(,;Dahletal.2010Dengetal.etal.,;2010bSeide,;2011Hinton2012aetal.,)tospeechrecognitionresultedinasuddendropoferrorrates,withsomeerrorratescutinhalf.WewillexplorethishistoryinmoredetailinSec..12.3Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionandimagesegmentation(,;Sermanetetal.2013Farabet2013Couprieetal.,;etal.,2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan23'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 38}, page_content='CHAPTER1.INTRODUCTION\\n\\ue031\\ue039\\ue035\\ue030\\ue031\\ue039\\ue038\\ue035\\ue032\\ue030\\ue030\\ue030\\ue032\\ue030\\ue031\\ue035\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue043\\ue06f\\ue06e\\ue06e\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue073\\ue020\\ue070\\ue065\\ue072\\ue020\\ue06e\\ue065\\ue075\\ue072\\ue06f\\ue06e\\ue031\\ue032\\ue033\\ue034\\ue035\\ue036\\ue037\\ue038\\ue039\\ue031\\ue030\\ue046\\ue072\\ue075\\ue069\\ue074\\ue020\\ue066\\ue06c\\ue079\\ue04d\\ue06f\\ue075\\ue073\\ue065\\ue043\\ue061\\ue074\\ue048\\ue075\\ue06d\\ue061\\ue06e\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue063\\ue06f\\ue06e\\ue06e\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue073\\ue020\\ue070\\ue065\\ue072\\ue020\\ue06e\\ue065\\ue075\\ue072\\ue06f\\ue06e\\ue020\\ue06f\\ue076\\ue065\\ue072\\ue020\\ue074\\ue069\\ue06d\\ue065\\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneuralnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetweenneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyasmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworkstohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehumanbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneuralnetworksizesfrom().Wikipedia20151.Adaptivelinearelement(,)WidrowandHoﬀ19602.Neocognitron(Fukushima1980,)3.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.20064.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)5.Unsupervisedconvolutionalnetwork(,)Jarrettetal.20096.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.20107.Distributedautoencoder(,)Leetal.20128.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.20129.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201310.GoogLeNet(,)Szegedyetal.2014a\\n24'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 39}, page_content='CHAPTER1.INTRODUCTIONetal.,).2012Atthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,sohasthecomplexityofthetasksthattheycansolve.()Goodfellowetal.2014dshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacterstranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividualelementsofthesequence(,).Recurrentneuralnetworks,GülçehreandBengio2013suchastheLSTMsequencemodelmentionedabove,arenowusedtomodelrelationshipsbetweensequencessequencesandotherratherthanjustﬁxedinputs.Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizinganotherapplication:machinetranslation(Sutskever2014Bahdanauetal.,;etal.,2015).ThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusionwiththeintroductionofneuralTuringmachines(Graves2014aetal.,)thatlearntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Suchneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.Forexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledandsortedsequences.Thisself-programmingtechnologyisinitsinfancy,butinthefuturecouldinprinciplebeappliedtonearlyanytask.Anothercrowningachievementofdeeplearningisitsextensiontothedomainofreinforcementlearning.Inthecontextofreinforcementlearning,anautonomousagentmustlearntoperformataskbytrialanderror,withoutanyguidancefromthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystembasedondeeplearningiscapableoflearningtoplayAtarivideogames,reachinghuman-levelperformanceonmanytasks(,).DeeplearninghasMnihetal.2015alsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics(,).Finnetal.2015Manyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearningisnowused bymanytoptechnologycompanies includingGoogle, Microsoft,Facebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftwareinfrastructure.SoftwarelibrariessuchasTheano(,;Bergstraetal.2010Bastienetal.etal.,),PyLearn2(2012Goodfellow,),Torch(,),2013cCollobertetal.2011bDistBelief(,),Caﬀe(,),MXNet(,),andDeanetal.2012Jia2013Chenetal.2015TensorFlow(,)haveallsupportedimportantresearchprojectsorAbadietal.2015commercialproducts.Deeplearninghasalsomadecontributionsbacktoothersciences.Modernconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing25'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 40}, page_content='CHAPTER1.INTRODUCTIONthatneuroscientistscanstudy(,).DeeplearningalsoprovidesusefulDiCarlo2013toolsforprocessingmassiveamountsofdataandmakingusefulpredictionsinscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteractinordertohelppharmaceuticalcompaniesdesignnewdrugs(,),Dahletal.2014tosearchforsubatomicparticles(,),andtoautomaticallyparseBaldietal.2014microscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-Barley2014etal.,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁcﬁeldsinthefuture.Insummary,deeplearningisanapproachtomachinelearningthathasdrawnheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasitdevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendousgrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-puters,largerdatasetsandtechniquestotraindeepernetworks.Theyearsaheadarefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherandbringittonewfrontiers.\\n26'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 41}, page_content='CHAPTER1.INTRODUCTION\\n1950198520002015205610−210−110010110210310410510610710810910101011Numberofneurons(logarithmicscale)\\n1234567891011121314151617181920\\nSpongeRoundwormLeechAntBeeFrogOctopusHumanIncreasingneuralnetworksizeovertime\\nFigure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom().Wikipedia20151.Perceptron(,,)Rosenblatt195819622.Adaptivelinearelement(,)WidrowandHoﬀ19603.Neocognitron(Fukushima1980,)4.Earlyback-propagationnetwork(,)Rumelhartetal.1986b5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)6.Multilayerperceptronforspeechrecognition(,)Bengioetal.19917.Meanﬁeldsigmoidbeliefnetwork(,)Sauletal.19968.LeNet-5(,)LeCunetal.1998b9.Echostatenetwork(,)JaegerandHaas200410.Deepbeliefnetwork(,)Hintonetal.200611.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.200612.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)13.GPU-accelerateddeepbeliefnetwork(,)Rainaetal.200914.Unsupervisedconvolutionalnetwork(,)Jarrettetal.200915.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.201016.OMP-1network(,)CoatesandNg201117.Distributedautoencoder(,)Leetal.201218.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.201219.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201320.GoogLeNet(,)Szegedyetal.2014a\\n27'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 42}, page_content='CHAPTER1.INTRODUCTION\\n201020112012201320142015000.005.010.015.020.025.030.ILSVRC classiﬁcationerrorrateDecreasingerrorrateovertime\\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNetLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetitioneveryyear,andyieldedlowerandlowererrorrateseachtime. DatafromRussakovskyetal.etal.()and2014bHe().2015\\n28'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 43}, page_content='PartIAppliedMathandMachineLearningBasics\\n29'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 44}, page_content='Thispartofthebookintroducesthebasicmathematicalconceptsneededtounderstanddeeplearning.Webeginwithgeneralideasfromappliedmaththatallowustodeﬁnefunctionsofmanyvariables,ﬁndthehighestandlowestpointsonthesefunctionsandquantifydegreesofbelief.Next,wedescribethefundamentalgoalsofmachinelearning.Wedescribehowtoaccomplishthesegoalsbyspecifyingamodelthatrepresentscertainbeliefs,designingacostfunctionthatmeasureshowwellthosebeliefscorrespondwithrealityandusingatrainingalgorithmtominimizethatcostfunction.Thiselementaryframeworkisthebasisforabroadvarietyofmachinelearningalgorithms,includingapproachestomachinelearningthatarenotdeep. Inthesubsequentpartsofthebook,wedevelopdeeplearningalgorithmswithinthisframework.\\n30'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 45}, page_content='Chapter2LinearAlgebraLinearalgebraisabranchofmathematicsthatiswidelyusedthroughoutscienceandengineering.However,becauselinearalgebraisaformofcontinuousratherthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.Agoodunderstandingoflinearalgebraisessentialforunderstandingandworkingwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.Wethereforeprecedeourintroductiontodeeplearningwithafocusedpresentationofthekeylinearalgebraprerequisites.Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.Ifyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreferencesheettoreviewkeyformulas,werecommendTheMatrixCookbook(PetersenandPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapterwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualsoconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchasShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebratopicsthatarenotessentialforunderstandingdeeplearning.2.1Scalars,Vectors,MatricesandTensorsThestudyoflinearalgebrainvolvesseveraltypesofmathematicalobjects:•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheotherobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.Whenweintroducethem,wespecifywhatkindofnumbertheyare.For31'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 46}, page_content='CHAPTER2.LINEARALGEBRAexample,wemightsay“Lets∈Rbetheslopeoftheline,”whiledeﬁningareal-valuedscalar,or“Letn∈Nbethenumberofunits,”whiledeﬁninganaturalnumberscalar.•Vectors:Avectorisanarrayofnumbers.Thenumbersarearrangedinorder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.Typicallywegivevectorslowercasenameswritteninboldtypeface,suchasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalictypeface,withasubscript.Theﬁrstelementofxisx1,thesecondelementisx2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredinthevector.IfeachelementisinR,andthevectorhasnelements,thenthevectorliesinthesetformedbytakingtheCartesianproductofRntimes,denotedasRn.Whenweneedtoexplicitlyidentifytheelementsofavector,wewritethemasacolumnenclosedinsquarebrackets:x=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1x2...xn\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fb.(2.1)Wecanthinkofvectorsasidentifyingpointsinspace,witheachelementgivingthecoordinatealongadiﬀerentaxis.Sometimesweneedtoindexasetofelementsofavector.Inthiscase,wedeﬁneasetcontainingtheindicesandwritethesetasasubscript.Forexample,toaccessx1,x3andx6,wedeﬁnethesetS={1,3,6}andwritexS.Weusethe−signtoindexthecomplementofaset.Forexamplex−1isthevectorcontainingallelementsofxexceptforx1,andx−Sisthevectorcontainingalloftheelementsofexceptforxx1,x3andx6.•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁedbytwoindicesinsteadofjustone.Weusuallygivematricesupper-casevariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhasaheightofmandawidthofn,thenwesaythatA∈Rmn×.Weusuallyidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,andtheindicesarelistedwithseparatingcommas.Forexample,A11,istheupperleftentryofAandAm,nisthebottomrightentry.Wecanidentifyallofthenumberswithverticalcoordinateibywritinga“”forthehorizontal:coordinate.Forexample,Ai,:denotesthehorizontalcrosssectionofAwithverticalcoordinatei.Thisisknownasthei-throwofA.Likewise,A:,iis32'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 47}, page_content='CHAPTER2.LINEARALGEBRA\\nA=\\uf8ee\\uf8f0A11,A12,A21,A22,A31,A32,\\uf8f9\\uf8fb⇒A\\ue021=\\ue025A11,A21,A31,A12,A22,A32,\\ue026Figure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthemaindiagonal.thei-thcolumnofA.Whenweneedtoexplicitlyidentifytheelementsofamatrix,wewritethemasanarrayenclosedinsquarebrackets:\\ue014A11,A12,A21,A22,\\ue015.(2.2)Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjustasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdonotconvertanythingtolowercase.Forexample,f(A)i,jgiveselement(i,j)ofthematrixcomputedbyapplyingthefunctionto.fA•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwithavariablenumberofaxesisknownasaWedenoteatensornamed“A”tensor.withthistypeface:A.WeidentifytheelementofAatcoordinates(i,j,k)bywritingAi,j,k.Oneimportantoperationonmatricesisthetranspose.Thetransposeofamatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemaindiagonal,runningdownandtotheright,startingfromitsupperleftcorner.SeeFig.foragraphicaldepictionofthisoperation.Wedenotethetransposeofa2.1matrixasAA\\ue03e,anditisdeﬁnedsuchthat(A\\ue03e)i,j= Aj,i.(2.3)Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.Thetransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe33'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 48}, page_content='CHAPTER2.LINEARALGEBRAdeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,thenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,x= [x1,x2,x3]\\ue03e.Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,wecanseethatascalarisitsowntranspose:aa= \\ue03e.Wecanaddmatricestoeachother,aslongastheyhavethesameshape,justbyaddingtheircorrespondingelements:whereCAB= +Ci,j= Ai,j+Bi,j.Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,justbyperformingthatoperationoneachelementofamatrix:D=a·B+cwhereDi,j= aB·i,j+c.Inthecontextofdeeplearning,wealsousesomelessconventionalnotation.Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,whereCi,j=Ai,j+bj.Inotherwords,thevectorbisaddedtoeachrowofthematrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedintoeachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocationsiscalledbroadcasting.2.2MultiplyingMatricesandVectorsOneofthemostimportantoperationsinvolvingmatricesismultiplicationoftwomatrices.ThematrixproductofmatricesAandBisathirdmatrixC.Inorderforthisproducttobedeﬁned,AmusthavethesamenumberofcolumnsasBhasrows.IfAisofshapemn×andBisofshapenp×,thenCisofshapemp×.Wecanwritethematrixproductjustbyplacingtwoormorematricestogether,e.g.CAB= .(2.4)TheproductoperationisdeﬁnedbyCi,j=\\ue058kAi,kBk,j.(2.5)Notethatthestandardproductoftwomatricesisjustamatrixcontainingnottheproductoftheindividualelements.Suchanoperationexistsandiscalledtheelement-wiseproductHadamardproductor,andisdenotedas.AB\\ue00cThedotproductbetweentwovectorsxandyofthesamedimensionalityisthematrixproductx\\ue03ey.WecanthinkofthematrixproductC=ABascomputingCi,jasthedotproductbetweenrowofandcolumnof.iAjB34'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 49}, page_content='CHAPTER2.LINEARALGEBRAMatrixproductoperationshavemanyusefulpropertiesthatmakemathematicalanalysis ofmatrices moreconvenient.For example, matrix multiplication isdistributive:ABCABAC(+) = +.(2.6)Itisalsoassociative:ABCABC() = ().(2.7)Matrixmultiplicationiscommutative(theconditionnotAB=BAdoesnotalwayshold),unlikescalarmultiplication.However,thedotproductbetweentwovectorsiscommutative:x\\ue03eyy= \\ue03ex.(2.8)Thetransposeofamatrixproducthasasimpleform:()AB\\ue03e= B\\ue03eA\\ue03e.(2.9)ThisallowsustodemonstrateEq.,byexploitingthefactthatthevalueof2.8suchaproductisascalarandthereforeequaltoitsowntranspose:x\\ue03ey=\\ue010x\\ue03ey\\ue011\\ue03e= y\\ue03ex.(2.10)Sincethefocusofthistextbookisnotlinearalgebra,wedonotattempttodevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,butthereadershouldbeawarethatmanymoreexist.Wenowknowenoughlinearalgebranotationtowritedownasystemoflinearequations:Axb= (2.11)whereA∈Rmn×isaknownmatrix,b∈Rmisaknownvector,andx∈Rnisavectorofunknownvariableswewouldliketosolvefor.Eachelementxiofxisoneoftheseunknownvariables.EachrowofAandeachelementofbprovideanotherconstraint.WecanrewriteEq.as:2.11A1:,x= b1(2.12)A2:,x= b2(2.13)...(2.14)Am,:x= bm(2.15)or,evenmoreexplicitly,as:A11,x1+A12,x2++···A1,nxn= b1(2.16)35'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 50}, page_content='CHAPTER2.LINEARALGEBRA\\uf8ee\\uf8f0100010001\\uf8f9\\uf8fbFigure2.2::ThisisExampleidentitymatrixI3.A21,x1+A22,x2++···A2,nxn= b2(2.17)...(2.18)Am,1x1+Am,2x2++···Am,nxn= bm.(2.19)Matrix-vectorproductnotationprovidesamorecompactrepresentationforequationsofthisform.2.3IdentityandInverseMatricesLinearalgebraoﬀersapowerfultoolcalledthatallowsustomatrixinversionanalyticallysolveEq.formanyvaluesof.2.11ATodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentitymatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwemultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreservesn-dimensionalvectorsasIn.Formally,In∈Rnn×,and∀∈xRn,Inxx= .(2.20)Thestructureoftheidentitymatrixissimple:alloftheentriesalongthemaindiagonalare1,whilealloftheotherentriesarezero.SeeFig.foranexample.2.2TheofmatrixinverseAisdenotedasA−1,anditisdeﬁnedasthematrixsuchthatA−1AI= n.(2.21)WecannowsolveEq.bythefollowingsteps:2.11Axb= (2.22)A−1AxA= −1b(2.23)InxA= −1b(2.24)36'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 51}, page_content='CHAPTER2.LINEARALGEBRAxA= −1b.(2.25)Ofcourse,thisdependsonitbeingpossibletoﬁndA−1.WediscusstheconditionsfortheexistenceofA−1inthefollowingsection.WhenA−1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmanytimesfordiﬀerentvaluesofb.However,A−1isprimarilyusefulasatheoreticaltool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.BecauseA−1canberepresentedwithonlylimitedprecisiononadigitalcomputer,algorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurateestimatesof.x2.4LinearDependenceandSpanInorderforA−1toexist,Eq.musthaveexactlyonesolutionforeveryvalue2.11ofb.However,itisalsopossibleforthesystemofequationstohavenosolutionsorinﬁnitelymanysolutionsforsomevaluesofb.Itisnotpossibletohavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;ifbothxandyaresolutionsthenzxy= α+(1)−α(2.26)isalsoasolutionforanyreal.αToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumnsofAasspecifyingdiﬀerentdirectionswecantravelfromthe(thepointoriginspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareofreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelineachofthesedirections,withxispecifyinghowfartomoveinthedirectionofcolumn:iAx=\\ue058ixiA:,i.(2.27)Ingeneral,thiskindofoperationiscalledalinearcombination.Formally,alinearcombinationofsomesetofvectors{v(1),...,v()n}isgivenbymultiplyingeachvectorv()ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\\ue058iciv()i.(2.28)Thespanofasetofvectorsisthesetofallpointsobtainablebylinearcombinationoftheoriginalvectors.37'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 52}, page_content='CHAPTER2.LINEARALGEBRADeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherbisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumnspacerangeortheof.AInorderforthesystemAx=btohaveasolutionforallvaluesofb∈Rm,wethereforerequirethatthecolumnspaceofAbeallofRm.IfanypointinRmisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathasnosolution.TherequirementthatthecolumnspaceofAbeallofRmimpliesimmediatelythatAmusthaveatleastmcolumns,i.e.,nm≥. Otherwise,thedimensionalityofthecolumnspacewouldbelessthanm.Forexample,considera3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofxatbestallowsustotraceouta2-DplanewithinR3.Theequationhasasolutionifandonlyifliesonthatplane.bHavingnm≥isonlyanecessaryconditionforeverypointtohaveasolution.Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnstoberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.Thishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthereplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailstoencompassallofR2,eventhoughtherearetwocolumns.Formally,thiskindofredundancyisknownaslineardependence.Asetofvectorsislinearlyindependentifnovectorinthesetisalinearcombinationoftheothervectors.Ifweaddavectortoasetthatisalinearcombinationoftheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’sspan.ThismeansthatforthecolumnspaceofthematrixtoencompassallofRm,thematrixmustcontainatleastonesetofmlinearlyindependentcolumns.ThisconditionisbothnecessaryandsuﬃcientforEq.tohaveasolutionforeveryvalueof2.11b.Notethattherequirementisforasettohaveexactlymlinearindependentcolumns,notatleastm.Nosetofm-dimensionalvectorscanhavemorethanmmutuallylinearlyindependentcolumns,butamatrixwithmorethancolumnsmmayhavemorethanonesuchset.Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethatEq.hasonesolutionforeachvalueof2.11atmostb.Todoso,weneedtoensurethatthematrixhasatmostmcolumns.Otherwisethereismorethanonewayofparametrizingeachsolution.Together,thismeansthatthematrixmustbesquare,thatis,werequirethatm=nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrixwithlinearlydependentcolumnsisknownas.singularIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe38'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 53}, page_content='CHAPTER2.LINEARALGEBRAequation.However,wecannotusethemethodofmatrixinversiontoﬁndthesolution.Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itisalsopossibletodeﬁneaninversethatismultipliedontheright:AA−1= I.(2.29)Forsquarematrices,theleftinverseandrightinverseareequal.2.5NormsSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusuallymeasurethesizeofvectorsusingafunctioncalleda.Formally,thenormLpnormisgivenby||||xp=\\ue020\\ue058i|xi|p\\ue0211p(2.30)forp,p.∈R≥1Norms,includingtheLpnorm,arefunctionsmappingvectorstonon-negativevalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefromtheorigintothepointx.Morerigorously,anormisanyfunctionfthatsatisﬁesthefollowingproperties:•⇒f() = 0 xx= 0•≤f(+) xyff()+x()y(thetriangleinequality)•∀∈||αR,fα(x) = αf()xTheL2norm,withp= 2,isknownastheEuclideannorm. ItissimplytheEuclideandistancefromtheorigintothepointidentiﬁedbyx.TheL2normisusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,withthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing2thesquaredL2norm,whichcanbecalculatedsimplyasx\\ue03ex.ThesquaredL2normismoreconvenienttoworkwithmathematicallyandcomputationallythantheL2normitself.Forexample,thederivativesofthesquaredL2normwithrespecttoeachelementofxeachdependonlyonthecorrespondingelementofx,whileallofthederivativesoftheL2normdependontheentirevector.Inmanycontexts,thesquaredL2normmaybeundesirable39'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 54}, page_content='CHAPTER2.LINEARALGEBRAbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearningapplications,itisimportanttodiscriminatebetweenelementsthatareexactlyzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunctionthatgrowsatthesamerateinalllocations,butretainsmathematicalsimplicity:theL1norm.TheL1normmaybesimpliﬁedto||||x1=\\ue058i|xi|.(2.31)TheL1normiscommonlyusedinmachinelearningwhenthediﬀerencebetweenzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmovesawayfrom0by,the\\ue00fL1normincreasesby.\\ue00fWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzeroelements.Someauthorsrefertothisfunctionasthe“L0norm,”butthisisincorrectterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,becausescalingthevectorbyαdoesnotchangethenumberofnonzeroentries. TheL1normisoftenusedasasubstituteforthenumberofnonzeroentries.OneothernormthatcommonlyarisesinmachinelearningistheL∞norm,alsoknownasthe. Thisnormsimpliﬁestotheabsolutevalueofthemaxnormelementwiththelargestmagnitudeinthevector,||||x∞= maxi|xi|.(2.32)Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontextofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscureFrobeniusnorm||||AF=\\ue073\\ue058i,jA2i,j,(2.33)whichisanalogoustotheL2normofavector.Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,x\\ue03eyx= ||||2||||y2cosθ(2.34)whereistheanglebetweenand.θxy2.6SpecialKindsofMatricesandVectorsSomespecialkindsofmatricesandvectorsareparticularlyuseful.40'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 55}, page_content='CHAPTER2.LINEARALGEBRADiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalongthemaindiagonal. Formally,amatrixDisdiagonalifandonlyifDi,j=0foralli\\ue036=j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentitymatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquarediagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrixisverycomputationallyeﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeachelementxibyvi.Inotherwords,diag(v)x=vx\\ue00c.Invertingasquarediagonalmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,andinthatcase,diag(v)−1=diag([1/v1,...,1/vn]\\ue03e).Inmanycases,wemayderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsomematricestobediagonal.Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangulardiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstillpossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,theproductDxwillinvolvescalingeachelementofx,andeitherconcatenatingsomezerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelastelementsofthevectorifiswiderthanitistall.DAmatrixisanymatrixthatisequaltoitsowntranspose:symmetricAA= \\ue03e.(2.35)Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionoftwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,ifAisamatrixofdistancemeasurements,withAi,jgivingthedistancefrompointitopoint,thenjAi,j= Aj,ibecausedistancefunctionsaresymmetric.Aunitvectorunitnormisavectorwith:||||x2= 1.(2.36)Avectorxandavectoryareorthogonaltoeachotherifx\\ue03ey=0. Ifbothvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeachother.InRn,atmostnvectorsmaybemutuallyorthogonalwithnonzeronorm.Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthemorthonormal.Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonormalandwhosecolumnsaremutuallyorthonormal:A\\ue03eAAA= \\ue03e= I.(2.37)41'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 56}, page_content='CHAPTER2.LINEARALGEBRAThisimpliesthatA−1= A\\ue03e,(2.38)soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,theirrowsarenotmerelyorthogonalbutfullyorthonormal.Thereisnospecialtermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.2.7EigendecompositionManymathematicalobjectscanbeunderstoodbetterbybreakingthemintoconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcausedbythewaywechoosetorepresentthem.Forexample,integerscanbedecomposedintoprimefactors.Thewaywerepresentthenumberwillchangedependingonwhetherwewriteitinbaseten12orinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentationwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany125integermultipleofwillbedivisibleby.123Muchaswecandiscoversomethingaboutthetruenatureofanintegerbydecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthatshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromtherepresentationofthematrixasanarrayofelements.Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-decomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsandeigenvalues.AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmultipli-cationbyaltersonlythescaleof:AvAvv= λ.(2.39)Thescalarλisknownasthecorrespondingtothiseigenvector.(Oneeigenvaluecanalsoﬁndalefteigenvectorsuchthatv\\ue03eA=λv\\ue03e,butweareusuallyconcernedwithrighteigenvectors).IfvisaneigenvectorofA,thensoisanyrescaledvectorsvfors,s∈R\\ue036= 0.Moreover,svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylookforuniteigenvectors.SupposethatamatrixAhasnlinearlyindependenteigenvectors,{v(1),...,v()n},withcorrespondingeigenvalues{λ1,...,λn}.Wemayconcatenateallofthe42'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 57}, page_content='CHAPTER2.LINEARALGEBRA\\n\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue031\\ue076\\ue028\\ue031\\ue029\\ue076\\ue028\\ue032\\ue029\\ue042\\ue065\\ue066\\ue06f\\ue072\\ue065\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\n\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\ue030\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\ue031\\ue076\\ue028\\ue031\\ue029\\ue0b8\\ue031\\ue076\\ue028\\ue031\\ue029\\ue076\\ue028\\ue032\\ue029\\ue0b8\\ue032\\ue076\\ue028\\ue032\\ue029\\ue041\\ue066\\ue074\\ue065\\ue072\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\ue045\\ue066\\ue066\\ue065\\ue063\\ue074\\ue020\\ue06f\\ue066\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue065\\ue063\\ue074\\ue06f\\ue072\\ue073\\ue020\\ue061\\ue06e\\ue064\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue061\\ue06c\\ue075\\ue065\\ue073\\nFigure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehaveamatrixAwithtwoorthonormaleigenvectors,v(1)witheigenvalueλ1andv(2)witheigenvalueλ2.(Left)Weplotthesetofallunitvectorsu∈R2asaunitcircle.(Right)WeplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,wecanseethatitscalesspaceindirectionv()ibyλi.eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v(1),...,v()n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [λ1,...,λn]\\ue03e.TheeigendecompositionofisthengivenbyAAVλV= diag()−1.(2.40)Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-torsallowsustostretchspaceindesireddirections. However,weoftenwanttodecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelpustoanalyzecertainpropertiesofthematrix,muchasdecomposinganintegerintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome43'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 58}, page_content='CHAPTER2.LINEARALGEBRAcases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassofmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetricmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectorsandeigenvalues:AQQ= Λ\\ue03e,(2.41)whereQisanorthogonalmatrixcomposedofeigenvectorsofA,andΛisadiagonalmatrix.TheeigenvalueΛi,iisassociatedwiththeeigenvectorincolumniofQ,denotedasQ:,i.BecauseQisanorthogonalmatrix,wecanthinkofAasscalingspacebyλiindirectionv()i.SeeFig.foranexample.2.3WhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectorssharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspanarealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesofΛindescendingorder.Underthisconvention,theeigendecompositionisuniqueonlyifalloftheeigenvaluesareunique.Theeigendecompositionof amatrix tellsus many usefulfactsabout thematrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.Theeigendecompositionofarealsymmetricmatrixcanalsobeusedtooptimizequadraticexpressionsoftheformf(x) =x\\ue03eAxsubjectto||||x2= 1.WheneverxisequaltoaneigenvectorofA,ftakesonthevalueofthecorrespondingeigenvalue.Themaximumvalueoffwithintheconstraintregionisthemaximumeigenvalueanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.Amatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.Amatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,andifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positivesemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx,\\ue03eAx≥0.Positivedeﬁnitematricesadditionallyguaranteethatx\\ue03eAxx= 0 ⇒= 0.2.8SingularValueDecompositionInSec.,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues.2.7Thesingularvaluedecomposition(SVD)providesanotherwaytofactorizeamatrix,intosingularvectorssingularvaluesand.TheSVDallowsustodiscoversomeofthesamekindofinformationastheeigendecomposition.However,theSVDis44'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 59}, page_content='CHAPTER2.LINEARALGEBRAmoregenerallyapplicable.Everyrealmatrixhasasingularvaluedecomposition,butthesameisnottrueoftheeigenvaluedecomposition.Forexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwemustuseasingularvaluedecompositioninstead.RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscoveramatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewriteAasAVλV= diag()−1.(2.42)Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteAasaproductofthreematrices:AUDV= \\ue03e.(2.43)SupposethatAisanmn×matrix.ThenUisdeﬁnedtobeanmm×matrix,DVtobeanmatrix,andmn×tobeanmatrix.nn×Eachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesUandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobeadiagonalmatrix.Notethatisnotnecessarilysquare.DTheelementsalongthediagonalofDareknownastheofthesingularvaluesmatrixA.ThecolumnsofUareknownastheleft-singularvectors.ThecolumnsofareknownasastheVright-singularvectors.WecanactuallyinterpretthesingularvaluedecompositionofAintermsoftheeigendecompositionoffunctionsofA.Theleft-singularvectorsofAaretheeigenvectorsofAA\\ue03e.Theright-singularvectorsofAaretheeigenvectorsofA\\ue03eA.Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofA\\ue03eA.ThesameistrueforAA\\ue03e.PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartiallygeneralizematrixinversiontonon-squarematrices,aswewillseeinthenextsection.2.9TheMoore-PenrosePseudoinverseMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewanttomakealeft-inverseofamatrix,sothatwecansolvealinearequationBAAxy= (2.44)45'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 60}, page_content='CHAPTER2.LINEARALGEBRAbyleft-multiplyingeachsidetoobtainxBy= .(2.45)Dependingonthestructureoftheproblem,itmaynotbepossibletodesignauniquemappingfromto.ABIfAistallerthanitiswide, thenitispossibleforthisequationtohavenosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossiblesolutions.TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayinthesecases.ThepseudoinverseofisdeﬁnedasamatrixAA+=limα\\ue0260(A\\ue03eAI+α)−1A\\ue03e.(2.46)Practicalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-tion,butrathertheformulaA+= VD+U\\ue03e,(2.47)whereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverseD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zeroelementsthentakingthetransposeoftheresultingmatrix.WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthepseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovidesthesolutionx=A+ywithminimalEuclideannorm||||x2amongallpossiblesolutions.WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseaspossibletointermsofEuclideannormy||−||Axy2.2.10TheTraceOperatorThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:Tr() =A\\ue058iAi,i.(2.48)Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatarediﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing46'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 61}, page_content='CHAPTER2.LINEARALGEBRAmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovidesanalternativewayofwritingtheFrobeniusnormofamatrix:||||AF=\\ue071Tr(AA\\ue03e).(2.49)Writinganexpressionintermsofthetraceoperatoropensupopportunitiestomanipulatetheexpressionusingmanyusefulidentities. Forexample,thetraceoperatorisinvarianttothetransposeoperator:Tr() = Tr(AA\\ue03e).(2.50)Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvarianttomovingthelastfactorintotheﬁrstposition,iftheshapesofthecorrespondingmatricesallowtheresultingproducttobedeﬁned:Tr() = Tr() = Tr()ABCCABBCA(2.51)ormoregenerally,Tr(n\\ue059i=1F()i) = Tr(F()nn−1\\ue059i=1F()i).(2.52)Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasadiﬀerentshape.Forexample,forA∈Rmn×andB∈Rnm×,wehaveTr() = Tr()ABBA(2.53)eventhoughAB∈Rmm×andBA∈Rnn×.Anotherusefulfacttokeepinmindisthatascalarisitsowntrace:a=Tr(a).2.11TheDeterminantThedeterminantofa squarematrix, denoteddet(A), isa functionmappingmatricesto realscalars.Thedeterminantisequal totheproductof alltheeigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethoughtofasameasureofhowmuchmultiplicationbythematrixexpandsorcontractsspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleastonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,thenthetransformationisvolume-preserving.47'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 62}, page_content='CHAPTER2.LINEARALGEBRA2.12Example:PrincipalComponentsAnalysisOnesimplemachinelearningalgorithm,principalcomponentsanalysisPCAorcanbederivedusingonlyknowledgeofbasiclinearalgebra.Supposewehaveacollectionofmpoints{x(1),...,x()m}inRn.Supposewewouldliketoapplylossycompressiontothesepoints.Lossycompressionmeansstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.Wewouldliketoloseaslittleprecisionaspossible.Onewaywecanencodethesepointsistorepresentalower-dimensionalversionofthem.Foreachpointx()i∈Rnwewillﬁndacorrespondingcodevectorc()i∈Rl.Iflissmallerthann,itwilltakelessmemorytostorethecodepointsthantheoriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecodeforaninput,f(x) =c,andadecodingfunctionthatproducesthereconstructedinputgivenitscode,.xx≈gf(())PCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethedecoderverysimple,wechoosetousematrixmultiplicationtomapthecodebackintoRn.Let,whereg() = cDcD∈Rnl×isthematrixdeﬁningthedecoding.Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.Tokeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonaltoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unlessln= )Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewecanincreasethescaleofD:,iifwedecreaseciproportionallyforallpoints.Togivetheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitDnorm.Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrstthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗foreachinputpointx.Onewaytodothisistominimizethedistancebetweentheinputpointxanditsreconstruction,g(c∗).Wecanmeasurethisdistanceusinganorm.Intheprincipalcomponentsalgorithm,weusetheL2norm:c∗= argminc||−||xg()c2.(2.54)WecanswitchtothesquaredL2norminsteadoftheL2normitself,becausebothareminimizedbythesamevalueofc.ThisisbecausetheL2normisnon-negativeandthesquaringoperationismonotonicallyincreasingfornon-negative48'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 63}, page_content='CHAPTER2.LINEARALGEBRAarguments.c∗= argminc||−||xg()c22.(2.55)Thefunctionbeingminimizedsimpliﬁesto(())x−gc\\ue03e(())x−gc(2.56)(bythedeﬁnitionoftheL2norm,Eq.)2.30= x\\ue03exx−\\ue03egg()c−()c\\ue03exc+(g)\\ue03eg()c(2.57)(bythedistributiveproperty)= x\\ue03exx−2\\ue03egg()+c()c\\ue03eg()c(2.58)(becausethescalarg()x\\ue03exisequaltothetransposeofitself).Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,sincethistermdoesnotdependon:cc∗= argminc−2x\\ue03egg()+c()c\\ue03eg.()c(2.59)Tomakefurtherprogress,wemustsubstituteinthedeﬁnitionof:g()cc∗= argminc−2x\\ue03eDcc+\\ue03eD\\ue03eDc(2.60)= argminc−2x\\ue03eDcc+\\ue03eIlc(2.61)(bytheorthogonalityandunitnormconstraintson)D= argminc−2x\\ue03eDcc+\\ue03ec(2.62)Wecansolvethisoptimizationproblemusingvectorcalculus(seeSec.if4.3youdonotknowhowtodothis):∇c(2−x\\ue03eDcc+\\ue03ec) = 0(2.63)−2D\\ue03exc+2= 0(2.64)cD= \\ue03ex.(2.65)Thismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusingamatrix-vectoroperation.Toencodeavector,weapplytheencoderfunctionf() = xD\\ue03ex.(2.66)49'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 64}, page_content='CHAPTER2.LINEARALGEBRAUsingafurthermatrixmultiplication,wecanalsodeﬁnethePCAreconstructionoperation:rgf() = x(()) = xDD\\ue03ex.(2.67)Next,weneedtochoosetheencodingmatrixD.Todoso,werevisittheideaofminimizingtheL2distancebetweeninputsandreconstructions.However,sincewewillusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthepointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrixoferrorscomputedoveralldimensionsandallpoints:D∗= argminD\\ue073\\ue058i,j\\ue010x()ij−r(x()i)j\\ue0112subjecttoD\\ue03eDI= l(2.68)ToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecasewherel= 1.Inthiscase,Disjustasinglevector,d.SubstitutingEq.into2.67Eq.andsimplifyinginto,theproblemreducesto2.68Ddd∗= argmind\\ue058i||x()i−dd\\ue03ex()i||22subjectto||||d2= 1.(2.69)Theaboveformulationisthemostdirectwayofperformingthesubstitution,butisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthescalarvalued\\ue03ex()iontherightofthevectord.Itismoreconventionaltowritescalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywritesuchaformulaasd∗= argmind\\ue058i||x()i−d\\ue03ex()id||22subjectto||||d2= 1,(2.70)or,exploitingthefactthatascalarisitsowntranspose,asd∗= argmind\\ue058i||x()i−x()i\\ue03edd||22subjectto||||d2= 1.(2.71)Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements.Atthispoint,itcanbehelpfultorewritetheproblemintermsofasingledesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.Thiswillallowustousemorecompactnotation.LetX∈Rmn×bethematrixdeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatXi,:=x()i\\ue03e.Wecannowrewritetheproblemasd∗= argmind||−XXdd\\ue03e||2Fsubjecttod\\ue03ed= 1.(2.72)50'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 65}, page_content='CHAPTER2.LINEARALGEBRADisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnormportionasfollows:argmind||−XXdd\\ue03e||2F(2.73)= argmindTr\\ue012\\ue010XXdd−\\ue03e\\ue011\\ue03e\\ue010XXdd−\\ue03e\\ue011\\ue013(2.74)(byEq.)2.49= argmindTr(X\\ue03eXX−\\ue03eXdd\\ue03e−dd\\ue03eX\\ue03eXdd+\\ue03eX\\ue03eXdd\\ue03e)(2.75)= argmindTr(X\\ue03eX)Tr(−X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.76)= argmind−Tr(X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.77)(becausetermsnotinvolvingdonotaﬀectthe)dargmin= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.78)(becausewecancycletheorderofthematricesinsideatrace,Eq.)2.52= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)(2.79)(usingthesamepropertyagain)Atthispoint,were-introducetheconstraint:argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)subjecttod\\ue03ed= 1(2.80)= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.81)(duetotheconstraint)= argmind−Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.82)= argmaxdTr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.83)= argmaxdTr(d\\ue03eX\\ue03eXdd)subjectto\\ue03ed= 1(2.84)51'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 66}, page_content='CHAPTER2.LINEARALGEBRAThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,theoptimaldisgivenbytheeigenvectorofX\\ue03eXcorrespondingtothelargesteigenvalue.Inthegeneralcase,wherel>1,thematrixDisgivenbytheleigenvectorscorrespondingtothelargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommendwritingthisproofasanexercise.Linearalgebraisoneofthefundamentalmathematicaldisciplinesthatisnecessarytounderstanddeeplearning.Anotherkeyareaofmathematicsthatisubiquitousinmachinelearningisprobabilitytheory,presentednext.\\n52'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 67}, page_content='Chapter3ProbabilityandInformationTheoryInthischapter,wedescribeprobabilitytheoryandinformationtheory.Probabilitytheoryisamathematicalframeworkforrepresentinguncertainstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderivingnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobabilitytheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystemsshouldreason,sowedesignouralgorithmstocomputeorapproximatevariousexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityandstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceandengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundisprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycanunderstandthematerialinthisbook.Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasoninthepresenceofuncertainty,informationallowsustoquantifytheamountofuncertaintyinaprobabilitydistribution.Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,youmaywishtoskipallofthischapterexceptforSec.,whichdescribesthe3.14graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.Ifyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershouldbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedosuggestthatyouconsultanadditionalresource,suchasJaynes2003().53'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 68}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.1WhyProbability?Manybranchesofcomputersciencedealmostlywithentitiesthatareentirelydeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwillexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butarerareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccountforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkinarelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearningmakesheavyuseofprobabilitytheory.Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities,andsometimesmayalsoneedtodealwithstochastic(non-deterministic)quantities.Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemadecompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleastthe1980s.ManyoftheargumentspresentedherearesummarizedfromorinspiredbyPearl1988().Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.Infact,beyondmathematicalstatementsthataretruebydeﬁnition,itisdiﬃculttothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutelyguaranteedtooccur.Therearethreepossiblesourcesofuncertainty:1.Inherentstochasticityinthesystembeingmodeled.Forexample,mostinterpretationsofquantummechanicsdescribethedynamicsofsubatomicparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthatwepostulatetohaverandomdynamics,suchasahypotheticalcardgamewhereweassumethatthecardsaretrulyshuﬄedintoarandomorder.2.Incompleteobservability.Evendeterministicsystemscanappearstochasticwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthesystem.Forexample,intheMontyHallproblem,agameshowcontestantisaskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosendoor.Twodoorsleadtoagoatwhileathirdleadstoacar. Theoutcomegiventhecontestant’schoiceisdeterministic,butfromthecontestant’spointofview,theoutcomeisuncertain.3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeofthe informationwehave observed, the discarded informationresults inuncertaintyinthemodel’spredictions.Forexample,supposewebuildarobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe54'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 69}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,thenthediscretizationmakestherobotimmediatelybecomeuncertainabouttheprecisepositionofobjects: eachobjectcouldbeanywherewithinthediscretecellthatitwasobservedtooccupy.Inmanycases,itismorepracticaltouseasimplebutuncertainruleratherthanacomplexbutcertainone,evenifthetrueruleisdeterministicandourmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,thesimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearuleoftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedtoﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirdsincludingthecassowary,ostrichandkiwi...” isexpensivetodevelop,maintainandcommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.Giventhatweneedameansofrepresentingandreasoningaboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovideallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheorywasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytoseehowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandofcardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.Whenwesaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeatedtheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportionpoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnotseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctoranalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasofthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatientwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.Inthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresentadegreeofbelief,with1indicatingabsolutecertaintythatthepatienthastheﬂuand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu. Theformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,isknownasfrequentistprobability,whilethelatter,relatedtoqualitativelevelsofcertainty,isknownasBayesianprobability.Ifwelistseveralpropertiesthatweexpectcommonsensereasoningaboutuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreatBayesianprobabilitiesasbehavingexactlythesameasfrequentistprobabilities.Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapokergamegiventhatshehasacertainsetofcards,weuseexactlythesameformulasaswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe55'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 70}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsenseassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,see().Ramsey1926Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logicprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedtobetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrueorfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthelikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.3.2RandomVariablesArandomvariableisavariablethatcantakeondiﬀerentvaluesrandomly. Wetypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,andthevaluesitcantakeonwithlowercasescriptletters.Forexample,x1andx2arebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valuedvariables,wewouldwritetherandomvariableasxandoneofitsvaluesasx.Onitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;itmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachofthesestatesare.Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariableisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthesestatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthatarenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableisassociatedwitharealvalue.3.3ProbabilityDistributionsAprobabilitydistributionis adescription ofhowlikelyarandomvariable orsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywedescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteorcontinuous.3.3.1DiscreteVariablesandProbabilityMassFunctionsAprobabilitydistributionoverdiscretevariablesmaybedescribedusingaproba-bilitymassfunction(PMF).WetypicallydenoteprobabilitymassfunctionswithacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability56'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 71}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmassfunctionandthereadermustinferwhichprobabilitymassfunctiontousebasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;PP()xisusuallynotthesameas()y.Theprobabilitymassfunctionmapsfromastateofarandomvariabletotheprobabilityofthatrandomvariabletakingonthatstate.Theprobabilitythatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xiscertainandaprobabilityof0indicatingthatx=xisimpossible.SometimestodisambiguatewhichPMFtouse,wewritethenameoftherandomvariableexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationtospecifywhichdistributionitfollowslater:xx.∼P()Probabilitymassfunctionscanactonmanyvariablesatthesametime.Suchaprobabilitydistributionovermanyvariablesisknownasajointprobabilitydistribution.P(x=x,y=y)denotestheprobabilitythatx=xandy=ysimultaneously.Wemayalsowriteforbrevity.Px,y()Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.P•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan0 belessprobablethanthat.Likewise,aneventthatisguaranteedtohappenhasprobability,andnostatecanhaveagreaterchanceofoccurring.1•\\ue050x∈xP(x) = 1.Werefertothispropertyasbeingnormalized.Withoutthisproperty,wecouldobtainprobabilitiesgreaterthanonebycomputingtheprobabilityofoneofmanyeventsoccurring.Forexample,considerasinglediscreterandomvariablexwithkdiﬀerentstates.Wecanplaceaonuniformdistributionx—thatis,makeeachofitsstatesequallylikely—bysettingitsprobabilitymassfunctiontoPx(= xi) =1k(3.1)foralli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.Thevalue1kispositivebecauseisapositiveinteger.Wealsoseethatk\\ue058iPx(= xi) =\\ue058i1k=kk= 1,(3.2)sothedistributionisproperlynormalized.57'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 72}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.3.2ContinuousVariablesandProbabilityDensityFunctionsWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydis-tributionsusingaprobabilitydensityfunction(PDF)ratherthanaprobabilitymassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.p•∀∈≥≤xx,px() 0() .pNotethatwedonotrequirex1.•\\ue052pxdx()= 1.Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁcstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwithvolumeisgivenby.δxpxδx()Wecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofasetofpoints.Speciﬁcally,theprobabilitythatxliesinsomesetSisgivenbytheintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatxliesintheintervalisgivenby[]a,b\\ue052[]a,bpxdx().Foranexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁcprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-tiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),whereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans“parametrizedby”;weconsiderxtobetheargumentofthefunction,whileaandbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobabilitymassoutsidetheinterval,wesayu(x;a,b)=0forallx\\ue036∈[a,b][.Withina,b],uxa,b(;) =1ba−.Wecanseethatthisisnonnegativeeverywhere.Additionally,itintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]bywritingx.∼Ua,b()3.4MarginalProbabilitySometimesweknowtheprobabilitydistributionoverasetofvariablesandwewanttoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobabilitydistributionoverthesubsetisknownasthemarginalprobabilitydistribution.Forexample,supposewehavediscreterandomvariablesxandy,andweknowP,(xy.Wecanﬁndxwiththe:)P()sumrule∀∈xxx,P(= ) =x\\ue058yPx,y.(= xy= )(3.3)58'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 73}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThename“marginalprobability”comesfromtheprocessofcomputingmarginalprobabilitiesonpaper.WhenthevaluesofP(xy,)arewritteninagridwithdiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturaltosumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjusttotherightoftherow.Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:px() =\\ue05apx,ydy.()(3.4)3.5ConditionalProbabilityInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsomeothereventhashappened.Thisiscalledaconditionalprobability.Wedenotetheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).ThisconditionalprobabilitycanbecomputedwiththeformulaPyx(= y|x= ) =Py,x(= yx= )Px(= x).(3.5)TheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcomputetheconditionalprobabilityconditionedonaneventthatneverhappens.Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhatwouldhappenifsomeactionwereundertaken.TheconditionalprobabilitythatapersonisfromGermanygiventhattheyspeakGermanisquitehigh,butifarandomlyselectedpersonistaughttospeakGerman,theircountryoforigindoesnotchange.Computingtheconsequencesofanactioniscalledmakinganinterventionquery.Interventionqueriesarethedomainofcausalmodeling,whichwedonotexploreinthisbook.3.6TheChainRuleofConditionalProbabilitiesAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposedintoconditionaldistributionsoveronlyonevariable:P(x(1),...,x()n) = (Px(1))Πni=2P(x()i|x(1),...,x(1)i−).(3.6)Thisobservationisknownastheorchainruleproductruleofprobability.ItfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinEq..For3.559'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 74}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYexample,applyingthedeﬁnitiontwice,wegetP,,P,P,(abc)=(ab|c)(bc)P,PP(bc)=()bc|()cP,,P,PP.(abc)=(ab|c)()bc|()c3.7IndependenceandConditionalIndependenceTworandomvariablesxandyareindependentiftheirprobabilitydistributioncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolvingonlyy:∀∈∈xx,yyxyxy(3.7),p(= x,= ) = (yp= )(xp= )y.Tworandomvariablesxandyareconditionallyindependentgivenarandomvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthiswayforeveryvalueofz:∀∈∈∈|||xx,yy,zzxy,p(= x,= yzx= ) = (zp= xzy= )(zp= yz= )z.(3.8)We candenoteindependence andconditionalindependence withcompactnotation:xy⊥meansthatxandyareindependent,whilexyz⊥|meansthatxandyareconditionallyindependentgivenz.3.8Expectation,VarianceandCovarianceTheexpectationexpectedvalueorofsomefunctionf(x) withrespecttoaprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenxisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation:PEx∼P[()] =fx\\ue058xPxfx,()()(3.9)whileforcontinuousvariables,itiscomputedwithanintegral:Ex∼p[()] =fx\\ue05apxfxdx.()()(3.10)60'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 75}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYWhentheidentityofthedistributionisclearfromthecontext,wemaysimplywritethenameoftherandomvariablethattheexpectationisover,asinEx[f(x)].Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthesubscriptentirely,asinE[f(x)].Bydefault,wecanassumethatE[·]averagesoverthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereisnoambiguity,wemayomitthesquarebrackets.Expectationsarelinear,forexample,Ex[()+()] = αfxβgxαEx[()]+fxβEx[()]gx,(3.11)whenandarenotdependenton.αβxThevariancegivesameasureofhowmuchthevaluesofafunctionofarandomvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:Var(()) = fxE\\ue068(()[()])fx−Efx2\\ue069.(3.12)Whenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.Thesquarerootofthevarianceisknownasthestandarddeviation.Thecovariancegivessomesenseofhowmuchtwovaluesarelinearlyrelatedtoeachother,aswellasthescaleofthesevariables:Cov(()()) = [(()[()])(()[()])]fx,gyEfx−Efxgy−Egy.(3.13)Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuchandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthecovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvaluessimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendstotakeonarelativelyhighvalueatthetimesthattheothertakesonarelativelylowvalueandviceversa.Othermeasuressuchascorrelationnormalizethecontributionofeachvariableinordertomeasureonlyhowmuchthevariablesarerelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.Thenotionsofcovarianceanddependencearerelated,butareinfactdistinctconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezerocovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-ever,independenceisadistinctpropertyfromcovariance.Fortwovariablestohavezerocovariance,theremustbenolineardependencebetweenthem.Independenceisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludesnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthavezerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfromauniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable61'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 76}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYs.Withprobability12,wechoosethevalueofstobe.Otherwise,wechoose1thevalueofstobe−1.Wecanthengeneratearandomvariableybyassigningy=sx.Clearly,xandyarenotindependent,becausexcompletelydeterminesthemagnitudeof.However,yCov() = 0x,y.Thecovariancematrixofarandomvectorx∈Rnisannn×matrix,suchthatCov()xi,j= Cov(xi,xj).(3.14)Thediagonalelementsofthecovariancegivethevariance:Cov(xi,xi) = Var(xi).(3.15)3.9CommonProbabilityDistributionsSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachinelearning.3.9.1BernoulliDistributionThedistributionisadistributionoverasinglebinaryrandomvariable.BernoulliItiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityoftherandomvariablebeingequalto1.Ithasthefollowingproperties:Pφ(= 1) = x(3.16)Pφ(= 0) = 1x−(3.17)Pxφ(= x) = x(1)−φ1−x(3.18)Ex[] = xφ(3.19)Varx() = (1)xφ−φ(3.20)3.9.2MultinoulliDistributionTheormultinoullicategoricaldistributionisadistributionoverasinglediscretevariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedbyMurphy2012().Themultinoullidistributionisaspecialcaseofthedistribution.Amultinomialmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmanytimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifyingthattheyreferonlytothecase.n= 162'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 77}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYparametrizedbyavectorp∈[0,1]k−1,wherepigivestheprobabilityofthei-thstate.Theﬁnal,k-thstate’sprobabilityisgivenby1−1\\ue03ep.Notethatwemustconstrain1\\ue03ep≤1.Multinoullidistributionsareoftenusedtorefertodistributionsovercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumericalvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectationorvarianceofmultinoulli-distributedrandomvariables.TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-butionovertheirdomain.Thisisbecausetheymodeldiscretevariablesforwhichitisfeasibletosimplyenumerateallofthestates.Whendealingwithcontinuousvariables,thereareuncountablymanystates,soanydistributiondescribedbyasmallnumberofparametersmustimposestrictlimitsonthedistribution.3.9.3GaussianDistributionThemostcommonlyuseddistributionoverrealnumbersisthe,normaldistributionalsoknownasthe:GaussiandistributionN(;xµ,σ2) =\\ue07212πσ2exp\\ue012−12σ2()xµ−2\\ue013.(3.21)SeeFig.foraplotofthedensityfunction.3.1Thetwoparametersµ∈Randσ∈(0,∞)controlthenormaldistribution.Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanofthedistribution:E[x] =µ.Thestandarddeviationofthedistributionisgivenbyσ,andthevariancebyσ2.WhenweevaluatethePDF,weneedtosquareandinvertσ.WhenweneedtofrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientwayofparametrizingthedistributionistouseaparameterβ∈(0,∞)tocontroltheprecisionorinversevarianceofthedistribution:N(;xµ,β−1) =\\ue072β2πexp\\ue012−12βxµ(−)2\\ue013.(3.22)Normaldistributionsareasensiblechoiceformanyapplications.Intheabsenceofpriorknowledgeaboutwhatformadistributionovertherealnumbersshouldtake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.First,manydistributionswewishtomodelaretrulyclosetobeingnormaldistributions.Thecentrallimittheoremshowsthatthesumofmanyindependentrandomvariablesisapproximatelynormallydistributed.Thismeansthatin63'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 78}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue030\\ue02e\\ue030\\ue035\\ue030\\ue02e\\ue031\\ue030\\ue030\\ue02e\\ue031\\ue035\\ue030\\ue02e\\ue032\\ue030\\ue030\\ue02e\\ue032\\ue035\\ue030\\ue02e\\ue033\\ue030\\ue030\\ue02e\\ue033\\ue035\\ue030\\ue02e\\ue034\\ue030\\ue070\\ue028\\ue078\\ue029\\ue04d\\ue061\\ue078\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue061\\ue074\\ue020\\ue078\\ue0b9\\ue03d\\ue049\\ue06e\\ue066\\ue06c\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue073\\ue020\\ue061\\ue074\\ue020\\ue020\\ue020\\ue020\\ue020\\ue020\\ue078\\ue0b9\\ue0be\\ue03d\\ue0a7\\ue054\\ue068\\ue065\\ue020\\ue06e\\ue06f\\ue072\\ue06d\\ue061\\ue06c\\ue020\\ue064\\ue069\\ue073\\ue074\\ue072\\ue069\\ue062\\ue075\\ue074\\ue069\\ue06f\\ue06e\\nFigure3.1::ThenormaldistributionThenormaldistributionN(x;µ,σ2) exhibitsaclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,andthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormaldistribution,withand.µ= 0σ= 1practice, manycomplicatedsystemscanbemodeledsuccessfullyasnormallydistributednoise,evenifthesystemcanbedecomposedintopartswithmorestructuredbehavior.Second,outofallpossibleprobabilitydistributionswiththesamevariance,thenormaldistributionencodesthemaximumamountofuncertaintyovertherealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheonethatinsertstheleastamountofpriorknowledgeintoamodel. Fullydevelopingandjustifyingthisidearequiresmoremathematicaltools,andispostponedtoSec.19.4.2.ThenormaldistributiongeneralizestoRn,inwhichcaseitisknownasthemultivariatenormaldistribution.Itmaybeparametrizedwithapositivedeﬁnitesymmetricmatrix:ΣN(;) =xµ,Σ\\ue0731(2)πndet()Σexp\\ue012−12()xµ−\\ue03eΣ−1()xµ−\\ue013.(3.23)Theparameterµstillgivesthemeanofthedistribution,thoughnowitisvector-valued.TheparameterΣgivesthecovariancematrixofthedistribution.Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor64'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 79}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationallyeﬃcientwaytoparametrizethedistribution,sinceweneedtoinvertΣtoevaluatethePDF.Wecaninsteaduseaprecisionmatrixβ:N(;xµβ,−1) =\\ue073det()β(2)πnexp\\ue012−12()xµ−\\ue03eβxµ(−)\\ue013.(3.24)Weoftenﬁxthecovariancematrixtobeadiagonalmatrix.AnevensimplerversionistheisotropicGaussiandistribution,whosecovariancematrixisascalartimestheidentitymatrix.3.9.4ExponentialandLaplaceDistributionsInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistributionwithasharppointatx=0.Toaccomplishthis,wecanusetheexponentialdistribution:pxλλ(;) = 1x≥0exp()−λx.(3.25)Theexponentialdistributionusestheindicatorfunction1x≥0toassignprobabilityzerotoallnegativevaluesof.xAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeakofprobabilitymassatanarbitrarypointistheµLaplacedistributionLaplace(;) =xµ,γ12γexp\\ue012−|−|xµγ\\ue013.(3.26)3.9.5TheDiracDistributionandEmpiricalDistributionInsomecases,wewishtospecifythatallofthemassinaprobabilitydistributionclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusingtheDiracdeltafunction,:δx()pxδxµ.() = (−)(3.27)TheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthatassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindofmathematicalobjectcalledageneralizedfunctionthatisdeﬁnedintermsofitspropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthelimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsotherthan.µ65'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 80}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYBydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowandinﬁnitelyhighpeakofprobabilitymasswhere.xµ= AcommonuseoftheDiracdeltadistributionisasacomponentofanempiricaldistribution,ˆp() =x1mm\\ue058i=1δ(xx−()i)(3.28)whichputsprobabilitymass1moneachofthempointsx(1),...,x()mformingagivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessarytodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,thesituationissimpler:anempiricaldistributioncanbeconceptualizedasamultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvaluethatissimplyequaltotheempiricalfrequencyofthatvalueinthetrainingset.Wecanviewtheempiricaldistributionformedfromadatasetoftrainingexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodelonthisdataset. Anotherimportantperspectiveontheempiricaldistributionisthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata(seeSec.).5.53.9.6MixturesofDistributionsItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimplerprobabilitydistributions.Onecommon wayof combining distributionsis toconstructamixturedistribution.Amixturedistributionismadeupofseveralcomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistributiongeneratesthesampleisdeterminedbysamplingacomponentidentityfromamultinoullidistribution:P() =x\\ue058iPiPi(= c)(= xc|)(3.29)wherecisthemultinoullidistributionovercomponentidentities.P()Wehavealreadyseenoneexampleofamixturedistribution:theempiricaldistributionoverreal-valuedvariablesisamixturedistributionwithoneDiraccomponentforeachtrainingexample.Themixturemodelisonesimplestrategyforcombiningprobabilitydistributionstocreatearicherdistribution.InChapter,weexploretheartofbuildingcomplex16probabilitydistributionsfromsimpleonesinmoredetail.66'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 81}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThemixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeofparamountimportancelater—the.Alatentvariableisarandomlatentvariablevariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthemixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthroughthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)overthelatentvariableandthedistributionP(xc|)relatingthelatentvariablestothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhoughitispossibletodescribeP(x)withoutreferencetothelatentvariable.LatentvariablesarediscussedfurtherinSec..16.5AverypowerfulandcommontypeofmixturemodelistheGaussianmixturemodel,inwhichthecomponentsp(x|c=i)areGaussians.Eachcomponenthasaseparatelyparametrizedmeanµ()iandcovarianceΣ()i.Somemixturescanhavemoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponentsviatheconstraintΣ()i=Σ∀i.AswithasingleGaussiandistribution,themixtureofGaussiansmightconstrainthecovariancematrixforeachcomponenttobediagonalorisotropic.Inadditiontothemeansandcovariances,theparametersofaGaussianmixturespecifythepriorprobabilityαi=P(c=i)giventoeachcomponenti.Theword“prior”indicatesthatitexpressesthemodel’sbeliefsaboutcbeforeithasobservedx.Bycomparison,P(c|x)isaposteriorprobability,becauseitiscomputedafterobservation ofx.AGaussian mixturemodel isa universalapproximatorofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithanyspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenoughcomponents.Fig.showssamplesfromaGaussianmixturemodel.3.23.10UsefulPropertiesofCommonFunctionsCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especiallytheprobabilitydistributionsusedindeeplearningmodels.Oneofthesefunctionsisthelogisticsigmoid:σx() =11+exp()−x.(3.30)ThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoullidistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvaluesfortheφparameter.SeeFig.foragraphofthesigmoidfunction.Thesigmoid3.367'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 82}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nx1x2\\nFigure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethreecomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonalcovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligneddirection.Thisexamplehasmorevariancealongthex2axisthanalongthex1axis.Thethirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevarianceseparatelyalonganarbitrarybasisofdirections.functionsaturateswhenitsargumentisverypositiveorverynegative,meaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinitsinput.Anothercommonlyencounteredfunctionisthefunction(,softplusDugasetal.2001):ζxx.() = log(1+exp())(3.31)Thesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormaldistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulatingexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthefactthatitisasmoothedor“softened”versionofx+= max(0),x.(3.32)SeeFig.foragraphofthesoftplusfunction.3.4Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorizethem:σx() =exp()xexp()+exp(0)x(3.33)ddxσxσxσx() = ()(1−())(3.34)68'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 83}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\U000f0913\\ue031\\ue030\\U000f0913\\ue035\\ue030\\ue035\\ue031\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue038\\ue031\\ue02e\\ue030\\ue0be\\ue078\\ue028\\ue029\\ue054\\ue068\\ue065\\ue020\\ue06c\\ue06f\\ue067\\ue069\\ue073\\ue074\\ue069\\ue063\\ue020\\ue073\\ue069\\ue067\\ue06d\\ue06f\\ue069\\ue064\\ue020\\ue066\\ue075\\ue06e\\ue063\\ue074\\ue069\\ue06f\\ue06e\\nFigure3.3:Thelogisticsigmoidfunction.\\n\\U000f0913\\ue031\\ue030\\U000f0913\\ue035\\ue030\\ue035\\ue031\\ue030\\ue030\\ue032\\ue034\\ue036\\ue038\\ue031\\ue030\\ue0b3\\ue078\\ue028\\ue029\\ue054\\ue068\\ue065\\ue020\\ue073\\ue06f\\ue066\\ue074\\ue070\\ue06c\\ue075\\ue073\\ue020\\ue066\\ue075\\ue06e\\ue063\\ue074\\ue069\\ue06f\\ue06e\\nFigure3.4:Thesoftplusfunction.69'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 84}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY1() = ()−σxσ−x(3.35)log() = ()σx−ζ−x(3.36)ddxζxσx() = ()(3.37)∀∈x(01),,σ−1() = logx\\ue012x1−x\\ue013(3.38)∀x>,ζ0−1() = log(exp()1)xx−(3.39)ζx() =\\ue05ax−∞σydy()(3.40)ζxζxx()−(−) = (3.41)Thefunctionσ−1(x)iscalledthelogitinstatistics,butthistermismorerarelyusedinmachinelearning.Eq.providesextrajustiﬁcationforthename“softplus.”Thesoftplus3.41functionisintendedasasmoothedversionofthepositivepartfunction,x+=max{0,x}.Thepositivepartfunctionisthecounterpartofthenegativepartfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothenegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepartandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverxusingthesamerelationshipbetweenand,asshowninEq..ζx()ζx(−)3.413.11Bayes’RuleWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknowP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantityusing:Bayes’ruleP() =xy|PP()x()yx|P()y.(3.42)NotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocomputeP() =y\\ue050xPxPxP(y|)(),sowedonotneedtobeginwithknowledgeof()y.Bayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditionalprobability,butitisusefultoknowthenameofthisformulasincemanytextsrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrstdiscoveredaspecialcaseoftheformula.ThegeneralversionpresentedherewasindependentlydiscoveredbyPierre-SimonLaplace.70'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 85}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.12TechnicalDetailsofContinuousVariablesAproperformalunderstandingofcontinuousrandomvariablesandprobabilitydensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchofmathematicsknownasmeasuretheory.Measuretheoryisbeyondthescopeofthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryisemployedtoresolve.InSec.,wesawthattheprobabilityofacontinuousvector-valued3.3.2xlyinginsomesetSisgivenbytheintegralofp(x)overthesetS.SomechoicesofsetScanproduceparadoxes.Forexample,itispossibletoconstructtwosetsS1andS2suchthatp(x∈S1)+p(x∈S2)>1butS1∩S2=∅.Thesesetsaregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofrealnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedbytransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasuretheoryistoprovideacharacterizationofthesetofsetsthatwecancomputetheprobabilityofwithoutencounteringparadoxes.Inthisbook,weonlyintegrateoversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheoryneverbecomesarelevantconcern.Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthatapplytomostpointsinRnbutdonotapplytosomecornercases.Measuretheoryprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Suchasetissaidtohave“measurezero.” Wedonotformallydeﬁnethisconceptinthistextbook.However,itisusefultounderstandtheintuitionthatasetofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,withinR2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysetsthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherationalnumbershasmeasurezero,forinstance).Anotherusefultermfrommeasuretheoryis“almosteverywhere.” Apropertythatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetofmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,theycanbesafelyignoredformanyapplications.Someimportantresultsinprobabilitytheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuousvalues.Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuousrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehavetworandomvariables,xandy,suchthaty=g(x),wheregisaninvertible,con-2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.71'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 86}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYtinuous,diﬀerentiabletransformation.Onemightexpectthatpy(y) =px(g−1(y)).Thisisactuallynotthecase.Asasimpleexample,supposewehavescalarrandomvariablesxandy.Supposey=x2andx∼U(0,1).Ifweusetherulepy(y)=px(2y)thenpywillbe0everywhereexcepttheinterval[0,12]1,anditwillbeonthisinterval.Thismeans\\ue05apy()=ydy12,(3.43)whichviolatesthedeﬁnitionofaprobabilitydistribution.Thiscommonmistakeiswrongbecauseitfailstoaccountforthedistortionofspaceintroducedbythefunctiong. Recallthattheprobabilityofxlyinginaninﬁnitesimallysmallregionwithvolumeδxisgivenbyp(x)δx. Sincegcanexpandorcontractspace,theinﬁnitesimalvolumesurroundingxinxspacemayhavediﬀerentvolumeinspace.yToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedtopreservetheproperty|py(())= gxdy||px()xdx.|(3.44)Solvingfromthis,weobtainpy() = ypx(g−1())y\\ue00c\\ue00c\\ue00c\\ue00c∂x∂y\\ue00c\\ue00c\\ue00c\\ue00c(3.45)orequivalentlypx() = xpy(())gx\\ue00c\\ue00c\\ue00c\\ue00c∂gx()∂x\\ue00c\\ue00c\\ue00c\\ue00c.(3.46)Inhigherdimensions,thederivativegeneralizestothedeterminantoftheJacobianmatrix—thematrixwithJi,j=∂xi∂yj.Thus,forreal-valuedvectorsand,xypx() = xpy(())gx\\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g()x∂x\\ue013\\ue00c\\ue00c\\ue00c\\ue00c.(3.47)3.13InformationTheoryInformationtheory isa branchof appliedmathematics thatrevolvesaroundquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinventedtostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchascommunicationviaradiotransmission.Inthiscontext,informationtheorytellshowtodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom72'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 87}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextofmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariableswheresomeofthesemessagelengthinterpretationsdonotapply.Thisﬁeldisfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthistextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterizeprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.Formoredetailoninformationtheory,seeCoverandThomas2006MacKay()or().2003Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikelyeventhas occurredismoreinformativethanlearningthata likely eventhasoccurred.Amessagesaying“thesunrosethismorning”issouninformativeastobeunnecessarytosend,butamessagesaying“therewasasolareclipsethismorning”isveryinformative.Wewouldliketoquantifyinformationinawaythatformalizesthisintuition.Speciﬁcally,•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,eventsthatareguaranteedtohappenshouldhavenoinformationcontentwhatsoever.•Lesslikelyeventsshouldhavehigherinformationcontent.•Independenteventsshouldhaveadditiveinformation.Forexample,ﬁndingoutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceasmuchinformationasﬁndingoutthatatossedcoinhascomeupasheadsonce.Inordertosatisfyallthreeoftheseproperties,wedeﬁnetheself-informationofaneventxtobe= xIxPx.() = log−()(3.48)Inthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.OurdeﬁnitionofI(x)isthereforewritteninunitsof.Onenatistheamountofnatsinformationgainedbyobservinganeventofprobability1e.Othertextsusebase-2logarithmsandunitscalledor;informationmeasuredinbitsisjustbitsshannonsarescalingofinformationmeasuredinnats.Whenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,butsomeofthepropertiesfromthediscretecasearelost.Forexample,aneventwithunitdensitystillhaszeroinformation,despitenotbeinganeventthatisguaranteedtooccur.73'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 88}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue038\\ue031\\ue02e\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue031\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue033\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue037\\ue053\\ue068\\ue061\\ue06e\\ue06e\\ue06f\\ue06e\\ue020\\ue065\\ue06e\\ue074\\ue072\\ue06f\\ue070\\ue079\\ue020\\ue069\\ue06e\\ue020\\ue06e\\ue061\\ue074\\ue073\\ue053\\ue068\\ue061\\ue06e\\ue06e\\ue06f\\ue06e\\ue020\\ue065\\ue06e\\ue074\\ue072\\ue06f\\ue070\\ue079\\ue020\\ue06f\\ue066\\ue020\\ue061\\ue020\\ue062\\ue069\\ue06e\\ue061\\ue072\\ue079\\ue020\\ue072\\ue061\\ue06e\\ue064\\ue06f\\ue06d\\ue020\\ue076\\ue061\\ue072\\ue069\\ue061\\ue062\\ue06c\\ue065\\nFigure3.5:ThisplotshowshowdistributionsthatareclosertodeterministichavelowShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequalto.Theentropyisgivenby1(p−1)log(1−p)−pplog.Whenpisnear0,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwooutcomes.Self-informationdealsonlywithasingleoutcome.WecanquantifytheamountofuncertaintyinanentireprobabilitydistributionusingtheShannonentropy:H() = xEx∼P[()] = Ix−Ex∼P[log()]Px.(3.49)alsodenotedH(P).Inotherwords,theShannonentropyofadistributionistheexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgivesalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunitsarediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)havelowentropy;distributionsthatareclosertouniformhavehighentropy.SeeFig.forademonstration.When3.5xiscontinuous,theShannonentropyisknownasthediﬀerentialentropy.IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesamerandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusingtheKullback-Leibler(KL)divergence:DKL() = PQ\\ue06bEx∼P\\ue014logPx()Qx()\\ue015= Ex∼P[log()log()]Px−Qx.(3.50)74'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 89}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYInthecaseofdiscretevariables,itistheextraamountofinformation(measuredinbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats2andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawnfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimizethelengthofmessagesdrawnfromprobabilitydistribution.QTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributioninthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuousvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerencebetweentwodistributions,itisoftenconceptualizedasmeasuringsomesortofdistancebetweenthesedistributions.However,itisnotatruedistancemeasurebecauseitisnotsymmetric:DKL(PQ\\ue06b)\\ue036=DKL(QP\\ue06b)forsomePandQ. ThisasymmetrymeansthatthereareimportantconsequencestothechoiceofwhethertouseDKL()PQ\\ue06borDKL()QP\\ue06b.SeeFig.formoredetail.3.6AquantitythatiscloselyrelatedtotheKLdivergenceisthecross-entropyH(P,Q) =H(P)+DKL(PQ\\ue06b),whichissimilartotheKLdivergencebutlackingthetermontheleft:HP,Q() = −Ex∼Plog()Qx.(3.51)Minimizingthecross-entropywithrespecttoQisequivalenttominimizingtheKLdivergence,becausedoesnotparticipateintheomittedterm.QWhencomputingmanyofthesequantities,itiscommontoencounterexpres-sionsoftheform0log0.Byconvention,inthecontextofinformationtheory,wetreattheseexpressionsaslimx→0xxlog= 0.3.14StructuredProbabilisticModelsMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveraverylargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolvedirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctiontodescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(bothcomputationallyandstatistically).Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,wecansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.Forexample,supposewehavethreerandomvariables:a,bandc.Supposethatainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcareindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree75'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 90}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nxProbability Densityq∗= argminqDKL()pq\\ue06bpx()q∗()x\\nxProbability Densityq∗= argminqDKL()qp\\ue06bp()xq∗()x\\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)andwishtoapproximateitwithanotherdistributionq(x).WehavethechoiceofminimizingeitherDKL(pq\\ue06b)orDKL(qp\\ue06b).WeillustratetheeﬀectofthischoiceusingamixtureoftwoGaussiansforp,andasingleGaussianforq. ThechoiceofwhichdirectionoftheKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximationthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshighprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshighprobabilityanywherethatthetruedistributionplaceslowprobability.ThechoiceofthedirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeachapplication.(Left)TheeﬀectofminimizingDKL(pq\\ue06b).Inthiscase,weselectaqthathashighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosestoblurthemodestogether,inordertoputhighprobabilitymassonallofthem.The(Right)eﬀectofminimizingDKL(qp\\ue06b).Inthiscase,weselectaqthathaslowprobabilitywherephaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,asinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inordertoavoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,weillustratetheoutcomewhenqischosentoemphasizetheleftmode.WecouldalsohaveachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodesarenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionoftheKLdivergencecanstillchoosetoblurthemodes.76'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 91}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYvariablesasaproductofprobabilitydistributionsovertwovariables:p,,ppp.(abc) = ()a()ba|()cb|(3.52)Thesefactorizationscangreatlyreducethenumberofparametersneededtodescribethedistribution.Eachfactorusesanumberofparametersthatisexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatlyreducethecostofrepresentingadistributionifweareabletoﬁndafactorizationintodistributionsoverfewervariables.Wecandescribethesekindsoffactorizationsusinggraphs. Hereweusetheword“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeachotherwithedges.Whenwerepresentthefactorizationofaprobabilitydistributionwithagraph,wecallitastructuredprobabilisticmodelgraphicalormodel.Therearetwomainkindsofstructuredprobabilisticmodels:directedandundirected.BothkindsofgraphicalmodelsuseagraphGinwhicheachnodeinthegraphcorrespondstoarandomvariable, andanedgeconnectingtworandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirectinteractionsbetweenthosetworandomvariables.Directedmodelsusegraphswithdirectededges,andtheyrepresentfactoriza-tionsintoconditionalprobabilitydistributions,asintheexampleabove.Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablexiinthedistribution,andthatfactorconsistsoftheconditionaldistributionoverxigiventheparentsofxi,denotedPaG(xi):p() =x\\ue059ip(xi|PaG(xi)).(3.53)SeeFig.foranexampleofadirectedgraphandthefactorizationofprobability3.7distributionsitrepresents.Undirectedmodelsusegraphswithundirectededges,andtheyrepresentfac-torizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctionsareusuallynotprobabilitydistributionsofanykind.AnysetofnodesthatareallconnectedtoeachotherinGiscalledaclique.EachcliqueC()iinanundirectedmodelisassociatedwithafactorφ()i(C()i).Thesefactorsarejustfunctions,notprobabilitydistributions.Theoutputofeachfactormustbenon-negative,butthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobabilitydistribution.Theprobabilityofaconﬁgurationofrandomvariablesisproportionaltotheproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare77'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 92}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b\\ne ed dFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,ppp,pp.(abcde) = ()a()ba|(ca|b)()db|()ec|(3.54)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.WethereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegraloverallstatesoftheproductoftheφfunctions,inordertoobtainanormalizedprobabilitydistribution:p() =x1Z\\ue059iφ()i\\ue010C()i\\ue011.(3.55)See Fig.foran exampleof anundirected graph andthe factorizationof3.8 probabilitydistributionsitrepresents.Keep inmind thatthese graphicalrepresentationsof factorizationsare alanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusivefamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotapropertyofaprobabilitydistribution; itisapropertyofa particularofadescriptionprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinbothways.ThroughoutPartandPartofthisbook,wewillusestructuredprobabilisticIIImodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationshipsdiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstandingofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,inPart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreaterIIIdetail.78'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 93}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b\\ne ed dFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,(abcde) =1Zφ(1)()abc,,φ(2)()bd,φ(3)()ce,.(3.56)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.Thischapterhasreviewedthebasicconceptsofprobabilitytheorythataremostrelevanttodeeplearning.Onemoresetoffundamentalmathematicaltoolsremains:numericalmethods.\\n79'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 94}, page_content='Chapter4NumericalComputationMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-tation.Thistypicallyreferstoalgorithmsthatsolvemathematicalproblemsbymethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthananalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-lution.Commonoperationsincludeoptimization(ﬁndingthevalueofanargumentthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.Evenjustevaluatingamathematicalfunctiononadigitalcomputercanbediﬃcultwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedpreciselyusingaﬁniteamountofmemory.4.1OverﬂowandUnderﬂowThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputeristhatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumberofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursomeapproximationerrorwhenwerepresentthenumberinthecomputer.Inmanycases,thisisjustroundingerror.Roundingerrorisproblematic,especiallywhenitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkintheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationofroundingerror.Oneformofroundingerrorthatisparticularlydevastatingis.Under-underﬂowﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctionsbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmallpositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(somesoftware80'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 95}, page_content='CHAPTER4.NUMERICALCOMPUTATIONenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturnaresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(thisisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformanyfurtherarithmeticoperations).Anotherhighlydamagingformofnumericalerroris.Overﬂowoccursoverﬂowwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Furtherarithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowandoverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredicttheprobabilitiesassociatedwithamultinoullidistribution.Thesoftmaxfunctionisdeﬁnedtobesoftmax()xi=exp(xi)\\ue050nj=1exp(xj).(4.1)Considerwhathappenswhenallofthexiareequaltosomeconstantc.Analytically,wecanseethatalloftheoutputsshouldbeequalto1n.Numerically,thismaynotoccurwhenchaslargemagnitude.Ifcisverynegative,thenexp(c)willunderﬂow.Thismeansthedenominatorofthesoftmaxwillbecome0,sotheﬁnalresultisundeﬁned.Whencisverylargeandpositive,exp(c)willoverﬂow,againresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃcultiescanberesolvedbyinsteadevaluatingsoftmax(z)wherez=x−maxixi.Simplealgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallybyaddingorsubtractingascalarfromtheinputvector.Subtractingmaxixiresultsinthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.Likewise,atleastoneterminthedenominatorhasavalueof1,whichrulesoutthepossibilityofunderﬂowinthedenominatorleadingtoadivisionbyzero.Thereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcausetheexpressionasawholetoevaluatetozero.Thismeansthatifweimplementlogsoftmax(x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresulttothelogfunction,wecoulderroneouslyobtain−∞.Instead,wemustimplementaseparatefunctionthatcalculateslogsoftmaxinanumericallystableway.Thelogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilizethefunction.softmaxForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderationsinvolvedinimplementingthevariousalgorithmsdescribedinthisbook.Developersoflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementingdeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-levellibrariesthatprovidestableimplementations.Insomecases,itispossibletoimplementanewalgorithmandhavethenewimplementationautomatically81'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 96}, page_content='CHAPTER4.NUMERICALCOMPUTATIONstabilized.Theano(,;,)isanexampleBergstraetal.2010Bastienetal.2012ofasoftwarepackagethatautomaticallydetectsandstabilizesmanycommonnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.4.2PoorConditioningConditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchangesinitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightlycanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputscanresultinlargechangesintheoutput.Considerthefunctionf(x)=A−1x.WhenA∈Rnn×hasaneigenvaluedecomposition,itsconditionnumberismaxi,j\\ue00c\\ue00c\\ue00c\\ue00cλiλj\\ue00c\\ue00c\\ue00c\\ue00c.(4.2)Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.Whenthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresultofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplifypre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,theerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.4.3Gradient-BasedOptimizationMostdeeplearningalgorithmsinvolveoptimizationofsomesort. Optimizationreferstothetaskofeitherminimizingormaximizingsomefunctionf(x) byalteringx. Weusuallyphrasemostoptimizationproblemsintermsofminimizingf(x).Maximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing−f()x.Thefunctionwewanttominimizeormaximizeiscalledtheobjectivefunctionor.Whenweareminimizingit,wemayalsocallitthecriterioncostfunction,lossfunctionerrorfunction,or.Inthisbook,weusethesetermsinterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaningtosomeoftheseterms.Weoftendenotethevaluethatminimizesormaximizesafunctionwithasuperscript.Forexample,wemightsay∗x∗= argmin()fx.82'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 97}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue078\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue047\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue061\\ue074\\ue020\\ue02e\\ue078\\ue03d\\ue030\\ue053\\ue069\\ue06e\\ue063\\ue065\\ue020\\ue066\\ue030\\ue028\\ue029\\ue03d\\ue030\\ue078\\ue02c\\ue020\\ue067\\ue072\\ue061\\ue064\\ue069\\ue065\\ue06e\\ue074\\ue064\\ue065\\ue073\\ue063\\ue065\\ue06e\\ue074\\ue020\\ue068\\ue061\\ue06c\\ue074\\ue073\\ue020\\ue068\\ue065\\ue072\\ue065\\ue02e\\ue046\\ue06f\\ue072\\ue020\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue068\\ue061\\ue076\\ue065\\ue020\\ue078\\ue03c\\ue030\\ue066\\ue030\\ue028\\ue029\\ue030\\ue078\\ue03c\\ue02c\\ue073\\ue06f\\ue020\\ue077\\ue065\\ue020\\ue063\\ue061\\ue06e\\ue020\\ue064\\ue065\\ue063\\ue072\\ue065\\ue061\\ue073\\ue065\\ue020\\ue066\\ue020\\ue062\\ue079\\ue06d\\ue06f\\ue076\\ue069\\ue06e\\ue067\\ue020\\ue072\\ue069\\ue067\\ue068\\ue074\\ue077\\ue061\\ue072\\ue064\\ue02e\\ue046\\ue06f\\ue072\\ue020\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue068\\ue061\\ue076\\ue065\\ue020\\ue078\\ue03e\\ue030\\ue066\\ue030\\ue028\\ue029\\ue030\\ue078\\ue03e\\ue02c\\ue073\\ue06f\\ue020\\ue077\\ue065\\ue020\\ue063\\ue061\\ue06e\\ue020\\ue064\\ue065\\ue063\\ue072\\ue065\\ue061\\ue073\\ue065\\ue020\\ue066\\ue020\\ue062\\ue079\\ue06d\\ue06f\\ue076\\ue069\\ue06e\\ue067\\ue020\\ue06c\\ue065\\ue066\\ue074\\ue077\\ue061\\ue072\\ue064\\ue02e\\ue047\\ue072\\ue061\\ue064\\ue069\\ue065\\ue06e\\ue074\\ue020\\ue064\\ue065\\ue073\\ue063\\ue065\\ue06e\\ue074\\n\\ue066\\ue078\\ue028\\ue029\\ue03d\\ue031\\ue032\\ue078\\ue032\\ue066\\ue030\\ue028\\ue029\\ue03d\\ue078\\ue078Figure4.1:Anillustrationofhowthederivativesofafunctioncanbeusedtofollowthefunctiondownhilltoaminimum.Thistechniqueiscalledgradientdescent.Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabriefreviewofhowcalculusconceptsrelatetooptimizationhere.Supposewehaveafunctiony=f(x),wherebothxandyarerealnumbers.Theofthisfunctionisdenotedasderivativef\\ue030(x)orasdydx.Thederivativef\\ue030(x)givestheslopeoff(x)atthepointx.Inotherwords,itspeciﬁeshowtoscaleasmallchangeintheinputinordertoobtainthecorrespondingchangeintheoutput:fx\\ue00ffx\\ue00ff(+) ≈()+\\ue030()x.Thederivativeisthereforeusefulforminimizingafunctionbecauseittellsushowtochangexinordertomakeasmallimprovementiny.Forexample,weknowthatf(x\\ue00f−sign(f\\ue030(x)))islessthanf(x)forsmallenough\\ue00f.Wecanthusreducef(x)bymovingxinsmallstepswithoppositesignofthederivative.Thistechniqueiscalledgradientdescent(Cauchy1847,).SeeFig.foranexampleof4.1thistechnique.Whenf\\ue030(x) = 0,thederivativeprovidesnoinformationaboutwhichdirectiontomove.Pointswheref\\ue030(x) = 0areknownascriticalpointsstationarypointsor.Alocalminimumisapointwheref(x)islowerthanatallneighboringpoints,soitisnolongerpossibletodecreasef(x)bymakinginﬁnitesimalsteps.Alocalmaximumisapointwheref(x)ishigherthanatallneighboringpoints,soitis83'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 98}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\ue04d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue04d\\ue061\\ue078\\ue069\\ue06d\\ue075\\ue06d\\ue053\\ue061\\ue064\\ue064\\ue06c\\ue065\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue054\\ue079\\ue070\\ue065\\ue073\\ue020\\ue06f\\ue066\\ue020\\ue063\\ue072\\ue069\\ue074\\ue069\\ue063\\ue061\\ue06c\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue073\\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointisapointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthantheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,orasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.notpossibletoincreasef(x)bymakinginﬁnitesimalsteps.Somecriticalpointsareneithermaximanorminima.Theseareknownassaddlepoints.SeeFig.4.2forexamplesofeachtypeofcriticalpoint.Apointthatobtainstheabsolutelowestvalueoff(x)isaglobalminimum.Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaofthefunction.Itisalsopossiblefortheretobelocalminimathatarenotgloballyoptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhavemanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedbyveryﬂatregions.Allofthismakesoptimizationverydiﬃcult,especiallywhentheinputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndingavalueoffthatisverylow,butnotnecessarilyminimalinanyformalsense.SeeFig.foranexample.4.3Weoftenminimizefunctionsthathavemultipleinputs:f:Rn→R.Fortheconceptof“minimization” tomakesense,theremuststillbeonlyone(scalar)output.Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptofpartialderivatives.Thepartialderivative∂∂xif(x)measureshowfchangesasonlythevariablexiincreasesatpointx.Thegradientgeneralizesthenotionofderivativetothecasewherethederivativeiswithrespecttoavector:thegradientoffisthevectorcontainingallofthepartialderivatives,denoted∇xf(x).Elementiofthegradientisthepartialderivativeoffwithrespecttoxi.Inmultipledimensions,84'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 99}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue066\\ue078\\ue028\\ue029\\ue049\\ue064\\ue065\\ue061\\ue06c\\ue06c\\ue079\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue077\\ue06f\\ue075\\ue06c\\ue064\\ue020\\ue06c\\ue069\\ue06b\\ue065\\ue020\\ue074\\ue06f\\ue020\\ue061\\ue072\\ue072\\ue069\\ue076\\ue065\\ue020\\ue061\\ue074\\ue020\\ue074\\ue068\\ue065\\ue020\\ue067\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue02c\\ue020\\ue062\\ue075\\ue074\\ue020\\ue074\\ue068\\ue069\\ue073\\ue020\\ue06d\\ue069\\ue067\\ue068\\ue074\\ue020\\ue06e\\ue06f\\ue074\\ue020\\ue062\\ue065\\ue020\\ue070\\ue06f\\ue073\\ue073\\ue069\\ue062\\ue06c\\ue065\\ue02e\\ue054\\ue068\\ue069\\ue073\\ue020\\ue06c\\ue06f\\ue063\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue070\\ue065\\ue072\\ue066\\ue06f\\ue072\\ue06d\\ue073\\ue020\\ue06e\\ue065\\ue061\\ue072\\ue06c\\ue079\\ue020\\ue061\\ue073\\ue020\\ue077\\ue065\\ue06c\\ue06c\\ue020\\ue061\\ue073\\ue074\\ue068\\ue065\\ue020\\ue067\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06f\\ue06e\\ue065\\ue02c\\ue073\\ue06f\\ue020\\ue069\\ue074\\ue020\\ue069\\ue073\\ue020\\ue061\\ue06e\\ue020\\ue061\\ue063\\ue063\\ue065\\ue070\\ue074\\ue061\\ue062\\ue06c\\ue065\\ue068\\ue061\\ue06c\\ue074\\ue069\\ue06e\\ue067\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue02e\\ue054\\ue068\\ue069\\ue073\\ue020\\ue06c\\ue06f\\ue063\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue070\\ue065\\ue072\\ue066\\ue06f\\ue072\\ue06d\\ue073\\ue070\\ue06f\\ue06f\\ue072\\ue06c\\ue079\\ue02c\\ue020\\ue061\\ue06e\\ue064\\ue020\\ue073\\ue068\\ue06f\\ue075\\ue06c\\ue064\\ue020\\ue062\\ue065\\ue020\\ue061\\ue076\\ue06f\\ue069\\ue064\\ue065\\ue064\\ue02e\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue078\\ue069\\ue06d\\ue061\\ue074\\ue065\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue069\\ue07a\\ue061\\ue074\\ue069\\ue06f\\ue06e\\nFigure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhentherearemultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerallyacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespondtosigniﬁcantlylowvaluesofthecostfunction.criticalpointsarepointswhereeveryelementofthegradientisequaltozero.Thedirectionalderivativeindirectionu(aunitvector)istheslopeofthefunctionfindirectionu.Inotherwords,thedirectionalderivativeisthederivativeofthefunctionf(x+αu)withrespecttoα,evaluatedatα= 0.Usingthechainrule,wecanseethat∂∂αfα(+xuu) = \\ue03e∇xf()x.Tominimizef,wewouldliketoﬁndthedirectioninwhichfdecreasesthefastest.Wecandothisusingthedirectionalderivative:minuu,\\ue03eu=1u\\ue03e∇xf()x(4.3)=minuu,\\ue03eu=1||||u2||∇xf()x||2cosθ(4.4)whereθistheanglebetweenuandthegradient.Substitutingin||||u2= 1andignoringfactorsthatdonotdependonu,thissimpliﬁestominucosθ.Thisisminimizedwhenupointsintheoppositedirectionasthegradient.Inotherwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectlydownhill.Wecandecreasefbymovinginthedirectionofthenegativegradient.Thisisknownasthemethodofsteepestdescentgradientdescentor.Steepestdescentproposesanewpointx\\ue030= x−∇\\ue00fxf()x(4.5)85'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 100}, page_content='CHAPTER4.NUMERICALCOMPUTATIONwhere\\ue00fisthelearningrate,apositivescalardeterminingthesizeofthestep.Wecanchoose\\ue00finseveraldiﬀerentways.Apopularapproachistoset\\ue00ftoasmallconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectionalderivativevanish.Anotherapproachistoevaluatef\\ue00f(x−∇xf())xforseveralvaluesof\\ue00fandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.Thislaststrategyiscalledalinesearch.Steepestdescentconvergeswheneveryelementofthegradientiszero(or,inpractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthisiterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingtheequation∇xf() = 0xfor.xAlthoughgradientdescentislimitedtooptimizationincontinuousspaces,thegeneralconceptofmakingsmallmoves(thatareapproximatelythebestsmallmove)towardsbetterconﬁgurationscanbegeneralizedtodiscretespaces.Ascendinganobjectivefunctionofdiscreteparametersiscalled(,hillclimbingRusselandNorvig2003).4.3.1BeyondtheGradient:JacobianandHessianMatricesSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinputandoutputarebothvectors.ThematrixcontainingallsuchpartialderivativesisknownasaJacobianmatrix.Speciﬁcally,ifwehaveafunctionf:Rm→Rn,thentheJacobianmatrixJ∈Rnm×ofisdeﬁnedsuchthatfJi,j=∂∂xjf()xi.Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknownasasecondderivative.Forexample,forafunctionf:Rn→R,thederivativewithrespecttoxiofthederivativeoffwithrespecttoxjisdenotedas∂2∂xi∂xjf.Inasingledimension,wecandenoted2dx2fbyf\\ue030\\ue030(x).Thesecondderivativetellsushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportantbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovementaswewouldexpectbasedonthegradientalone.Wecanthinkofthesecondderivativeasmeasuringcurvature.Supposewehaveaquadraticfunction(manyfunctionsthatariseinpracticearenotquadraticbutcanbeapproximatedwellasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,thenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredictedusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize1\\ue00falongthenegativegradient,andthecostfunctionwilldecreaseby\\ue00f.Ifthesecondderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwillactuallydecreasebymorethan\\ue00f.Finally,ifthesecondderivativeispositive,thefunctioncurvesupward,sothecostfunctioncandecreasebylessthan\\ue00f.SeeFig.86'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 101}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\nxfx()Negativecurvature\\nxfx()Nocurvature\\nxfx()Positivecurvature\\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshowquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecostfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradientstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfasterthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecreasecorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpectedandeventuallybeginstoincrease,sotoolargeofstepsizescanactuallyincreasethefunctioninadvertently.4.4toseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetweenthevalueofthecostfunctionpredictedbythegradientandthetruevalue.Whenourfunctionhasmultipleinputdimensions,therearemanysecondderivatives.ThesederivativescanbecollectedtogetherintoamatrixcalledtheHessianmatrix.TheHessianmatrixisdeﬁnedsuchthatHx()(f)Hx()(f)i,j=∂2∂xi∂xjf.()x(4.6)Equivalently,theHessianistheJacobianofthegradient.Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerentialoperatorsarecommutative,i.e.theirordercanbeswapped:∂2∂xi∂xjf() =x∂2∂xj∂xif.()x(4.7)ThisimpliesthatHi,j=Hj,i,sotheHessianmatrixissymmetricatsuchpoints.MostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetricHessianalmosteverywhere. BecausetheHessianmatrixisrealandsymmetric,wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof87'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 102}, page_content='CHAPTER4.NUMERICALCOMPUTATIONeigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunitvectordisgivenbyd\\ue03eHd.WhendisaneigenvectorofH,thesecondderivativeinthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsofd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,withweightsbetween0and1,andeigenvectorsthathavesmalleranglewithdreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecondderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.The(directional)secondderivativetellsushowwellwecanexpectagradientdescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximationtothefunctionaroundthecurrentpointf()xx(0):ff() x≈(x(0))+(xx−(0))\\ue03eg+12(xx−(0))\\ue03eHxx(−(0)).(4.8)wheregisthegradientandHistheHessianatx(0). Ifweusealearningrateof\\ue00f,thenthenewpointxwillbegivenbyx(0)−\\ue00fg.Substitutingthisintoourapproximation,weobtainf(x(0)−≈\\ue00fg) f(x(0))−\\ue00fg\\ue03eg+12\\ue00f2g\\ue03eHg.(4.9)Therearethree termshere:theoriginalvalue ofthefunction, theexpectedimprovementduetotheslopeofthefunction,andthecorrectionwemustapplytoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,thegradientdescentstepcanactuallymoveuphill.Wheng\\ue03eHgiszeroornegative,theTaylorseriesapproximationpredictsthatincreasing\\ue00fforeverwilldecreasefforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge\\ue00f,soonemustresorttomoreheuristicchoicesof\\ue00finthiscase.Wheng\\ue03eHgispositive,solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximationofthefunctionthemostyields\\ue00f∗=g\\ue03egg\\ue03eHg.(4.10)Intheworstcase,whengalignswiththeeigenvectorofHcorrespondingtothemaximaleigenvalueλmax,thenthisoptimalstepsizeisgivenby1λmax.Totheextentthatthefunctionweminimizecanbeapproximatedwellbyaquadraticfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearningrate.Thesecondderivativecanbeusedtodeterminewhetheracriticalpointisalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacriticalpoint,f\\ue030(x)=0.Whenf\\ue030\\ue030(x)>0,thismeansthatf\\ue030(x)increasesaswemovetotheright,andf\\ue030(x)decreasesaswemovetotheleft.Thismeansf\\ue030(x\\ue00f−)<0and88'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 103}, page_content='CHAPTER4.NUMERICALCOMPUTATIONf\\ue030(x+\\ue00f)>0 forsmallenough\\ue00f.Inotherwords,aswemoveright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslopebeginstopointuphilltotheleft.Thus,whenf\\ue030(x) = 0andf\\ue030\\ue030(x)>0,wecanconcludethatxisalocalminimum.Similarly,whenf\\ue030(x) = 0andf\\ue030\\ue030(x)<0,wecanconcludethatxisalocalmaximum.Thisisknownasthesecondderivativetest.Unfortunately,whenf\\ue030\\ue030(x) = 0,thetestisinconclusive.Inthiscasexmaybeasaddlepoint,orapartofaﬂatregion.Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthefunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralizethesecondderivativetesttomultipledimensions.Atacriticalpoint,where∇xf(x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhetherthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.WhentheHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocalminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivativeinanydirectionmustbepositive,andmakingreferencetotheunivariatesecondderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvaluesarenegative),thepointisalocalmaximum.Inmultipledimensions,itisactuallypossibletoﬁndpositiveevidenceofsaddlepointsinsomecases. Whenatleastoneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthatxisalocalmaximumononecrosssectionoffbutalocalminimumonanothercrosssection.SeeFig.foranexample.Finally,themultidimensionalsecond4.5derivativetestcanbeinconclusive,justliketheunivariateversion.Thetestisinconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butatleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestisinconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.Inmultipledimensions,therecanbeawidevarietyofdiﬀerentsecondderivativesatasinglepoint,becausethereisadiﬀerentsecondderivativeforeachdirection.TheconditionnumberoftheHessianmeasureshowmuchthesecondderivativesvary.WhentheHessianhasapoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinonedirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreasesslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnotknowthatitneedstoexplorepreferentiallyinthedirectionwherethederivativeremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoinguphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthestepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithlesscurvature.SeeFig.foranexample.4.6ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixto89'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 104}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue031\\U000f0913\\ue031\\ue035\\ue030\\ue031\\ue035\\ue078\\ue032\\U000f0913\\ue031\\ue035\\ue030\\ue031\\ue035\\ue066\\ue078\\ue028\\ue031\\ue03b\\ue078\\ue032\\ue029\\U000f0913\\ue035\\ue030\\ue030\\ue030\\ue035\\ue030\\ue030\\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunctioninthisexampleisf(x)=x21−x22.Alongtheaxiscorrespondingtox1,thefunctioncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.Alongtheaxiscorrespondingtox2,thefunctioncurvesdownward.ThisdirectionisaneigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfromthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunctionwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalueof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegativeeigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocalmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\\n90'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 105}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n−−−30201001020x1−30−20−1001020x2\\nFigure4.6:GradientdescentfailstoexploitthecurvatureinformationcontainedintheHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunctionf(x) whoseHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvaturehasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themostcurvatureisinthedirection[1,1]\\ue03eandtheleastcurvatureisinthedirection[1,−1]\\ue03e.Theredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadraticfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescendingcanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhattoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedstodescendtheoppositecanyonwallonthenextiteration.ThelargepositiveeigenvalueoftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthatthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedontheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearchdirectioninthiscontext.\\n91'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 106}, page_content='CHAPTER4.NUMERICALCOMPUTATIONguidethesearch.ThesimplestmethodfordoingsoisknownasNewton’smethod.Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansiontoapproximatenearsomepointf()xx(0):ff() x≈(x(0))+(xx−(0))\\ue03e∇xf(x(0))+12(xx−(0))\\ue03eHx()(f(0))(xx−(0)).(4.11)Ifwethensolveforthecriticalpointofthisfunction,weobtain:x∗= x(0)−Hx()(f(0))−1∇xf(x(0)).(4.12)Whenfisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsofapplyingEq.oncetojumptotheminimumofthefunctiondirectly.When4.12fisnottrulyquadraticbutcanbelocallyapproximatedasapositivedeﬁnitequadratic,Newton’smethodconsistsofapplyingEq.multipletimes.Iterativelyupdating4.12theapproximationandjumpingtotheminimumoftheapproximationcanreachthecriticalpointmuchfasterthangradientdescentwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmfulpropertynearasaddlepoint.AsdiscussedinSec.,Newton’smethodisonlyappropriatewhenthenearby8.2.3criticalpointisaminimum(alltheeigenvaluesoftheHessianarepositive),whereasgradientdescentisnotattractedtosaddlepointsunlessthegradientpointstowardthem.Optimizationalgorithmssuchasgradientdescentthatuseonlythegradientarecalledﬁrst-orderoptimizationalgorithms.OptimizationalgorithmssuchasNew-ton’smethodthatalsousetheHessianmatrixarecalledsecond-orderoptimizationalgorithms(NocedalandWright2006,).The optimizationalgorithms employedin mostcontextsin thisbook areapplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.Thisisbecausethefamilyoffunctionsusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominantapproachtooptimizationistodesignoptimizationalgorithmsforalimitedfamilyoffunctions.Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-ingourselvestofunctionsthatareeitherLipschitzcontinuousorhaveLipschitzcontinuousderivatives.ALipschitzcontinuousfunctionisafunctionfwhoserateofchangeisboundedbyaLipschitzconstantL:∀∀|−|≤L||−||x,y,f()xf()yxy2.(4.13)Thispropertyisusefulbecauseitallowsustoquantifyourassumptionthatasmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhaveasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,92'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 107}, page_content='CHAPTER4.NUMERICALCOMPUTATIONandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuouswithrelativelyminormodiﬁcations.Perhapsthemostsuccessfulﬁeldofspecializedoptimizationisconvexoptimiza-tion.Convexoptimizationalgorithmsareabletoprovidemanymoreguaranteesbymakingstrongerrestrictions.Convexoptimizationalgorithmsareapplicableonlytoconvexfunctions—functionsforwhichtheHessianispositivesemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddlepointsandalloftheirlocalminimaarenecessarilyglobalminima.However,mostproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.Convexoptimizationisusedonlyasasubroutineofsomedeeplearningalgorithms.Ideasfromtheanalysisofconvexoptimizationalgorithmscanbeusefulforprovingtheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportanceofconvexoptimizationisgreatlydiminishedinthecontextofdeeplearning.Formoreinformationaboutconvexoptimization,seeBoydandVandenberghe2004()orRockafellar1997().4.4ConstrainedOptimizationSometimeswewishnotonlytomaximizeorminimizeafunctionf(x)overallpossiblevaluesofx.Insteadwemaywishtoﬁndthemaximalorminimalvalueoff(x)forvaluesofxinsomesetS.Thisisknownasconstrainedoptimization.PointsxthatliewithinthesetSarecalledfeasiblepointsinconstrainedoptimizationterminology.Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommonapproachinsuchsituationsistoimposeanormconstraint,suchas.||||≤x1Onesimpleapproachtoconstrainedoptimizationissimplytomodifygradientdescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize\\ue00f,wecanmakegradientdescentsteps,thenprojecttheresultbackintoS.Ifweusealinesearch,wecansearchonlyoverstepsizes\\ue00fthatyieldnewxpointsthatarefeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradientintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginningthelinesearch(,).Rosen1960Amoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,constrainedoptimizationproblem.Forexample,ifwewanttominimizef(x)forx∈R2withxconstrainedtohaveexactlyunitL2norm,wecaninsteadminimize93'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 108}, page_content='CHAPTER4.NUMERICALCOMPUTATIONg(θ) =f([cossinθ,θ]\\ue03e)withrespecttoθ,thenreturn[cossinθ,θ]asthesolutiontotheoriginalproblem.Thisapproachrequirescreativity;thetransformationbetweenoptimizationproblemsmustbedesignedspeciﬁcallyforeachcaseweencounter.TheKarush–Kuhn–Tucker(KKT)approach1providesaverygeneralsolutiontoconstrainedoptimization.WiththeKKTapproach,weintroduceanewfunctioncalledthegeneralizedLagrangiangeneralizedLagrangefunctionor.TodeﬁnetheLagrangian,weﬁrstneedtodescribeSintermsofequationsandinequalities. WewantadescriptionofSintermsofmfunctionsg()iandnfunctionsh()jsothatS={|∀xi,g()i(x) = 0and∀j,h()j(x)≤0}.Theequationsinvolvingg()iarecalledtheequalityconstraintsandtheinequalitiesinvolvingh()jarecalledinequalityconstraints.Weintroducenewvariablesλiandαjforeachconstraint,thesearecalledtheKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedasL,,f(xλα) = ()+x\\ue058iλig()i()+x\\ue058jαjh()j()x.(4.14)WecannowsolveaconstrainedminimizationproblemusingunconstrainedoptimizationofthegeneralizedLagrangian.Observethat,solongasatleastonefeasiblepointexistsandisnotpermittedtohavevalue,thenf()x∞minxmaxλmaxαα,≥0L,,.(xλα)(4.15)hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsasxminx∈Sf.()x(4.16)Thisfollowsbecauseanytimetheconstraintsaresatisﬁed,maxλmaxαα,≥0L,,f,(xλα) = ()x(4.17)whileanytimeaconstraintisviolated,maxλmaxαα,≥0L,,.(xλα) = ∞(4.18)Thesepropertiesguaranteethatnoinfeasiblepointwilleverbeoptimal,andthattheoptimumwithinthefeasiblepointsisunchanged.1TheKKTapproachgeneralizesthemethodofLagrangemultiplierswhichallowsequalityconstraintsbutnotinequalityconstraints.94'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 109}, page_content='CHAPTER4.NUMERICALCOMPUTATIONToperformconstrainedmaximization,wecanconstructthegeneralizedLa-grangefunctionof,whichleadstothisoptimizationproblem:−f()xminxmaxλmaxαα,≥0−f()+x\\ue058iλig()i()+x\\ue058jαjh()j()x.(4.19)Wemayalsoconvertthistoaproblemwithmaximizationintheouterloop:maxxminλminαα,≥0f()+x\\ue058iλig()i()x−\\ue058jαjh()j()x.(4.20)Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneitwithadditionorsubtractionaswewish,becausetheoptimizationisfreetochooseanysignforeachλi.Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstrainth()i(x)isifactiveh()i(x∗) = 0.Ifaconstraintisnotactive,thenthesolutiontotheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionifthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludesothersolutions.Forexample,aconvexproblemwithanentireregionofgloballyoptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthisregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocalstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,thepointfoundatconvergenceremainsastationarypointwhetherornottheinactiveconstraintsareincluded.Becauseaninactiveh()ihasnegativevalue,thenthesolutiontominxmaxλmaxαα,≥0L(xλα,,)willhaveαi=0.Wecanthusobservethatatthesolution,αh(x) =0.Inotherwords,foralli,weknowthatatleastoneoftheconstraintsαi≥0andh()i(x)≤0mustbeactiveatthesolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolutionisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultipliertoinﬂuencethesolutiontox,ortheinequalityhasnoinﬂuenceonthesolutionandwerepresentthisbyzeroingoutitsKKTmultiplier.ThepropertiesthatthegradientofthegeneralizedLagrangianiszero,allconstraintsonbothxandtheKKTmultipliersaresatisﬁed,andαh\\ue00c(x) =0arecalledtheKarush-Kuhn-Tucker(KKT)conditions(,;Karush1939KuhnandTucker1951,).Together,thesepropertiesdescribetheoptimalpointsofconstrainedoptimizationproblems.FormoreinformationabouttheKKTapproach,seeNocedalandWright2006().95'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 110}, page_content='CHAPTER4.NUMERICALCOMPUTATION4.5Example:LinearLeastSquaresSupposewewanttoﬁndthevalueofthatminimizesxf() =x12||−||Axb22.(4.21)Therearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.However,wecanalsoexplorehowtosolveitusinggradient-basedoptimizationasasimpleexampleofhowthesetechniqueswork.First,weneedtoobtainthegradient:∇xf() = xA\\ue03e() = Axb−A\\ue03eAxA−\\ue03eb.(4.22)Wecanthenfollowthisgradientdownhill,takingsmallsteps.SeeAlgorithm4.1fordetails.Algorithm4.1Analgorithmtominimizef(x) =12||−||Axb22withrespecttoxusinggradientdescent.Setthestepsize()andtolerance()tosmall,positivenumbers.\\ue00fδwhile||A\\ue03eAxA−\\ue03eb||2>δdoxx←−\\ue00f\\ue000A\\ue03eAxA−\\ue03eb\\ue001endwhileOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,becausethetruefunctionisquadratic,thequadraticapproximationemployedbyNewton’smethodisexact,andthealgorithmconvergestotheglobalminimuminasinglestep.Nowsuppose we wishto minimizethesame function,butsubject totheconstraintx\\ue03ex≤1.Todoso,weintroducetheLagrangianL,λfλ(x) = ()+x\\ue010x\\ue03ex−1\\ue011.(4.23)Wecannowsolvetheproblemminxmaxλ,λ≥0L,λ.(x)(4.24)Thesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybefoundusingtheMoore-Penrosepseudoinverse:x=A+b.Ifthispointisfeasible,thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda96'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 111}, page_content='CHAPTER4.NUMERICALCOMPUTATIONsolutionwheretheconstraintisactive.BydiﬀerentiatingtheLagrangianwithrespectto,weobtaintheequationxA\\ue03eAxA−\\ue03ebx+2λ= 0.(4.25)ThistellsusthatthesolutionwilltaketheformxA= (\\ue03eAI+2λ)−1A\\ue03eb.(4.26)Themagnitudeofλmustbechosensuchthattheresultobeystheconstraint.Wecanﬁndthisvaluebyperforminggradientascenton.Todoso,observeλ∂∂λL,λ(x) = x\\ue03ex−1.(4.27)Whenthenormofxexceeds1,thisderivativeispositive,sotofollowthederivativeuphillandincreasetheLagrangianwithrespecttoλ,weincreaseλ.Becausethecoeﬃcientonthex\\ue03expenaltyhasincreased,solvingthelinearequationforxwillnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequationandadjustingλcontinuesuntilxhasthecorrectnormandthederivativeonλis0.Thisconcludesthemathematicalpreliminariesthatweusetodevelopmachinelearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedgedlearningsystems.\\n97'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 112}, page_content='Chapter5MachineLearningBasicsDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstanddeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesofmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneralprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersorthosewhowantawiderperspectiveareencouragedtoconsidermachinelearningtextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy2012Bishop2006()or().Ifyouarealreadyfamiliarwithmachinelearningbasics,feelfreetoskipaheadtoSec..Thatsectioncoverssomeper-5.11spectivesontraditionalmachinelearningtechniquesthathavestronglyinﬂuencedthedevelopmentofdeeplearningalgorithms.Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentanexample:thelinearregressionalgorithm. Wethenproceedtodescribehowthechallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatternsthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettingscalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithmitself;wediscusshowtosettheseusingadditionaldata.Machinelearningisessentiallyaformofappliedstatisticswithincreasedemphasisontheuseofcomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasisonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthetwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervisedlearningandunsupervisedlearning;wedescribethesecategoriesandgivesomeexamplesofsimplelearningalgorithmsfromeachcategory.Mostdeeplearningalgorithmsare basedonan optimizationalgorithmcalled stochasticgradientdescent.Wedescribehowtocombinevariousalgorithmcomponentssuchasan98'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 113}, page_content='CHAPTER5.MACHINELEARNINGBASICSoptimizationalgorithm,acostfunction,amodel,andadatasettobuildamachinelearningalgorithm.Finally,inSec.,wedescribesomeofthefactorsthathave5.11limitedtheabilityoftraditionalmachinelearningtogeneralize.Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthatovercometheseobstacles.5.1LearningAlgorithmsAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.Butwhatdowemeanbylearning?()providesthedeﬁnition“AcomputerMitchell1997programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksTandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,improveswithexperienceE.”OnecanimagineaverywidevarietyofexperiencesE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthisbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthediﬀerentkindsoftasks,performancemeasuresandexperiencesthatcanbeusedtoconstructmachinelearningalgorithms.5.1.1TheTask,TMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewithﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcandphilosophicalpointofview,machinelearningisinterestingbecausedevelopingourunderstandingofmachinelearningentailsdevelopingourunderstandingoftheprinciplesthatunderlieintelligence.Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearningitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthetask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywriteaprogramthatspeciﬁeshowtowalkmanually.Machinelearningtasksareusuallydescribedintermsofhowthemachinelearningsystemshouldprocessan.Anexampleisacollectionofexamplefeaturesthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewantthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasavectorx∈Rnwhereeachentryxiofthevectorisanotherfeature.Forexample,thefeaturesofanimageareusuallythevaluesofthepixelsintheimage.99'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 114}, page_content='CHAPTER5.MACHINELEARNINGBASICSManykindsoftaskscanbesolvedwithmachinelearning.Someofthemostcommonmachinelearningtasksincludethefollowing:•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecifywhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearningalgorithmisusuallyaskedtoproduceafunctionf:Rn→{1,...,k}.Wheny=f(x),themodelassignsaninputdescribedbyvectorxtoacategoryidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcationtask,forexample,wherefoutputsaprobabilitydistributionoverclasses.Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinputisanimage(usuallydescribedasasetofpixelbrightnessvalues),andtheoutputisanumericcodeidentifyingtheobjectintheimage.Forexample,theWillowGaragePR2robotisabletoactasawaiterthatcanrecognizediﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwithdeeplearning(,;,).ObjectKrizhevskyetal.2012IoﬀeandSzegedy2015recognitionisthesamebasictechnologythatallowscomputerstorecognizefaces(Taigman2014etal.,),whichcanbeusedtoautomaticallytagpeopleinphotocollectionsandallowcomputerstointeractmorenaturallywiththeirusers.•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechallengingifthecomputerprogramisnotguaranteedthateverymeasurementinitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcationtask,thelearningalgorithmonlyhastodeﬁneafunctionmappingfromavectorsingleinputtoacategoricaloutput.Whensomeoftheinputsmaybemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearningalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassifyingsetxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituationarisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltestsareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargesetoffunctionsistolearnaprobabilitydistributionoveralloftherelevantvariables,thensolvetheclassiﬁcationtaskbymarginalizingoutthemissingvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁcationfunctionsneededforeachpossiblesetofmissinginputs,butweonlyneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.SeeGoodfellowetal.()foranexampleofadeepprobabilisticmodelappliedtosuchatask2013binthisway. Manyoftheothertasksdescribedinthissectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcationwithmissinginputsisjustoneexampleofwhatmachinelearningcando.100'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 115}, page_content='CHAPTER5.MACHINELEARNINGBASICS•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredictanumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithmisaskedtooutputafunctionf:Rn→R.Thistypeoftaskissimilartoclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleofaregressiontaskisthepredictionoftheexpectedclaimamountthataninsuredpersonwillmake(usedtosetinsurancepremiums),orthepredictionoffuturepricesofsecurities.Thesekindsofpredictionsarealsousedforalgorithmictrading.•Transcription:Inthistypeoftask,themachinelearningsystemisaskedtoobservearelativelyunstructuredrepresentationofsomekindofdataandtranscribeitintodiscrete,textualform.Forexample,inopticalcharacterrecognition,thecomputerprogramisshownaphotographcontaininganimageoftextandisaskedtoreturnthistextintheformofasequenceofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewusesdeeplearningtoprocessaddressnumbersinthisway(Goodfellowetal.,2014d).Anotherexampleisspeechrecognition,wherethecomputerprogramisprovidedanaudiowaveformandemitsasequenceofcharactersorwordIDcodesdescribingthewordsthatwerespokenintheaudiorecording.DeeplearningisacrucialcomponentofmodernspeechrecognitionsystemsusedatmajorcompaniesincludingMicrosoft,IBMandGoogle(Hintonetal.,2012b).•Machinetranslation:Inamachinetranslationtask,theinputalreadyconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogrammustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisiscommonlyappliedtonaturallanguages,suchastotranslatefromEnglishtoFrench.Deeplearninghasrecentlybeguntohaveanimportantimpactonthiskindoftask(Sutskever2014Bahdanau2015etal.,;etal.,).•Structuredoutput:Structuredoutputtasksinvolveanytaskwheretheoutputisavector(orotherdatastructurecontainingmultiplevalues)withimportantrelationshipsbetweenthediﬀerentelements.Thisisabroadcategory,andsubsumesthetranscriptionandtranslationtasksdescribedabove,butalsomanyothertasks.Oneexampleisparsing—mappinganaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructureandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.See()Collobert2011foranexampleofdeeplearningappliedtoaparsingtask.Anotherexampleispixel-wisesegmentationofimages,wherethecomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.Forexample,deeplearningcan101'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 116}, page_content='CHAPTER5.MACHINELEARNINGBASICSbeusedtoannotatethelocationsofroadsinaerialphotographs(MnihandHinton2010,). Theoutputneednothaveitsformmirrorthestructureoftheinputascloselyasintheseannotation-styletasks.Forexample,inimagecaptioning,thecomputerprogramobservesanimageandoutputsanaturallanguagesentencedescribingtheimage(,,;,Kirosetal.2014abMaoetal.2015Vinyals2015bDonahue2014KarpathyandLi2015;etal.,;etal.,;,;Fang2015Xu2015etal.,;etal.,).Thesetasksarecalledstructuredoutputtasksbecausetheprogrammustoutputseveralvaluesthatarealltightlyinter-related.Forexample,thewordsproducedbyanimagecaptioningprogrammustformavalidsentence.•Anomalydetection:Inthistypeoftask,thecomputerprogramsiftsthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusualoratypical.Anexampleofananomalydetectiontaskiscreditcardfrauddetection.Bymodelingyourpurchasinghabits,acreditcardcompanycandetectmisuseofyourcards. Ifathiefstealsyourcreditcardorcreditcardinformation,thethief’spurchaseswilloftencomefromadiﬀerentprobabilitydistributionoverpurchasetypesthanyourown.Thecreditcardcompanycanpreventfraudbyplacingaholdonanaccountassoonasthatcardhasbeenusedforanuncharacteristicpurchase.See()forasurveyofChandolaetal.2009anomalydetectionmethods.•Synthesisandsampling:Inthistypeoftask,themachinelearningalgorithmisaskedtogeneratenewexamplesthataresimilartothoseinthetrainingdata.Synthesisandsamplingviamachinelearningcanbeusefulformediaapplicationswhereitcanbeexpensiveorboringforanartisttogeneratelargevolumesofcontentbyhand. Forexample,videogamescanautomaticallygeneratetexturesforlargeobjectsorlandscapes,ratherthanrequiringanartisttomanuallylabeleachpixel(,).Insomecases,weLuoetal.2013wantthesamplingorsynthesisproceduretogeneratesomespeciﬁckindofoutputgiventheinput.Forexample,inaspeechsynthesistask,weprovideawrittensentenceandasktheprogramtoemitanaudiowaveformcontainingaspokenversionofthatsentence.Thisisakindofstructuredoutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrectoutputforeachinput,andweexplicitlydesirealargeamountofvariationintheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.•Imputationofmissingvalues:Inthistypeoftask,themachinelearningalgorithmisgivenanewexamplex∈Rn,butwithsomeentriesxiofxmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissingentries.102'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 117}, page_content='CHAPTER5.MACHINELEARNINGBASICS•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenininputacorruptedexample˜x∈Rnobtainedbyanunknowncorruptionprocessfromacleanexamplex∈Rn.Thelearnermustpredictthecleanexamplexfromitscorruptedversion˜x,ormoregenerallypredicttheconditionalprobabilitydistributionp(x|˜x).•Densityestimationprobabilitymassfunctionestimationor:Inthedensityestimationproblem, themachinelearningalgorithmisaskedtolearnafunctionpmodel:Rn→R,wherepmodel(x) canbeinterpretedasaprobabilitydensityfunction(ifxiscontinuous)oraprobabilitymassfunction(ifxisdiscrete)onthespacethattheexamplesweredrawnfrom.Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwediscussperformancemeasuresP), the algorithmneeds tolearn thestructure ofthe dataithasseen.Itmustknowwhereexamplesclustertightlyandwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequirethatthelearningalgorithmhasat leastimplicitlycaptured thestructure oftheprobabilitydistribution.Densityestimationallowsustoexplicitlycapturethatdistribution.Inprinciple,wecanthenperformcomputationsonthatdistributioninordertosolvetheothertasksaswell.Forexample,ifwehaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),wecanusethatdistributiontosolvethemissingvalueimputationtask.Ifavaluexiismissingandalloftheothervalues,denotedx−i,aregiven,thenweknowthedistributionoveritisgivenbyp(xi|x−i).Inpractice,densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,becauseinmanycasestherequiredoperationsonp(x)arecomputationallyintractable.Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftaskswelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcando,nottodeﬁnearigidtaxonomyoftasks.5.1.2ThePerformanceMeasure,PInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesignaquantitativemeasureofitsperformance.UsuallythisperformancemeasurePisspeciﬁctothetaskbeingcarriedoutbythesystem.TFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtranscrip-tion,weoftenmeasuretheaccuracyofthemodel.Accuracyisjusttheproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecanalsoobtain103'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 118}, page_content='CHAPTER5.MACHINELEARNINGBASICSequivalentinformationbymeasuringtheerrorrate,theproportionofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenrefertotheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0ifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1loss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodelacontinuous-valuedscoreforeachexample.Themostcommonapproachistoreporttheaveragelog-probabilitythemodelassignstosomeexamples.Usuallyweareinterestedinhowwellthemachinelearningalgorithmperformsondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhendeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusingaofdatathatisseparatefromthedatausedfortrainingthemachinetestsetlearningsystem.Thechoiceofperformancemeasuremayseemstraightforwardandobjective,butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswelltothedesiredbehaviorofthesystem.Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracyofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grainedperformancemeasurethatgivespartialcreditforgettingsomeelementsofthesequencecorrect?Whenperformingaregressiontask,shouldwepenalizethesystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakesverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.Inothercases,weknowwhatquantitywewouldideallyliketomeasure,butmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextofdensityestimation.Manyofthebestprobabilisticmodelsrepresentprobabilitydistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedtoaspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,onemustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,ordesignagoodapproximationtothedesiredcriterion.5.1.3TheExperience,EMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedsu-orpervisedbywhatkindofexperiencetheyareallowedtohaveduringthelearningprocess.Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowedtoexperienceanentire.Adatasetisacollectionofmanyexamples,asdataset104'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 119}, page_content='CHAPTER5.MACHINELEARNINGBASICSdeﬁnedinSec..Sometimeswewillalsocallexamples5.1.1datapoints.Oneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-searchersistheIrisdataset(,).ItisacollectionofmeasurementsofFisher1936diﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.Thefeatureswithineachexamplearethemeasurementsofeachofthepartsoftheplant:thesepallength,sepalwidth,petallengthandpetalwidth. Thedatasetalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesarerepresentedinthedataset.Unsupervisedlearningalgorithmsexperienceadatasetcontainingmanyfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontextofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthatgeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfortaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithmsperformotherroles,likeclustering,whichconsistsofdividingthedatasetintoclustersofsimilarexamples.Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures,buteachexampleisalsoassociatedwithalabeltargetor.Forexample,theIrisdatasetisannotatedwiththespeciesofeachirisplant.AsupervisedlearningalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothreediﬀerentspeciesbasedontheirmeasurements.Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamplesofarandomvectorx,andattemptingtoimplicitlyorexplicitlylearntheproba-bilitydistributionp(x),orsomeinterestingpropertiesofthatdistribution,whilesupervisedlearninginvolvesobservingseveralexamplesofarandomvectorxandanassociatedvalueorvectory,andlearningtopredictyfromx,usuallybyestimatingp(yx|).Thetermsupervisedlearningoriginatesfromtheviewofthetargetybeingprovidedbyaninstructororteacherwhoshowsthemachinelearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructororteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescanbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystatesthatforavectorx∈Rn,thejointdistributioncanbedecomposedasp() =xn\\ue059i=1p(xi|x1,...,xi−1).(5.1)Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemofmodelingp(x) bysplittingitintonsupervisedlearningproblems.Alternatively,we105'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 120}, page_content='CHAPTER5.MACHINELEARNINGBASICScansolvethesupervisedlearningproblemoflearningp(y|x)byusingtraditionalunsupervised learningtechnologiesto learn thejointdistributionp(x,y)andinferringpy(|x) =p,y(x)\\ue050y\\ue030p,y(x\\ue030).(5.2)Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalordistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowithmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcationandstructuredoutputproblemsassupervisedlearning.Densityestimationinsupportofothertasksisusuallyconsideredunsupervisedlearning.Othervariantsofthelearningparadigmarepossible.Forexample,insemi-supervisedlearning,someexamplesincludeasupervisiontargetbutothersdonot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledascontainingornotcontaininganexampleofaclass,buttheindividualmembersofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearningwithdeepmodels,see().Kotziasetal.2015Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.Forexample,reinforcementlearningalgorithmsinteractwithanenvironment,sothereisafeedbackloopbetweenthelearningsystemanditsexperiences.Suchalgorithmsarebeyondthescopeofthisbook.Pleasesee()orSuttonandBarto1998BertsekasandTsitsiklis1996Mnih()forinformationaboutreinforcementlearning,andetal.()forthedeeplearningapproachtoreinforcementlearning.2013Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcanbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,whichareinturncollectionsoffeatures.Onecommonwayofdescribingadatasetiswitha.Adesigndesignmatrixmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthematrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains150exampleswithfourfeaturesforeachexample.ThismeanswecanrepresentthedatasetwithadesignmatrixX∈R1504×,whereXi,1isthesepallengthofplanti,Xi,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearningalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibletodescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographswithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerentnumbersofpixels,sonotallofthephotographsmaybedescribedwiththesamelengthofvector.Sec.andChapterdescribehowtohandlediﬀerenttypes9.710106'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 121}, page_content='CHAPTER5.MACHINELEARNINGBASICSofsuchheterogeneousdata.Incaseslikethese,ratherthandescribingthedatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:{x(1),x(2),...,x()m}.Thisnotationdoesnotimplythatanytwoexamplevectorsx()iandx()jhavethesamesize.Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetaswellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithmtoperformobjectrecognitionfromphotographs,weneedtospecifywhichobjectappearsineachofthephotos.Wemightdothiswithanumericcode,with0signifyingaperson,1signifyingacar,2signifyingacat,etc.OftenwhenworkingwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealsoprovideavectoroflabels,withyyiprovidingthelabelforexample.iOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.Forexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentiresentences,thenthelabelforeachexamplesentenceisasequenceofwords.Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedherecovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.5.1.4Example:LinearRegressionOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapableofimprovingacomputerprogram’sperformanceatsometaskviaexperienceissomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofasimplemachinelearningalgorithm:linearregression.Wewillreturntothisexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelptounderstanditsbehavior.Asthenameimplies,linearregressionsolvesaregressionproblem. Inotherwords,thegoalistobuildasystemthatcantakeavectorx∈Rnasinputandpredictthevalueofascalary∈Rasitsoutput.Inthecaseoflinearregression,theoutputisalinearfunctionoftheinput. Letˆybethevaluethatourmodelpredictsshouldtakeon.Wedeﬁnetheoutputtobeyˆy= w\\ue03ex(5.3)wherew∈Rnisavectorofparameters.Parametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,wiisthecoeﬃcientthatwemultiplybyfeaturexibeforesummingupthecontributionsfromallthefeatures.Wecanthinkofwasasetofthatdeterminehowweightseachfeatureaﬀectstheprediction.Ifafeaturexireceivesapositiveweightwi,107'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 122}, page_content='CHAPTER5.MACHINELEARNINGBASICSthenincreasingthevalueofthatfeatureincreasesthevalueofourpredictionˆy.Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeaturedecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,thenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasnoeﬀectontheprediction.WethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputtingˆy= w\\ue03ex.Nextweneedadeﬁnitionofourperformancemeasure,.PSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnotusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohaveavectorofregressiontargetsprovidingthecorrectvalueofyforeachoftheseexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetestset.WerefertothedesignmatrixofinputsasX()testandthevectorofregressiontargetsasy()test.Onewayofmeasuringtheperformanceofthemodelistocomputethemeansquarederrorofthemodelonthetestset. Ifˆy()testgivesthepredictionsofthemodelonthetestset,thenthemeansquarederrorisgivenbyMSEtest=1m\\ue058i(ˆy()test−y()test)2i.(5.4)Intuitively,onecanseethatthiserrormeasuredecreasesto0whenˆy()test=y()test.WecanalsoseethatMSEtest=1m||ˆy()test−y()test||22,(5.5)sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictionsandthetargetsincreases.Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthatwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithmisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).Oneintuitivewayofdoingthis(whichwewilljustifylater,inSec.)isjustto5.5.1minimizethemeansquarederroronthetrainingset,MSEtrain.TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis:0∇wMSEtrain= 0(5.6)⇒∇w1m||ˆy()train−y()train||22= 0(5.7)⇒1m∇w||X()trainwy−()train||22= 0(5.8)108'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 123}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n−−10.05000510....x1−3−2−10123yLinearregressionexample\\n051015...w1020.025.030.035.040.045.050.055.MSE(train)Optimizationofw\\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorwcontainsonlyasingleparametertolearn,w1.(Left)Observethatlinearregressionlearnstosetw1suchthattheliney=w1xcomesascloseaspossibletopassingthroughallthetrainingpoints.Theplottedpointindicatesthevalueof(Right)w1foundbythenormalequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.⇒∇w\\ue010X()trainwy−()train\\ue011\\ue03e\\ue010X()trainwy−()train\\ue011= 0(5.9)⇒∇w\\ue010w\\ue03eX()train\\ue03eX()trainww−2\\ue03eX()train\\ue03ey()train+y()train\\ue03ey()train\\ue011= 0(5.10)⇒2X()train\\ue03eX()trainwX−2()train\\ue03ey()train= 0(5.11)⇒w=\\ue010X()train\\ue03eX()train\\ue011−1X()train\\ue03ey()train(5.12)ThesystemofequationswhosesolutionisgivenbyEq.isknownasthe5.12normalequations.EvaluatingEq.constitutesasimplelearningalgorithm.5.12Foranexampleofthelinearregressionlearningalgorithminaction,seeFig..5.1Itisworthnotingthatthetermlinearregressionisoftenusedtorefertoaslightlymoresophisticatedmodelwithoneadditionalparameter—aninterceptterm.Inthismodelbˆy= w\\ue03ex+b(5.13)sothemappingfromparameterstopredictionsisstillalinearfunctionbutthemappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensiontoaﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikealine,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameterb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan109'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 124}, page_content='CHAPTER5.MACHINELEARNINGBASICSextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry11playstheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”whenreferringtoaﬃnefunctionsthroughoutthisbook.Theintercepttermbisoftencalledtheparameteroftheaﬃnetransfor-biasmation.Thisterminologyderivesfromthepointofviewthattheoutputofthetransformationisbiasedtowardbeingbintheabsenceofanyinput. Thistermisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimationalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,butitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequentsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithmdesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicatedlearningalgorithms.5.2Capacity,OverﬁttingandUnderﬁttingThecentralchallengeinmachinelearningisthatwemustperformwellonnew,previouslyunseeninputs—notjustthoseonwhichourmodelwastrained.Theabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.Typically,whentrainingamachinelearningmodel,wehaveaccesstoatrainingset,wecancomputesomeerrormeasureonthetrainingsetcalledthetrainingerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimplyanoptimizationproblem.Whatseparatesmachinelearningfromoptimizationisthatwewantthegeneralizationerrortesterror,alsocalledthe,tobelowaswell.Thegeneralizationerrorisdeﬁnedastheexpectedvalueoftheerroronanewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawnfromthedistributionofinputsweexpectthesystemtoencounterinpractice.Wetypicallyestimatethegeneralizationerrorofamachinelearningmodelbymeasuringitsperformanceonaofexamplesthatwerecollectedseparatelytestsetfromthetrainingset.Inourlinearregressionexample,wetrainedthemodelbyminimizingthetrainingerror,1m()train||X()trainwy−()train||22,(5.14)butweactuallycareaboutthetesterror,1m()test||X()testwy−()test||22.Howcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythetrainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.Ifthe110'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 125}, page_content='CHAPTER5.MACHINELEARNINGBASICStrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecando.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtestsetarecollected,thenwecanmakesomeprogress.Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasetscalledthedatageneratingprocess.WetypicallymakeasetofassumptionsknowncollectivelyastheTheseassumptionsarethattheexamplesi.i.d.assumptionsineachdatasetareindependentfromeachother,andthatthetrainsetandtestsetareidenticallydistributed,drawnfromthesameprobabilitydistributionaseachother.Thisassumptionallowsustodescribethedatageneratingprocesswithaprobabilitydistributionoverasingleexample.Thesamedistributionisthenusedtogenerateeverytrainexampleandeverytestexample.Wecallthatsharedunderlyingdistributionthedatageneratingdistribution,denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowustomathematicallystudytherelationshipbetweentrainingerrorandtesterror.Oneimmediateconnectionwecanobservebetweenthetrainingandtesterroristhattheexpectedtrainingerrorofarandomlyselectedmodelisequaltotheexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetestset.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameastheexpectedtestseterror,becausebothexpectationsareformedusingthesamedatasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthenameweassigntothedatasetwesample.Ofcourse, when weuseamachinelearningalgorithm, wedo notﬁxtheparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethetestset. Underthisprocess,theexpectedtesterrorisgreaterthanorequaltotheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachinelearningalgorithmwillperformareitsabilityto:1. Makethetrainingerrorsmall.2. Makethegapbetweentrainingandtesterrorsmall.Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:underﬁttingoverﬁttingand.Underﬁttingoccurswhenthemodelisnotabletoobtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhenthegapbetweenthetrainingerrorandtesterroristoolarge.Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyalteringitscapacity. Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof111'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 126}, page_content='CHAPTER5.MACHINELEARNINGBASICSfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Modelswithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdonotservethemwellonthetestset.Onewaytocontrolthecapacityofalearningalgorithmisbychoosingitshypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedtoselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthesetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralizelinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,initshypothesisspace.Doingsoincreasesthemodel’scapacity.Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwearealreadyfamiliar,withpredictionˆybwx.= +(5.15)Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,wecanlearnamodelthatisquadraticasafunctionof:xˆybw= +1xw+2x2.(5.16)Thoughthismodelimplementsaquadraticfunctionofitsinput,theoutputisstillalinearfunctionoftheparameters,sowecanstillusethenormalequationstotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxasadditionalfeatures,forexampletoobtainapolynomialofdegree9:ˆyb= +9\\ue058i=1wixi.(5.17)Machinelearningalgorithmswillgenerallyperformbestwhentheircapacityisappropriateinregardtothetruecomplexityofthetasktheyneedtoperformandtheamountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacityareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplextasks,butwhentheircapacityishigherthanneededtosolvethepresenttasktheymayoverﬁt.Fig.showsthisprincipleinaction.Wecomparealinear,quadraticand5.2degree-9predictorattemptingtoﬁtaproblemwherethetrueunderlyingfunctionisquadratic.Thelinearfunctionisunabletocapturethecurvatureinthetrueun-derlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableofrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitelymanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewehavemore112'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 127}, page_content='CHAPTER5.MACHINELEARNINGBASICSparametersthantrainingexamples.Wehavelittlechanceofchoosingasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.Inthisexample,thequadraticmodelisperfectlymatchedtothetruestructureofthetasksoitgeneralizeswelltonewdata.\\n\\ue078\\ue030\\ue079\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\n\\ue078\\ue030\\ue079\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\n\\ue078\\ue030\\ue079\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\nFigure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawasgeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministicallybyevaluatingaquadraticfunction.(Left)Alinearfunctionﬁttothedatasuﬀersfromunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata.()ACenterquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfromasigniﬁcantamountofoverﬁttingorunderﬁtting.()Apolynomialofdegree9ﬁttoRightthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolvetheunderdeterminednormalequations.Thesolutionpassesthroughallofthetrainingpointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrueunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetruefunctiondecreasesinthisarea.Sofarwehaveonlydescribedchangingamodel’scapacitybychangingthenumber ofinput features ithas (andsimultaneouslyadding newparametersassociatedwiththosefeatures).Thereareinfactmanywaysofchangingamodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.Themodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefromwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalledtherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebestfunctionwithinthisfamilyisaverydiﬃcultoptimizationproblem.Inpractice,thelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyonethatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchastheimperfection113'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 128}, page_content='CHAPTER5.MACHINELEARNINGBASICSoftheoptimizationalgorithm,meanthatthelearningalgorithm’seﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodelfamily.OurmodernideasaboutimprovingthegeneralizationofmachinelearningmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearlyasPtolemy.ManyearlyscholarsinvokeaprincipleofparsimonythatisnowmostwidelyknownasOccam’srazor(c. 1287-1347). Thisprinciplestatesthatamongcompetinghypothesesthatexplainknownobservationsequallywell,oneshouldchoosethe“simplest”one.Thisideawasformalizedandmademorepreciseinthe20thcenturybythefoundersofstatisticallearningtheory(VapnikandChervonenkis1971Vapnik1982Blumer1989Vapnik1995,;,;etal.,;,).Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,orVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.TheVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthereexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.Quantifyingthecapacityofthemodelallowsstatisticallearningtheorytomakequantitativepredictions.Themostimportantresultsinstatisticallearningtheoryshowthatthediscrepancybetweentrainingerrorandgeneralizationerrorisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbutshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,1971Vapnik1982Blumer1989Vapnik1995;,;etal.,;,).Theseboundsprovideintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyarerarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisinpartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequitediﬃculttodeterminethecapacityofdeeplearningalgorithms. Theproblemofdeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausetheeﬀectivecapacityislimitedbythecapabilitiesoftheoptimizationalgorithm,andwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimizationproblemsinvolvedindeeplearning.Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize(tohaveasmallgapbetweentrainingandtesterror)wemuststillchooseasuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,trainingerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodelcapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,generalizationerrorhasaU-shapedcurveasafunctionofmodelcapacity.ThisisillustratedinFig..5.3Toreachthemostextremecaseofarbitrarilyhighcapacity,weintroducetheconceptofnon-parametricmodels.Sofar, wehaveseenonlyparametric114'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 129}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n0OptimalCapacityCapacityErrorUnderﬁttingzoneOverﬁttingzoneGeneralizationgapTrainingerrorGeneralizationerror\\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterrorbehavediﬀerently. Attheleftendofthegraph,trainingerrorandgeneralizationerrorarebothhigh.Thisistheunderﬁttingregime. Asweincreasecapacity,trainingerrordecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁttingregime,wherecapacityistoolarge,abovetheoptimalcapacity.models,suchaslinearregression.Parametricmodelslearnafunctiondescribedbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.Non-parametricmodelshavenosuchlimitation.Sometimes,non-parametricmodelsarejusttheoreticalabstractions(suchasanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannotbeimplementedinpractice.However,wecanalsodesignpracticalnon-parametricmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexampleofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,whichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodelsimplystorestheXandyfromthetrainingset.Whenaskedtoclassifyatestpointx,themodellooksupthenearestentryinthetrainingsetandreturnstheassociatedregressiontarget.Inotherwords,ˆy=yiwherei=argmin||Xi,:−||x22.ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,suchaslearneddistancemetrics(,).IfthealgorithmisallowedGoldbergeretal.2005tobreaktiesbyaveragingtheyivaluesforallXi,:thataretiedfornearest,thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(whichmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerentoutputs)onanyregressiondataset.Finally,wecanalsocreateanon-parametriclearningalgorithmbywrappingaparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber115'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 130}, page_content='CHAPTER5.MACHINELEARNINGBASICSofparametersasneeded.Forexample,wecouldimagineanouterloopoflearningthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofapolynomialexpansionoftheinput.Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistributionthatgeneratesthedata.Evensuchamodelwillstillincursomeerroronmanyproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecaseofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,orymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthoseincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetruedistributioniscalledthep,y(x)Bayeserror.Trainingandgeneralizationerrorvaryasthesizeofthetrainingsetvaries.Expectedgeneralizationerrorcanneverincreaseasthenumberoftrainingexamplesincreases.Fornon-parametricmodels,moredatayieldsbettergeneralizationuntilthebestpossibleerrorisachieved.AnyﬁxedparametricmodelwithlessthanoptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.SeeFig.foranillustration.Notethatitispossibleforthemodeltohaveoptimal5.4capacityandyetstillhavealargegapbetweentrainingandgeneralizationerror.Inthissituation,wemaybeabletoreducethisgapbygatheringmoretrainingexamples.5.2.1TheNoFreeLunchTheoremLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfromaﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesoflogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,isnotlogicallyvalid. Tologicallyinferaruledescribingeverymemberofaset,onemusthaveinformationabouteverymemberofthatset.Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,ratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machinelearningpromisestoﬁndrulesthatareprobablymostcorrectaboutmembersofthesettheyconcern.Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofreelunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedoverallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthesameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,insomesense,nomachinelearningalgorithmisuniversallyanybetterthananyother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverageperformance(overallpossibletasks)asmerelypredictingthateverypointbelongstothesameclass.116'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 131}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue032\\ue02e\\ue035\\ue033\\ue02e\\ue030\\ue033\\ue02e\\ue035\\ue045\\ue072\\ue072\\ue06f\\ue072\\ue020\\ue028\\ue04d\\ue053\\ue045\\ue029\\ue042\\ue061\\ue079\\ue065\\ue073\\ue020\\ue065\\ue072\\ue072\\ue06f\\ue072\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029\\n\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue030\\ue035\\ue031\\ue030\\ue031\\ue035\\ue032\\ue030\\ue04f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue020\\ue028\\ue070\\ue06f\\ue06c\\ue079\\ue06e\\ue06f\\ue06d\\ue069\\ue061\\ue06c\\ue020\\ue064\\ue065\\ue067\\ue072\\ue065\\ue065\\ue029Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellasontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedonaddingmoderateamountofnoisetoadegree5polynomial,generatedasingletestset,andthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40diﬀerenttrainingsetsinordertoploterrorbarsshowing95%conﬁdenceintervals.(Top)TheMSEonthetrainandtestsetfortwodiﬀerentmodels: aquadraticmodel,andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.Forthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadraticmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotestoahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.ThetrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithmtomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,thetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoatleasttheBayeserror.Asthetrainingsetsizeincreases,theoptimalcapacity(Bottom)(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimalcapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.117'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 132}, page_content='CHAPTER5.MACHINELEARNINGBASICSFortunately,theseresultsholdonlywhenweaverageoverpossibledataallgeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobabilitydistributionsweencounterinreal-worldapplications,thenwecandesignlearningalgorithmsthatperformwellonthesedistributions.Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversallearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalistounderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAIagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellondatadrawnfromthekindsofdatageneratingdistributionswecareabout.5.2.2RegularizationThenofreelunchtheoremimpliesthatwemustdesignourmachinelearningalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetofpreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwiththelearningproblemsweaskthealgorithmtosolve,itperformsbetter.Sofar,theonlymethodofmodifyingalearningalgorithmwehavediscussedistoincreaseordecreasethemodel’scapacitybyaddingorremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithmisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthedegreeofapolynomialforaregressionproblem.Theviewwehavedescribedsofarisoversimpliﬁed.Thebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewemakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentityofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.Theselinearfunctionscanbeveryusefulforproblemswheretherelationshipbetweeninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblemsthatbehaveinaverynonlinearfashion.Forexample,linearregressionwouldnotperformverywellifwetriedtouseittopredictsin(x)fromx.Wecanthuscontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionsweallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthesefunctions.Wecanalsogivealearningalgorithmapreferenceforonesolutioninitshypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butoneispreferred. Theunpreferredsolutionbechosenonlyifitﬁtsthetrainingdatasigniﬁcantlybetterthanthepreferredsolution.Forexample, wecanmodifythetrainingcriterionforlinearregressiontoincludeweightdecay.Toperformlinearregressionwithweightdecay,weminimize118'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 133}, page_content='CHAPTER5.MACHINELEARNINGBASICSasumcomprisingboththemeansquarederroronthetrainingandacriterionJ(w)thatexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,J() = wMSEtrain+λw\\ue03ew,(5.18)whereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreferenceforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcestheweightstobecomesmaller.MinimizingJ(w)resultsinachoiceofweightsthatmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesussolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asanexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweightdecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvaluesof.SeeFig.fortheresults.λ5.5\\n\\ue078\\ue030\\ue079\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\ue028\\ue045\\ue078\\ue063\\ue065\\ue073\\ue073\\ue069\\ue076\\ue065\\ue020\\ue0b8\\ue029\\n\\ue078\\ue030\\ue079\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue077\\ue065\\ue069\\ue067\\ue068\\ue074\\ue020\\ue064\\ue065\\ue063\\ue061\\ue079\\ue028\\ue04d\\ue065\\ue064\\ue069\\ue075\\ue06d\\ue020\\ue0b8\\ue029\\n\\ue078\\ue030\\ue079\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\ue028\\ue030\\ue029\\ue0b8\\ue021\\nFigure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingsetfromFig..Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9.5.2Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.(Left)Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeatall. Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction. ()WithaCentermediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape.λEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicatedshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmallercoeﬃcients.()Withweightdecayapproachingzero(i.e.,usingtheMoore-PenroseRightpseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),thedegree-9polynomialoverﬁtssigniﬁcantly,aswesawinFig..5.2Moregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)byaddingapenaltycalledaregularizertothecostfunction. Inthecaseofweightdecay,theregularizerisΩ(w) =w\\ue03ew.InChapter,wewillseethatmanyother7119'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 134}, page_content='CHAPTER5.MACHINELEARNINGBASICSregularizersarepossible.Expressingpreferencesforonefunctionoveranotherisamoregeneralwayofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthehypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceasexpressinganinﬁnitelystrongpreferenceagainstthatfunction.Inourweightdecayexample,weexpressedourpreferenceforlinearfunctionsdeﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionweminimize.Thereare many otherwaysof expressingpreferencesfor diﬀerentsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproachesareknownasregularization.Regularizationisanymodiﬁcationwemaketoalearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachinelearning,rivaledinitsimportanceonlybyoptimization.Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachinelearningalgorithm,and,inparticular,nobestformofregularization.Insteadwemustchooseaformofregularizationthatiswell-suitedtotheparticulartaskwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookinparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasksthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeformsofregularization.5.3HyperparametersandValidationSetsMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrolthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparameters.Thevaluesofhyperparametersarenotadaptedbythelearningalgorithmitself(thoughwecandesignanestedlearningprocedurewhereonelearningalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).InthepolynomialregressionexamplewesawinFig.,thereisasinglehyper-5.2parameter:thedegreeofthepolynomial,whichactsasacapacityhyperparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanotherexampleofahyperparameter.Sometimesasettingischosentobeahyperparameterthatthelearningalgo-rithmdoesnotlearnbecauseitisdiﬃculttooptimize. Morefrequently,wedonotlearnthehyperparameterbecauseitisnotappropriatetolearnthathyper-parameteronthetrainingset.Thisappliestoallhyperparametersthatcontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameterswouldalways120'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 135}, page_content='CHAPTER5.MACHINELEARNINGBASICSchoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refertoFig.).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher5.3degreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalowerdegreepolynomialandapositiveweightdecaysetting.Tosolvethisproblem,weneedaofexamplesthatthetrainingvalidationsetalgorithmdoesnotobserve.Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfromthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralizationerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthetestexamplesarenotusedinanywaytomakechoicesaboutthemodel,includingitshyperparameters. Forthisreason,noexamplefromthetestsetcanbeusedinthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthetrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.Oneofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidationset,usedtoestimatethegeneralizationerrorduringoraftertraining,allowingforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedtolearntheparametersisstilltypicallycalledthetrainingset,eventhoughthismaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.Thesubsetofdatausedtoguidetheselectionofhyperparametersiscalledthevalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters,thevalidationseterrorwillunderestimatethegeneralizationerror,thoughtypicallybyasmalleramountthanthetrainingerror.Afterallhyperparameteroptimizationiscomplete,thegeneralizationerrormaybeestimatedusingthetestset.Inpractice, whenthesametestsethasbeenusedrepeatedlytoevaluateperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsideralltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-the-artperformanceonthattestset,weenduphavingoptimisticevaluationswiththetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthetrueﬁeldperformanceofatrainedsystem.Thankfully,thecommunitytendstomoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.5.3.1Cross-ValidationDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematicifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertaintyaroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithmAworksbetterthanalgorithmonthegiventask.B121'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 136}, page_content='CHAPTER5.MACHINELEARNINGBASICSWhenthedatasethashundredsofthousandsofexamplesormore,thisisnotaseriousissue.Whenthedatasetistoosmall,therearealternativeprocedures,whichallowonetousealloftheexamplesintheestimationofthemeantesterror,atthepriceofincreasedcomputationalcost.Theseproceduresarebasedontheideaofrepeatingthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplitsoftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validationprocedure,showninAlgorithm,inwhichapartition5.1ofthedatasetisformedbysplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimatedbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthedataisusedasthetestsetandtherestofthedataisusedasthetrainingset.Oneproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverageerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypicallyused.5.4Estimators,BiasandVarianceTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachinelearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.Foundationalconceptssuchasparameterestimation,biasandvarianceareusefultoformallycharacterizenotionsofgeneralization,underﬁttingandoverﬁtting.5.4.1PointEstimationPointestimationistheattempttoprovidethesingle“best”predictionofsomequantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameteroravectorofparametersinsomeparametricmodel,suchastheweightsinourlinearregressionexampleinSec.,butitcanalsobeawholefunction.5.1.4Inordertodistinguishestimatesofparametersfromtheirtruevalue, ourconventionwillbetodenoteapointestimateofaparameterbyθˆθ.Let{x(1),...,x()m}beasetofmindependentandidenticallydistributed(i.i.d.)datapoints.Apointestimatorstatisticorisanyfunctionofthedata:ˆθm= (gx(1),...,x()m).(5.19)Thedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrueθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.Thisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofanestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,122'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 137}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nAlgorithm5.1Thek-foldcross-validationalgorithm.ItcanbeusedtoestimategeneralizationerrorofalearningalgorithmAwhenthegivendatasetDistoosmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationofgeneralizationerror,becausethemeanofalossLonasmalltestsetmayhavetoohighvariance.ThedatasetDcontainsaselementstheabstractexamplesz()i(forthei-thexample),whichcouldstandforan(input,target)pairz()i= (x()i,y()i)inthecaseofsupervisedlearning,orforjustaninputz()i=x()iinthecaseofunsupervisedlearning. ThealgorithmreturnsthevectoroferrorseforeachexampleinD,whosemeanistheestimatedgeneralizationerror. Theerrorsonindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean(Eq.).Whiletheseconﬁdenceintervalsarenotwell-justiﬁedaftertheuseof5.47cross-validation,itisstillcommonpracticetousethemtodeclarethatalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerrorofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm.BDeﬁneKFoldXV():D,A,L,kRequire:D,thegivendataset,withelementsz()iRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetasinputandoutputsalearnedfunctionRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfandanexamplez()i∈∈DtoascalarRRequire:k,thenumberoffoldsSplitintomutuallyexclusivesubsetsDkDi,whoseunionis.Dfordoikfromto1fi= (ADD\\\\i)forz()jinDidoej= (Lfi,z()j)endforendforReturne\\n123'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 138}, page_content='CHAPTER5.MACHINELEARNINGBASICSagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthatgeneratedthetrainingdata.Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassumethatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimateˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,anyfunctionofthedataisrandom.Thereforeˆθisarandomvariable.Pointestimationcanalsorefertotheestimationoftherelationshipbetweeninputandtargetvariables.Werefertothesetypesofpointestimatesasfunctionestimators.FunctionEstimationAswementionedabove,sometimesweareinterestedinperformingfunctionestimation(orfunctionapproximation).Herewearetryingtopredictavariableygivenaninputvectorx.Weassumethatthereisafunctionf(x)thatdescribestheapproximaterelationshipbetweenyandx.Forexample,wemayassumethaty=f(x)+\\ue00f,where\\ue00fstandsforthepartofythatisnotpredictablefromx. Infunctionestimation,weareinterestedinapproximatingfwithamodelorestimateˆf.Functionestimationisreallyjustthesameasestimatingaparameterθ;thefunctionestimatorˆfissimplyapointestimatorinfunctionspace.Thelinearregressionexample(discussedaboveinSec.)and5.1.4thepolynomialregressionexample(discussedinSec.)arebothexamplesof5.2scenariosthatmaybeinterpretedeitherasestimatingaparameterworestimatingafunctionˆfymappingfromtox.Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsanddiscusswhattheytellusabouttheseestimators.5.4.2BiasThebiasofanestimatorisdeﬁnedas:bias(ˆθm) = (Eˆθm)−θ(5.20)wheretheexpectationisoverthedata(seenassamplesfromarandomvariable)andθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistribution.Anestimatorˆθmissaidtobeunbiasedifbias(ˆθm) =0,whichimpliesthatE(ˆθm) =θ.Anestimatorˆθmissaidtobeasymptoticallyunbiasediflimm→∞bias(ˆθm) =0,whichimpliesthatlimm→∞E(ˆθm) = θ.Example:BernoulliDistributionConsiderasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-124'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 139}, page_content='CHAPTER5.MACHINELEARNINGBASICSbutionwithmean:θPx(()i;) = θθx()i(1)−θ(1−x()i).(5.21)Acommonestimatorfortheθparameterofthisdistributionisthemeanofthetrainingsamples:ˆθm=1mm\\ue058i=1x()i.(5.22)Todeterminewhetherthisestimatorisbiased,wecansubstituteEq.intoEq.5.225.20:bias(ˆθm) = [Eˆθm]−θ(5.23)= E\\ue0221mm\\ue058i=1x()i\\ue023−θ(5.24)=1mm\\ue058i=1E\\ue068x()i\\ue069−θ(5.25)=1mm\\ue058i=11\\ue058x()i=0\\ue010x()iθx()i(1)−θ(1−x()i)\\ue011−θ(5.26)=1mm\\ue058i=1()θ−θ(5.27)= = 0θθ−(5.28)Sincebias(ˆθ) = 0,wesaythatourestimatorˆθisunbiased.Example:GaussianDistributionEstimatoroftheMeanNow,considerasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaGaussiandistributionp(x()i) =N(x()i;µ,σ2),wherei∈{1,...,m}.RecallthattheGaussianprobabilitydensityfunctionisgivenbypx(()i;µ,σ2) =1√2πσ2exp\\ue020−12(x()i−µ)2σ2\\ue021.(5.29)AcommonestimatoroftheGaussianmeanparameterisknownasthesamplemean:ˆµm=1mm\\ue058i=1x()i(5.30)125'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 140}, page_content='CHAPTER5.MACHINELEARNINGBASICSTodeterminethebiasofthesamplemean,weareagaininterestedincalculatingitsexpectation:bias(ˆµm) = [ˆEµm]−µ(5.31)= E\\ue0221mm\\ue058i=1x()i\\ue023−µ(5.32)=\\ue0201mm\\ue058i=1E\\ue068x()i\\ue069\\ue021−µ(5.33)=\\ue0201mm\\ue058i=1µ\\ue021−µ(5.34)= = 0µµ−(5.35)ThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmeanparameter.Example:EstimatorsoftheVarianceofaGaussianDistributionAsanexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofaGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.Theﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:ˆσ2m=1mm\\ue058i=1\\ue010x()i−ˆµm\\ue0112,(5.36)whereˆµmisthesamplemean,deﬁnedabove.Moreformally,weareinterestedincomputingbias(ˆσ2m) = [ˆEσ2m]−σ2(5.37)WebeginbyevaluatingthetermE[ˆσ2m]:E[ˆσ2m] =E\\ue0221mm\\ue058i=1\\ue010x()i−ˆµm\\ue0112\\ue023(5.38)=m−1mσ2(5.39)ReturningtoEq.,weconcludethatthebiasof5.37ˆσ2mis−σ2/m.Therefore,thesamplevarianceisabiasedestimator.126'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 141}, page_content='CHAPTER5.MACHINELEARNINGBASICSTheunbiasedsamplevarianceestimator˜σ2m=1m−1m\\ue058i=1\\ue010x()i−ˆµm\\ue0112(5.40)providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.Thatis,weﬁndthatE[˜σ2m] = σ2:E[˜σ2m] = E\\ue0221m−1m\\ue058i=1\\ue010x()i−ˆµm\\ue0112\\ue023(5.41)=mm−1E[ˆσ2m](5.42)=mm−1\\ue012m−1mσ2\\ue013(5.43)= σ2.(5.44)Wehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiasedestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswewillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.5.4.3VarianceandStandardErrorAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuchweexpectittovaryasafunctionofthedatasample.Justaswecomputedtheexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.ThevarianceofanestimatorissimplythevarianceVar(ˆθ)(5.45)wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthevarianceiscalledthestandarderror,denotedSE(ˆθ).Thevarianceorthestandarderrorofanestimatorprovidesameasureofhowwewouldexpecttheestimatewecomputefromdatatovaryasweindependentlyresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswemightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelativelylowvariance.Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimateofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhaveobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave127'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 142}, page_content='CHAPTER5.MACHINELEARNINGBASICSbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceoferrorthatwewanttoquantify.ThestandarderrorofthemeanisgivenbySE(ˆµm) =\\ue076\\ue075\\ue075\\ue074Var[1mm\\ue058i=1x()i] =σ√m,(5.46)whereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoftenestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootofthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevarianceprovideanunbiasedestimateofthestandarddeviation. Bothapproachestendtounderestimatethetruestandarddeviation,butarestillusedinpractice.Thesquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.Forlarge,theapproximationisquitereasonable.mThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.Weoftenestimatethegeneralizationerrorbycomputingthesamplemeanoftheerroronthetestset.Thenumberofexamplesinthetestsetdeterminestheaccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,whichtellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectationfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredonthemeanisˆµmis(ˆµm−196SE(ˆ.µm)ˆ,µm+196SE(ˆ.µm)),(5.47)underthenormaldistributionwithmeanˆµmandvarianceSE(ˆµm)2.Inmachinelearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithmBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAislessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithmB.Example: BernoulliDistributionWeonceagainconsiderasetofsamples{x(1),...,x()m}drawnindependentlyandidenticallyfromaBernoullidistribution(recallP(x()i;θ) =θx()i(1−θ)(1−x()i)).Thistimeweareinterestedincomputingthevarianceoftheestimatorˆθm=1m\\ue050mi=1x()i.Var\\ue010ˆθm\\ue011= Var\\ue0201mm\\ue058i=1x()i\\ue021(5.48)128'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 143}, page_content='CHAPTER5.MACHINELEARNINGBASICS=1m2m\\ue058i=1Var\\ue010x()i\\ue011(5.49)=1m2m\\ue058i=1θθ(1−)(5.50)=1m2mθθ(1−)(5.51)=1mθθ(1−)(5.52)Thevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamplesinthedataset.Thisisacommonpropertyofpopularestimatorsthatwewillreturntowhenwediscussconsistency(seeSec.).5.4.55.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquaredErrorBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Biasmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.Varianceontheotherhand,providesameasureofthedeviationfromtheexpectedestimatorvaluethatanyparticularsamplingofthedataislikelytocause.Whathappenswhenwearegivenachoicebetweentwoestimators,onewithmorebiasandonewithmorevariance?Howdowechoosebetweenthem?Forexample,imaginethatweareinterestedinapproximatingthefunctionshowninFig.andweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand5.2onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation.Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-natively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:MSE = [(Eˆθm−θ)2](5.53)= Bias(ˆθm)2+Var(ˆθm)(5.54)TheMSEmeasurestheoverallexpecteddeviation—inasquarederrorsense—betweentheestimatorandthetruevalueoftheparameterθ.AsisclearfromEq.,evaluatingtheMSEincorporatesboththebiasandthevariance.Desirable5.54estimatorsarethosewithsmallMSEandtheseareestimatorsthatmanagetokeepboththeirbiasandvariancesomewhatincheck.Therelationshipbetweenbiasandvarianceistightlylinkedtothemachinelearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-129'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 144}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nCapacityBiasGeneralizationerrorVarianceOptimalcapacityOverﬁtting zoneUnderﬁtting zone\\nFigure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(boldcurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁttingwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationshipissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedinSec.andFig..5.25.3eralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningfulcomponentsofgeneralizationerror),increasingcapacitytendstoincreasevarianceanddecreasebias.ThisisillustratedinFig.,whereweseeagaintheU-shaped5.6curveofgeneralizationerrorasafunctionofcapacity.5.4.5ConsistencySofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetofﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorastheamountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumberofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetruevalueofthecorrespondingparameters.Moreformally,wewouldlikethatlimm→∞ˆθmp→θ.(5.55)Thesymbolp→meansthattheconvergenceisinprobability,i.e.forany\\ue00f>0,P(|ˆθm−|θ>\\ue00f)→0asm→∞.The conditiondescribed by Eq.is5.55knownas consistency.Itissometimesreferredto asweakconsistency, withstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almostsure130'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 145}, page_content='CHAPTER5.MACHINELEARNINGBASICSconvergenceofasequenceofrandomvariablesx(1),x(2),...toavaluexoccurswhenp(limm→∞x()m= ) = 1x.Consistencyensuresthat thebias inducedbytheestimatorisassuredtodiminishasthenumberofdataexamplesgrows.However,thereverseisnottrue—asymptoticunbiasednessdoesnotimplyconsistency.Forexample,considerestimatingthemeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsistingofmsamples:{x(1),...,x()m}.Wecouldusetheﬁrstsamplex(1)ofthedatasetasanunbiasedestimator:ˆθ=x(1). Inthatcase,E(ˆθm)=θsotheestimatorisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,impliesthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistentestimatorasitisthecasethatnotˆθm→→∞θmas.5.5MaximumLikelihoodEstimationPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzedtheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessingthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasandvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁcfunctionsthataregoodestimatorsfordiﬀerentmodels.Themostcommonsuchprincipleisthemaximumlikelihoodprinciple.ConsiderasetofmexamplesX={x(1),...,x()m}drawnindependentlyfromthetruebutunknowndatageneratingdistributionpdata()x.Letpmodel(x;θ)beaparametricfamilyofprobabilitydistributionsoverthesamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationxtoarealnumberestimatingthetrueprobabilitypdata()x.ThemaximumlikelihoodestimatorforisthendeﬁnedasθθML= argmaxθpmodel(;)Xθ(5.56)= argmaxθm\\ue059i=1pmodel(x()i;)θ(5.57)Thisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenientbutequivalentoptimizationproblem,weobservethattakingthelogarithmofthelikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct131'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 146}, page_content='CHAPTER5.MACHINELEARNINGBASICSintoasum:θML= argmaxθm\\ue058i=1logpmodel(x()i;)θ.(5.58)Becausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecandividebymtoobtainaversionofthecriterionthatisexpressedasanexpectationwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:θML= argmaxθEx∼ˆpdatalogpmodel(;)xθ.(5.59)Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizingthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetrainingsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwomeasuredbytheKLdivergence.TheKLdivergenceisgivenbyDKL(ˆpdata\\ue06bpmodel) = Ex∼ˆpdata[log ˆpdata()logx−pmodel()]x.(5.60)Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthemodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,weneedonlyminimize−Ex∼ˆpdata[logpmodel()]x(5.61)whichisofcoursethesameasthemaximizationinEq..5.59MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-entropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”toidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacrossentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthemodel.Forexample,meansquarederroristhecross-entropybetweentheempiricaldistributionandaGaussianmodel.Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-tributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatchthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothisdistribution.WhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthelikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctionsarediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.Maximumlikelihoodthusbecomesminimizationofthenegativelog-likelihood(NLL),orequivalently,minimizationofthecrossentropy.TheperspectiveofmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscasebecausetheKLdivergencehasaknownminimumvalueofzero.Thenegativelog-likelihoodcanactuallybecomenegativewhenisreal-valued.x132'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 147}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.5.1ConditionalLog-LikelihoodandMeanSquaredErrorThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhereourgoalistoestimateaconditionalprobabilityP(yx|;θ)inordertopredictygivenx.Thisisactuallythemostcommonsituationbecauseitformsthebasisformostsupervisedlearning.IfXrepresentsallourinputsandYallourobservedtargets,thentheconditionalmaximumlikelihoodestimatorisθML= argmaxθP.(;)YX|θ(5.62)Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedintoθML= argmaxθm\\ue058i=1log(Py()i|x()i;)θ.(5.63)Example:LinearRegressionasMaximumLikelihoodLinearregression,introducedearlierinSec.,maybejustiﬁedasamaximumlikelihoodprocedure.5.1.4Previously,wemotivatedlinearregressionasanalgorithmthatlearnstotakeaninputxandproduceanoutputvalueˆy. Themappingfromxtoˆyischosentominimizemeansquarederror,acriterionthatweintroducedmoreorlessarbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximumlikelihoodestimation.Insteadofproducingasinglepredictionˆy,wenowthinkofthemodelasproducingaconditionaldistributionp(y|x).Wecanimaginethatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexampleswiththesameinputvaluexbutdiﬀerentvaluesofy.Thegoalofthelearningalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvaluesthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithmweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunctionˆy(x;w)givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethatthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthischoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimationproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincetheexamplesareassumedtobei.i.d.,theconditionallog-likelihood(Eq.)isgivenby5.63m\\ue058i=1log(py()i|x()i;)θ(5.64)=log−mσ−m2log(2)π−m\\ue058i=1|ˆy()i−y()i||22σ2(5.65)133'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 148}, page_content='CHAPTER5.MACHINELEARNINGBASICSwhereˆy()iistheoutputofthelinearregressiononthei-thinputx()iandmisthenumberofthetrainingexamples.Comparingthelog-likelihoodwiththemeansquarederror,MSEtrain=1mm\\ue058i=1||ˆy()i−y()i||2,(5.66)weimmediatelyseethatmaximizingthelog-likelihoodwithrespecttowyieldsthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.ThisjustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswewillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.5.5.2PropertiesofMaximumLikelihoodThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshowntobethebestestimatorasymptotically,asthenumberofexamplesm→∞,intermsofitsrateofconvergenceasincreases.mUnderappropriateconditions,maximumlikelihoodestimatorhasthepropertyofconsistency(seeSec.above),meaningthatasthenumberoftraining5.4.5examplesapproachesinﬁnity,themaximumlikelihoodestimateofaparameterconvergestothetruevalueoftheparameter.Theseconditionsare:•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).Otherwise,noestimatorcanrecoverpdata.•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeabletodeterminewhichvalueofwasusedbythedatageneratingprocessing.θThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestimator,manyofwhichsharethepropertyofbeingconsistentestimators.However,consis-tentestimatorscandiﬀerintheir,meaningthatoneconsistentstatisticeﬃciencyestimatormayobtainlowergeneralizationerrorforaﬁxednumberofsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelofgeneralizationerror.Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinearregression)whereourgoalistoestimatethevalueofaparameter(andassumingitispossibletoidentifythetrueparameter),notthevalueofafunction.Awaytomeasurehowclosewearetothetrueparameterisbytheexpectedmeansquarederror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter134'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 149}, page_content='CHAPTER5.MACHINELEARNINGBASICSvalues,wheretheexpectationisovermtrainingsamplesfromthedatageneratingdistribution.Thatparametricmeansquarederrordecreasesasmincreases,andformlarge,theCramér-Raolowerbound(,;,)showsthatnoRao1945Cramér1946consistentestimatorhasalowermeansquarederrorthanthemaximumlikelihoodestimator.Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoftenconsideredthepreferredestimatortouseformachinelearning.Whenthenumberofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategiessuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihoodthathaslessvariancewhentrainingdataislimited.5.6BayesianStatisticsSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimatingasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatoneestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakingaprediction.Thelatteristhedomainof.BayesianstatisticsAsdiscussedinSec.,thefrequentistperspectiveisthatthetrueparameter5.4.1valueθisﬁxedbutunknown,whilethepointestimateˆθisarandomvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).TheBayesianperspectiveonstatisticsisquitediﬀerent. TheBayesianusesprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetisdirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθisunknownoruncertainandthusisrepresentedasarandomvariable.Beforeobservingthedata,werepresentourknowledgeofθusingthepriorprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).Gen-erally,themachinelearningpractitionerselectsapriordistributionthatisquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthevalueofθbeforeobservinganydata.Forexample,onemightassumethataprioriθliesinsomeﬁniterangeorvolume,withauniformdistribution.Manypriorsinsteadreﬂectapreferencefor“simpler”solutions(suchassmallermagnitudecoeﬃcients,orafunctionthatisclosertobeingconstant).Nowconsiderthatwehaveasetofdatasamples{x(1),...,x()m}.Wecanrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihoodpx((1),...,x()m|θ)withthepriorviaBayes’rule:px(θ|(1),...,x()m) =px((1),...,x()m|θθ)(p)px((1),...,x()m)(5.67)135'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 150}, page_content='CHAPTER5.MACHINELEARNINGBASICSInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasarelativelyuniformorGaussiandistributionwithhighentropy,andtheobservationofthedatausuallycausestheposteriortoloseentropyandconcentratearoundafewhighlylikelyvaluesoftheparameters.Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwoimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakespredictionsusingapointestimateofθ,theBayesianapproachistomakepredictionsusingafulldistributionoverθ.Forexample,afterobservingmexamples,thepredicteddistributionoverthenextdatasample,x(+1)m,isgivenbypx((+1)m|x(1),...,x()m) =\\ue05apx((+1)m||θθ)(px(1),...,x()m)d.θ(5.68)Hereeachvalueofθwithpositiveprobabilitydensitycontributestothepredictionofthenextexample,withthecontributionweightedbytheposteriordensityitself.Afterhavingobserved{x(1),...,x()m},ifwearestillquiteuncertainaboutthevalueofθ,thenthisuncertaintyisincorporateddirectlyintoanypredictionswemightmake.InSec.,wediscussedhowthefrequentistapproachaddressestheuncertainty5.4inagivenpointestimateofθbyevaluatingitsvariance.Thevarianceoftheestimatorisanassessmentofhowtheestimatemightchangewithalternativesamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodealwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendstoprotectwellagainstoverﬁtting.Thisintegralisofcoursejustanapplicationofthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethefrequentistmachineryforconstructinganestimatorisbasedontheratheradhocdecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepointestimate.ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimationandthemaximumlikelihoodapproachisduetothecontributionoftheBayesianpriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensitytowardsregionsoftheparameterspacethatarepreferred.Inpractice,aprioritheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehumanjudgmentimpactingthepredictions.Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdataisavailable,buttypicallysuﬀerfromhighcomputationalcostwhenthenumberoftrainingexamplesislarge.136'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 151}, page_content='CHAPTER5.MACHINELEARNINGBASICSExample:BayesianLinearRegressionHereweconsidertheBayesianesti-mationapproachtolearningthelinearregressionparameters.Inlinearregression,welearnalinearmappingfromaninputvectorx∈Rntopredictthevalueofascalar.Thepredictionisparametrizedbythevectory∈Rw∈Rn:ˆy= w\\ue03ex.(5.69)Givenasetofmtrainingsamples(X()train,y()train),wecanexpressthepredictionofovertheentiretrainingsetas:yˆy()train= X()trainw.(5.70)ExpressedasaGaussianconditionaldistributionony()train,wehavep(y()train|X()train,wy) = (N()train;X()trainwI,)(5.71)∝exp\\ue012−12(y()train−X()trainw)\\ue03e(y()train−X()trainw)\\ue013,(5.72)wherewefollowthestandardMSEformulationinassumingthattheGaussianvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto(X()train,y()train)()assimplyXy,.Todeterminetheposteriordistributionoverthemodelparametervectorw,weﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebeliefaboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnaturaltoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewetypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertaintyaboutθ. Forreal-valuedparametersitiscommontouseaGaussianasapriordistribution:p() = (;wNwµ0,Λ0) exp∝\\ue012−12(wµ−0)\\ue03eΛ−10(wµ−0)\\ue013(5.73)whereµ0andΛ0arethepriordistributionmeanvectorandcovariancematrixrespectively.1Withthepriorthusspeciﬁed,wecannowproceedindeterminingtheposteriordistributionoverthemodelparameters.p,p,p(wX|y) ∝(yX|w)()w(5.74)1Unlessthereisareasontoassumeaparticularcovariancestructure,wetypicallyassumeadiagonalcovariancematrixΛ0= diag(λ0).137'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 152}, page_content='CHAPTER5.MACHINELEARNINGBASICS∝exp\\ue012−12()yXw−\\ue03e()yXw−\\ue013exp\\ue012−12(wµ−0)\\ue03eΛ−10(wµ−0)\\ue013(5.75)∝exp\\ue012−12\\ue010−2y\\ue03eXww+\\ue03eX\\ue03eXww+\\ue03eΛ−10wµ−2\\ue03e0Λ−10w\\ue011\\ue013.(5.76)WenowdeﬁneΛm=\\ue000X\\ue03eX+Λ−10\\ue001−1andµm= Λm\\ue000X\\ue03ey+Λ−10µ0\\ue001.Usingthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussiandistribution:p,(wX|y) exp∝\\ue012−12(wµ−m)\\ue03eΛ−1m(wµ−m)+12µ\\ue03emΛ−1mµm\\ue013(5.77)∝exp\\ue012−12(wµ−m)\\ue03eΛ−1m(wµ−m)\\ue013.(5.78)Alltermsthatdonotincludetheparametervectorwhavebeenomitted;theyareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1Eq.showshowtonormalizeamultivariateGaussiandistribution.3.23ExaminingthisposteriordistributionallowsustogainsomeintuitionfortheeﬀectofBayesianinference.Inmostsituations,wesetµ0to0.IfwesetΛ0=1αI,thenµmgivesthesameestimateofwasdoesfrequentistlinearregressionwithaweightdecaypenaltyofαw\\ue03ew.OnediﬀerenceisthattheBayesianestimateisundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearningprocesswithaninﬁnitelywideprioronw.ThemoreimportantdiﬀerenceisthattheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthediﬀerentvaluesofare,ratherthanprovidingonlytheestimatewµm.5.6.1Maximum(MAP)EstimationAPosterioriWhilethemostprincipledapproachistomakepredictionsusingthefullBayesianposteriordistributionovertheparameterθ,itisstilloftendesirabletohaveasinglepointestimate. OnecommonreasonfordesiringapointestimateisthatmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsareintractable,andapointestimateoﬀersatractableapproximation.Ratherthansimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeofthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoiceofthepointestimate. Onerationalwaytodothisistochoosethemaximumaposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointofmaximal138'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 153}, page_content='CHAPTER5.MACHINELEARNINGBASICSposteriorprobability(ormaximalprobabilitydensityinthemorecommoncaseofcontinuous):θθMAP= argmaxθp() = argmaxθx|θlog()+log()pxθ|pθ.(5.79)Werecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-likelihoodterm,and,correspondingtothepriordistribution.log()pθAsanexample,consideralinearregressionmodelwithaGaussianpriorontheweightsw.IfthispriorisgivenbyN(w;0,1λI2),thenthelog-priorterminEq.5.79isproportionaltothefamiliarλw\\ue03ewweightdecaypenalty,plusatermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAPBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweightdecay.AswithfullBayesianinference,MAPBayesianinferencehastheadvantageofleveraginginformationthatisbroughtbythepriorandcannotbefoundinthetrainingdata.ThisadditionalinformationhelpstoreducethevarianceintheMAPpointestimate(incomparisontotheMLestimate).However,itdoessoatthepriceofincreasedbias.Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearningregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsofaddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).NotallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,someregularizertermsmaynotbethelogarithmofaprobabilitydistribution.Otherregularizationtermsdependonthedata,whichofcourseapriorprobabilitydistributionisnotallowedtodo.MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicatedyetinterpretableregularizationterms.Forexample,amorecomplicatedpenaltytermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussiandistribution,astheprior(NowlanandHinton1992,).5.7SupervisedLearningAlgorithmsRecallfromSec.thatsupervisedlearningalgorithmsare,roughlyspeaking,5.1.3learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givenatrainingsetofexamplesofinputsxandoutputsy. Inmanycasestheoutputsymaybediﬃculttocollectautomaticallyandmustbeprovidedbyahuman“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswerecollectedautomatically.139'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 154}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.7.1ProbabilisticSupervisedLearningMost supervised learning algorithms inthis book are based onestimating aprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximumlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamilyofdistributions.py(|xθ;)Wehavealreadyseenthatlinearregressioncorrespondstothefamilypyy(|Nxθ;) = (;θ\\ue03exI,).(5.80)Wecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁningadiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0andclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.Theprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovaluesmustaddupto1.Thenormaldistributionoverreal-valuednumbersthatweusedforlinearregressionisparametrizedintermsofamean.Anyvaluewesupplyforthismeanisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,becauseitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistousethelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintotheinterval(0,1)andinterpretthatvalueasaprobability:pyσ(= 1 ;) = |xθ(θ\\ue03ex).(5.81)Thisapproachisknownaslogisticregression(asomewhatstrangenamesinceweusethemodelforclassiﬁcationratherthanregression).Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsbysolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.Thereisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchforthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegativelog-likelihood(NLL)usinggradientdescent.Thissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,bywritingdownaparametricfamilyofconditionalprobabilitydistributionsovertherightkindofinputandoutputvariables.5.7.2SupportVectorMachinesOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvectormachine(,;Boseretal.1992CortesandVapnik1995,).Thismodelissimilartologisticregressioninthatitisdrivenbyalinearfunctionw\\ue03ex+b.Unlikelogistic140'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 155}, page_content='CHAPTER5.MACHINELEARNINGBASICSregression,thesupportvectormachinedoesnotprovideprobabilities,butonlyoutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhenw\\ue03ex+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhenw\\ue03ex+bisnegative.Onekeyinnovationassociatedwithsupportvectormachinesisthe.kerneltrickThekerneltrickconsistsofobservingthatmanymachinelearningalgorithmscanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecanbere-writtenasw\\ue03ex+= +bbm\\ue058i=1αix\\ue03ex()i(5.82)wherex()iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthelearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeaturefunctionφ(x) andthedotproductwithafunctionk(xx,()i) =φ(x)·φ(x()i) calleda.Thekernel·operatorrepresentsaninnerproductanalogoustoφ(x)\\ue03eφ(x()i).Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.Insomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,forexample,innerproductsbasedonintegrationratherthansummation.Acompletedevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.Afterreplacingdotproductswithkernelevaluations,wecanmakepredictionsusingthefunctionfb() = x+\\ue058iαik,(xx()i).(5.83)Thisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)andf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.Thekernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplyingφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodelsthatarenonlinearasafunctionofxusingconvexoptimizationtechniquesthatareguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedandoptimizeonlyα,i.e.,theoptimizationalgorithmcanviewthedecisionfunctionasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmitsanimplementationthatissigniﬁcantlymorecomputationaleﬃcientthannaivelyconstructingtwovectorsandexplicitlytakingtheirdotproduct.φ()xInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultinaninﬁnitecomputationalcostforthenaive,explicitapproach.Inmanycases,k(xx,\\ue030)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As141'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 156}, page_content='CHAPTER5.MACHINELEARNINGBASICSanexampleofaninﬁnite-dimensionalfeaturespacewithatractablekernel,weconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethatthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.Wecanwriteakernelfunctionk(x,x()i) =min(x,x()i)thatisexactlyequivalenttothecorrespondinginﬁnite-dimensionaldotproduct.ThemostcommonlyusedkernelistheGaussiankernelk,,σ(uvuv) = (N−;02I)(5.84)whereN(x;µ,Σ)isthestandardnormaldensity.Thiskernelisalsoknownastheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglinesinvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadotproductinaninﬁnite-dimensionalspace,butthederivationofthisspaceislessstraightforwardthaninourexampleofthekernelovertheintegers.minWecanthinkoftheGaussiankernelasperformingakindof.templatematchingAtrainingexamplexassociatedwithtraininglabelybecomesatemplateforclassy.Whenatestpointx\\ue030isnearxaccordingtoEuclideandistance,theGaussiankernelhasalargeresponse,indicatingthatx\\ue030isverysimilartothextemplate.Themodelthenputsalargeweightontheassociatedtraininglabely. Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythesimilarityofthecorrespondingtrainingexamples.Supportvectormachinesarenottheonlyalgorithmthatcanbeenhancedusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.Thecategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachinesorkernelmethods(,;WilliamsandRasmussen1996Schölkopf1999etal.,).Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecisionfunctionislinearinthenumberoftrainingexamples,becausethei-thexamplecontributesatermαik(xx,()i)tothedecisionfunction.Supportvectormachinesareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyforthetrainingexamplesthathavenon-zeroαi.Thesetrainingexamplesareknownassupportvectors.Kernelmachinesalsosuﬀerfromahighcomputationalcostoftrainingwhenthedatasetislarge.WewillrevisitthisideainSec. .Kernelmachineswith5.9generickernelsstruggletogeneralizewell.WewillexplainwhyinSec..The5.11modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsofkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM2006ontheMNISTbenchmark.142'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 157}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.7.3OtherSimpleSupervisedLearningAlgorithmsWehavealreadybrieﬂyencounteredanothernon-probabilisticsupervisedlearningalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsisafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asanon-parametriclearningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxednumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithmasnothavinganyparameters,butratherimplementingasimplefunctionofthetrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,weﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturntheaverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentiallyanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.Inthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithcy= 1andci= 0forallothervaluesofi.Wecantheninterprettheaverageovertheseone-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametriclearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,supposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe1numberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayeserrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequallydistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsxwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthealgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingoneofthem,theprocedureconvergestotheBayeserrorrate. Thehighcapacityofk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.However,itdoessoathighcomputationalcost,anditmaygeneralizeverybadlygivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatitcannotlearnthatonefeatureismorediscriminativethananother.Forexample,imaginewehavearegressiontaskwithx∈R100drawnfromanisotropicGaussiandistribution,butonlyasinglevariablex1isrelevanttotheoutput.Supposefurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inallcases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberoffeaturesx2throughx100,notbythelonefeaturex1. Thustheoutputonsmalltrainingsetswillessentiallyberandom.143'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 158}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n0101\\n11101\\n011\\n1111111011010010001110111111010010001001111111\\n11Figure5.7:Diagramsdescribinghowadecisiontreeworks.(Top)Eachnodeofthetreechoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeontheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeisdisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtainedbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).(Bottom)Thetreedividesspaceintoregions. The2DplaneshowshowadecisiontreemightdivideR2.Thenodesofthetreeareplottedinthisplane,witheachinternalnodedrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthecenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,withonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitisnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthenumberoftrainingexamples.144'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 159}, page_content='CHAPTER5.MACHINELEARNINGBASICSAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregionsandhasseparateparametersforeachregionisthedecisiontree(,Breimanetal.1984)anditsmanyvariants.AsshowninFig.,eachnodeofthedecisiontree5.7isassociatedwitharegionintheinputspace,andinternalnodesbreakthatregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-alignedcut). Spaceisthussub-dividedintonon-overlappingregions,withaone-to-onecorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymapseverypointinitsinputregiontothesameoutput.Decisiontreesareusuallytrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.Thelearningalgorithmcanbeconsiderednon-parametricifitisallowedtolearnatreeofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraintsthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyaretypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,struggletosolvesomeproblemsthatareeasyevenforlogisticregression.Forexample,ifwehaveatwo-classproblemandthepositiveclassoccurswhereverx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthusneedtoapproximatethedecisionboundarywithmanynodes,implementingastepfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunctionwithaxis-alignedsteps.Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemanylimitations.Nonetheless,theyareusefullearningalgorithmswhencomputationalresourcesareconstrained.Wecanalsobuildintuitionformoresophisticatedlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetweensophisticatedalgorithmsand-NNordecisiontreebaselines.kSee(),(), ()orothermachineMurphy2012Bishop2006Hastieetal.2001learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.5.8UnsupervisedLearningAlgorithmsRecallfromSec.thatunsupervisedalgorithmsarethosethatexperienceonly5.1.3“features”butnotasupervisionsignal.Thedistinctionbetweensupervisedandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisnoobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedbyasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextractinformationfromadistributionthatdonotrequirehumanlabortoannotateexamples.Thetermisusuallyassociatedwithdensityestimation,learningtodrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof145'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 160}, page_content='CHAPTER5.MACHINELEARNINGBASICSrelatedexamples.Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthedata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelookingforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhileobeyingsomepenaltyorconstraintaimedatkeepingtherepresentationorsimplermoreaccessiblethanitself.xTherearemultiplewaysofdeﬁningarepresentation.Threeofthesimpler mostcommonincludelowerdimensionalrepresentations,sparserepresentationsandindependentrepresentations.Low-dimensionalrepresentationsattempttocompressasmuchinformationaboutxaspossibleinasmallerrepresentation.Sparserepresentations(,;,;Barlow1989OlshausenandField1996HintonandGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesaremostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequiresincreasingthedimensionalityoftherepresentation,sothattherepresentationbecomingmostlyzeroesdoesnotdiscardtoomuchinformation.Thisresultsinanoverallstructureoftherepresentationthattendstodistributedataalongtheaxesoftherepresentationspace.Independentrepresentationsattempttodisentanglethesourcesofvariationunderlyingthedatadistributionsuchthatthedimensionsoftherepresentationarestatisticallyindependent.Of coursethese three criteriaare certainly notmutuallyexclusive.Low-dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewaytoreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifyingandremovingmoreredundancyallowsthedimensionalityreductionalgorithmtoachievemorecompressionwhilediscardinglessinformation.Thenotionofrepresentationisoneofthecentralthemesofdeeplearningandthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsomesimpleexamplesofrepresentationlearningalgorithms.Together,theseexamplealgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostoftheremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthatdevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.5.8.1PrincipalComponentsAnalysisInSec.,wesawthattheprincipalcomponentsanalysisalgorithmprovidesa2.12meansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearningalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedontwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa146'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 161}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n−−201001020x1−20−1001020x2−−201001020z1−20−1001020z2\\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariancewiththeaxesofthenewspace.(Left)Theoriginaldataconsistsofsamplesofx.Inthisspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.The(Right)transformeddataz=x\\ue03eWnowvariesmostalongtheaxisz1.Thedirectionofsecondmostvarianceisnowalongz2.representationthathaslowerdimensionalitythantheoriginalinput.Italsolearnsarepresentationwhoseelementshavenolinearcorrelationwitheachother.Thisisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsarestatisticallyindependent.Toachievefullindependence,arepresentationlearningalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsaninputxtoarepresentationzasshowninFig..InSec.,wesawthatwe5.82.12couldlearnaone-dimensionalrepresentationthatbestreconstructstheoriginaldata(inthesenseofmeansquarederror)andthatthisrepresentationactuallycorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCAasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuchoftheinformationinthedataaspossible(again,asmeasuredbyleast-squaresreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentationdecorrelatestheoriginaldatarepresentation.XLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethatthedatahasameanofzero,E[x] =0.Ifthisisnotthecase,thedatacaneasilybecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.Theunbiasedsamplecovariancematrixassociatedwithisgivenby:XVar[] =x1m−1X\\ue03eX.(5.85)147'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 162}, page_content='CHAPTER5.MACHINELEARNINGBASICSPCAﬁndsarepresentation(throughlineartransformation)z=x\\ue03eWwhereVar[]zisdiagonal.InSec.,wesawthattheprincipalcomponentsofadesignmatrix2.12XaregivenbytheeigenvectorsofX\\ue03eX.Fromthisview,X\\ue03eXWW= Λ\\ue03e.(5.86)Inthissection,weexploitanalternativederivationoftheprincipalcomponents.Theprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbetherightsingularvectorsinthedecompositionX=UWΣ\\ue03e. Wethenrecovertheoriginaleigenvectorequationwithastheeigenvectorbasis:WX\\ue03eX=\\ue010UWΣ\\ue03e\\ue011\\ue03eUWΣ\\ue03e= WΣ2W\\ue03e.(5.87)TheSVDishelpfultoshowthatPCAresultsinadiagonalVar[z].UsingtheSVDof,wecanexpressthevarianceofas:XXVar[] =x1m−1X\\ue03eX(5.88)=1m−1(UWΣ\\ue03e)\\ue03eUWΣ\\ue03e(5.89)=1m−1WΣ\\ue03eU\\ue03eUWΣ\\ue03e(5.90)=1m−1WΣ2W\\ue03e,(5.91)whereweusethefactthatU\\ue03eU=IbecausetheUmatrixofthesingularvaluedeﬁnitionisdeﬁnedtobeorthonormal.Thisshowsthatifwetakez=x\\ue03eW,wecanensurethatthecovarianceofisdiagonalasrequired:zVar[] =z1m−1Z\\ue03eZ(5.92)=1m−1W\\ue03eX\\ue03eXW(5.93)=1m−1W\\ue03eWΣ2W\\ue03eW(5.94)=1m−1Σ2,(5.95)wherethistimeweusethefactthatW\\ue03eW=I,againfromthedeﬁnitionoftheSVD.148'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 163}, page_content='CHAPTER5.MACHINELEARNINGBASICSTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelineartransformationW,theresultingrepresentationhasadiagonalcovariancematrix(asgivenbyΣ2)whichimmediatelyimpliesthattheindividualelementsofzaremutuallyuncorrelated.ThisabilityofPCAtotransformdataintoarepresentationwheretheelementsaremutuallyuncorrelatedisaveryimportantpropertyofPCA.Itisasimpleexampleofarepresentationthatattempttodisentangletheunknownfactorsofvariationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakestheformofﬁndingarotationoftheinputspace(describedbyW)thatalignstheprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociatedwith.zWhilecorrelationisanimportantcategoryofdependencybetweenelementsofthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemorecomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhatcanbedonewithasimplelineartransformation.5.8.2-meansClusteringkAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.Thek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclustersofexamplesthatareneareachother.Wecanthusthinkofthealgorithmasprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifxbelongstoclusteri,thenhi= 1andallotherentriesoftherepresentationharezero.Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparserepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,wewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,wheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodesareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁtsofadistributedrepresentation.Theone-hotcodestillconferssomestatisticaladvantages(itnaturallyconveystheideathatallexamplesinthesameclusteraresimilartoeachother)anditconfersthecomputationaladvantagethattheentirerepresentationmaybecapturedbyasingleinteger.Thek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ()k}todiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexofthenearestcentroidµ()i.Intheotherstep,eachcentroidµ()iisupdatedtothemeanofalltrainingexamplesx()jassignedtocluster.i149'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 164}, page_content='CHAPTER5.MACHINELEARNINGBASICSOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherentlyill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwellaclusteringofthedatacorrespondstotherealworld.WecanmeasurepropertiesoftheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothemembersofthecluster.Thisallowsustotellhowwellweareabletoreconstructthetrainingdatafromtheclusterassignments.Wedonotknowhowwelltheclusterassignmentscorrespondtopropertiesoftherealworld. Moreover,theremaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyoftherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebutobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.Forexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingofimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgraycars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmayﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterofredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclusteringalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassigntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.Thisnewclusteringnowatleastcapturesinformationaboutbothattributes,butithaslostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgraycars,justastheyareinadiﬀerentclusterfromgraytrucks. Theoutputoftheclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycarsthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisallweknow.Theseissuesillustratesomeofthereasonsthatwemaypreferadistributedrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhavetwoattributesforeachvehicle—onerepresentingitscolorandonerepresentingwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimaldistributedrepresentationis(howcanthelearningalgorithmknowwhetherthetwoattributesweareinterestedinarecolorandcar-versus-truckratherthanmanufacturerandage?)buthavingmanyattributesreducestheburdenonthealgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasuresimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributesinsteadofjusttestingwhetheroneattributematches.5.9StochasticGradientDescentNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochasticgradientdescentSGDor.Stochasticgradientdescentisanextensionofthegradient150'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 165}, page_content='CHAPTER5.MACHINELEARNINGBASICSdescentalgorithmintroducedinSec..4.3Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessaryforgoodgeneralization,butlargetrainingsetsarealsomorecomputationallyexpensive.Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasasumovertrainingexamplesofsomeper-examplelossfunction.Forexample,thenegativeconditionallog-likelihoodofthetrainingdatacanbewrittenasJ() = θEx,y∼ˆpdataL,y,(xθ) =1mm\\ue058i=1L(x()i,y()i,θ)(5.96)whereistheper-examplelossLL,y,py.(xθ) = log−(|xθ;)Fortheseadditivecostfunctions,gradientdescentrequirescomputing∇θJ() =θ1mm\\ue058i=1∇θL(x()i,y()i,.θ)(5.97)ThecomputationalcostofthisoperationisO(m).Asthetrainingsetsizegrowstobillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitivelylong.Theinsightofstochasticgradientdescentisthatthegradientisanexpectation.Theexpectationmaybeapproximatelyestimatedusingasmallsetofsamples.Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamplesB={x(1),...,x(m\\ue030)}drawnuniformlyfromthetrainingset.Theminibatchsizem\\ue030istypicallychosentobearelativelysmallnumberofexamples,rangingfrom1toafewhundred.Crucially,m\\ue030isusuallyheldﬁxedasthetrainingsetsizemgrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputedononlyahundredexamples.Theestimateofthegradientisformedasg=1m\\ue030∇θm\\ue030\\ue058i=1L(x()i,y()i,.θ)(5.98)usingexamplesfromtheminibatch.ThestochasticgradientdescentalgorithmBthenfollowstheestimatedgradientdownhill:θθg←−\\ue00f,(5.99)whereisthelearningrate.\\ue00f151'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 166}, page_content='CHAPTER5.MACHINELEARNINGBASICSGradientdescentingeneralhasoftenbeenregardedassloworunreliable.Inthepast,theapplicationofgradientdescenttonon-convexoptimizationproblemswasregardedasfoolhardyorunprincipled.Today,weknowthatthemachinelearningmodelsdescribedinPartworkverywellwhentrainedwithgradientIIdescent.Theoptimizationalgorithmmaynotbeguaranteedtoarriveatevenalocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalueofthecostfunctionquicklyenoughtobeuseful.Stochasticgradientdescenthasmanyimportantusesoutsidethecontextofdeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylargedatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthetrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsizeincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreachconvergenceusuallyincreaseswithtrainingsetsize. However,asmapproachesinﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbeforeSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnotextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletesterror.Fromthispointofview,onecanarguethattheasymptoticcostoftrainingamodelwithSGDisasafunctionof.O(1)mPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodelswastousethekerneltrickincombinationwithalinearmodel.Manykernellearningalgorithmsrequireconstructinganmm×matrixGi,j=k(x()i,x()j).ConstructingthismatrixhascomputationalcostO(m2),whichisclearlyundesirablefordatasetswith billionsof examples.In academia, starting in2006,deep learningwasinitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetterthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensofthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestinindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlargedatasets.StochasticgradientdescentandmanyenhancementstoitaredescribedfurtherinChapter.85.10BuildingaMachineLearningAlgorithmNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesofafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,anoptimizationprocedureandamodel.Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof152'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 167}, page_content='CHAPTER5.MACHINELEARNINGBASICSXyand,thecostfunctionJ,b(w) = −Ex,y∼ˆpdatalogpmodel()y|x,(5.100)themodelspeciﬁcationpmodel(y|x) =N(y;x\\ue03ew+b,1),and,inmostcases,theoptimizationalgorithmdeﬁnedbysolvingforwherethegradientofthecostiszerousingthenormalequations.Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependentlyfromtheothers,wecanobtainaverywidevarietyofalgorithms.Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearningprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthenegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximumlikelihoodestimation.Thecostfunctionmayalsoincludeadditionalterms,suchasregularizationterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunctiontoobtainJ,bλ(w) = ||||w22−Ex,py∼datalogpmodel()y|x.(5.101)Thisstillallowsclosed-formoptimization.Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolongerbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumericaloptimizationprocedure,suchasgradientdescent.Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,andoptimizationalgorithmssupportsbothsupervisedandunsupervisedlearning.Thelinearregressionexampleshowshowtosupportsupervisedlearning.UnsupervisedlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandprovidinganappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrstPCAvectorbyspecifyingthatourlossfunctionisJ() = wEx∼pdata||−||xr(;)xw22(5.102)whileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunctionr() = xw\\ue03exw.Insomecases,thecostfunctionmaybeafunctionthatwecannotactuallyevaluate,forcomputationalreasons.Inthesecases,wecanstillapproximatelyminimizeitusingiterativenumericaloptimizationsolongaswehavesomewayofapproximatingitsgradients.Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynotimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor153'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 168}, page_content='CHAPTER5.MACHINELEARNINGBASICShand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Somemodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecausetheircostfunctionshaveﬂatregionsthatmaketheminappropriateforminimizationbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithmscanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofataxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,ratherthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.5.11ChallengesMotivatingDeepLearningThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellonawidevarietyofimportantproblems.However,theyhavenotsucceededinsolvingthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.ThedevelopmentofdeeplearningwasmotivatedinpartbythefailureoftraditionalalgorithmstogeneralizewellonsuchAItasks.Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomesexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhowthemechanismsusedtoachievegeneralizationintraditionalmachinelearningareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Suchspacesalsooftenimposehighcomputationalcosts.Deeplearningwasdesignedtoovercometheseandotherobstacles.5.11.1TheCurseofDimensionalityManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumberof dimensionsin thedata ishigh.This phenomenonis knownas thecurseofdimensionality. Ofparticularconcernisthatthenumberofpossibledistinctconﬁgurationsofasetofvariablesincreasesexponentiallyasthenumberofvariablesincreases.\\n154'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 169}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromlefttoright),thenumberofconﬁgurationsofinterestmaygrowexponentially.(Left)Inthisone-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregioncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithineachregion(andpossiblyinterpolatebetweenneighboringregions).With2(Center)dimensions(center)itismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable.Weneedtokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplestocoverallthoseregions.With3dimensionsthisgrowsto(Right)103= 1000regionsandatleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeachaxis,weseemtoneedO(vd)regionsandexamples.Thisisaninstanceofthecurseofdimensionality.FiguregraciouslyprovidedbyNicolasChapados.Thecurseofdimensionalityarisesinmanyplacesincomputerscience,andespeciallysoinmachinelearning.Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge.AsillustratedinFig.,astatisticalchallengearisesbecausethenumberof5.9possibleconﬁgurationsofxismuchlargerthanthenumberoftrainingexamples.Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoagrid,likeintheﬁgure.Inlowdimensionswecandescribethisspacewithalownumberofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdatapoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamplesthatlieinthesamecellasthenewinput. Forexample,ifestimatingtheprobabilitydensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesinthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftrainingexamplesinthesamecell.Ifwearedoingregressionwecanaveragethetargetvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhichwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberofconﬁgurationsisgoingtobehuge,muchlargerthanournumberofexamples,mostconﬁgurationswillhavenotrainingexampleassociatedwithit.155'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 170}, page_content='CHAPTER5.MACHINELEARNINGBASICSHowcouldwepossiblysaysomethingmeaningfulaboutthesenewconﬁgurations?Manytraditionalmachinelearningalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximatelythesameastheoutputatthenearesttrainingpoint.5.11.2LocalConstancyandSmoothnessRegularizationInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbypriorbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseenthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributionsoverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsasdirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparametersfunctionviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsasbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosingsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingourdegreeofbeliefinvariousfunctions.Amongthemostwidelyusedoftheseimplicit“priors”isthesmoothnesspriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearnshouldnotchangeverymuchwithinasmallregion.Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,andasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-leveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroducesadditional(explicit andimplicit)priorsinorderto reducethegeneralizationerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneisinsuﬃcientforthesetasks.Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbeliefthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerentmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗thatsatisﬁestheconditionf∗() x≈f∗(+)x\\ue00f(5.103)formostconﬁgurationsxandsmallchange\\ue00f.Inotherwords,ifweknowagoodanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthatanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswersinsomeneighborhoodwewouldcombinethem(bysomeformofaveragingorinterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchaspossible.Anextremeexampleofthelocalconstancyapproachisthek-nearestneighbors156'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 171}, page_content='CHAPTER5.MACHINELEARNINGBASICSfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereachregioncontainingallthepointsxthathavethesamesetofknearestneighborsinthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemorethanthenumberoftrainingexamples.Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytrainingexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociatedwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocalkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfartherapartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunctionthatperformstemplatematching,bymeasuringhowcloselyatestexamplexresembleseachtrainingexamplex()i. Muchofthemodernmotivationfordeeplearningisderivedfromstudyingthelimitationsoflocaltemplatematchingandhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails(,).Bengioetal.2006bDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-basedlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereareleavesanduseaseparateparameter(orsometimesmanyparametersforextensionsofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithatleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesarerequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatisticalconﬁdenceinthepredictedoutput.Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethodsrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parametersassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,whereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustratedinFig..5.10Isthereawaytorepresentacomplexfunctionthathasmanymoreregionstobedistinguishedthanthenumberoftrainingexamples?Clearly,assumingonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.For example, imaginethat thetargetfunctionisakindofcheckerboard.Acheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.Imaginewhathappenswhenthenumberoftrainingexamplesissubstantiallysmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Basedononlylocalgeneralizationandthesmoothnessorlocalconstancyprior,wewouldbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesamecheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearnercouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdonotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan157'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 172}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspaceintoregions. Anexample(representedherebyacircle)withineachregiondeﬁnestheregionboundary(representedherebythelines).Theyvalueassociatedwitheachexampledeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. TheregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoidiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumberoftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighboralgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthelocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexampleonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediatelysurroundingthatexample.\\n158'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 173}, page_content='CHAPTER5.MACHINELEARNINGBASICSexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsoftheentirecheckerboardrightistocovereachofitscellswithatleastoneexample.Thesmoothnessassumptionandtheassociatednon-parametriclearningalgo-rithmsworkextremelywellsolongasthereareenoughexamplesforthelearningalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleysofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthefunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutinadiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerentlyindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetoftrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahugenumberofregionscomparedtothenumberofexamples),isthereanyhopetogeneralizewell?Theanswertobothofthesequestionsisyes.Thekeyinsightisthataverylargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solongasweintroducesomedependenciesbetweentheregionsviaadditionalassumptionsabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactuallygeneralizenon-locally(,;,).ManyBengioandMonperrus2005Bengioetal.2006cdiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatarereasonableforabroadrangeofAItasksinordertocapturetheseadvantages.Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyprovidingtheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuchstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralizetoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoocomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,sowewantlearningalgorithmsthatembodymoregeneral-purposeassumptions.Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedbythecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningalgorithms.Theseapparentlymildassumptionsallowanexponentialgainintherelationshipbetweenthenumberofexamplesandthenumberofregionsthatcanbedistinguished.TheseexponentialgainsaredescribedmorepreciselyinSec.,Sec.,andSec..Theexponentialadvantagesconferredbythe6.4.115.415.5useofdeep,distributedrepresentationscountertheexponentialchallengesposedbythecurseofdimensionality.159'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 174}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.11.3ManifoldLearningAnimportantconceptunderlyingmanyideasinmachinelearningisthatofamanifold.Aisaconnectedregion.Mathematically,itisasetofpoints,associatedmanifoldwithaneighborhoodaroundeachpoint.Fromanygivenpoint,themanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperiencethesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin3-Dspace.Thedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistenceoftransformationsthatcanbeappliedtomoveonthemanifoldfromonepositiontoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecanwalknorth,south,east,orwest.Althoughthereisaformalmathematicalmeaningtotheterm“manifold,”inmachinelearningittendstobeusedmorelooselytodesignateaconnectedsetofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberofdegreesoffreedom,ordimensions,embeddedinahigher-dimensionalspace.Eachdimensioncorrespondstoalocaldirectionofvariation.SeeFig.foran5.11exampleoftrainingdatalyingnearaone-dimensionalmanifoldembeddedintwo-dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionalityofthemanifoldtovaryfromonepointtoanother.Thisoftenhappenswhenamanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingledimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\\n0510152025303540........−10.−05.00.05.10.15.20.25.\\nFigure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactuallyconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicatestheunderlyingmanifoldthatthelearnershouldinfer.160'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 175}, page_content='CHAPTER5.MACHINELEARNINGBASICSManymachinelearningproblemsseemhopelessifweexpectthemachinelearningalgorithmto learnfunctions withinterestingvariations acrossall ofRn.ManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmostofRnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalongacollectionofmanifoldscontainingasmallsubsetofpoints,withinterestingvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirectionsthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwemovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecaseofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthisprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthesupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassishighlyconcentrated.Theassumptionthatthedataliesalongalow-dimensionalmanifoldmaynotalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchasthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionisatleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsistsoftwocategoriesofobservations.Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheprobabilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeishighlyconcentrated. Uniformnoiseessentiallyneverresemblesstructuredinputsfromthesedomains.Fig.showshow,instead,uniformlysampledpointslooklikethe5.12patternsofstaticthatappearonanalogtelevisionsetswhennosignalisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyatrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-languagetext?Almostzero,again,becausemostofthelongsequencesoflettersdonotcorrespondtoanaturallanguagesequence:thedistributionofnaturallanguagesequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\\n161'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 176}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixelaccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-zeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencounteredinAIapplications,weneveractuallyobservethishappeninginpractice.ThissuggeststhattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthevolumeofimagespace.Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshowthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalsoestablishthattheexamplesweencounterareconnectedtoeachotherbyother162'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 177}, page_content='CHAPTER5.MACHINELEARNINGBASICSexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthatmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecondargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuchneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,wecancertainlythinkofmanypossibletransformationsthatallowustotraceoutamanifoldinimagespace:wecangraduallydimorbrightenthelights,graduallymoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesofobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmostapplications.Forexample,themanifoldofimagesofhumanfacesmaynotbeconnectedtothemanifoldofimagesofcatfaces.Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-tuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,;andMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum,;etal.,;,;etal.,2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger;,;,;,;andSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsofinterestinAI.Whenthedataliesonalow-dimensionalmanifold,itcanbemostnaturalformachinelearningalgorithmstorepresentthedataintermsofcoordinatesonthemanifold,ratherthanintermsofcoordinatesinRn.Ineverydaylife,wecanthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionstospeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notintermsofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,butholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneralprincipleisappliedinmanycontexts.Fig.showsthemanifoldstructureofa5.13datasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthemethodsnecessarytolearnsuchamanifoldstructure. InFig.,wewillsee20.6howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.ThisconcludesPart,whichhasprovidedthebasicconceptsinmathematicsIandmachinelearningwhichareemployedthroughouttheremainingpartsofthebook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\\n163'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 178}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset(,)Gongetal.2000forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensionalmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobeabletodiscoveranddisentanglesuchmanifoldcoordinates.Fig.illustratessucha20.6feat.\\n164'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 179}, page_content='PartIIDeepNetworks:ModernPractices\\n165'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 180}, page_content='Thispartofthebooksummarizesthestateofmoderndeeplearningasitisusedtosolvepracticalapplications.Deeplearninghasalonghistoryandmanyaspirations.Severalapproacheshavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoalshaveyettoberealized.Theseless-developedbranchesofdeeplearningappearintheﬁnalpartofthebook.Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-nologiesthatarealreadyusedheavilyinindustry.Modern deeplearning provides avery powerful framework forsupervisedlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcanrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappinganinputvectortoanoutputvector,andthatareeasyforapersontodorapidly,canbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃcientlylargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribedasassociatingonevectortoanother,orthatarediﬃcultenoughthatapersonwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remainbeyondthescopeofdeeplearningfornow.Thispartofthebookdescribesthecoreparametricfunctionapproximationtechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.We begin by describingthe feedforward deepnetworkmodelthatisusedtorepresentthesefunctions.Next,wepresentadvancedtechniquesforregularizationandoptimizationofsuchmodels.Scalingthesemodelstolargeinputssuchashighresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroducetheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneuralnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelinesforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringanapplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeeplearning.Thesechaptersarethemostimportantforapractitioner—someonewhowantstobeginimplementingandusingdeeplearningalgorithmstosolvereal-worldproblemstoday.\\n166'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 181}, page_content='Chapter6DeepFeedforwardNetworksDeepfeedforwardnetworksfeedforwardneuralnetworksmulti-,alsooftencalled,orlayerperceptronsMLPs(),arethequintessentialdeeplearningmodels.Thegoalofafeedforwardnetworkistoapproximatesomefunctionf∗. Forexample,foraclassiﬁer,y=f∗(x)mapsaninputxtoacategoryy.Afeedforwardnetworkdeﬁnesamappingy=f(x;θ)andlearnsthevalueoftheparametersθthatresultinthebestfunctionapproximation.Thesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthefunctionbeingevaluatedfromx,throughtheintermediatecomputationsusedtodeﬁnef,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhichoutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworksareextendedtoincludefeedbackconnections,theyarecalledrecurrentneuralnetworks,presentedinChapter.10Feedforwardnetworksareofextremeimportancetomachinelearningpracti-tioners.Theyformthebasisofmanyimportantcommercialapplications.Forexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosareaspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptualsteppingstoneonthepathtorecurrentnetworks,whichpowermanynaturallanguageapplications.Feedforwardneuralnetworksarecalledbecausetheyaretypicallyrep-networksresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisassociatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposedtogether.Forexample,wemighthavethreefunctionsf(1),f(2),andf(3)connectedinachain,toformf(x)=f(3)(f(2)(f(1)(x))).Thesechainstructuresarethemostcommonlyusedstructuresofneuralnetworks.Inthiscase,f(1)iscalledtheﬁrstlayerofthenetwork,f(2)iscalledthesecondlayer,andsoon.Theoveralllength167'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 182}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofthechaingivestheofthemodel.Itisfromthisterminologythatthedepthname“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalledtheoutputlayer.Duringneuralnetworktraining,wedrivef(x)tomatchf∗(x).Thetrainingdataprovidesuswithnoisy,approximateexamplesoff∗(x)evaluatedatdiﬀerenttrainingpoints. Eachexamplexisaccompaniedbyalabelyf≈∗(x).Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpointx;itmustproduceavaluethatisclosetoy.Thebehavioroftheotherlayersisnotdirectlyspeciﬁedbythetrainingdata. Thelearningalgorithmmustdecidehowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoesnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmustdecidehowtousetheselayerstobestimplementanapproximationoff∗.Becausethetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,theselayersarecalled.hiddenlayersFinally,thesenetworksarecalledneuralbecausetheyarelooselyinspiredbyneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.Thedimensionalityofthesehiddenlayersdeterminestheofthemodel.Eachwidthelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,wecanalsothinkofthelayerasconsistingofmanythatactinparallel,unitseachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuroninthesensethatitreceivesinputfrommanyotherunitsandcomputesitsownactivationvalue. Theideaofusingmanylayersofvector-valuedrepresentationisdrawnfromneuroscience.Thechoiceofthefunctionsf()i(x)usedtocomputetheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsaboutthefunctionsthatbiologicalneuronscompute.However,modernneuralnetworkresearchisguidedbymanymathematicalandengineeringdisciplines,andthegoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkoffeedforwardnetworksasfunctionapproximationmachinesthataredesignedtoachievestatisticalgeneralization,occasionallydrawingsomeinsightsfromwhatweknowaboutthebrain,ratherthanasmodelsofbrainfunction.Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodelsandconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogisticregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃcientlyandreliably,eitherinclosedformorwithconvexoptimization.Linearmodelsalsohavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,sothemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapplythelinearmodelnottoxitselfbuttoatransformedinputφ(x),whereφisa168'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 183}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedinSec.,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying5.7.2theφmapping.Wecanthinkofφasprovidingasetoffeaturesdescribingx,orasprovidinganewrepresentationfor.xThequestionisthenhowtochoosethemapping.φ1.Oneoptionistouseaverygenericφ,suchastheinﬁnite-dimensionalφthatisimplicitlyusedbykernelmachinesbasedontheRBFkernel. Ifφ(x)isofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthetrainingset,butgeneralizationtothetestsetoftenremainspoor.Verygenericfeaturemappingsareusuallybasedonlyontheprincipleoflocalsmoothnessanddonotencodeenoughpriorinformationtosolveadvancedproblems.2.Anotheroptionistomanuallyengineerφ.Untiltheadventofdeeplearning,thiswasthedominantapproach.Thisapproachrequiresdecadesofhumaneﬀortforeach separatetask, withpractitionersspecializing indiﬀerentdomainssuch asspeechrecognitionor computervision, andwithlittletransferbetweendomains.3.Thestrategyofdeeplearningistolearnφ.Inthisapproach,wehaveamodely=f(x;θw,) =φ(x;θ)\\ue03ew.Wenowhaveparametersθthatweusetolearnφfromabroadclassoffunctions,andparameterswthatmapfromφ(x)tothedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,withφdeﬁningahiddenlayer.Thisapproachistheonlyoneofthethreethatgivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweightheharms.Inthisapproach,weparametrizetherepresentationasφ(x;θ)andusetheoptimizationalgorithmtoﬁndtheθthatcorrespondstoagoodrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrstapproachbybeinghighlygeneric—wedosobyusingaverybroadfamilyφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.Humanpractitionerscanencodetheirknowledgetohelpgeneralizationbydesigningfamiliesφ(x;θ)thattheyexpectwillperformwell.Theadvantageisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunctionfamilyratherthanﬁndingpreciselytherightfunction.Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyondthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeeplearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic169'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 184}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmappingsfromxtoythatlackfeedbackconnections.Othermodelspresentedlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctionswithfeedback,andlearningprobabilitydistributionsoverasinglevector.Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesigndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecostfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-basedlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareuniquetofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofahiddenlayer,andthisrequiresustochoosethethatwillbeactivationfunctionsusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitectureofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthesenetworksshouldbeconnectedtoeachother,andhowmanyunitsshouldbeineachlayer.Learningindeepneuralnetworksrequirescomputingthegradientsofcomplicatedfunctions.Wepresenttheback-propagationalgorithmanditsmoderngeneralizations,whichcanbeusedtoeﬃcientlycomputethesegradients.Finally,weclosewithsomehistoricalperspective.6.1Example:LearningXORTomaketheideaofafeedforwardnetworkmoreconcrete, webeginwithanexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learningtheXORfunction.TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues,x1andx2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction1returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction1y=f∗(x)thatwewanttolearn.Ourmodelprovidesafunctiony=f(x;θ)andourlearningalgorithmwilladapttheparametersθtomakefassimilaraspossibletof∗.Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.WewantournetworktoperformcorrectlyonthefourpointsX={[0,0]\\ue03e,[0,1]\\ue03e,[1,0]\\ue03e,and[1,1]\\ue03e}. Wewilltrainthenetworkonallfourofthesepoints.Theonlychallengeistoﬁtthetrainingset.Wecantreatthisproblemasaregressionproblemanduseameansquarederrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthisexampleasmuchaspossible.Wewillseelaterthatthereareother,moreappropriate170'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 185}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSapproachesformodelingbinarydata.Evaluatedonourwholetrainingset,theMSElossfunctionisJ() =θ14\\ue058x∈X(f∗()(;))x−fxθ2.(6.1)Nowwemustchoosetheformofourmodel,f(x;θ).Supposethatwechoosealinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobeθwbf,b(;xw) = x\\ue03ew+b.(6.2)WecanminimizeJ(θ)inclosedformwithrespecttowandbusingthenormalequations.Aftersolvingthenormalequations,weobtainw=0andb=12. Thelinearmodelsimplyoutputs0.5everywhere.Whydoesthishappen?Fig.showshow6.1alinearmodelisnotabletorepresenttheXORfunction.Onewaytosolvethisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhichalinearmodelisabletorepresentthesolution.Speciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithonehiddenlayercontainingtwohiddenunits.SeeFig.foranillustrationof6.2thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatarecomputedbyafunctionf(1)(x;Wc,).Thevaluesofthesehiddenunitsarethenusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthenetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitisappliedtohratherthantox.Thenetworknowcontainstwofunctionschainedtogether:h=f(1)(x;Wc,)andy=f(2)(h;w,b),withthecompletemodelbeingf,,,bf(;xWcw) = (2)(f(1)())x.Whatfunctionshouldf(1)compute?Linearmodelshaveserveduswellsofar,anditmaybetemptingtomakef(1)belinearaswell.Unfortunately,iff(1)werelinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionofitsinput.Ignoringtheintercepttermsforthemoment,supposef(1)(x) =W\\ue03exandf(2)(h) =h\\ue03ew.Thenf(x) =w\\ue03eW\\ue03ex.Wecouldrepresentthisfunctionasf() = xx\\ue03ew\\ue030wherew\\ue030= Ww.Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneuralnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,followedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethatstrategyhere,bydeﬁningh=g(W\\ue03ex+c),whereWprovidestheweightsofalineartransformationandcthebiases.Previously,todescribealinearregressionmodel,weusedavectorofweightsandascalarbiasparametertodescribean171'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 186}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n01x101x2OriginalSpacex\\n012h101h2LearnedSpaceh\\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbersprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.(Left)AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXORfunction.Whenx1= 0,themodel’soutputmustincreaseasx2increases.Whenx1= 1,themodel’soutputmustdecreaseasx2increases.Alinearmodelmustapplyaﬁxedcoeﬃcientw2tox2.Thelinearmodelthereforecannotusethevalueofx1tochangethecoeﬃcientonx2andcannotsolvethisproblem.(Right)Inthetransformedspacerepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolvetheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen1collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshavemappedbothx= [1,0]\\ue03eandx= [0,1]\\ue03etoasinglepointinfeaturespace,h= [1,0]\\ue03e.Thelinearmodelcannowdescribethefunctionasincreasinginh1anddecreasinginh2.Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodelcapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learnedrepresentationscanalsohelpthemodeltogeneralize.\\n172'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 187}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSy yh hx xWwy yh1h1x1x1h2h2x2x2Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehiddenlayercontainingtwounits.(Left)Inthisstyle,wedraweveryunitasanodeinthegraph.Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexampleitcanconsumetoomuchspace.Inthisstyle,wedrawanodeinthe(Right)graphforeachentirevectorrepresentingalayer’sactivations.Thisstyleismuchmorecompact.Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthatdescribetherelationshipbetweentwolayers. Here,weindicatethatamatrixWdescribesthemappingfromxtoh,andavectorwdescribesthemappingfromhtoy.Wetypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskindofdrawing.aﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribeanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbiasparametersisneeded.Theactivationfunctiongistypicallychosentobeafunctionthatisappliedelement-wise,withhi=g(x\\ue03eW:,i+ci).Inmodernneuralnetworks,thedefaultrecommendationistousetherectiﬁedlinearunitorReLU(Jarrettetal.etal.,;,;2009NairandHinton2010Glorot,)deﬁnedbytheactivation2011afunctiondepictedinFig..gz,z() = max0{}6.3Wecannowspecifyourcompletenetworkasf,,,b(;xWcw) = w\\ue03emax0{,W\\ue03exc+}+b.(6.3)WecannowspecifyasolutiontotheXORproblem.LetW=\\ue0141111\\ue015,(6.4)c=\\ue0140−1\\ue015,(6.5)w=\\ue0141−2\\ue015,(6.6)173'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 188}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0z0gz,z() = max0{}TheRectiﬁedLinearActivationFunction\\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefaultactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applyingthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinearfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,theypreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodelsgeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuildcomplicatedsystemsfromminimalcomponents. MuchasaTuringmachine’smemoryneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximatorfromrectiﬁedlinearfunctions.\\n174'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 189}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSand.b= 0Wecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,withoneexampleperrow:X=\\uf8ee\\uf8ef\\uf8ef\\uf8f000011011\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.7)Theﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrstlayer’sweightmatrix:XW=\\uf8ee\\uf8ef\\uf8ef\\uf8f000111122\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.8)Next,weaddthebiasvector,toobtainc\\uf8ee\\uf8ef\\uf8ef\\uf8f001−101021\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.9)Inthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong1thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto.010Alinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalueofforeachexample,weapplytherectiﬁedlineartransformation:h\\uf8ee\\uf8ef\\uf8ef\\uf8f000101021\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.10)Thistransformationhaschangedtherelationshipbetweentheexamples.Theynolongerlieonasingleline.AsshowninFig.,theynowlieinaspacewherea6.1linearmodelcansolvetheproblem.Weﬁnishbymultiplyingbytheweightvector:w\\uf8ee\\uf8ef\\uf8ef\\uf8f00110\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.11)175'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtainedzeroerror. Inarealsituation,theremightbebillionsofmodelparametersandbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedidhere.Instead,agradient-basedoptimizationalgorithmcanﬁndparametersthatproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisataglobalminimumofthelossfunction,sogradientdescentcouldconvergetothispoint.ThereareotherequivalentsolutionstotheXORproblemthatgradientdescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsontheinitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynotﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresentedhere.6.2Gradient-BasedLearningDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermachinelearningmodelwithgradientdescent.InSec.,wedescribed5.10howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,acostfunction,andamodelfamily.Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneuralnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestinglossfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusuallytrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecostfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrainlinearregressionmodelsortheconvexoptimizationalgorithmswithglobalconver-genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimizationconvergesstartingfromanyinitialparameters(intheory—inpracticeitisveryrobustbutcanencounternumericalproblems).Stochasticgradientdescentappliedtonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitivetothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itisimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybeinitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-mizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeepmodelswillbedescribedindetailinChapter,withparameterinitializationin8particulardiscussedinSec. .Forthemoment,itsuﬃcestounderstandthat8.4thetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthecostfunctioninonewayoranother.Thespeciﬁcalgorithmsareimprovementsandreﬁnementsontheideasofgradientdescent,introducedinSec.,and,4.3176'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 191}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescentalgorithm,introducedinSec..5.9Wecanofcourse,trainmodelssuchaslinearregressionandsupportvectormachineswithgradientdescenttoo,andinfactthisiscommonwhenthetrainingsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightlymorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.Sec.willdescribehowtoobtainthegradientusingtheback-propagation6.5algorithmandmoderngeneralizationsoftheback-propagationalgorithm.Aswithothermachinelearningmodels,toapplygradient-basedlearningwemustchooseacostfunction,andwemustchoosehowtorepresenttheoutputofthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasisontheneuralnetworksscenario.6.2.1CostFunctionsAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthecostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorlessthesameasthoseforotherparametricmodels,suchaslinearmodels.Inmostcases,ourparametricmodeldeﬁnesadistributionp(yx|;θ)andwesimplyusethe principleof maximum likelihood.Thismeansweuse thecross-entropybetweenthetrainingdataandthemodel’spredictionsasthecostfunction.Sometimes,wetakeasimplerapproach,whereratherthanpredictingacompleteprobabilitydistributionovery,wemerelypredictsomestatisticofyconditionedon.Specializedlossfunctionsallowustotrainapredictoroftheseestimates.xThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineoneoftheprimarycostfunctionsdescribedherewitharegularizationterm.WehavealreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsinSec.5.2.2.Theweightdecayapproachusedforlinearmodelsisalsodirectlyapplicabletodeepneuralnetworksandisamongthemostpopularregularizationstrategies.MoreadvancedregularizationstrategiesforneuralnetworkswillbedescribedinChapter.76.2.1.1LearningConditionalDistributionswithMaximumLikelihoodMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeansthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed177'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 192}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSasthecross-entropybetweenthetrainingdataandthemodeldistribution.ThiscostfunctionisgivenbyJ() = θ−Exy,∼ˆpdatalogpmodel()yx|.(6.12)Thespeciﬁcformofthecostfunctionchangesfrommodeltomodel,dependingonthespeciﬁcformoflogpmodel.Theexpansionoftheaboveequationtypicallyyieldssometermsthatdonotdependonthemodelparametersandmaybediscarded.Forexample,aswesawinSec.,if5.5.1pmodel(yx|) =N(y;f(x;θ),I),thenwerecoverthemeansquarederrorcost,Jθ() =12Exy,∼ˆpdata||−||yf(;)xθ2+const,(6.13)uptoascalingfactorof12andatermthatdoesnotdependon.ThediscardedθconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscasewechosenottoparametrize.Previously,wesawthattheequivalencebetweenmaximumlikelihoodestimationwithanoutputdistributionandminimizationofmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholdsregardlessoftheusedtopredictthemeanoftheGaussian.f(;)xθAnadvantageofthisapproachofderivingthecostfunctionfrommaximumlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.Specifyingamodelp(yx|)automaticallydeterminesacostfunctionlogp(yx|).Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientofthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguideforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)underminethisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycasesthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthehiddenunitsortheoutputunitssaturate.Thenegativelog-likelihoodhelpstoavoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunctionthatcansaturatewhenitsargumentisverynegative.Thelogfunctioninthenegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.WewilldiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitinSec..6.2.2Oneunusualpropertyofthecross-entropycostusedtoperformmaximumlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenappliedtothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,mostmodelsareparametrizedinsuchawaythattheycannotrepresentaprobabilityofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregressionisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel178'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 193}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKScancontrolthedensityoftheoutputdistribution(forexample,bylearningthevarianceparameterofaGaussianoutputdistribution)thenitbecomespossibletoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingincross-entropyapproachingnegativeinﬁnity.RegularizationtechniquesdescribedinChapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso7thatthemodelcannotreapunlimitedrewardinthisway.6.2.1.2LearningConditionalStatisticsInsteadoflearningafullprobabilitydistributionp(yx|;θ)weoftenwanttolearnjustoneconditionalstatisticofgiven.yxForexample,wemayhaveapredictorf(x;θ) thatwewishtopredictthemeanof.yIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneuralnetworkasbeingabletorepresentanyfunctionffromawideclassoffunctions,withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundednessratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,wecanviewthecostfunctionasbeingaratherthanjustafunction. Afunctionalfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkoflearningaschoosingafunctionratherthanmerelychoosingasetofparameters.Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁcfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveitsminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematicaltoolcalledcalculusofvariations,describedinSec..Itisnotnecessaryto19.4.2understandcalculusofvariationstounderstandthecontentofthischapter.Atthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybeusedtoderivethefollowingtworesults.Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-tionproblemf∗= argminfExy,∼pdata||−||yf()x2(6.14)yieldsf∗() = xEy∼pdata()yx|[]y,(6.15)solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwecouldtrainoninﬁnitelymanysamplesfromthetruedata-generatingdistribution,minimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthemeanofforeachvalueof.yx179'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 194}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusingcalculusofvariationsisthatf∗= argminfExy,∼pdata||−||yf()x1(6.16)yieldsafunctionthatpredictsthemedianvalueofyforeachx,solongassuchafunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscostfunctioniscommonlycalledmeanabsoluteerror.Unfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoorresultswhenusedwithgradient-basedoptimization.Someoutputunitsthatsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmeansquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimateanentiredistribution.p()yx|6.2.2OutputUnitsThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Mostofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthemodeldistribution. Thechoiceofhowtorepresenttheoutputthendeterminestheformofthecross-entropyfunction.Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobeusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthemodel,butinprincipletheycanbeusedinternallyaswell.WerevisittheseunitswithadditionaldetailabouttheiruseashiddenunitsinSec..6.3Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesasetofhiddenfeaturesdeﬁnedbyh=f(x;θ).Theroleoftheoutputlayeristhentoprovidesomeadditionaltransformationfromthefeaturestocompletethetaskthatthenetworkmustperform.6.2.2.1LinearUnitsforGaussianOutputDistributionsOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformationwithnononlinearity.Theseareoftenjustcalledlinearunits.Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=W\\ue03eh+b.LinearoutputlayersareoftenusedtoproducethemeanofaconditionalGaussiandistribution:p() = (;yx|NyˆyI,).(6.17)180'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 195}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquarederror.ThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthecovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbeafunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositivedeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinearoutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.Approachestomodelingthecovariancearedescribedshortly,inSec..6.2.2.4Becauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimizationalgorithms.6.2.2.2SigmoidUnitsforBernoulliOutputDistributionsManytasksrequirepredictingthevalueofabinaryvariabley.Classiﬁcationproblemswithtwoclassescanbecastinthisform.Themaximum-likelihoodapproachistodeﬁneaBernoullidistributionoveryconditionedon.xABernoullidistributionisdeﬁnedbyjustasinglenumber.TheneuralnetneedstopredictonlyP(y= 1|x).Forthisnumbertobeavalidprobability,itmustlieintheinterval[0,1].Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposeweweretousealinearunit,andthresholditsvaluetoobtainavalidprobability:Py(= 1 ) = max|x\\ue06e0min,\\ue06e1,w\\ue03eh+b\\ue06f\\ue06f.(6.18)Thiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeabletotrainitveryeﬀectivelywithgradientdescent.Anytimethatw\\ue03eh+bstrayedoutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespecttoitsparameterswouldbe0.Agradientof0istypicallyproblematicbecausethelearningalgorithmnolongerhasaguideforhowtoimprovethecorrespondingparameters.Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysastronggradientwheneverthemodelhasthewronganswer.Thisapproachisbasedonusingsigmoidoutputunitscombinedwithmaximumlikelihood.Asigmoidoutputunitisdeﬁnedbyˆyσ= \\ue010w\\ue03eh+b\\ue011(6.19)181'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 196}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSwhereisthelogisticsigmoidfunctiondescribedinSec..σ3.10Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,itusesalinearlayertocomputez=w\\ue03eh+b.Next,itusesthesigmoidactivationfunctiontoconvertintoaprobability.zWeomitthedependenceonxforthemomenttodiscusshowtodeﬁneaprobabilitydistributionoveryusingthevaluez.Thesigmoidcanbemotivatedbyconstructinganunnormalizedprobabilitydistribution˜P(y),whichdoesnotsumto1.Wecanthendividebyanappropriateconstanttoobtainavalidprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalizedlogprobabilitiesarelinearinyandz,wecanexponentiatetoobtaintheunnormalizedprobabilities.WethennormalizetoseethatthisyieldsaBernoullidistributioncontrolledbyasigmoidaltransformationof:zlog˜Pyyz() = (6.20)˜Pyyz() = exp()(6.21)Py() =exp()yz\\ue0501y\\ue030=0exp(y\\ue030z)(6.22)Pyσyz.() = ((2−1))(6.23)Probabilitydistributionsbasedonexponentiationandnormalizationarecommonthroughoutthestatisticalmodelingliterature.Thezvariabledeﬁningsuchadistributionoverbinaryvariablesiscalledalogit.Thisapproachtopredictingtheprobabilitiesinlog-spaceisnaturaltousewithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximumlikelihoodis−logP(y|x),theloginthecostfunctionundoestheexpofthesigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-based learningfrom making good progress.The lossfunction for maximumlikelihoodlearningofaBernoulliparametrizedbyasigmoidisJPy() = logθ−(|x)(6.24)= log((21))−σy−z(6.25)= ((12))ζ−yz.(6.26)ThisderivationmakesuseofsomepropertiesfromSec..Byrewriting3.10thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen(1−2y)zisverynegative.Saturationthusoccursonlywhenthemodelalreadyhastherightanswer—wheny= 1andzisverypositive,ory= 0andzisverynegative.Whenzhasthewrongsign,theargumenttothesoftplusfunction,182'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 197}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS(1−2y)z,maybesimpliﬁedto||z.As||zbecomeslargewhilezhasthewrongsign,thesoftplusfunctionasymptotestowardsimplyreturningitsargument||z.Thederivativewithrespecttozasymptotestosign(z),so,inthelimitofextremelyincorrectz,thesoftplusfunctiondoesnotshrinkthegradientatall.Thispropertyisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquicklycorrectamistaken.zWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscansaturateanytimeσ(z)saturates.Thesigmoidactivationfunctionsaturatesto0whenzbecomesverynegativeandsaturatestowhen1zbecomesverypositive.Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoidoutputunits.Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,becausethesigmoidreturnsvaluesrestrictedtotheopeninterval(0,1),ratherthanusingtheentireclosedintervalofvalidprobabilities[0,1].Insoftwareimplementations,toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasafunctionofz,ratherthanasafunctionofˆy=σ(z).Ifthesigmoidfunctionunderﬂowstozero,thentakingthelogarithmofˆyyieldsnegativeinﬁnity.6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributionsAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariablewithnpossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasageneralizationofthesigmoidfunctionwhichwasusedtorepresentaprobabilitydistributionoverabinaryvariable.Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresenttheprobabilitydistributionoverndiﬀerentclasses.Morerarely,softmaxfunctionscanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneofndiﬀerentoptionsforsomeinternalvariable.Inthecaseofbinaryvariables,wewishedtoproduceasinglenumberˆyPy.= (= 1 )|x(6.27)Becausethisnumberneededtoliebetweenand,andbecausewewantedthe01logarithmofthenumbertobewell-behavedforgradient-basedoptimizationofthelog-likelihood,wechosetoinsteadpredictanumberz=log˜P(y=1|x).ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythesigmoidfunction.183'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 198}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTogeneralizetothecaseofadiscretevariablewithnvalues,wenowneedtoproduceavectorˆy,withˆyi=P(y=i|x).Werequirenotonlythateachelementofˆyibebetweenand,butalsothattheentirevectorsumstosothat011itrepresentsavalidprobabilitydistribution.ThesameapproachthatworkedfortheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinearlayerpredictsunnormalizedlogprobabilities:zW= \\ue03ehb+,(6.28)wherezi=log˜P(y=i|x).Thesoftmaxfunctioncanthenexponentiateandnormalizetoobtainthedesiredzˆy.Formally,thesoftmaxfunctionisgivenbysoftmax()zi=exp(zi)\\ue050jexp(zj).(6.29)Aswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhentrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.Inthiscase,wewishtomaximizelogP(y=i;z)=logsoftmax(z)i.Deﬁningthesoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundotheofthesoftmax:explogsoftmax()zi= zi−log\\ue058jexp(zj).(6.30)TheﬁrsttermofEq.showsthattheinput6.30zialwayshasadirectcon-tributiontothecostfunction.Becausethistermcannotsaturate,weknowthatlearningcanproceed,evenifthecontributionofzitothesecondtermofEq.6.30becomesverysmall.Whenmaximizingthelog-likelihood,theﬁrsttermencourageszitobepushedup,whilethesecondtermencouragesallofztobepusheddown.Togainsomeintuitionforthesecondterm,log\\ue050jexp(zj),observethatthistermcanberoughlyapproximatedbymaxjzj.Thisapproximationisbasedontheideathatexp(zk)isinsigniﬁcantforanyzkthatisnoticeablylessthanmaxjzj.Theintuitionwecangainfromthisapproximationisthatthenegativelog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrectprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,thenthe−zitermandthelog\\ue050jexp(zj)≈maxjzj=zitermswillroughlycancel. Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbedominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.Sofarwehavediscussedonlyasingleexample.Overall,unregularizedmaximumlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict184'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 199}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSthefractionofcountsofeachoutcomeobservedinthetrainingset:softmax((;))zxθi≈\\ue050mj=11y()j=i,x()j=x\\ue050mj=11x()j=x.(6.31)Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappensolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.Inpractice,limitedmodelcapacityandimperfectoptimizationwillmeanthatthemodelisonlyabletoapproximatethesefractions.Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswellwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogtoundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomesverynegative,causingthegradienttovanish.Inparticular,squarederrorisapoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeitsoutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexaminethesoftmaxfunctionitself.Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhasasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremelypositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.Theseoutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecomeextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmaxalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofitsinputs:softmax() = softmax(+)zzc.(6.32)Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:softmax() = softmax(maxzz−izi).(6.33)Thereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumericalerrorsevenwhenzcontainsextremelylargeorextremelynegativenumbers.Ex-aminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdrivenbytheamountthatitsargumentsdeviatefrommaxizi.Anoutputsoftmax(z)isaturatestowhenthecorrespondinginputismaximal1(zi=maxizi)andziismuchgreaterthanalloftheotherinputs.Theoutputsoftmax(z)icanalsosaturatetowhen0ziisnotmaximalandthemaximumismuchgreater.Thisisageneralizationofthewaythatsigmoidunitssaturate,and185'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 200}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKScancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedtocompensateforit.Theargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutputeveryelementofz,asdescribedaboveusingthelinearlayerz=W\\ue03eh+b.Whilestraightforward,thisapproachactuallyoverparametrizesthedistribution.Theconstraintthatthenoutputsmustsumtomeansthatonly1n−1parametersarenecessary;theprobabilityofthen-thvaluemaybeobtainedbysubtractingtheﬁrstn−1 1probabilitiesfrom.Wecanthusimposearequirementthatoneelementofzbeﬁxed. Forexample,wecanrequirethatzn=0.Indeed,thisisexactlywhatthesigmoidunitdoes.DeﬁningP(y= 1|x) =σ(z)isequivalenttodeﬁningP(y= 1|x) =softmax(z)1withatwo-dimensionalzandz1= 0.Boththen−1argumentandthenargumentapproachestothesoftmaxcandescribethesamesetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,thereisrarelymuchdiﬀerencebetweenusingtheoverparametrizedversionortherestrictedversion,anditissimplertoimplementtheoverparametrizedversion.Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxasawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:thesoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarilycorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateralinhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Attheextreme(whenthediﬀerencebetweenthemaximalaiandtheothersislargeinmagnitude)itbecomesaformof(oneoftheoutputsisnearly1winner-take-allandtheothersarenearly0).Thename“softmax”canbesomewhatconfusing.Thefunctionismorecloselyrelatedtotheargmaxfunctionthanthemaxfunction. Theterm“soft”derivesfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable.Theargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuousordiﬀerentiable.Thesoftmaxfunctionthusprovidesa“softened”versionoftheargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)\\ue03ez.Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthecurrentnameisanentrenchedconvention.6.2.2.4OtherOutputTypesThelinear, sigmoid,andsoftmaxoutputunitsdescribed above arethemostcommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthatwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign186'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSagoodcostfunctionfornearlyanykindofoutputlayer.Ingeneral,ifwedeﬁneaconditionaldistributionp(yx|;θ),theprincipleofmaximumlikelihoodsuggestsweuseasourcostfunction.−|log(pyxθ;)Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunctionf(x;θ).Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,f(x;θ) =ωprovidestheparametersforadistributionovery.Ourlossfunctioncanthenbeinterpretedas.−log(;())pyωxForexample,wemaywishtolearnthevarianceofaconditionalGaussianfory,givenx.Inthesimplecase,wherethevarianceσ2isaconstant,thereisaclosedformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplytheempiricalmeanofthesquareddiﬀerencebetweenobservationsyandtheirexpectedvalue.Acomputationallymoreexpensiveapproachthatdoesnotrequirewritingspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthedistributionp(y|x)thatiscontrolledbyω=f(x;θ).Thenegativelog-likelihood−logp(y;ω(x))willthenprovideacostfunctionwiththeappropriatetermsnecessarytomakeouroptimizationprocedureincrementallylearnthevariance.Inthesimplecasewherethestandarddeviationdoesnotdependontheinput,wecanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnewparametermightbeσitselforcouldbeaparametervrepresentingσ2oritcouldbeaparameterβrepresenting1σ2,dependingonhowwechoosetoparametrizethedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvarianceinyfordiﬀerentvaluesofx. Thisiscalledaheteroscedasticmodel.Intheheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneofthevaluesoutputbyf(x;θ).AtypicalwaytodothisistoformulatetheGaussiandistributionusingprecision,ratherthanvariance,asdescribedinEq.3.22.Inthemultivariatecaseitismostcommontouseadiagonalprecisionmatrixdiag(6.34)()β.Thisformulationworkswellwithgradientdescentbecausetheformulaforthelog-likelihoodoftheGaussiandistributionparametrizedbyβinvolvesonlymul-tiplicationbyβiandadditionoflogβi.Thegradientofmultiplication,addition,andlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrizedtheoutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunctionbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,arbitrarilylargegradientsusuallyresultininstability.Ifweparametrizedtheoutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperationcanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.187'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 202}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemustensurethatthecovariancematrixoftheGaussianispositivedeﬁnite.Becausetheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesofthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixispositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,thentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.Ifwesupposethataistherawactivationofthemodelusedtodeterminethediagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecisionvector:β=ζ(a).Thissamestrategyappliesequallyifusingvarianceorstandarddeviationratherthanprecisionorifusingascalartimesidentityratherthandiagonalmatrix.Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethandiagonal. Ifthecovarianceisfullandconditional,thenaparametrizationmustbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.ThiscanbeachievedbywritingΣ() = ()xBxB\\ue03e()x,whereBisanunconstrainedsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthelikelihoodisexpensive,withadd×matrixrequiringO(d3)computationforthedeterminantandinverseofΣ(x)(orequivalently,andmorecommonlydone,itseigendecompositionorthatof).Bx()Weoftenwanttoperformmultimodalregression,thatis,topredictrealvaluesthatcomefromaconditionaldistributionp(yx|)thatcanhaveseveraldiﬀerentpeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureisanaturalrepresentationfortheoutput(,;,).Jacobsetal.1991Bishop1994NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixturedensitynetworks.AGaussianmixtureoutputwithncomponentsisdeﬁnedbytheconditionalprobabilitydistributionp() =yx|n\\ue058i=1pi(= c|Nx)(;yµ()i()x,Σ()i())x.(6.35)Theneuralnetworkmusthavethreeoutputs:avectordeﬁningp(c=i|x),amatrixprovidingµ()i(x)foralli,andatensorprovidingΣ()i(x)foralli.Theseoutputsmustsatisfydiﬀerentconstraints:1.Mixturecomponentsp(c=i|x):theseformamultinoullidistributionoverthendiﬀerentcomponentsassociatedwithlatentvariable1c,andcan1Weconsiderctobelatentbecausewedonotobserveitinthedata:giveninputxandtargety,itisnotpossibletoknowwithcertaintywhichGaussiancomponentwasresponsiblefory,butwecanimaginethatywasgeneratedbypickingoneofthem,andmakethatunobservedchoicearandomvariable.188'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 203}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKStypicallybeobtainedbyasoftmaxoverann-dimensionalvector,toguaranteethattheseoutputsarepositiveandsumto1.2.Meansµ()i(x):theseindicatethecenterormeanassociatedwiththei-thGaussiancomponent,andareunconstrained(typicallywithnononlinearityatallfortheseoutputunits).Ifyisad-vector,thenthenetworkmustoutputannd×matrixcontainingallnofthesed-dimensionalvectors. Learningthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthanlearningthemeansofadistributionwithonlyoneoutputmode. Weonlywanttoupdatethemeanforthecomponentthatactuallyproducedtheobservation.Inpractice,wedonotknowwhichcomponentproducedeachobservation.Theexpressionforthenegativelog-likelihoodnaturallyweightseachexample’scontributiontothelossforeachcomponentbytheprobabilitythatthecomponentproducedtheexample.3.CovariancesΣ()i(x):thesespecifythecovariancematrixforeachcomponenti.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonalmatrixtoavoidneedingtocomputedeterminants.Aswithlearningthemeansofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassignpartialresponsibilityforeachpointtoeachmixturecomponent.Gradientdescentwillautomaticallyfollowthecorrectprocessifgiventhecorrectspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.Ithasbeenreportedthatgradient-basedoptimizationofconditionalGaussianmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseonegetsdivisions(bythevariance)whichcanbenumericallyunstable(whensomevariancegetstobesmallforaparticularexample,yieldingverylargegradients).Onesolutionistoclipgradients(seeSec.)whileanotheristoscalethe10.11.1gradientsheuristically(MurrayandLarochelle2014,).Gaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsofspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).Themixturedensitystrategygivesawayforthenetworktorepresentmultipleoutputmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtainingahighdegreeofqualityinthesereal-valueddomains.AnexampleofamixturedensitynetworkisshowninFig..6.4Ingeneral,wemaywishtocontinuetomodellargervectorsycontainingmorevariables,andtoimposericherandricherstructuresontheseoutputvariables.Forexample,wemaywishforourneuralnetworktooutputasequenceofcharactersthatforms asentence.Inthesecases, wemaycontinueto usetheprincipleofmaximumlikelihoodappliedtoourmodelp(y;ω(x)),butthemodelweuse189'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nxy\\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.Theinputxissampledfromauniformdistributionandtheoutputyissampledfrompmodel(yx|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputtotheparametersoftheoutputdistribution. Theseparametersincludetheprobabilitiesgoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellastheparametersforeachmixturecomponent.EachmixturecomponentisGaussianwithpredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareabletovarywithrespecttotheinput,andtodosoinnonlinearways.xtodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.Chapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels10oversequences,andPartdescribesadvancedtechniquesformodelingarbitraryIIIprobabilitydistributions.6.3HiddenUnitsSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthatarecommontomostparametricmachinelearningmodelstrainedwithgradient-basedoptimization.Nowweturntoanissuethatisuniquetofeedforwardneuralnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthemodel.Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnotyethavemanydeﬁnitiveguidingtheoreticalprinciples.Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyothertypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentousewhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice). We190'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 205}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSdescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.Theseintuitionscanbeusedtosuggestwhentotryouteachoftheseunits.Itisusuallyimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsistsoftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthentraininganetworkwiththatkindofhiddenunitandevaluatingitsperformanceonavalidationset.Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableatallinputpoints.Forexample,therectiﬁedlinearfunctiong(z) =max{0,z}isnotdiﬀerentiableatz= 0.Thismayseemlikeitinvalidatesgforusewithagradient-basedlearningalgorithm.Inpractice,gradientdescentstillperformswellenoughforthesemodelstobeusedformachinelearningtasks. Thisisinpartbecauseneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumofthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshowninFig..TheseideaswillbedescribedfurtherinChapter.Becausewedonot4.38expecttrainingtoactuallyreachapointwherethegradientis0,itisacceptablefortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.Hiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiableatonlyasmallnumberofpoints.Ingeneral,afunctiong(z)hasaleftderivativedeﬁnedbytheslopeofthefunctionimmediatelytotheleftofzandarightderivativedeﬁnedbytheslopeofthefunctionimmediatelytotherightofz.Afunctionisdiﬀerentiableatzonlyifboththeleftderivativeandtherightderivativearedeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneuralnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthecaseofg(z) =max{0,z},theleftderivativeatz= 00isandtherightderivativeis.Softwareimplementationsofneuralnetworktrainingusuallyreturnoneof1theone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedorraisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-basedoptimizationonadigitalcomputerissubjecttonumericalerroranyway.Whenafunctionisaskedtoevaluateg(0),itisveryunlikelythattheunderlyingvaluetrulywas.Instead,itwaslikelytobesomesmallvalue0\\ue00fthatwasroundedto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but0theseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthatinpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunitactivationfunctionsdescribedbelow.Unlessindicatedotherwise,mosthiddenunitscanbedescribedasacceptingavectorofinputsx,computinganaﬃnetransformationz=W\\ue03ex+b,andthenapplyinganelement-wisenonlinearfunctiong(z).Mosthiddenunitsaredistinguishedfromeachotheronlybythechoiceoftheformoftheactivationfunction.g()z191'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 206}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.1RectiﬁedLinearUnitsandTheirGeneralizationsRectiﬁedlinearunitsusetheactivationfunction.gz,z() = max0{}Rectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinearunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitisthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain.Thismakesthederivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeoftherectifyingoperationisalmosteverywhere,andthederivativeoftherectifying0operationiseverywherethattheunitisactive.Thismeansthatthegradient1directionisfarmoreusefulforlearningthanitwouldbewithactivationfunctionsthatintroducesecond-ordereﬀects.Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:hW= (g\\ue03exb+).(6.36)Wheninitializingtheparametersoftheaﬃnetransformation,itcanbeagoodpracticetosetallelementsofbtoasmall,positivevalue,suchas0.1.Thismakesitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputsinthetrainingsetandallowthederivativestopassthrough.Severalgeneralizationsofrectiﬁedlinearunitsexist.Mostofthesegeneral-izationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperformbetter.Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-based methods onexamples forwhich theiractivation iszero.A varietyofgeneralizationsofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-where.Threegeneralizationsofrectiﬁedlinearunitsarebasedonusinganon-zeroslopeαiwhenzi<0:hi=g(zα,)i=max(0,zi)+αimin(0,zi). Absolutevaluerectiﬁcationﬁxesαi=−1toobtaing(z)=||z. Itisusedforobjectrecognitionfromimages(,),whereitmakessensetoseekfeaturesthatareJarrettetal.2009invariantunderapolarityreversaloftheinputillumination.Othergeneralizationsofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maasetal.2013)ﬁxesαitoasmallvaluelike0.01whileaparametricReLUPReLUortreatsαiasalearnableparameter(,).Heetal.2015Maxoutunits(,)generalizerectiﬁedlinearunitsfurther.Goodfellowetal.2013aInsteadofapplyinganelement-wisefunctiong(z),maxoutunitsdividezintogroupsofkvalues.Eachmaxoutunitthenoutputsthemaximumelementofone192'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 207}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofthesegroups:g()zi=maxj∈G()izj(6.37)whereG()iistheindicesoftheinputsforgroupi,{(i−1)k+1,...,ik}.Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultipledirectionsintheinputspace.xAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithuptokpieces.Maxoutunitscanthusbeseenaslearningtheactivationfunctionitselfratherthanjusttherelationshipbetweenunits.Withlargeenoughk,amaxoutunitcanlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,amaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionoftheinputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolutevaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearntoimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcoursebeparametrizeddiﬀerentlyfromanyoftheseotherlayertypes,sothelearningdynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthesamefunctionofasoneoftheotherlayertypes.xEachmaxoutunitisnowparametrizedbykweightvectorsinsteadofjustone,somaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.Theycanworkwellwithoutregularizationifthetrainingsetislargeandthenumberofpiecesperunitiskeptlow(,).Caietal.2013Maxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-tisticalandcomputationaladvantagesbyrequiringfewerparameters.Speciﬁcally,ifthefeaturescapturedbyndiﬀerentlinearﬁlterscanbesummarizedwithoutlosinginformationbytakingthemaxovereachgroupofkfeatures,thenthenextlayercangetbywithtimesfewerweights.kBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-dancythathelpsthemtoresistaphenomenoncalledcatastrophicforgettinginwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedoninthepast(,).Goodfellowetal.2014aRectiﬁedlinearunitsandallofthesegeneralizationsofthemarebasedontheprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimizationalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscanlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentrainingthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismucheasierwhensomelinearcomputations(withsomedirectionalderivativesbeingofmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork193'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 208}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSarchitectures,theLSTM,propagatesinformationthroughtimeviasummation—aparticularstraightforwardkindofsuchlinearactivation.ThisisdiscussedfurtherinSec..10.106.3.2LogisticSigmoidandHyperbolicTangentPriortotheintroductionofrectiﬁedlinearunits,mostneuralnetworksusedthelogisticsigmoidactivationfunctiongzσz() = ()(6.38)orthehyperbolictangentactivationfunctiongzz.() = tanh()(6.39)Theseactivationfunctionsarecloselyrelatedbecause.tanh() = 2(2)1zσz−Wehave alreadyseen sigmoidunitsasoutputunits, used topredict theprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal1unitssaturateacrossmostoftheirdomain—theysaturatetoahighvaluewhenzisverypositive,saturatetoalowvaluewhenzisverynegative,andareonlystronglysensitivetotheirinputwhenzisnear0.Thewidespreadsaturationofsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruseasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenanappropriatecostfunctioncanundothesaturationofthesigmoidintheoutputlayer.Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangentactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresemblestheidentityfunctionmoreclosely,inthesensethattanh(0) = 0whileσ(0) =12.Becausetanhissimilartoidentitynear,trainingadeepneuralnetwork0ˆy=w\\ue03etanh(U\\ue03etanh(V\\ue03ex))resemblestrainingalinearmodelˆy=w\\ue03eU\\ue03eV\\ue03exsolongastheactivationsofthenetworkcanbekeptsmall.Thismakestrainingthetanhnetworkeasier.Sigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-forwardnetworks.Recurrentnetworks,manyprobabilisticmodels, andsomeautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewiselinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethedrawbacksofsaturation.194'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 209}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.3OtherHiddenUnitsManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.Ingeneral,awidevarietyofdiﬀerentiablefunctionsperformperfectlywell.Manyunpublishedactivationfunctionsperformjustaswellasthepopularones.Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusingh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivationfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommontotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationsonstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunittypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcantimprovement.Newhiddenunittypesthatperformroughlycomparablytoknowntypesaresocommonastobeuninteresting.Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappearedintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.Onepossibilityistonothaveanactivationg(z)atall.Onecanalsothinkofthisasusingtheidentityfunctionastheactivationfunction.Wehavealreadyseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork. Itmayalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonlylineartransformations,thenthenetworkasawholewillbelinear.However,itisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consideraneuralnetworklayerwithninputsandpoutputs,h=g(W\\ue03ex+b).Wemayreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheotherusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehaveessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.Thefactoredapproachistocomputeh=g(V\\ue03eU\\ue03ex+b).IfUproducesqoutputs,thenUandVtogethercontainonly(n+p)qparameters,whileWcontainsnpparameters.Forsmallq,thiscanbeaconsiderablesavinginparameters.Itcomesatthecostofconstrainingthelineartransformationtobelow-rank,buttheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeraneﬀectivewayofreducingthenumberofparametersinanetwork.Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(asdescribedinSec.)butmaysometimesbeusedasahiddenunit.Softmax6.2.2.3unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewithkpossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhiddenunitsareusuallyonlyusedinmoreadvancedarchitecturesthatexplicitlylearntomanipulatememory,describedinSec..10.12195'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 210}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAfewotherreasonablycommonhiddenunittypesinclude:•Radial basis functionRBFor unit:hi=exp\\ue010−1σ2i||W:,i−||x2\\ue011.ThisfunctionbecomesmoreactiveasxapproachesatemplateW:,i.Becauseitsaturatestoformost,itcanbediﬃculttooptimize.0x•Softplus:g(a) =ζ(a) =log(1+ea).Thisisasmoothversionoftherectiﬁer,introducedby()forfunctionapproximationandbyDugasetal.2001NairandHinton2010()fortheconditionaldistributionsofundirectedprobabilisticmodels.()comparedthesoftplusandrectiﬁerandfoundGlorotetal.2011abetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.Thesoftplusdemonstratesthattheperformanceofhiddenunittypescanbeverycounterintuitive—onemightexpectittohaveanadvantageovertherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatinglesscompletely,butempiricallyitdoesnot.•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlikethelatter,itisbounded,g(a)=max(−1,min(1,a)).Itwasintroducedby().Collobert2004Hiddenunitdesignremainsanactiveareaofresearchandmanyusefulhiddenunittypesremaintobediscovered.6.4ArchitectureDesignAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.Thewordarchitecturereferstotheoverallstructureofthenetwork:howmanyunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.Mostneuralnetworksareorganizedintogroupsofunitscalledlayers. Mostneuralnetworkarchitecturesarrangetheselayersinachainstructure,witheachlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayerisgivenbyh(1)= g(1)\\ue010W(1)\\ue03exb+(1)\\ue011,(6.40)thesecondlayerisgivenbyh(2)= g(2)\\ue010W(2)\\ue03eh(1)+b(2)\\ue011,(6.41)andsoon.196'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 211}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInthesechain-basedarchitectures,themainarchitecturalconsiderationsaretochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,anetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deepernetworksoftenareabletousefarfewerunitsperlayerandfarfewerparametersandoftengeneralizetothetestset,butarealsooftenhardertooptimize. Theidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedbymonitoringthevalidationseterror.6.4.1UniversalApproximationPropertiesandDepthAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication,canbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasytotrainbecausemanylossfunctionsresultinconvexoptimizationproblemswhenappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequiresdesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-mationframework.Speciﬁcally,theuniversalapproximationtheorem(,Horniketal.1989Cybenko1989;,)statesthatafeedforwardnetworkwithalinearoutputlayerandatleastonehiddenlayerwithany“squashing”activationfunction(suchasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurablefunctionfromoneﬁnite-dimensionalspacetoanotherwithanydesirednon-zeroamountoferror,providedthatthenetworkisgivenenoughhiddenunits.Thederivativesofthefeedforwardnetworkcanalsoapproximatethederivativesofthefunctionarbitrarilywell(,).TheconceptofBorelmeasurabilityHorniketal.1990isbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatanycontinuousfunctiononaclosedandboundedsubsetofRnisBorelmeasurableandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmayalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespacetoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswithactivationfunctionsthatsaturatebothforverynegativeandforverypositivearguments,universalapproximationtheoremshavealsobeenprovenforawiderclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinearunit(,).Leshnoetal.1993Theuniversalapproximationtheoremmeansthatregardlessofwhatfunctionwearetryingtolearn,weknowthatalargeMLPwillbeabletorepresentthisfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeabletolearnthatfunction.EveniftheMLPisabletorepresentthefunction,learningcanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining197'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 212}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesiredfunction.Second,thetrainingalgorithmmightchoosethewrongfunctionduetooverﬁtting.RecallfromSec.thatthe“nofreelunch”theoremshowsthat5.2.1thereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworksprovideauniversalsystemforrepresentingfunctions,inthesensethat,givenafunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.Thereisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesandchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.Theuniversalapproximationtheoremsaysthatthereexistsanetworklargeenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnotsayhowlargethisnetworkwillbe.()providessomeboundsontheBarron1993sizeofasingle-layernetworkneededtoapproximateabroadclassoffunctions.Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possiblywithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobedistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:thenumberofpossiblebinaryfunctionsonvectorsv∈{0,1}nis22nandselectingonesuchfunctionrequires2nbits,whichwillingeneralrequireO(2n)degreesoffreedom.Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresentanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnandgeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethenumberofunitsrequiredtorepresentthedesiredfunctionandcanreducetheamountofgeneralizationerror.Thereexistfamiliesoffunctionswhichcanbeapproximatedeﬃcientlybyanarchitecturewithdepthgreaterthansomevalued,butwhichrequireamuchlargermodelifdepthisrestrictedtobelessthanorequaltod.Inmanycases,thenumberofhiddenunitsrequiredbytheshallowmodelisexponentialinn. Suchresultswereﬁrstprovenformodelsthatdonotresemblethecontinuous,diﬀerentiableneuralnetworksusedformachinelearning,buthavesincebeenextendedtothesemodels.Theﬁrstresultswereforcircuitsoflogicgates(,).LaterHåstad1986workextendedtheseresultstolinearthresholdunitswithnon-negativeweights(,;,),andthentonetworkswithHåstadandGoldmann1991Hajnaletal.1993continuous-valuedactivations(,;,). ManymodernMaass1992Maassetal.1994neuralnetworksuserectiﬁedlinearunits.()demonstratedLeshnoetal.1993thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,includingrectiﬁedlinearunits,haveuniversalapproximationproperties,buttheseresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythatasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Pascanuetal.198'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 213}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS()and()showedthatfunctionsrepresentablewitha2013bMontufaretal.2014deeprectiﬁernetcanrequireanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtainedfromrectiﬁernonlinearitiesormaxoutunits)canrepresentfunctionswithanumberofregionsthatisexponentialinthedepthofthenetwork.Fig.illustrateshowanetworkwithabsolutevaluerectiﬁcationcreates6.5mirrorimagesofthefunctioncomputedontopofsomehiddenunit,withrespecttotheinputofthathiddenunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreatemirrorresponses(onbothsidesoftheabsolutevaluenonlinearity).Bycomposingthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewiselinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeperrectiﬁernetworksformallyshownbyPascanu2014aMontufar2014etal.()andbyetal.().(Left)Anabsolutevaluerectiﬁcationunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxisofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.Afunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimageofasimplerpatternacrossthataxisofsymmetry.(Center)Thefunctioncanbeobtainedbyfoldingthespacearoundtheaxisofsymmetry.(Right)Anotherrepeatingpatterncanbefoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry(whichisnowrepeatedfourtimes,withtwohiddenlayers).Moreprecisely,themaintheoremin()statesthattheMontufaretal.2014numberoflinearregionscarvedoutbyadeeprectiﬁernetworkwithdinputs,depth,andunitsperhiddenlayer,islnO\\ue020\\ue012nd\\ue013dl(−1)nd\\ue021,(6.42)i.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersperlkunit,thenumberoflinearregionsisO\\ue010k(1)+l−d\\ue011.(6.43)199'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 214}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearninapplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytimewechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsomesetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshouldlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwewanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbeinterpretedfromarepresentationlearningpointofviewassayingthatwebelievethelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariationthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsofvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressingabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingofmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput. Theseintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbeanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternalprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralizationforawidevarietyoftasks(,;,;,;Bengioetal.2007Erhanetal.2009Bengio2009Mesnil2011Ciresan2012Krizhevsky2012Sermanetetal.,;etal.,;etal.,;etal.,2013Farabet2013Couprie2013Kahou2013Goodfellow;etal.,;etal.,;etal.,;etal.etal.,;2014dSzegedy,).SeeFig.andFig.forexamplesofsome2014a6.66.7oftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoesindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.6.4.2OtherArchitecturalConsiderationsSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthemainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.Inpractice,neuralnetworksshowconsiderablymorediversity.Manyneuralnetworkarchitectureshavebeendevelopedforspeciﬁctasks.SpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksaredescribedinChapter.Feedforwardnetworksmayalsobegeneralizedtothe9recurrentneuralnetworksforsequenceprocessing,describedinChapter,which10havetheirownarchitecturalconsiderations.Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthemostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextraarchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayeritolayeri+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfromoutputlayerstolayersnearertheinput.200'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 215}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n\\ue033\\ue034\\ue035\\ue036\\ue037\\ue038\\ue039\\ue031\\ue030\\ue031\\ue031\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue068\\ue069\\ue064\\ue064\\ue065\\ue06e\\ue020\\ue06c\\ue061\\ue079\\ue065\\ue072\\ue073\\ue039\\ue032\\ue02e\\ue030\\ue039\\ue032\\ue02e\\ue035\\ue039\\ue033\\ue02e\\ue030\\ue039\\ue033\\ue02e\\ue035\\ue039\\ue034\\ue02e\\ue030\\ue039\\ue034\\ue02e\\ue035\\ue039\\ue035\\ue02e\\ue030\\ue039\\ue035\\ue02e\\ue035\\ue039\\ue036\\ue02e\\ue030\\ue039\\ue036\\ue02e\\ue035\\ue054\\ue065\\ue073\\ue074\\ue020\\ue061\\ue063\\ue063\\ue075\\ue072\\ue061\\ue063\\ue079\\ue020\\ue028\\ue025\\ue029\\ue045\\ue066\\ue066\\ue065\\ue063\\ue074\\ue020\\ue06f\\ue066\\ue020\\ue044\\ue065\\ue070\\ue074\\ue068\\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenusedtotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellowetal.(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See2014dFig.foracontrolexperimentdemonstratingthatotherincreasestothemodelsizedo6.7notyieldthesameeﬀect.\\n201'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 216}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n000204060810......Numberofparameters×10891929394959697Testaccuracy(%)EﬀectofNumberofParameters3,convolutional3,fullyconnected11,convolutional\\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelislarger.ThisexperimentfromGoodfellow2014detal.()showsthatincreasingthenumberofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnotnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthofnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeoftheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthiscontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhavingover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceoverthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthefunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresulteitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,cornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependentsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognizethem).\\n202'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 217}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnectapairoflayerstoeachother.InthedefaultneuralnetworklayerdescribedbyalineartransformationviaamatrixW,everyinputunitisconnectedtoeveryoutputunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,sothateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsintheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreducethenumberofparametersandtheamountofcomputationrequiredtoevaluatethenetwork,butareoftenhighlyproblem-dependent.Forexample,convolutionalnetworks,describedinChapter,usespecializedpatternsofsparseconnections9thatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃculttogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneuralnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthathavebeenfoundtoworkwellfordiﬀerentapplicationdomains.6.5Back-PropagationandOtherDiﬀerentiationAlgo-rithmsWhenweuseafeedforwardneuralnetworktoacceptaninputxandproduceanoutputˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovidetheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayerandﬁnallyproducesˆy.Thisiscalledforwardpropagation.Duringtraining,forwardpropagationcancontinueonwarduntilitproducesascalarcostJ(θ).Theback-propagationbackpropalgorithm(,),oftensimplycalledRumelhartetal.1986a,allowstheinformationfromthecosttothenﬂowbackwardsthroughthenetwork,inordertocomputethegradient.Computingananalyticalexpressionforthegradientisstraightforward,butnumericallyevaluatingsuchanexpressioncanbecomputationallyexpensive.Theback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.Thetermback-propagationisoftenmisunderstood asmeaningthewholelearningalgorithmformulti-layerneuralnetworks.Actually,back-propagationrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.Furthermore,back-propagationisoftenmisunderstoodasbeingspeciﬁctomulti-layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthefunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient∇xf(xy,)foranarbitraryfunctionf,wherexisasetofvariableswhosederivativesaredesired,andyisanadditionalsetofvariablesthatareinputstothefunction203'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 218}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemostoftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,∇θJ(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,eitheraspartofthe learningprocess, or toanalyze thelearned model.The back-propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestrictedtocomputingthegradientofthecostfunctionwithrespecttotheparameters.Theideaofcomputingderivativesbypropagatinginformationthroughanetworkisverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunctionfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonlyusedcasewherehasasingleoutput.f6.5.1ComputationalGraphsSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohaveamoreprecisecomputationalgraphlanguage.Manywaysofformalizingcomputationasgraphsarepossible.Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemaybeascalar,vector,matrix,tensor,orevenavariableofanothertype.Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguageisaccompaniedbyasetofallowableoperations.Functionsmorecomplicatedthantheoperationsinthissetmaybedescribedbycomposingmanyoperationstogether.Withoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingleoutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhavemultipleentries,suchasavector.Softwareimplementationsofback-propagationusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinourdescriptionbecauseitintroducesmanyextradetailsthatarenotimportanttoconceptualunderstanding.Ifavariableyiscomputedbyapplyinganoperationtoavariablex,thenwedrawadirectededgefromxtoy. Wesometimesannotatetheoutputnodewiththenameoftheoperationapplied,andothertimesomitthislabelwhentheoperationisclearfromcontext.ExamplesofcomputationalgraphsareshowninFig..6.8204'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 219}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz zx xy y(a)×x xw w(b)u(1)u(1)dotb bu(2)u(2)+ˆyˆyσ\\n(c)X XW WU(1)U(1)matmulb bU(2)U(2)+H Hrelu\\nx xw w(d)ˆyˆydotλ λu(1)u(1)sqru(2)u(2)sumu(3)u(3)×\\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe(a)×operationtocomputez=xy. Thegraphforthelogisticregressionprediction(b)ˆy=σ\\ue000x\\ue03ew+b\\ue001.Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpressionbutneednamesinthegraph.Wesimplynamethei-thsuchvariableu()i.The(c)computationalgraphfortheexpressionH=max{0,XW+b},whichcomputesadesignmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatchofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit(d)ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthatappliesmorethanoneoperationtotheweightswofalinearregressionmodel.Theweightsareusedtomaketheboththepredictionˆyandtheweightdecaypenaltyλ\\ue050iw2i.205'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 220}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.5.2ChainRuleofCalculusThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)isusedtocomputethederivativesoffunctionsformedbycomposingotherfunctionswhosederivativesareknown.Back-propagationisanalgorithmthatcomputesthechainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.Letxbearealnumber,andletfandgbothbefunctionsmappingfromarealnumbertoarealnumber.Supposethaty=g(x)andz=f(g(x)) =f(y).Thenthechainrulestatesthatdzdx=dzdydydx.(6.44)Wecangeneralizethisbeyondthescalarcase.Supposethatx∈Rm,y∈Rn,gmapsfromRmtoRn,andfmapsfromRntoR.Ify=g(x) andz=f(y),then∂z∂xi=\\ue058j∂z∂yj∂yj∂xi.(6.45)Invectornotation,thismaybeequivalentlywrittenas∇xz=\\ue012∂y∂x\\ue013\\ue03e∇yz,(6.46)where∂y∂xistheJacobianmatrixof.nm×gFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplyingaJacobianmatrix∂y∂xbyagradient∇yz.Theback-propagationalgorithmconsistsofperformingsuchaJacobian-gradientproductforeachoperationinthegraph.Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,butrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythesameasback-propagationwithvectors.Theonlydiﬀerenceishowthenumbersarearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensorintoavectorbeforewerunback-propagation,computingavector-valuedgradient,andthenreshapingthegradientbackintoatensor.Inthisrearrangedview,back-propagationisstilljustmultiplyingJacobiansbygradients.TodenotethegradientofavaluezwithrespecttoatensorX,wewrite∇Xz,justasifXwereavector.TheindicesintoXnowhavemultiplecoordinates—forexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisawaybyusingasinglevariableitorepresentthecompletetupleofindices.Forallpossibleindextuplesi,(∇Xz)igives∂z∂Xi.Thisisexactlythesameashowforall206'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 221}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSpossibleintegerindicesiintoavector,(∇xz)igives∂z∂xi.Usingthisnotation,wecanwritethechainruleasitappliestotensors.Ifand,thenYX= (g)zf= ()Y∇Xz=\\ue058j(∇XYj)∂z∂Yj.(6.47)6.5.3RecursivelyApplyingtheChainRuletoObtainBackpropUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionforthegradientofascalarwithrespecttoanynodeinthecomputationalgraphthatproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputerintroducessomeextraconsiderations.Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithintheoverallexpressionforthegradient.Anyprocedurethatcomputesthegradientwillneedtochoosewhethertostorethesesubexpressionsortorecomputethemseveraltimes.AnexampleofhowtheserepeatedsubexpressionsariseisgiveninFig. . Insomecases,computingthesamesubexpressiontwicewouldsimply6.9bewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthesewastedcomputations,makinganaiveimplementationofthechainruleinfeasible.Inothercases,computingthesamesubexpressiontwicecouldbeavalidwaytoreducememoryconsumptionatthecostofhigherruntime.Weﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁestheactualgradientcomputationdirectly(AlgorithmalongwithAlgorithm6.26.1fortheassociatedforwardcomputation),intheorderitwillactuallybedoneandaccordingtotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthesecomputationsorviewthedescriptionofthealgorithmasasymbolicspeciﬁcationofthecomputationalgraphforcomputingtheback-propagation.How-ever,thisformulationdoesnotmakeexplicitthemanipulationandtheconstructionofthesymbolicgraphthatperformsthegradientcomputation.SuchaformulationispresentedbelowinSec.,withAlgorithm,wherewealsogeneralizeto6.5.66.5nodesthatcontainarbitrarytensors.Firstconsideracomputationalgraphdescribinghowtocomputeasinglescalaru()n(saythelossonatrainingexample).Thisscalaristhequantitywhosegradientwewanttoobtain,withrespecttotheniinputnodesu(1)tou(ni). Inotherwordswewishtocompute∂u()n∂u()iforalli∈{1,2,...,ni}.Intheapplicationofback-propagationtocomputinggradientsforgradientdescentoverparameters,u()nwillbethecostassociatedwithanexampleoraminibatch,whileu(1)tou(ni)correspondtotheparametersofthemodel.207'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 222}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSWewillassumethatthenodesofthegraphhavebeenorderedinsuchawaythatwecancomputetheiroutputoneaftertheother,startingatu(ni+1)andgoinguptou()n.AsdeﬁnedinAlgorithm,eachnode6.1u()iisassociatedwithanoperationf()iandiscomputedbyevaluatingthefunctionu()i= (fA()i)(6.48)whereA()iisthesetofallnodesthatareparentsofu()i.Algorithm6.1Aprocedurethatperformsthecomputationsmappingniinputsu(1)tou(ni)toanoutputu()n.Thisdeﬁnesacomputationalgraphwhereeachnodecomputesnumericalvalueu()ibyapplyingafunctionf()itothesetofargumentsA()ithatcomprisesthevaluesofpreviousnodesu()j,j<i,withjPa∈(u()i).Theinputtothecomputationalgraphisthevectorx,andissetintotheﬁrstninodesu(1)tou(ni).Theoutputofthecomputationalgraphisreadoﬀthelast(output)nodeu()n.fori,...,n= 1idou()i←xiendforforin= i+1,...,ndoA()i←{u()j|∈jPau(()i)}u()i←f()i(A()i)endforreturnu()nThatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecouldputinagraphG.Inordertoperformback-propagation,wecanconstructacomputationalgraphthatdependsonGandaddstoitanextrasetofnodes.TheseformasubgraphBwithonenodepernodeofG.ComputationinBproceedsinexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputesthederivative∂u()n∂u()iassociatedwiththeforwardgraphnodeu()i.Thisisdoneusingthechainrulewithrespecttoscalaroutputu()n:∂u()n∂u()j=\\ue058ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()j(6.49)asspeciﬁedbyAlgorithm.Thesubgraph6.2Bcontainsexactlyoneedgeforeachedgefromnodeu()jtonodeu()iofG.Theedgefromu()jtou()iisassociatedwiththecomputationof∂u()i∂u()j.Inaddition,adotproductisperformedforeachnode,betweenthegradientalreadycomputedwithrespecttonodesu()ithatarechildren208'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 223}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofu()jandthevectorcontainingthepartialderivatives∂u()i∂u()jforthesamechildrennodesu()i.Tosummarize,theamountofcomputationrequiredforperformingtheback-propagationscaleslinearlywiththenumberofedgesinG,wherethecomputationforeachedgecorrespondstocomputingapartialderivative(ofonenodewithrespecttooneofitsparents)aswellasperformingonemultiplicationandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,whichisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemoreeﬃcientimplementations.Algorithm6.2Simpliﬁedversionoftheback-propagationalgorithmforcomputingthederivativesofu()nwithrespecttothevariablesinthegraph.Thisexampleisintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariablesarescalars,andwewishtocomputethederivativeswithrespecttou(1),...,u(ni).Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.Thecomputationalcostofthisalgorithmisproportionaltothenumberofedgesinthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequiresaconstanttime.Thisisofthesameorderasthenumberofcomputationsfortheforwardpropagation.Each∂u()i∂u()jisafunctionoftheparentsu()jofu()i,thuslinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagationgraph.Runforwardpropagation(Algorithmforthisexample)toobtaintheactiva-6.1tionsofthenetworkInitializegrad_table,adatastructurethatwillstorethederivativesthathavebeencomputed.Theentrygradtable_[u()i]willstorethecomputedvalueof∂u()n∂u()i.gradtable_[∂u()n] 1←fordojn= −1downto1Thenextlinecomputes∂u()n∂u()j=\\ue050ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()jusingstoredvalues:gradtable_[u()j] ←\\ue050ijPau:∈(()i)gradtable_[u()i]∂u()i∂u()jendforreturn{gradtable_[u()i] = 1|i,...,ni}Theback-propagationalgorithmisdesignedtoreducethenumberofcommonsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorderofoneJacobianproductpernodeinthegraph. ThiscanbeseenfromthefactinAlgorithmthatbackpropvisitseachedgefromnode6.2u()jtonodeu()iofthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂u()j∂u()i.Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.209'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 224}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSHowever,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperformingsimpliﬁcationsonthecomputationalgraph,ormaybeabletoconservememorybyrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideasafterdescribingtheback-propagationalgorithmitself.6.5.4Back-PropagationComputationinFully-ConnectedMLPToclarifytheabovedeﬁnitionoftheback-propagationcomputation,letusconsiderthespeciﬁcgraphassociatedwithafully-connectedmulti-layerMLP.Algorithmﬁrstshowstheforwardpropagation,whichmapsparametersto6.3thesupervisedlossL(ˆyy,)associatedwithasingle(input,target)trainingexample()xy,,withˆytheoutputoftheneuralnetworkwhenisprovidedininput.xAlgorithm then shows the corresponding computation tobe done for6.4applyingtheback-propagationalgorithmtothisgraph.AlgorithmandAlgorithmaredemonstrationsthatarechosentobe6.36.4simpleandstraightforwardtounderstand.However,theyarespecializedtoonespeciﬁcproblem.Modernsoftwareimplementationsarebasedonthegeneralizedformofback-propagationdescribedinSec.below,whichcanaccommodateanycomputa-6.5.6tionalgraphbyexplicitlymanipulatingadatastructureforrepresentingsymboliccomputation.6.5.5Symbol-to-SymbolDerivativesAlgebraicexpressionsand computationalgraphsbothoperate onsymbols, orvariables thatdo nothave speciﬁc values.These algebraicandgraph-basedrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseortrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.Wereplaceasymbolicinputtothenetworkxwithaspeciﬁcvalue,suchasnumeric[12376518].,.,−.\\ue03e.Someapproachestoback-propagationtakeacomputationalgraphandasetofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumericalvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-to-number”diﬀerentiation.ThisistheapproachusedbylibrariessuchasTorch(,)andCaﬀe(,).Collobertetal.2011bJia2013Anotherapproachistotakeacomputationalgraphandaddadditionalnodestothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This210'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 225}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nx xy y\\nw wfff\\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputingthegradient.Letw∈Rbetheinputtothegraph.Weusethesamefunctionf:RR→astheoperationthatweapplyateverystepofachain:x=f(w),y=f(x),z=f(y).Tocompute∂z∂w,weapplyEq.andobtain:6.44∂z∂w(6.50)=∂z∂y∂y∂x∂x∂w(6.51)=f\\ue030()yf\\ue030()xf\\ue030()w(6.52)=f\\ue030((()))ffwf\\ue030(())fwf\\ue030()w(6.53)Eq.suggestsanimplementationinwhichwecomputethevalueof6.52f(w)onlyonceandstoreitinthevariablex.Thisistheapproachtakenbytheback-propagationalgorithm.AnalternativeapproachissuggestedbyEq. ,wherethesubexpression6.53f(w)appearsmorethanonce.Inthealternativeapproach,f(w)isrecomputedeachtimeitisneeded. Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,theback-propagationapproachofEq.isclearlypreferablebecauseofitsreduced6.52runtime.However,Eq.isalsoavalidimplementationofthechainrule,andisuseful6.53whenmemoryislimited.211'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 226}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkandthecomputationofthecostfunction.ThelossL(ˆyy,)dependsontheoutputˆyandonthetargety(seeSec.forexamplesoflossfunctions).Toobtainthe6.2.1.1totalcostJ,thelossmaybeaddedtoaregularizerΩ(θ),whereθcontainsalltheparameters(weightsandbiases).Algorithmshowshowtocomputegradients6.4ofJwithrespecttoparametersWandb.Forsimplicity,thisdemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshoulduseaminibatch.SeeSec.foramorerealisticdemonstration.6.5.7Require:Networkdepth,lRequire:W()i,i,...,l,∈{1}theweightmatricesofthemodelRequire:b()i,i,...,l,∈{1}thebiasparametersofthemodelRequire:x,theinputtoprocessRequire:y,thetargetoutputh(0)= xfordok,...,l= 1a()k= b()k+W()kh(1)k−h()k= (fa()k)endforˆyh= ()lJL= (ˆyy,)+Ω()λθistheapproachtakenbyTheano(,;,)Bergstraetal.2010Bastienetal.2012andTensorFlow(,).AnexampleofhowthisapproachworksAbadietal.2015is illustratedin Fig..Theprimary advantage ofthis approach isthat6.10thederivativesaredescribedinthesamelanguageastheoriginalexpression.Becausethederivativesarejustanothercomputationalgraph,itispossibletorunback-propagationagain,diﬀerentiatingthederivativesinordertoobtainhigherderivatives.Computationofhigher-orderderivativesisdescribedinSec..6.5.10Wewillusethelatterapproachanddescribetheback-propagationalgorithmintermsofconstructingacomputationalgraphforthederivatives.Anysubsetofthegraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.Thisallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasitsparents’valuesareavailable.Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodasperformingexactlythesamecomputationsasaredoneinthegraphbuiltbythesymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number212'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 227}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.4Backwardcomputation forthedeepneuralnetwork ofAlgo-rithm,whichusesinadditiontotheinput6.3xatargety.Thiscomputationyieldsthegradientsontheactivationsa()kforeachlayerk,startingfromtheoutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,whichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchangetoreduceerror,onecanobtainthegradientontheparametersofeachlayer.Thegradientsonweightsandbiasescanbeimmediatelyusedaspartofastochas-ticgradientupdate(performingtheupdaterightafterthegradientshavebeencomputed)orusedwithothergradient-basedoptimizationmethods.Aftertheforwardcomputation,computethegradientontheoutputlayer:g←∇ˆyJ= ∇ˆyL(ˆy,y)fordokl,l,...,= −11Convertthegradient onthelayer’soutputinto agradient into thepre-nonlinearityactivation(element-wisemultiplicationifiselement-wise):fg←∇a()kJf= g\\ue00c\\ue030(a()k)Computegradientsonweightsandbiases(includingtheregularizationterm,whereneeded):∇b()kJλ= +g∇b()kΩ()θ∇W()kJ= gh(1)k−\\ue03e+λ∇W()kΩ()θPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:g←∇h(1)k−J= W()k\\ue03egendfor\\n213'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSz z\\nx xy y\\nw wfffzz\\nx xy y\\nw wfffdzdydzdyf\\ue021dydxdydxf\\ue021dzdxdzdx×dxdwdxdwf\\ue021dzdwdzdw×Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.Inthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactualspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghowtocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethederivativesforanyspeciﬁcnumericvalues.(Left)Inthisexample,webeginwithagraphrepresentingz=f(f(f(w))).Weruntheback-propagationalgorithm,instructing(Right)ittoconstructthegraphfortheexpressioncorrespondingtodzdw.Inthisexample,wedonotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustratewhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthederivative.approachdoesnotexposethegraph.6.5.6GeneralBack-PropagationTheback-propagationalgorithmisverysimple.Tocomputethegradientofsomescalarzwithrespecttooneofitsancestorsxinthegraph,webeginbyobservingthatthegradientwithrespecttozisgivenbydzdz=1.WecanthencomputethegradientwithrespecttoeachparentofzinthegraphbymultiplyingthecurrentgradientbytheJacobianoftheoperationthatproducedz.WecontinuemultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntilwereachx.Foranynodethatmaybereachedbygoingbackwardsfromzthroughtwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsatthatnode.Moreformally,eachnodeinthegraphGcorrespondstoavariable. Toachievemaximumgenerality,wedescribethisvariableasbeingatensorV. Tensorcan214'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 229}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSingeneralhaveanynumberofdimensions,andsubsumescalars,vectors,andmatrices.Weassumethateachvariableisassociatedwiththefollowingsubroutines:V•getoperation_(V):ThisreturnstheoperationthatcomputesV,repre-sentedbytheedgescomingintoVinthecomputationalgraph.Forexample,theremaybeaPythonorC++classrepresentingthematrixmultiplicationoperation,andtheget_operationfunction.Supposewehaveavariablethatiscreatedbymatrixmultiplication,C=AB.Thengetoperation_(V)returnsapointertoaninstanceofthecorrespondingC++class.•getconsumers_(V,G):ThisreturnsthelistofvariablesthatarechildrenofVinthecomputationalgraph.G•Ggetinputs_(V,):ThisreturnsthelistofvariablesthatareparentsofVinthecomputationalgraph.GEachoperationopisalsoassociatedwithabpropoperation.ThisbpropoperationcancomputeaJacobian-vectorproductasdescribedbyEq..This6.47ishowtheback-propagationalgorithmisabletoachievegreatgenerality. Eachoperationisresponsibleforknowinghowtoback-propagatethroughtheedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrixmultiplicationoperationtocreateavariableC=AB.SupposethatthegradientofascalarzwithrespecttoCisgivenbyG.Thematrixmultiplicationoperationisresponsiblefordeﬁningtwoback-propagationrules,oneforeachofitsinputarguments.IfwecallthebpropmethodtorequestthegradientwithrespecttoAgiventhatthegradientontheoutputisG,thenthebpropmethodofthematrixmultiplicationoperationmuststatethatthegradientwithrespecttoAisgivenbyGB\\ue03e.Likewise,ifwecallthebpropmethodtorequestthegradientwithrespecttoB,thenthematrixoperationisresponsibleforimplementingthebpropmethodandspecifyingthatthedesiredgradientisgivenbyA\\ue03eG.Theback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiationrules.Itonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,opbprop.(inputs,,XG)mustreturn\\ue058i(∇Xopfinputs.()i)Gi,(6.54)which isjust animplementation ofthechain ruleas expressed in Eq..6.47Here,inputsisalistofinputsthataresuppliedtotheoperation,op.fisthemathematicalfunctionthattheoperationimplements,Xistheinputwhosegradientwewishtocompute,andisthegradientontheoutputoftheoperation.G215'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 230}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinctfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassedtwocopiesofxtocomputex2,theop.bpropmethodshouldstillreturnxasthederivativewithrespecttobothinputs.Theback-propagationalgorithmwilllateraddbothoftheseargumentstogethertoobtain2x,whichisthecorrecttotalderivativeon.xSoftwareimplementationsofback-propagationusuallyprovideboththeopera-tionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesareabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrixmultiplication,exponents,logarithms,andsoon.Softwareengineerswhobuildanewimplementationofback-propagationoradvanceduserswhoneedtoaddtheirownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodforanynewoperationsmanually.Theback-propagationalgorithmisformallydescribedinAlgorithm.6.5Algorithm6.5Theoutermostskeletonoftheback-propagationalgorithm.Thisportiondoessimplesetupandcleanupwork.MostoftheimportantworkhappensinthesubroutineofAlgorithmbuild_grad6.6.Require:T,thetargetsetofvariableswhosegradientsmustbecomputed.Require:G,thecomputationalgraphRequire:z,thevariabletobediﬀerentiatedLetG\\ue030beGprunedtocontainonlynodesthatareancestorsofzanddescendentsofnodesin.TInitialize,adatastructureassociatingtensorstotheirgradientsgrad_tablegradtable_[] 1z←fordoVinTbuildgrad_(V,,GG\\ue030,gradtable_)endforReturnrestrictedtograd_tableTInSec.,wemotivatedback-propagationasastrategyforavoidingcomput-6.5.2ingthesamesubexpressioninthechainrulemultipletimes.Thenaivealgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstanditscom-putationalcost.Ifweassumethateachoperationevaluationhasroughlythesamecost,thenwemayanalyzethecomputationalcostintermsofthenumberofoperationsexecuted. Keepinmindherethatwerefertoanoperationasthefundamentalunitofourcomputationalgraph,whichmightactuallyconsistofvery216'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 231}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.6Theinnerloopsubroutinebuildgrad_(V,,GG\\ue030,gradtable_)oftheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁnedinAlgorithm.6.5Require:V,thevariablewhosegradientshouldbeaddedtoand.Ggrad_tableRequire:G,thegraphtomodify.Require:G\\ue030,therestrictionoftonodesthatparticipateinthegradient.GRequire:grad_table,adatastructuremappingnodestotheirgradientsifthenVisingrad_tableReturn_gradtable[]Vendifi←1forCVin_getconsumers(,G\\ue030)doopgetoperation←_()CDC←buildgrad_(,,GG\\ue030,gradtable_)G()i←Gopbpropgetinputs.(_(C,\\ue030)),,VD  ii←+1endforG←\\ue050iG()igradtable_[] = VGInsertandtheoperationscreatingitintoGGReturnGmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrixmultiplicationasasingleoperation).ComputingagradientinagraphwithnnodeswillneverexecutemorethanO(n2)operationsorstoretheoutputofmorethanO(n2) operations.Herewearecountingoperationsinthecomputationalgraph,notindividualoperationsexecutedbytheunderlyinghardware,soitisimportanttorememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondtoasingleoperationinthegraph.WecanseethatcomputingthegradientrequiresasmostO(n2) operationsbecausetheforwardpropagationstagewillatworstexecuteallnnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithmaddsoneJacobian-vectorproduct,whichshouldbeexpressedwithO(1)nodes,peredgeintheoriginalgraph.BecausethecomputationalgraphisadirectedacyclicgraphithasatmostO(n2)edges.Forthekindsofgraphsthatarecommonlyusedinpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsareroughlychain-structured,causingback-propagationtohaveO(n)cost.Thisisfar217'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 232}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymanynodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewritingtherecursivechainrule(Eq.)non-recursively:6.49∂u()n∂u()j=\\ue058path(u(π1),u(π2),...,u(πt)),fromπ1=tojπt=nt\\ue059k=2∂u(πk)∂u(πk−1).(6.55)Sincethenumberofpathsfromnodejtonodencangrowuptoexponentiallyinthelengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumberofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagationgraph.Thislargecostwouldbeincurredbecausethesamecomputationfor∂u()i∂u()jwouldberedonemanytimes. Toavoidsuchrecomputation,wecanthinkofback-propagationasatable-ﬁllingalgorithmthattakesadvantageofstoringintermediateresults∂u()n∂u()i.Eachnodeinthegraphhasacorrespondingslotinatabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,back-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁllingstrategyissometimescalleddynamicprogramming.6.5.7Example:Back-PropagationforMLPTrainingAsanexample,wewalkthroughtheback-propagationalgorithmasitisusedtotrainamultilayerperceptron.Herewedevelopaverysimplemultilayerperceptionwithasinglehiddenlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.Theback-propagationalgorithmisusedtocomputethegradientofthecostonasingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetrainingsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.ThenetworkcomputesalayerofhiddenfeaturesH=max{0,XW(1)}.Tosimplifythepresentationwedonotusebiasesinthismodel.Weassumethatourgraphlanguageincludesareluoperationthatcancomputemax{0,Z}element-wise.ThepredictionsoftheunnormalizedlogprobabilitiesoverclassesarethengivenbyHW(2).Weassumethatourgraphlanguageincludesacross_entropyoperationthatcomputesthecross-entropybetweenthetargetsyandtheprobabilitydistributiondeﬁnedbytheseunnormalizedlogprobabilities.Theresultingcross-entropydeﬁnesthecostJMLE.Minimizingthiscross-entropyperformsmaximumlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,218'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 233}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nX XW(1)W(1)U(1)U(1)matmulH Hrelu\\nU(3)U(3)sqru(4)u(4)sumλ λu(7)u(7)W(2)W(2)U(2)U(2)matmuly yJMLEJMLEcross_entropy\\nU(5)U(5)sqru(6)u(6)sumu(8)u(8)J J+×+\\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexampleofasingle-layerMLPusingthecross-entropylossandweightdecay.wealsoincludearegularizationterm.ThetotalcostJJ= MLE+λ\\uf8eb\\uf8ed\\ue058i,j\\ue010W(1)i,j\\ue0112+\\ue058i,j\\ue010W(2)i,j\\ue0112\\uf8f6\\uf8f8(6.56)consistsofthecross-entropyandaweightdecaytermwithcoeﬃcientλ.ThecomputationalgraphisillustratedinFig..6.11Thecomputationalgraphforthegradientofthisexampleislargeenoughthatitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁtsoftheback-propagationalgorithm,whichisthatitcanautomaticallygenerategradientsthatwouldbestraightforwardbuttediousforasoftwareengineertoderivemanually.Wecanroughlytraceoutthebehavioroftheback-propagationalgorithmbylookingattheforwardpropagationgraphinFig..Totrain,wewish6.11tocomputeboth∇W(1)Jand∇W(2)J.TherearetwodiﬀerentpathsleadingbackwardfromJtotheweights:onethroughthecross-entropycost,andonethroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwillalwayscontribute2λW()itothegradientonW()i.219'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 234}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheotherpaththroughthecross-entropycostisslightlymorecomplicated.LetGbethegradientontheunnormalizedlogprobabilitiesU(2)providedbythecross_entropyoperation.Theback-propagationalgorithmnowneedstoexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsH\\ue03eGtothegradientonW(2),usingtheback-propagationruleforthesecondargumenttothematrixmultiplicationoperation.Theotherbranchcorrespondstothelongerchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithmcomputes∇HJ=GW(2)\\ue03eusingtheback-propagationrulefortheﬁrstargumenttothematrixmultiplicationoperation. Next,thereluoperationusesitsback-propagationruletozerooutcomponentsofthegradientcorrespondingtoentriesofU(1)thatwerelessthan.Lettheresultbecalled0G\\ue030.Thelaststepoftheback-propagationalgorithmistousetheback-propagationruleforthesecondargumentoftheoperationtoaddmatmulX\\ue03eG\\ue030tothegradientonW(1).Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradientdescentalgorithm,oranotheroptimizationalgorithm,tousethesegradientstoupdatetheparameters.FortheMLP,thecomputationalcostisdominatedbythecostofmatrixmultiplication.Duringtheforwardpropagationstage,wemultiplybyeachweightmatrix,resultinginO(w) multiply-adds,wherewisthenumberofweights.Duringthebackwardpropagationstage,wemultiplybythetransposeofeachweightmatrix,whichhasthesamecomputationalcost.Themainmemorycostofthealgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshasreturnedtothesamepoint.ThememorycostisthusO(mnh),wheremisthenumberofexamplesintheminibatchandnhisthenumberofhiddenunits.6.5.8ComplicationsOurdescriptionoftheback-propagationalgorithmhereissimplerthantheimple-mentationsactuallyusedinpractice.Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobeafunctionthatreturnsasingletensor.Mostsoftwareimplementationsneedtosupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewishtocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itisbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcienttoimplementthisprocedureasasingleoperationwithtwooutputs.Wehavenotdescribedhow tocontrolthememoryconsumptionofback-propagation.Back-propagationofteninvolvessummationofmanytensorstogether.220'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 235}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,thenallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverlyhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerandaddingeachvaluetothatbuﬀerasitiscomputed.Real-worldimplementationsofback-propagationalsoneedtohandlevariousdatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign.Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthesecasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.Variousothertechnicalitiesmakereal-worlddiﬀerentiationmorecomplicated.Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekeyintellectualtoolsneededtocomputederivatives,butitisimportanttobeawarethatmanymoresubtletiesexist.6.5.9DiﬀerentiationoutsidetheDeepLearningCommunityThe deeplearningcommunityhas been somewhatisolatedfrom thebroadercomputersciencecommunityandhaslargelydevelopeditsownculturalattitudesconcerninghowtoperformdiﬀerentiation.Moregenerally,theﬁeldofautomaticdiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically.Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomaticdiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreversemodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechainruleindiﬀerentorders.Ingeneral,determiningtheorderofevaluationthatresultsinthelowestcomputationalcostisadiﬃcultproblem.FindingtheoptimalsequenceofoperationstocomputethegradientisNP-complete(,),intheNaumann2008sensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleastexpensiveform.Forexample,supposewehavevariablesp1,p2,...,pnrepresentingprobabilitiesandvariablesz1,z2,...,znrepresentingunnormalizedlogprobabilities.Supposewedeﬁneqi=exp(zi)\\ue050iexp(zi),(6.57)wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivisionoperations,and construct across-entropylossJ=−\\ue050ipilogqi.A humanmathematiciancanobservethatthederivativeofJwithrespecttozitakesaverysimpleform:qi−pi.Theback-propagationalgorithmisnotcapableofsimplifyingthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof221'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 236}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSthelogarithmandexponentiationoperationsintheoriginalgraph.SomesoftwarelibrariessuchasTheano(,;,)areabletoBergstraetal.2010Bastienetal.2012performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposedbythepureback-propagationalgorithm.WhentheforwardgraphGhasasingleoutputnodeandeachpartialderivative∂u()i∂u()jcanbecomputedwithaconstantamountofcomputation,back-propagationguaranteesthatthenumberofcomputationsforthegradientcomputationisofthesameorderasthenumberofcomputationsfortheforwardcomputation:thiscanbeseeninAlgorithmbecauseeachlocalpartialderivative6.2∂u()i∂u()jneedstobecomputedonlyoncealongwithanassociatedmultiplicationandadditionfortherecursivechain-ruleformulation(Eq.).Theoverallcomputationis6.49thereforeO(#edges).However,itcanpotentiallybereducedbysimplifyingthecomputationalgraphconstructedbyback-propagation,andthisisanNP-completetask. ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedonmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplifythegraph.Wedeﬁnedback-propagationonlyforthecomputationofagradientofascalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(eitherofkdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontainingkvalues).Anaiveimplementationmaythenneedktimesmorecomputation:foreachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementationcomputeskgradientsinsteadofasinglegradient.Whenthenumberofoutputsofthegraphislargerthanthenumberofinputs,itissometimespreferabletouseanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputationofgradientsinrecurrentnetworks,forexample(,).ThisWilliamsandZipser1989alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,tradingoﬀcomputationaleﬃciencyformemory.Therelationshipbetweenforwardmodeandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversusright-multiplyingasequenceofmatrices,suchasABCD,(6.58)wherethematricescanbethoughtofasJacobianmatrices.Forexample,ifDisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwithasingleoutputandmanyinputs,andstartingthemultiplicationsfromtheendandgoingbackwardsonlyrequiresmatrix-vectorproducts.Thiscorrespondstothebackwardmode. Instead,startingtomultiplyfromtheleftwouldinvolveaseriesofmatrix-matrixproducts,whichmakesthewholecomputationmuchmoreexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorunthemultiplicationsleft-to-right,correspondingtotheforwardmode.222'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 237}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInmanycommunitiesoutsideofmachinelearning, itismorecommontoimplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramminglanguagecode,suchasPythonorCcode,andautomaticallygeneratesprogramsthatdiﬀerentfunctionswrittenintheselanguages.Inthedeeplearningcommunity,computationalgraphsareusuallyrepresentedbyexplicitdatastructurescreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackofrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperationandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.However,thespecializedapproachalsohasthebeneﬁtofallowingcustomizedback-propagationrulestobedevelopedforeachoperation,allowingthedevelopertoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedurewouldpresumablybeunabletoreplicate.Back-propagationisthereforenottheonlywayortheoptimalwayofcomputingthegradient,butitisaverypracticalmethodthatcontinuestoservethedeeplearningcommunityverywell.Inthefuture,diﬀerentiationtechnologyfordeepnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvancesinthebroaderﬁeldofautomaticdiﬀerentiation.6.5.10Higher-OrderDerivativesSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthedeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsforderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.Thismeansthatthesymbolicdiﬀerentiationmachinerycanbeappliedtoderivatives.Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivativeofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessianmatrix.Ifwehaveafunctionf:Rn→R,thentheHessianmatrixisofsizenn×.Intypicaldeeplearningapplications,nwillbethenumberofparametersinthemodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixisthusinfeasibletoevenrepresent.InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproachistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesforperformingvariousoperationslikeapproximatelyinvertingamatrixorﬁndingapproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperationotherthanmatrix-vectorproducts.InordertouseKrylovmethodsontheHessian,weonlyneedtobeabletocomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A223'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 238}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSstraightforwardtechnique(,)fordoingsoistocomputeChristianson1992Hv= ∇x\\ue068(∇xfx())\\ue03ev\\ue069.(6.59)Bothofthegradientcomputationsinthisexpressionmaybecomputedautomati-callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpressiontakesthegradientofafunctionoftheinnergradientexpression.Ifvisitselfavectorproducedbyacomputationalgraph,itisimportanttospecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethroughthegraphthatproduced.vWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowithHessianvectorproducts.OnesimplycomputesHe()iforalli= 1,...,n,wheree()iistheone-hotvectorwithe()ii= 1andallotherentriesequalto0.6.6HistoricalNotesFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximatorsbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.Fromthispointofview,themodernfeedforwardnetworkistheculminationofcenturiesofprogressonthegeneralfunctionapproximationtask.Thechainrulethatunderliestheback-propagationalgorithmwasinventedinthe17thcentury(,;,).CalculusandalgebrahaveLeibniz1676L’Hôpital1696longbeenusedtosolveoptimizationproblemsinclosedform,butgradientdescentwasnotintroducedasatechniqueforiterativelyapproximatingthesolutiontooptimizationproblemsuntilthe19thcentury(Cauchy1847,).Beginninginthe1940s,thesefunctionapproximationtechniqueswereusedtomotivatemachinelearningmodelssuchastheperceptron.However,theearliestmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedoutseveraloftheﬂawsofthelinearmodelfamily,suchasitinabilitytolearntheXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-ceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcientapplicationsofthechainrulebasedondynamicprogrammingbegantoappearinthe1960sand1970s,mostlyforcontrolapplications(,;,Kelley1960BrysonandDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973;,;,;,)butalsoforsensitivityanalysis(,).Linnainmaa1976Werbos1981()proposedapplyingthesetechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydevelopedinpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985Parker,224'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS1985Rumelhart1986a;etal.,).ThebookParallelDistributedProcessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswithback-propagationinachapter(,)thatcontributedgreatlytothepopularizationRumelhartetal.1986bofback-propagationandinitiatedaveryactiveperiodofresearchinmulti-layerneuralnetworks.However,theideasputforwardbytheauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyondback-propagation.Theyincludecrucialideasaboutthepossiblecomputationalimplementationofseveralcentralaspectsofcognitionandlearning,whichcameunderthenameof“connectionism”becauseoftheimportancegiventheconnectionsbetweenneuronsasthelocusoflearningandmemory.Inparticular,theseideasincludethenotionofdistributedrepresentation(,).Hintonetal.1986Followingthesuccessofback-propagation,neuralnetworkresearchgainedpop-ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearningtechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethatbeganin2006.Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-stantiallysincethe1980s. Thesameback-propagationalgorithmandthesameapproachestogradientdescentarestillinuse.Mostoftheimprovementinneuralnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,largerdatasetshavereducedthedegreetowhichstatisticalgeneralizationisachallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,duetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,asmallnumberofalgorithmicchangeshaveimprovedtheperformanceofneuralnetworksnoticeably.Oneofthesealgorithmicchangeswasthereplacementofmeansquarederrorwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularinthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandtheprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunityandthemachinelearningcommunity.Theuseofcross-entropylossesgreatlyimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,whichhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemeansquarederrorloss.Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformanceoffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewiselinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0,z}functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleastasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearlymodels didnot userectiﬁedlinearunits, butinsteadappliedrectiﬁcationto225'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSnonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwaslargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetterwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunitswereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswithnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.Jarrett2009etal.()observedthat“usingarectifyingnonlinearityisthesinglemostimportantfactorinimprovingtheperformanceofarecognitionsystem”amongseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.Forsmalldatasets,()observedthatusingrectifyingnon-Jarrettetal.2009linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁedlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerentfeaturevectorstoclassidentities.Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledgetoexceedtheperformanceofrandomlychosenparameters.()Glorotetal.2011ashowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeepnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.Rectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthatneurosciencehascontinuedtohavean inﬂuenceonthedevelopmentof deeplearningalgorithms.()motivaterectiﬁedlinearunitsfromGlorotetal.2011abiologicalconsiderations.Thehalf-rectifyingnonlinearitywasintendedtocapturethesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsarecompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportionaltoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewheretheyareinactive(i.e.,theyshouldhavesparseactivations).Whenthemodernresurgenceofdeeplearningbeganin2006,feedforwardnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidelybelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassistedbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththerightresourcesandengineeringpractices,feedforwardnetworksperformverywell.Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelopprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarialnetworks,describedinChapter.Ratherthanbeingviewedasanunreliable20technologythatmustbesupportedbyothertechniques,gradient-basedlearninginfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythatmaybeappliedtomanyothermachinelearningtasks. In2006,thecommunityusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,itismorecommontousesupervisedlearningtosupportunsupervisedlearning.226'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 241}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSFeedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,weexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimizationalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.Thischapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthesubsequentchapters,weturntohowtousethesemodels—howtoregularizeandtrainthem.\\n227'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 242}, page_content='Chapter7RegularizationforDeepLearningAcentralprobleminmachinelearningishowtomakeanalgorithmthatwillperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategiesusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possiblyattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectivelyasregularization. Aswewillseethereareagreatmanyformsofregularizationavailabletothedeeplearningpractitioner.Infact, developingmoreeﬀectiveregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.Chapterintroducedthebasicconceptsofgeneralization,underﬁtting,overﬁt-5ting,bias,varianceandregularization.Ifyouarenotalreadyfamiliarwiththesenotions,pleaserefertothatchapterbeforecontinuingwiththisone.Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblockstoformdeepmodels.Somesectionsofthischapterdealwithstandardconceptsinmachinelearning.Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevantsections.However,mostofthischapterisconcernedwiththeextensionofthesebasicconceptstotheparticularcaseofneuralnetworks.InSec.,wedeﬁnedregularizationas“anymodiﬁcationwemaketoa5.2.2learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.”Therearemanyregularizationstrategies.Someputextraconstraints ona machine learning model,such asadding restrictionson theparametervalues.Someaddextratermsintheobjectivefunctionthatcanbethoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosencarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance228'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 243}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencodespeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenaltiesaredesignedtoexpressagenericpreferenceforasimplermodelclassinordertopromotegeneralization.Sometimespenaltiesandconstraintsarenecessarytomakeanunderdeterminedproblemdetermined.Otherformsofregularization,knownasensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.Inthecontextofdeeplearning,mostregularizationstrategiesarebasedonregularizingestimators.Regularizationofanestimatorworksbytradingincreasedbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtabletrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.WhenwediscussedgeneralizationandoverﬁttinginChapter,wefocusedonthree5situations,wherethemodelfamilybeingtrainedeither(1)excludedthetruedatageneratingprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetruedatageneratingprocess,or(3)includedthegeneratingprocessbutalsomanyotherpossiblegeneratingprocesses—theoverﬁttingregimewherevarianceratherthanbiasdominatestheestimationerror.Thegoalofregularizationistotakeamodelfromthethirdregimeintothesecondregime.Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethetargetfunctionorthetruedatageneratingprocess,orevenacloseapproximationofeither.Wealmostneverhaveaccesstothetruedatageneratingprocesssowecanneverknowforsureifthemodelfamilybeingestimatedincludesthegeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithmsaretodomainswherethetruedatageneratingprocessisalmostcertainlyoutsidethemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremelycomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetruegenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosomeextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)intoaroundhole(ourmodelfamily).Whatthismeansisthatcontrollingthecomplexityofthemodelisnotasimplematterofﬁndingthemodeloftherightsize,withtherightnumberofparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,wealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizinggeneralizationerror)isalargemodelthathasbeenregularizedappropriately.Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularizedmodel.229'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 244}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1ParameterNormPenaltiesRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linearmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,andeﬀectiveregularizationstrategies.Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-rameternormpenaltyΩ(θ)totheobjectivefunctionJ.Wedenotetheregularizedobjectivefunctionby˜J:˜J,J,α(;θXy) = (;θXy)+Ω()θ(7.1)whereα∈[0,∞)isahyperparameterthatweightstherelativecontributionofthenormpenaltyterm,,relativetothestandardobjectivefunctionΩJ(x;θ).Settingαto0resultsinnoregularization.Largervaluesofαcorrespondtomoreregularization.Whenourtrainingalgorithmminimizestheregularizedobjectivefunction˜JitwilldecreaseboththeoriginalobjectiveJonthetrainingdataandsomemeasureofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerentchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred.ΩInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenaltiesonthemodelparameters.Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethatforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩpenalizesonlytheweightsoftheaﬃnetransformationateachlayerandleavesthebiasesunregularized.Thebiasestypicallyrequirelessdatatoﬁtaccuratelythantheweights. Eachweightspeciﬁeshowtwovariablesinteract. Fittingtheweightwellrequiresobservingbothvariablesinavarietyofconditions.Eachbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuchvariancebyleavingthebiasesunregularized.Also,regularizingthebiasparameterscanintroduceasigniﬁcantamountofunderﬁtting.Wethereforeusethevectorwtoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethevectorθdenotesalloftheparameters,includingbothwandtheunregularizedparameters.Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparatepenaltywithadiﬀerentαcoeﬃcientforeachlayerofthenetwork.Becauseitcanbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstillreasonabletousethesameweightdecayatalllayersjusttoreducethesizeofsearchspace.230'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 245}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1.1L2ParameterRegularizationWehavealreadyseen,inSec.,oneofthesimplestandmostcommonkinds5.2.2ofparameternormpenalty:theL2parameternormpenaltycommonlyknownasweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1byaddingaregularizationtermΩ(θ)=12\\ue06b\\ue06bw22totheobjectivefunction.Inotheracademiccommunities,L2regularizationisalsoknownasridgeregressionorTikhonovregularization.Wecangainsomeinsightintothebehaviorofweightdecayregularizationbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythepresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthefollowingtotalobjectivefunction:˜J,(;wXy) =α2w\\ue03ewwXy+(J;,),(7.2)withthecorrespondingparametergradient∇w˜J,α(;wXy) = w+∇wJ,.(;wXy)(7.3)Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate:www←−\\ue00fα(+∇wJ,.(;wXy))(7.4)Writtenanotherway,theupdateis:ww←−(1\\ue00fα)−∇\\ue00fwJ,.(;wXy)(7.5)Wecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearningruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensinasinglestep.Butwhathappensovertheentirecourseoftraining?Wewillfurthersimplifytheanalysisbymakingaquadraticapproximationtotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthatobtainsminimalunregularizedtrainingcost,w∗=argminwJ(w).Iftheobjectivefunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith1Moregenerally,wecouldregularizetheparameterstobenearanyspeciﬁcpointinspaceand,surprisingly,stillgetaregularizationeﬀect,butbetterresultswillbeobtainedforavalueclosertothetrueone,withzerobeingadefaultvaluethatmakessensewhenwedonotknowifthecorrectvalueshouldbepositiveornegative.Sinceitisfarmorecommontoregularizethemodelparameterstowardszero,wewillfocusonthisspecialcaseinourexposition.231'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 246}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGmeansquarederror,thentheapproximationisperfect.TheapproximationˆJisgivenbyˆJJ() = θ(w∗)+12(ww−∗)\\ue03eHww(−∗),(7.6)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Thereisnoﬁrst-orderterminthisquadraticapproximation,becausew∗isdeﬁnedtobeaminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofaminimumof,wecanconcludethatispositivesemideﬁnite.JHTheminimumofˆJoccurswhereitsgradient∇wˆJ() = (wHww−∗)(7.7)isequalto.0Tostudytheeﬀectofweightdecay,wemodifyEq.byaddingtheweight7.7decaygradient.WecannowsolvefortheminimumoftheregularizedversionofˆJ.Weusethevariable˜wtorepresentthelocationoftheminimum.α˜wH+(˜ww−∗) = 0(7.8)(+)HαI˜wHw= ∗(7.9)˜wHI= (+α)−1Hw∗.(7.10)Asαapproaches0,theregularizedsolution˜wapproachesw∗.Butwhathappensasαgrows?BecauseHisrealandsymmetric,wecandecomposeitintoadiagonalmatrixΛandanorthonormalbasisofeigenvectors,Q,suchthatHQQ= Λ\\ue03e.ApplyingthedecompositiontoEq.,weobtain:7.10˜wQQ= (Λ\\ue03e+)αI−1QQΛ\\ue03ew∗(7.11)=\\ue068QIQ(+Λα)\\ue03e\\ue069−1QQΛ\\ue03ew∗(7.12)= (+)QΛαI−1ΛQ\\ue03ew∗.(7.13)Weseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedbytheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththei-theigenvectorofHisrescaledbyafactorofλiλi+α.(Youmaywishtoreviewhowthiskindofscalingworks,ﬁrstexplainedinFig.).2.3AlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,whereλi\\ue01dα,theeﬀectofregularizationisrelativelysmall.However,componentswithλi\\ue01cαwillbeshrunktohavenearlyzeromagnitude.ThiseﬀectisillustratedinFig..7.1232'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 247}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw1w2w∗˜w\\nFigure7.1:AnillustrationoftheeﬀectofL2(orweightdecay)regularizationonthevalueoftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularizedobjective.ThedottedcirclesrepresentcontoursofequalvalueoftheL2regularizer.Atthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,theeigenvalueoftheHessianofJissmall. Theobjectivefunctiondoesnotincreasemuchwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpressastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.Theregularizerpullsw1closetozero.Intheseconddimension,theobjectivefunctionisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,indicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionofw2relativelylittle.Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducingtheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonotcontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessiantellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.Componentsoftheweightvectorcorrespondingtosuchunimportantdirectionsaredecayedawaythroughtheuseoftheregularizationthroughouttraining.Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimizationofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelatetomachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,amodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothesamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewillbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnowphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis233'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 248}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGthesumofsquarederrors:()Xwy−\\ue03e()Xwy−.(7.14)WhenweaddL2regularization,theobjectivefunctionchangesto()Xwy−\\ue03e()+Xwy−12αw\\ue03ew.(7.15)ThischangesthenormalequationsforthesolutionfromwX= (\\ue03eX)−1X\\ue03ey(7.16)towX= (\\ue03eXI+α)−1X\\ue03ey.(7.17)ThematrixX\\ue03eXinEq.isproportionaltothecovariancematrix7.161mX\\ue03eX.UsingL2regularizationreplacesthismatrixwith\\ue000X\\ue03eXI+α\\ue001−1inEq..7.17Thenewmatrixisthesameastheoriginalone,butwiththeadditionofαtothediagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeachinputfeature.WecanseethatL2regularizationcausesthelearningalgorithmto“perceive”theinputXashavinghighervariance,whichmakesitshrinktheweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedtothisaddedvariance.7.1.2L1RegularizationWhileL2weightdecayisthemostcommonformofweightdecay,thereareotherwaystopenalizethesizeofthemodelparameters. AnotheroptionistouseL1regularization.Formally,L1regularizationonthemodelparameterisdeﬁnedas:wΩ() = θ||||w1=\\ue058i|wi|,(7.18)thatis,asthesumofabsolutevaluesoftheindividualparameters.2WewillnowdiscusstheeﬀectofL1regularizationonthesimplelinearregressionmodel,withnobiasparameter,thatwestudiedinouranalysisofL2regularization.Inparticular,weareinterestedindelineatingthediﬀerencesbetweenL1andL2forms2AswithL2regularization,wecouldregularizetheparameterstowardsavaluethatisnotzero,butinsteadtowardssomeparametervaluew()o.InthatcasetheL1regularizationwouldintroducethetermΩ() = θ||−ww()o||1=\\ue050i|wi−w()oi|.234'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 249}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofregularization.AswithL2weightdecay,L1weightdecaycontrolsthestrengthoftheregularizationbyscalingthepenaltyusingapositivehyperparameterΩα.Thus,theregularizedobjectivefunction˜J,(;wXy)isgivenby˜J,α(;wXy) = ||||w1+(;)JwXy,,(7.19)withthecorrespondinggradient(actually,sub-gradient):∇w˜J,α(;wXy) = sign()+w∇wJ,(Xyw;)(7.20)whereissimplythesignofappliedelement-wise.sign()wwByinspectingEq.,wecanseeimmediatelythattheeﬀectof7.20L1regu-larizationisquitediﬀerentfromthatofL2regularization.Speciﬁcally,wecanseethattheregularizationcontributiontothegradientnolongerscaleslinearlywitheachwi;insteaditisaconstantfactorwithasignequaltosign(wi).OneconsequenceofthisformofthegradientisthatwewillnotnecessarilyseecleanalgebraicsolutionstoquadraticapproximationsofJ(Xy,;w)aswedidforL2regularization.OursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresentviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylorseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradientinthissettingisgivenby∇wˆJ() = (wHww−∗),(7.21)where,again,istheHessianmatrixofwithrespecttoevaluatedatHJww∗.BecausetheL1penaltydoesnotadmitcleanalgebraicexpressionsinthecaseofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumptionthattheHessianisdiagonal,H=diag([H11,,...,Hn,n]),whereeachHi,i>0.Thisassumptionholdsifthedataforthelinearregressionproblemhasbeenpreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybeaccomplishedusingPCA.OurquadraticapproximationoftheL1regularizedobjectivefunctiondecom-posesintoasumovertheparameters:ˆJ,J(;wXy) = (w∗;)+Xy,\\ue058i\\ue01412Hi,i(wi−w∗i)2+αw|i|\\ue015.(7.22)Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution(foreachdimension),withthefollowingform:iwi= sign(w∗i)max\\ue01a|w∗i|−αHi,i,0\\ue01b.(7.23)235'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 250}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConsiderthesituationwherew∗i>i0forall.Therearetwopossibleoutcomes:1.Thecasewherew∗i≤αHi,i.Heretheoptimalvalueofwiundertheregularizedobjectiveissimplywi= 0.ThisoccursbecausethecontributionofJ(w;Xy,)totheregularizedobjective˜J(w;Xy,)isoverwhelmed—indirectioni—bytheL1regularizationwhichpushesthevalueofwitozero.2.Thecasewherew∗i>αHi,i.Inthiscase,theregularizationdoesnotmovetheoptimalvalueofwitozerobutinsteaditjustshiftsitinthatdirectionbyadistanceequaltoαHi,i.Asimilarprocesshappenswhenw∗i<0,butwiththeL1penaltymakingwilessnegativebyαHi,i,or0.IncomparisontoL2regularization,L1regularizationresultsinasolutionthatismoresparse. Sparsityinthiscontextreferstothefactthatsomeparametershaveanoptimalvalueofzero.ThesparsityofL1regularizationisaqualitativelydiﬀerentbehaviorthanariseswithL2regularization.Eq.gavethesolution7.13˜wforL2regularization.IfwerevisitthatequationusingtheassumptionofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisofL1regularization,weﬁndthat˜wi=Hi,iHi,i+αw∗i. Ifw∗iwasnonzero,then˜wiremainsnonzero.ThisdemonstratesthatL2regularizationdoesnotcausetheparameterstobecomesparse,whileL1regularizationmaydosoforlargeenough.αThesparsitypropertyinducedbyL1regularizationhasbeenusedextensivelyasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearningproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.Inparticular,thewellknownLASSO(,)(leastabsoluteshrinkageandTibshirani1995selectionoperator)modelintegratesanL1penaltywithalinearmodelandaleastsquarescostfunction.TheL1penaltycausesasubsetoftheweightstobecomezero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.InSec.,wesawthatmanyregularizationstrategiescanbeinterpretedas5.6.1MAPBayesianinference,andthatinparticular,L2regularizationisequivalenttoMAPBayesianinferencewithaGaussianpriorontheweights. ForL1regu-larization,thepenaltyαΩ(w)=α\\ue050i|wi|usedtoregularizeacostfunctionisequivalenttothelog-priortermthatismaximizedbyMAPBayesianinferencewhenthepriorisanisotropicLaplacedistribution(Eq.)over3.26w∈Rn:log() =pw\\ue058ilogLaplace(wi;0,1α) = −||||αw1+loglog2nαn−.(7.24)236'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 251}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGFromthepointofviewoflearningviamaximizationwithrespecttow,wecanignorethetermsbecausetheydonotdependon.loglog2α−w7.2NormPenaltiesasConstrainedOptimizationConsiderthecostfunctionregularizedbyaparameternormpenalty:˜J,J,α.(;θXy) = (;θXy)+Ω()θ(7.25)RecallfromSec.thatwecanminimizeafunctionsubjecttoconstraintsby4.4constructingageneralizedLagrangefunction,consistingoftheoriginalobjectivefunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,calledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresentingwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthansomeconstant,wecouldconstructageneralizedLagrangefunctionkL−(;) = (;)+(Ω()θ,αXy,JθXy,αθk.)(7.26)Thesolutiontotheconstrainedproblemisgivenbyθ∗= argminθmaxα,α≥0L()θ,α.(7.27)AsdescribedinSec.,solvingthisproblemrequiresmodifyingboth4.4θandα.Sec.providesaworkedexampleoflinearregressionwithan4.5L2constraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,whileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinallproceduresαmustincreasewheneverΩ(θ)>kanddecreasewheneverΩ(θ)<k.AllpositiveαencourageΩ(θ)toshrink.Theoptimalvalueα∗willencourageΩ(θ)toshrink,butnotsostronglytomakebecomelessthan.Ω()θkTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁxα∗andviewtheproblemasjustafunctionof:θθ∗= argminθL(θ,α∗) = argminθJ,α(;θXy)+∗Ω()θ.(7.28)Thisisexactlythesameastheregularizedtrainingproblemofminimizing˜J.Wecanthusthinkofaparameternormpenaltyasimposingaconstraintontheweights.IfistheΩL2norm,thentheweightsareconstrainedtolieinanL2ball. IfistheΩL1norm,thentheweightsareconstrainedtolieinaregionof237'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 252}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGlimitedL1norm.Usuallywedonotknowthesizeoftheconstraintregionthatweimposebyusingweightdecaywithcoeﬃcientα∗becausethevalueofα∗doesnotdirectlytellusthevalueofk.Inprinciple,onecansolvefork,buttherelationshipbetweenkandα∗dependsontheformofJ.Whilewedonotknowtheexactsizeoftheconstraintregion,wecancontrolitroughlybyincreasingordecreasingαinordertogroworshrinktheconstraintregion.Largerαwillresultinasmallerconstraintregion.Smallerwillresultinalargerconstraintregion.αSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.AsdescribedinSec.,wecanmodifyalgorithmssuchasstochasticgradientdescent4.4totakeastepdownhillonJ(θ)andthenprojectθbacktothenearestpointthatsatisﬁesΩ(θ)<k.Thiscanbeusefulifwehaveanideaofwhatvalueofkisappropriateanddonotwanttospendtimesearchingforthevalueofαthatcorrespondstothis.kAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcingconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimizationprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentrainingneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthefunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemareallverysmall. Whentrainingwithapenaltyonthenormoftheweights,theseconﬁgurationscanbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduceJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projectioncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweightstoapproachtheorigin.Explicitconstraintsimplementedbyre-projectiononlyhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraintregion.Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimposesomestabilityontheoptimizationprocedure.Whenusinghighlearningrates,itispossibletoencounterapositivefeedbackloopinwhichlargeweightsinducelargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdatesconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfromtheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojectionpreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweightswithoutbound.()recommendusingconstraintscombinedwithHintonetal.2012cahighlearningratetoallowrapidexplorationofparameterspacewhilemaintainingsomestability.Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebroandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix238'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 253}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentireweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyonehiddenunitfromhavingverylargeweights.IfweconvertedthisconstraintintoapenaltyinaLagrangefunction,itwouldbesimilartoL2weightdecaybutwithaseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKTmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunitobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedasanexplicitconstraintwithreprojection.7.3RegularizationandUnder-ConstrainedProblemsInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-erlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregressionandPCA,dependoninvertingthematrixX\\ue03eX.ThisisnotpossiblewheneverX\\ue03eXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-butiontrulyhasnovarianceinsomedirection,orwhennovarianceinobservedinsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinvertingX\\ue03eXI+αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.Theselinearproblemshaveclosedformsolutionswhentherelevantmatrixisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobeunderdetermined.Anexampleislogisticregressionappliedtoaproblemwheretheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfectclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.Aniterativeoptimizationprocedurelikestochasticgradientdescentwillcontinuallyincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumericalimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweightstocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowtheprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.Mostformsofregularizationareabletoguaranteetheconvergenceofiterativemethodsappliedtounderdeterminedproblems. Forexample,weightdecaywillcausegradientdescenttoquitincreasingthemagnitudeoftheweightswhentheslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.Theideaofusingregularizationtosolveunderdeterminedproblemsextendsbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebraproblems.AswesawinSec.,wecansolveunderdeterminedlinearequationsusing2.9239'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 254}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMoore-Penrosepseudoinverse.RecallthatonedeﬁnitionofthepseudoinverseX+ofamatrixisXX+=limα\\ue0260(X\\ue03eXI+α)−1X\\ue03e.(7.29)WecannowrecognizeEq.asperforminglinearregressionwithweightdecay.7.29Speciﬁcally,Eq.isthelimitofEq.astheregularizationcoeﬃcientshrinks7.297.17tozero.Wecanthusinterpretthepseudoinverseasstabilizingunderdeterminedproblemsusingregularization.7.4DatasetAugmentationThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainitonmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Onewaytogetaroundthisproblemistocreatefakedataandaddittothetrainingset.Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenewfakedata.Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-cated,highdimensionalinputxandsummarizeitwithasinglecategoryidentityy.Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevarietyoftransformations.Wecangeneratenew(x,y)pairseasilyjustbytransformingtheinputsinourtrainingset.xThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,itisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehavealreadysolvedthedensityestimationproblem.Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁcclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandincludeanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncanoftengreatlyimprovegeneralization,evenifthemodelhasalreadybeendesignedtobepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniquesdescribedinChapter.Manyotheroperationssuchasrotatingtheimageor9scalingtheimagehavealsoprovenquiteeﬀective.Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrectclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthediﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontalﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthesetasks.240'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 255}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariantto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannotbeimplementedasasimplegeometricoperationontheinputpixels.Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(JaitlyandHinton2013,).Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)canalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationandevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmallrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobusttonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustnessofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheirinputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuchasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworkswhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdatasetaugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowedthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthenoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbedescribedinSec.,canbeseenasaprocessofconstructingnewinputsby7.12multiplyingbynoise.Whencomparingmachinelearningbenchmarkresults,itisimportanttotaketheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddatasetaugmentationschemescandramaticallyreducethegeneralizationerrorofamachinelearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithmtoanother,itisnecessarytoperformcontrolledexperiments.WhencomparingmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessarytomakesurethatbothalgorithmswereevaluatedusingthesamehand-designeddatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywithnodatasetaugmentationandalgorithmBperformswellwhencombinedwithnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythesynthetictransformationscausedtheimprovedperformance,ratherthantheuseofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperimenthasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machinelearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdatasetaugmentation.Usually,operationsthataregenerallyapplicable(suchasaddingGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,whileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomlycroppinganimage)areconsideredtobeseparatepre-processingsteps.241'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 256}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.5NoiseRobustnessSec.hasmotivatedtheuseofnoiseappliedtotheinputsasadatasetaug-7.4mentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimalvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthenormoftheweights(,,).Inthegeneralcase,itisimportanttoBishop1995abrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinkingtheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noiseappliedtothehiddenunitsissuchanimportanttopicastomerititsownseparatediscussion;thedropoutalgorithmdescribedinSec.isthemaindevelopment7.12ofthatapproach.Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodelsisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthecontextofrecurrentneuralnetworks(,;Jimetal.1996Graves2011,). ThiscanbeinterpretedasastochasticimplementationofaBayesianinferenceovertheweights. TheBayesiantreatmentoflearningwouldconsiderthemodelweightstobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthisuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂectthisuncertainty(Graves2011,).Thiscanalsobeinterpretedasequivalent(undersomeassumptions)toamoretraditionalformofregularization.Addingnoisetotheweightshasbeenshowntobeaneﬀectiveregularizationstrategyinthecontextofrecurrentneuralnetworks(,;Jimetal.1996Graves2011,).Inthefollowing,wewillpresentananalysisoftheeﬀectofweightnoiseonastandardfeedforwardneuralnetwork(asintroducedinChapter).6Westudytheregressionsetting,wherewewishtotrainafunctionˆy(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squarescostfunctionbetweenthemodelpredictionsˆy()xandthetruevalues:yJ= Epx,y()\\ue002(ˆyy()x−)2\\ue003.(7.30)Thetrainingsetconsistsoflabeledexamplesm{(x(1),y(1))(,...,x()m,y()m)}.Wenowassumethatwitheachinputpresentationwealsoincludearandomperturbation\\ue00fW∼N(\\ue00f;0,ηI)ofthenetworkweights.Letusimaginethatwehaveastandardl-layerMLP.Wedenotetheperturbedmodelasˆy\\ue00fW(x).Despitetheinjectionofnoise,wearestillinterestedinminimizingthesquarederroroftheoutputofthenetwork.Theobjectivefunctionthusbecomes:˜JW= Ep,y,(x\\ue00fW)\\ue068(ˆy\\ue00fW())x−y2\\ue069(7.31)242'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 257}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING= Ep,y,(x\\ue00fW)\\ue002ˆy2\\ue00fW()2ˆx−yy\\ue00fW()+xy2\\ue003.(7.32)Forsmallη,theminimizationofJwithaddedweightnoise(withcovarianceηI)isequivalenttominimizationofJwithanadditionalregularizationterm:ηEp,y(x)\\ue002\\ue06b∇Wˆy()x\\ue06b2\\ue003.Thisformofregularizationencouragestheparameterstogotoregionsofparameterspacewheresmallperturbationsoftheweightshavearelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodelintoregionswherethemodelisrelativelyinsensitivetosmallvariationsintheweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedbyﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinearregression(where,forinstance,ˆy(x) =w\\ue03ex+b),thisregularizationtermcollapsesintoηEp()x\\ue002\\ue06b\\ue06bx2\\ue003,whichisnotafunctionofparametersandthereforedoesnotcontributetothegradientof˜JWwithrespecttothemodelparameters.7.5.1InjectingNoiseattheOutputTargetsMostdatasetshavesomeamountofmistakesintheylabels.Itcanbeharmfultomaximizelogp(y|x)whenyisamistake.Onewaytopreventthisistoexplicitlymodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmallconstant\\ue00f,thetrainingsetlabelyiscorrectwithprobability1−\\ue00f,andotherwiseanyoftheotherpossiblelabelsmightbecorrect. Thisassumptioniseasytoincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawingnoisesamples. Forexample,labelsmoothingregularizesamodelbasedonasoftmaxwithkoutputvaluesbyreplacingthehardandclassiﬁcation01targetswithtargetsof\\ue00fkand1−k−1k\\ue00f,respectively.Thestandardcross-entropylossmaythenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmaxclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcanneverpredictaprobabilityofexactlyorexactly,soitwillcontinuetolearn01largerandlargerweights,makingmoreextremepredictionsforever.Itispossibletopreventthisscenariousingotherregularizationstrategieslikeweightdecay.Labelsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithoutdiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980sandcontinuestobefeaturedprominentlyinmodernneuralnetworks(,).Szegedyetal.20157.6Semi-SupervisedLearningIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfromP(x)andlabeledexamplesfromP(xy,)areusedtoestimateP(yx|)orpredictyfrom243'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 258}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGx.Inthecontextofdeeplearning,semi-supervisedlearningusuallyreferstolearningarepresentationh=f(x).Thegoalistolearnarepresentationsothatexamplesfromthesameclasshavesimilarrepresentations.Unsupervisedlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentationspace.Examplesthatclustertightlyintheinputspaceshouldbemappedtosimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebettergeneralizationinmanycases(BelkinandNiyogi2002Chapelle2003,;etal.,).Along-standingvariantofthisapproachistheapplicationofprincipalcomponentsanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojecteddata).Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthemodel,onecanconstructmodelsinwhichagenerativemodelofeitherP(x)orP(xy,)sharesparameterswithadiscriminativemodelofP(yx|).Onecanthentrade-oﬀthesupervisedcriterion−logP(yx|)withtheunsupervisedorgenerativeone(suchas−logP(x)or−logP(xy,)).Thegenerativecriterionthenexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervisedlearningproblem(,),namelythatthestructureofLasserreetal.2006P(x)isconnectedtothestructureofP(yx|)inawaythatiscapturedbythesharedparametrization.Bycontrollinghowmuchofthegenerativecriterionisincludedinthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerativeorapurelydiscriminativetrainingcriterion(,;Lasserreetal.2006LarochelleandBengio2008,).SalakhutdinovandHinton2008()describeamethodforlearningthekernelfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeledexamplesformodelingimprovesquitesigniﬁcantly.P()xP()yx|See()formoreinformationaboutsemi-supervisedlearning.Chapelleetal.20067.7Multi-TaskLearningMulti-tasklearning(,)isawaytoimprovegeneralizationbypoolingCaruana1993theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)arisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamplesputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralizewell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismoreconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyieldingbettergeneralization.244'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 259}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGFig.illustratesaverycommonformofmulti-tasklearning,inwhichdiﬀerent7.2supervisedtasks(predictingy()igivenx)sharethesameinputx,aswellassomeintermediate-levelrepresentationh(shared)capturingacommonpooloffactors.Themodelcangenerallybedividedintotwokindsofpartsandassociatedparameters:1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtasktoachievegoodgeneralization).ThesearetheupperlayersoftheneuralnetworkinFig..7.22.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthepooleddataofallthetasks).ThesearethelowerlayersoftheneuralnetworkinFig..7.2\\nh(1)h(1)h(2)h(2)h(3)h(3)y(1)y(1)y(2)y(2)\\nh(shared)h(shared)x xFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworksandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbutinvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetheritissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)canbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectivelywiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldingasharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommonpooloffactorsthatexplainthevariationsintheinputx,whileeachtaskisassociatedwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-levelhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredictingy(1)andy(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.Intheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobeassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeoftheinputvariationsbutarenotrelevantforpredictingy(1)ory(2).245'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 260}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n\\ue030\\ue035\\ue030\\ue031\\ue030\\ue030\\ue031\\ue035\\ue030\\ue032\\ue030\\ue030\\ue032\\ue035\\ue030\\ue054\\ue069\\ue06d\\ue065\\ue020\\ue028\\ue065\\ue070\\ue06f\\ue063\\ue068\\ue073\\ue029\\ue030\\ue02e\\ue030\\ue030\\ue030\\ue02e\\ue030\\ue035\\ue030\\ue02e\\ue031\\ue030\\ue030\\ue02e\\ue031\\ue035\\ue030\\ue02e\\ue032\\ue030\\ue04c\\ue06f\\ue073\\ue073\\ue020\\ue028\\ue06e\\ue065\\ue067\\ue061\\ue074\\ue069\\ue076\\ue065\\ue020\\ue06c\\ue06f\\ue067\\ue020\\ue06c\\ue069\\ue06b\\ue065\\ue06c\\ue069\\ue068\\ue06f\\ue06f\\ue064\\ue029\\ue04c\\ue065\\ue061\\ue072\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue063\\ue075\\ue072\\ue076\\ue065\\ue073\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue073\\ue065\\ue074\\ue020\\ue06c\\ue06f\\ue073\\ue073\\ue056\\ue061\\ue06c\\ue069\\ue064\\ue061\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue073\\ue065\\ue074\\ue020\\ue06c\\ue06f\\ue073\\ue073\\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesovertime(indicatedasnumberoftrainingiterationsoverthedataset,orepochs).Inthisexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjectivedecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginstoincreaseagain,forminganasymmetricU-shapedcurve.Improvedgeneralizationandgeneralizationerrorbounds(,)canbeBaxter1995achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbegreatlyimproved(inproportionwiththeincreasednumberofexamplesforthesharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethiswillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetweenthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssomeofthetasks.Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthefollowing:amongthefactorsthatexplainthevariationsobservedinthedataassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.7.8EarlyStoppingWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁtthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,butvalidationseterrorbeginstoriseagain.SeeFig.foranexampleofthisbehavior.7.3Thisbehavioroccursveryreliably.Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,hopefullybettertestseterror)byreturningtotheparametersettingatthepoint246'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 261}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGintimewiththelowestvalidationseterror.Insteadofrunningouroptimizationalgorithmuntilwereacha(local)minimumofvalidationerror,werunituntiltheerroronthevalidationsethasnotimprovedforsomeamountoftime.Everytimetheerroronthevalidationsetimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithmterminates,wereturntheseparameters,ratherthanthelatestparameters.ThisprocedureisspeciﬁedmoreformallyinAlgorithm.7.1Algorithm 7.1Theearlystopping meta-algorithmfor determiningthe bestamountoftimetotrain.Thismeta-algorithmisageneralstrategythatworkswellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthevalidationset.Letbethenumberofstepsbetweenevaluations.nLetpbethe“patience,”thenumberoftimestoobserveworseningvalidationseterrorbeforegivingup.Letθobetheinitialparameters.θθ←oi←0j←0v←∞θ∗←θi∗←iwhiledoj<pUpdatebyrunningthetrainingalgorithmforsteps.θniin←+v\\ue030←ValidationSetError()θifv\\ue030<vthenj←0θ∗←θi∗←ivv←\\ue030elsejj←+1endifendwhileBestparametersareθ∗,bestnumberoftrainingstepsisi∗Thisstrategyisknownasearlystopping.Itisprobablythemostcommonlyusedformofregularizationindeeplearning.Itspopularityisduebothtoitseﬀectivenessanditssimplicity.247'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 262}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGOnewaytothinkofearlystoppingisasaveryeﬃcienthyperparameterselectionalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.WecanseeinFig.thatthishyperparameterhasaU-shapedvalidationset7.3performancecurve.MosthyperparametersthatcontrolmodelcapacityhavesuchaU-shapedvalidationsetperformancecurve,asillustratedinFig..Inthecaseof5.3earlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermininghowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbechosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameteratthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The“trainingtime” hyperparameterisuniqueinthatbydeﬁnitionasinglerunoftrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcosttochoosingthishyperparameterautomaticallyviaearlystoppingisrunningthevalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdoneinparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparateGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthecostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatissmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorlessfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthebestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostoretheseparametersinaslowerandlargerformofmemory(forexample,traininginGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadiskdrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduringtraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.Earlystoppingisaveryunobtrusiveformofregularization,inthatitrequiresalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,orthesetofallowableparametervalues.Thismeansthatitiseasytouseearlystoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweightdecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthenetworkinabadlocalminimumcorrespondingtoasolutionwithpathologicallysmallweights.Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjectivefunctiontoencouragebettergeneralization,itisrareforthebestgeneralizationtooccuratalocalminimumofthetrainingobjective.Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnotfedtothemodel.Tobestexploitthisextradata,onecanperformextratrainingaftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra248'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 263}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategiesonecanuseforthissecondtrainingprocedure.Onestrategy(Algorithm)istoinitializethemodelagainandretrainonall7.2ofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsastheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Therearesomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagoodwayofknowingwhethertoretrainforthesamenumberofparameterupdatesorthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethetrainingsetisbigger.Algorithm7.2Ameta-algorithmforusingearlystoppingtodeterminehowlongtotrain,thenretrainingonallthedata.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(Algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisreturnsi∗,theoptimalnumberofsteps.Settorandomvaluesagain.θTrainonX()trainandy()trainfori∗steps.Anotherstrategyforusingallofthedataistokeeptheparametersobtainedfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallofthedata.Atthisstage,wenownolongerhaveaguideforwhentostopintermsofanumberofsteps. Instead,wecanmonitortheaveragelossfunctiononthevalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetrainingsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoidsthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.Forexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwilleverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.ThisprocedureispresentedmoreformallyinAlgorithm.7.3Earlystoppingisalsousefulbecauseitreducesthecomputationalcostofthetrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumberoftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithoutrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationofthegradientsofsuchadditionalterms.249'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 264}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGAlgorithm7.3Meta-algorithmusingearlystoppingtodetermineatwhatobjec-tivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(Algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisupdates.θ\\ue00fJ,←(θX()subtrain,y()subtrain)whileJ,(θX(valid),y(valid)) >\\ue00fdoTrainonX()trainandy()trainforsteps.nendwhileHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearlystoppingaregularizationstrategy,butwehavesupportedthisclaimonlybyisshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.Whatistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop()and()arguedthatearlystoppinghastheeﬀectof1995aSjöbergandLjung1995restrictingtheoptimizationproceduretoarelativelysmallvolumeofparameterspaceintheneighborhoodoftheinitialparametervalueθo.Morespeciﬁcally,imaginetakingτoptimizationsteps(correspondingtoτtrainingiterations)andwithlearningrate\\ue00f.Wecanviewtheproduct\\ue00fτasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restrictingboththenumberofiterationsandthelearningratelimitsthevolumeofparameterspacereachablefromθo.Inthissense,\\ue00fτbehavesasifitwerethereciprocalofthecoeﬃcientusedforweightdecay.Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadraticerrorfunctionandsimplegradientdescent—earlystoppingisequivalenttoL2regularization.InordertocomparewithclassicalL2regularization,weexamineasimplesettingwheretheonlyparametersarelinearweights(θ=w).WecanmodelthecostfunctionJwithaquadraticapproximationintheneighborhoodoftheempiricallyoptimalvalueoftheweightsw∗:ˆJJ() = θ(w∗)+12(ww−∗)\\ue03eHww(−∗),(7.33)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Giventheassumptionthatw∗isaminimumofJ(w),weknowthatHispositivesemideﬁnite.250'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 265}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw1w2w∗˜ww1w2w∗˜wFigure7.4:Anillustrationoftheeﬀectofearlystopping.(Left)Thesolidcontourlinesindicatethecontoursofthenegativelog-likelihood.ThedashedlineindicatesthetrajectorytakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗thatminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.Anillustrationoftheeﬀectof(Right)L2regularizationforcomparison.ThedashedcirclesindicatethecontoursoftheL2penalty,whichcausestheminimumofthetotalcosttolienearertheoriginthantheminimumoftheunregularizedcost.UnderalocalTaylorseriesapproximation,thegradientisgivenby:∇wˆJ() = (wHww−∗).(7.34)Wearegoingtostudythetrajectoryfollowedbytheparametervectorduringtraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3thatisw(0)= 0.Letussupposethatweupdatetheparametersviagradientdescent:w()τ= w(1)τ−−∇\\ue00fwJ(w(1)τ−)(7.35)= w(1)τ−−\\ue00fHw((1)τ−−w∗)(7.36)w()τ−w∗= ()(IH−\\ue00fw(1)τ−−w∗)(7.37)LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploitingtheeigendecompositionofH:H=QQΛ\\ue03e,whereΛisadiagonalmatrixandQisanorthonormalbasisofeigenvectors.w()τ−w∗= (IQQ−\\ue00fΛ\\ue03e)(w(1)τ−−w∗)(7.38)3Forneuralnetworks,toobtainsymmetrybreakingbetweenhiddenunits,wecannotinitializealltheparametersto0,asdiscussedinSec..However,theargumentholdsforanyother6.2initialvaluew(0).251'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 266}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGQ\\ue03e(w()τ−w∗) = ()I−\\ue00fΛQ\\ue03e(w(1)τ−−w∗)(7.39)Assumingthatw(0)=0andthat\\ue00fischosentobesmallenoughtoguarantee|1−\\ue00fλi|<1,theparametertrajectoryduringtrainingafterτparameterupdatesisasfollows:Q\\ue03ew()τ= [()I−I−\\ue00fΛτ]Q\\ue03ew∗.(7.40)Now,theexpressionforQ\\ue03e˜winEq.for7.13L2regularizationcanberearrangedas:Q\\ue03e˜wI= (+Λα)−1ΛQ\\ue03ew∗(7.41)Q\\ue03e˜wII= [−(+Λα)−1α]Q\\ue03ew∗(7.42)ComparingEq.andEq.,weseethatifthehyperparameters7.407.42\\ue00f,α,andτarechosensuchthat()I−\\ue00fΛτ= (+)ΛαI−1α,(7.43)thenL2regularizationandearlystoppingcanbeseentobeequivalent(atleastunderthequadraticapproximationoftheobjectivefunction).Goingevenfurther,bytakinglogarithmsandusingtheseriesexpansionforlog(1+x),wecanconcludethatifallλiaresmall(thatis,\\ue00fλi\\ue01c1andλi/α\\ue01c1)thenτ≈1\\ue00fα,(7.44)α≈1τ\\ue00f.(7.45)Thatis,undertheseassumptions,thenumberoftrainingiterationsτplaysaroleinverselyproportionaltotheL2regularizationparameter,andtheinverseofτ\\ue00fplaystheroleoftheweightdecaycoeﬃcient.Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(oftheobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespondtodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameterscorrespondingtodirectionsoflesscurvature.ThederivationsinthissectionhaveshownthatatrajectoryoflengthτendsatapointthatcorrespondstoaminimumoftheL2-regularizedobjective.Earlystoppingisofcoursemorethanthemererestrictionofthetrajectorylength;instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorinordertostopthetrajectoryataparticularlygoodpointinspace.Earlystoppingthereforehastheadvantageoverweightdecaythatearlystoppingautomaticallydeterminesthecorrectamountofregularizationwhileweightdecayrequiresmanytrainingexperimentswithdiﬀerentvaluesofitshyperparameter.252'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 267}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.9ParameterTyingandParameterSharingThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenaltiestotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.Forexample,L2regularization(orweightdecay)penalizesmodelparametersfordeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedotherwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtakebutweknow,fromknowledgeofthedomainandmodelarchitecture,thatthereshouldbesomedependenciesbetweenthemodelparameters.Acommontypeofdependencythatweoftenwanttoexpressisthatcertainparametersshouldbeclosetooneanother.Considerthefollowingscenario:wehavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetofclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodelAwithparametersw()AandmodelBwithparametersw()B.Thetwomodelsmaptheinput totwo diﬀerent, but relatedoutputs:ˆy()A=f(w()A,x)andˆy()B= (gw()B,x).Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinputandoutputdistributions)thatwebelievethemodelparametersshouldbeclosetoeachother:∀i,w()Aishouldbeclosetow()Bi.Wecanleveragethisinformationthroughregularization.Speciﬁcally,wecanuseaparameternormpenaltyoftheform:Ω(w()A,w()B)=\\ue06bw()A−w()B\\ue06b22. HereweusedanL2penalty,butotherchoicesarealsopossible.Thiskindofapproachwasproposedby(),whoregularizedLasserreetal.2006theparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,tobeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm(tocapturethedistributionoftheobservedinputdata).Thearchitectureswereconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbepairedtocorrespondingparametersintheunsupervisedmodel.Whileaparameternormpenaltyisonewaytoregularizeparameterstobeclosetooneanother,themorepopularwayistouseconstraints:toforcesetsofparameterstobeequal.Thismethodofregularizationisoftenreferredtoasparametersharing,whereweinterpretthevariousmodelsormodelcomponentsassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharingoverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlyasubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertainmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcantreductioninthememoryfootprintofthemodel.253'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 268}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConvolutionalNeuralNetworksByfarthemostpopularandextensiveuseofparametersharingoccursinconvolutionalneuralnetworks(CNNs)appliedtocomputervision.Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixeltotheright.CNNstakethispropertyintoaccountbysharingparametersacrossmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁndacatwiththesamecatdetectorwhetherthecatappearsatcolumniorcolumni+1intheimage.ParametersharinghasallowedCNNstodramaticallylowerthenumberofuniquemodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringacorrespondingincreaseintrainingdata. Itremainsoneofthebestexamplesofhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.CNNswillbediscussedinmoredetailinChapter.97.10SparseRepresentationsWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Anotherstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicatedpenaltyonthemodelparameters.Wehavealreadydiscussed(inSec.)how7.1.2L1penalizationinducesasparseparametrization—meaningthatmanyoftheparametersbecomezero(orclosetozero).Representationalsparsity,ontheotherhand,describesarepresentationwheremanyoftheelementsoftherepresentationarezero(orclosetozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextoflinearregression:\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f018515−9−3\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0400200−001030−050000100104−−100050−\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f023−2−514\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fby∈RmA∈Rmn×x∈Rn(7.46)\\n254'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 269}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−14119223\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0312541−−423113−−−−−154232312303−−−−−−542251\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00200−30\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fby∈RmB∈Rmn×h∈Rn(7.47)Intheﬁrstexpression,wehaveanexampleofasparselyparametrizedlinearregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-tionhofthedatax.Thatis,hisafunctionofxthat,insomesense,representstheinformationpresentin,butdoessowithasparsevector.xRepresentationalregularizationisaccomplishedbythesamesortsofmechanismsthatwehaveusedinparameterregularization.NormpenaltyregularizationofrepresentationsisperformedbyaddingtothelossfunctionJanormpenaltyontherepresentation.ThispenaltyisdenotedΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜J:˜J,J,α(;θXy) = (;θXy)+Ω()h(7.48)whereα∈[0,∞)weightstherelativecontributionofthenormpenaltyterm,withlargervaluesofcorrespondingtomoreregularization.αJustasanL1penaltyontheparametersinducesparametersparsity,anL1penaltyontheelementsoftherepresentationinducesrepresentationalsparsity:Ω(h) =||||h1=\\ue050i|hi|. Ofcourse,theL1penaltyisonlyonechoiceofpenaltythatcanresultinasparserepresentation.OthersincludethepenaltyderivedfromaStudent-tpriorontherepresentation(,;,)OlshausenandField1996Bergstra2011andKLdivergencepenalties(,)thatareespeciallyLarochelleandBengio2008usefulforrepresentationswithelementsconstrainedtolieontheunitinterval.Lee2008Goodfellow2009etal.()andetal.()bothprovideexamplesofstrategiesbasedonregularizingtheaverageactivationacrossseveralexamples,1m\\ue050ih()i,tobenearsometargetvalue,suchasavectorwith.01foreachentry.Otherapproachesobtainrepresentationalsparsitywithahardconstraintontheactivation values.Forexample, orthogonal matchingpursuit(,Patietal.1993)encodesaninputxwiththerepresentationhthatsolvestheconstrainedoptimizationproblemargminhh,\\ue06b\\ue06b0<k\\ue06b−\\ue06bxWh2,(7.49)where\\ue06b\\ue06bh0isthenumberofnon-zeroentriesofh. ThisproblemcanbesolvedeﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled255'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 270}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGOMP-kwiththevalueofkspeciﬁedtoindicatethenumberofnon-zerofeaturesallowed.()demonstratedthatOMP-canbeaveryeﬀectiveCoatesandNg20111featureextractorfordeeparchitectures.Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughoutthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyofcontexts.7.11BaggingandOtherEnsembleMethodsBaggingbootstrapaggregating(shortfor)isatechniqueforreducinggeneralizationerrorbycombiningseveralmodels(,).TheideaistotrainseveralBreiman1994diﬀerentmodelsseparately,thenhaveallofthemodelsvoteontheoutputfortestexamples.Thisisanexampleofageneralstrategyinmachinelearningcalledmodelaveraging.Techniquesemployingthisstrategyareknownasensemblemethods.Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusuallynotmakeallthesameerrorsonthetestset.Considerforexampleasetofkregressionmodels.Supposethateachmodelmakesanerror\\ue00fioneachexample, withtheerrorsdrawnfromazero-meanmultivariatenormaldistributionwithvariancesE[\\ue00f2i] =vandcovariancesE[\\ue00fi\\ue00fj] =c. Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis1k\\ue050i\\ue00fi.TheexpectedsquarederroroftheensemblepredictorisE\\uf8ee\\uf8f0\\ue0201k\\ue058i\\ue00fi\\ue0212\\uf8f9\\uf8fb=1k2E\\uf8ee\\uf8f0\\ue058i\\uf8eb\\uf8ed\\ue00f2i+\\ue058ji\\ue036=\\ue00fi\\ue00fj\\uf8f6\\uf8f8\\uf8f9\\uf8fb(7.50)=1kv+k−1kc.(7.51)Inthecasewheretheerrorsareperfectlycorrelatedandc=v,themeansquarederrorreducestov,sothemodelaveragingdoesnothelpatall.Inthecasewheretheerrorsareperfectlyuncorrelatedandc= 0,theexpectedsquarederroroftheensembleisonly1kv.Thismeansthattheexpectedsquarederroroftheensembledecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemblewillperformatleastaswellasanyofitsmembers,andifthemembersmakeindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletelydiﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging256'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 271}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n88First ensemble memberSecond ensemble memberOriginal datasetFirst resampled datasetSecond resampled datasetFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan‘8’detectoronthedatasetdepictedabove,containingan‘8’,a‘6’anda‘9’.Supposewemaketwodiﬀerentresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasetsbysamplingwithreplacement.Theﬁrstdatasetomitsthe‘9’andrepeatsthe‘8’.Onthisdataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan‘8’.Ontheseconddataset,werepeatthe‘9’andomitthe‘6’.Inthiscase,thedetectorlearnsthatalooponthebottomofthedigitcorrespondstoan‘8’.Eachoftheseindividualclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,achievingmaximalconﬁdenceonlywhenbothloopsofthe‘8’arepresent.isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjectivefunctiontobereusedseveraltimes.Speciﬁcally,bagginginvolvesconstructingkdiﬀerentdatasets.Eachdatasethasthesamenumberofexamplesastheoriginaldataset,buteachdatasetisconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeansthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromtheoriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtrainingset,ifithasthesamesizeastheoriginal).Modeliisthentrainedondataseti.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultindiﬀerencesbetweenthetrainedmodels.SeeFig.foranexample.7.5Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycanoftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesamedataset.Diﬀerencesinrandominitialization,randomselectionofminibatches,diﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-deterministicimple-mentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersoftheensembletomakepartiallyindependenterrors.257'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 272}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGModelaveragingisanextremelypowerfulandreliablemethodforreducinggeneralizationerror.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithmsforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrandPrize(Koren2009,).Notalltechniquesforconstructingensemblesaredesignedtomaketheensemblemoreregularizedthantheindividualmodels.Forexample,atechniquecalledboosting(FreundandSchapire1996ba,,)constructsanensemblewithhighercapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensemblesofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneuralnetworkstotheensemble. Boostinghasalsobeenappliedinterpretinganindividualneuralnetworkasanensemble(,),incrementallyaddinghiddenunitsBengioetal.2006atotheneuralnetwork.7.12DropoutDropout(Srivastava 2014etal., )providesa computationallyinexpensivebutpowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensemblesofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpracticalwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuchnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensemblesofﬁvetotenneuralnetworks—()usedsixtowintheILSVRC—Szegedyetal.2014abutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensiveapproximationtotrainingandevaluatingabaggedensembleofexponentiallymanyneuralnetworks.Speciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthatcanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,asillustratedinFig..Inmostmodernneuralnetworks,basedonaseriesof7.6aﬃnetransformationsandnonlinearities,wecaneﬀectivelyremoveaunitfromanetworkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressomeslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtakethediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresentthedropoutalgorithmintermsofmultiplicationbyzeroforsimplicity,butitcan258'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 273}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthenetwork.Recallthattolearnwithbagging,wedeﬁnekdiﬀerentmodels,constructkdiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthentrainmodeliondataseti.Dropoutaimstoapproximatethisprocess,butwithanexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,weuseaminibatch-basedlearningalgorithmthatmakessmallsteps,suchasstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,werandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhiddenunitsinthenetwork.Themaskforeachunitissampledindependentlyfromalloftheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobeincluded)isahyperparameterﬁxedbeforetrainingbegins. Itisnotafunctionofthecurrentvalueofthemodelparametersortheinputexample.Typically,aninputunitisincludedwithprobability0.8andahiddenunitisincludedwithprobability0.5.Wethenrunforwardpropagation,back-propagation,andthelearningupdateasusual.Fig.illustrateshowtorunforwardpropagationwith7.7dropout.Moreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,andJ(θµ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.ThendropouttrainingconsistsinminimizingEµJ(θµ,).Theexpectationcontainsexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradientbysamplingvaluesof.µDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseofbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshareparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromtheparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentanexponentialnumberofmodelswithatractableamountofmemory.Inthecaseofbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthecaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,themodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-networkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossiblesub-networksareeachtrainedforasinglestep,andtheparametersharingcausestheremainingsub-networkstoarriveatgoodsettingsoftheparameters.Thesearetheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.Forexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetoftheoriginaltrainingsetsampledwithreplacement.Tomakeaprediction,abaggedensemblemustaccumulatevotesfromallofitsmembers.Werefertothisprocessasinferenceinthiscontext.Sofar,our259'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 274}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ny yh1h1h2h2x1x1x2x2y yh1h1h2h2x1x1x2x2y yh1h1h2h2x2x2y yh1h1h2h2x1x1y yh2h2x1x1x2x2y yh1h1x1x1x2x2y yh1h1h2h2y yx1x1x2x2y yh2h2x2x2y yh1h1x1x1y yh1h1x2x2y yh2h2x1x1y yx1x1y yx2x2y yh2h2y yh1h1y yBase network\\nEnsemble of Sub-NetworksFigure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbeconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,webeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteenpossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformedbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnectingtheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwiderlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomessmaller.260'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 275}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nˆx1ˆx1µx1µx1x1x1ˆx2ˆx2x2x2µx2µx2h1h1h2h2µh1µh1µh2µh2ˆh1ˆh1ˆh2ˆh2y yy yh1h1h2h2x1x1x2x2\\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusingdropout.(Top)Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,onehiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward(Bottom)propagationwithdropout,werandomlysampleavectorµwithoneentryforeachinputorhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependentlyfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually10.5forthehiddenlayersand0.8fortheinput.Eachunitinthenetworkismultipliedbythecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthenetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfromFig.andrunningforwardpropagationthroughit.7.6261'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 276}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGdescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitlyprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobabilitydistribution.Inthecaseofbagging,eachmodeliproducesaprobabilitydistributionp()i(y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofallofthesedistributions,1kk\\ue058i=1p()i()y|x.(7.52)Inthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-abilitydistributionp(y,|xµ).Thearithmeticmeanoverallmasksisgivenby\\ue058µppy,()µ(|xµ)(7.53)wherep(µ)istheprobabilitydistributionthatwasusedtosampleµattrainingtime.Becausethissumincludesanexponentialnumberofterms,itisintractabletoevaluateexceptincaseswherethestructureofthemodelpermitssomeformofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractablesimpliﬁcation.Instead, wecan approximatethe inferencewithsampling, byaveragingtogethertheoutputfrommanymasks.Even10-20masksareoftensuﬃcienttoobtaingoodperformance.However,thereisanevenbetterapproach,thatallowsustoobtainagoodapproximationtothepredictionsoftheentireensemble,atthecostofonlyoneforwardpropagation.Todoso,wechangetousingthegeometricmeanratherthanthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-Farley2014etal.()presentargumentsandempiricalevidencethatthegeometricmeanperformscomparablytothearithmeticmeaninthiscontext.Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobeaprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,weimposetherequirementthatnoneofthesub-modelsassignsprobability0toanyevent,andwerenormalizetheresultingdistribution.Theunnormalizedprobabilitydistributiondeﬁneddirectlybythegeometricmeanisgivenby˜pensemble() =y|x2d\\ue073\\ue059µpy,(|xµ)(7.54)wheredisthenumberofunitsthatmaybedropped.Hereweuseauniformdistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare262'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 277}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGalsopossible.Tomakepredictionswemustre-normalizetheensemble:pensemble() =y|x˜pensemble()y|x\\ue050y\\ue030˜pensemble(y\\ue030|x).(7.55)Akeyinsight(,)involvedindropoutisthatwecanapproxi-Hintonetal.2012cmatepensemblebyevaluatingp(y|x)inonemodel:themodelwithallunits,butwiththeweightsgoingoutofunitimultipliedbytheprobabilityofincludinguniti.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueoftheoutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximateinferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.Becauseweusuallyuseaninclusionprobabilityof12,theweightscalingruleusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing2 themodelasusual.Anotherwaytoachievethesameresultistomultiplythestatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat2theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpectedtotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimearemissingonaverage.Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweightscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregressionclassiﬁerwithinputvariablesrepresentedbythevector:nvPy(= y|v) = softmax\\ue010W\\ue03ev+b\\ue011y.(7.56)Wecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationoftheinputwithabinaryvector:dPy(= y|v;) = dsoftmax\\ue010W\\ue03e()+d\\ue00cvb\\ue011y.(7.57)Theensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverallensemblemembers’predictions:Pensemble(= ) =yy|v˜Pensemble(= )yy|v\\ue050y\\ue030˜Pensemble(= yy\\ue030|v)(7.58)where˜Pensemble(= ) =yy|v2n\\ue073\\ue059d∈{}01,nPy.(= y|v;)d(7.59)263'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 278}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGToseethattheweightscalingruleisexact,wecansimplify˜Pensemble:˜Pensemble(= ) =yy|v2n\\ue073\\ue059d∈{}01,nPy(= y|v;)d(7.60)=2n\\ue073\\ue059d∈{}01,nsoftmax(W\\ue03e()+)d\\ue00cvby(7.61)=2n\\ue076\\ue075\\ue075\\ue074\\ue059d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue001\\ue050y\\ue030exp\\ue010W\\ue03ey\\ue030,:()+d\\ue00cvb\\ue011(7.62)=2n\\ue071\\ue051d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue0012n\\ue072\\ue051d∈{}01,n\\ue050y\\ue030exp\\ue010W\\ue03ey\\ue030,:()+d\\ue00cvb\\ue011(7.63)Because˜Pwillbenormalized,wecansafelyignoremultiplicationbyfactorsthatareconstantwithrespectto:y˜Pensemble(= ) yy|v∝2n\\ue073\\ue059d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue001(7.64)= exp\\uf8eb\\uf8ed12n\\ue058d∈{}01,nW\\ue03ey,:()+d\\ue00cvb\\uf8f6\\uf8f8(7.65)= exp\\ue01212W\\ue03ey,:v+b\\ue013(7.66)SubstitutingthisbackintoEq.weobtainasoftmaxclassiﬁerwithweights7.5812W.Theweightscalingruleisalsoexactinothersettings,includingregressionnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehiddenlayerswithoutnonlinearities.However,theweightscalingruleisonlyanapproxi-mationfordeepmodelsthathavenonlinearities.Thoughtheapproximationhasnotbeentheoreticallycharacterized,itoftenworkswell,empirically.Goodfellowetal.()foundexperimentallythattheweightscalingapproximationcanwork2013abetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximationstotheensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwasallowedtosampleupto1,000sub-networks.()foundGalandGhahramani2015thatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand264'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 279}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinferenceapproximationisproblem-dependent.Srivastava2014etal.()showedthatdropoutismoreeﬀectivethanotherstandardcomputationallyinexpensiveregularizers,suchasweightdecay,ﬁlternormconstraintsandsparseactivityregularization.Dropoutmayalsobecombinedwithotherformsofregularizationtoyieldafurtherimprovement.Oneadvantageofdropoutisthatitisverycomputationallycheap.UsingdropoutduringtrainingrequiresonlyO(n)computationperexampleperupdate,togeneratenrandombinarynumbersandmultiplythembythestate.Dependingontheimplementation,itmayalsorequireO(n)memorytostorethesebinarynumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodelhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpaythecostofdividingtheweightsby2oncebeforebeginningtoruninferenceonexamples.Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimitthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearlyanymodelthatusesadistributedrepresentationandcanbetrainedwithstochasticgradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodelssuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrentneuralnetworks(BayerandOsendorfer2014Pascanu2014a,;etal.,).Manyotherregularizationstrategiesofcomparablepowerimposemoresevererestrictionsonthearchitectureofthemodel.Thoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,thecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropoutisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀsetthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidationseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuchlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylargedatasets,regularizationconferslittlereductioningeneralizationerror. Inthesecases,thecomputationalcostofusingdropoutandlargermodelsmayoutweighthebeneﬁtofregularization.Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutislesseﬀective.Bayesianneural networks(, )outperform dropout ontheNeal1996AlternativeSplicingDataset(,)wherefewerthan5,000examplesXiongetal.2011areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,unsupervisedfeaturelearningcangainanadvantageoverdropout.Wager2013etal.()showedthat,whenappliedtolinearregression,dropoutisequivalenttoL2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor265'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 280}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGeachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientisdeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeepmodels,dropoutisnotequivalenttoweightdecay.Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryfortheapproach’ssuccess.Itisjustameansofapproximatingthesumoverallsub-models.WangandManning2013()derivedanalyticalapproximationstothismarginalization.Theirapproximation,knownasfastdropoutresultedinfasterconvergencetimeduetothereducedstochasticityinthecomputationofthegradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled(butalsomorecomputationallyexpensive)approximationtotheaverageoverallsub-networksthantheweightscalingapproximation.Fastdropouthasbeenusedtonearlymatchtheperformanceofstandarddropoutonsmallneuralnetworkproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoalargeproblem.Justasstochasticityisnotnecessarytoachievetheregularizing eﬀectofdropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()designedcontrolexperimentsusingamethodcalleddropoutboostingthattheydesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlackitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointlymaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditionaldropoutisanalogoustobagging, thisapproachisanalogoustoboosting.Asintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀectcomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthattheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationofdropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleisonlyachievedwhenthestochasticallysampledensemblemembersaretrainedtoperformwellindependentlyofeachother.Dropouthasinspiredotherstochasticapproachestotrainingexponentiallylargeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseofdropoutwhereeachproductbetweenasinglescalarweightandasinglehiddenunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochasticpoolingisaformofrandomizedpooling(seeSec.)forbuildingensembles9.3ofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerentspatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidelyusedimplicitensemblemethod.Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochasticbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisionsimplementsaformofbaggingwithparametersharing.Earlier, wedescribed266'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 281}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGdropoutas bagginganensembleofmodelsformedbyincludingor excludingunits.However,thereisnoneedforthismodelaveragingstrategytobebasedoninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.Inpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareabletolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafastapproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrizedbyavectorµastraininganensembleconsistingofp(y,|xµ)forallpossiblevaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.Forexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingtheweightsbyµ∼N(1,I)canoutperformdropoutbasedonbinarymasks.BecauseE[µ] =1thestandardnetworkautomaticallyimplementsapproximateinferenceintheensemble,withoutneedinganyweightscaling.Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,approximatebagging.However,thereisanotherviewofdropoutthatgoesfurtherthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensembleofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeabletoperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunitsmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves2012cswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressureforgenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerentorganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheirenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeaturesofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobenotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts. Warde-Farley2014etal.()compareddropouttrainingtotrainingoflargeensemblesandconcludedthatdropoutoﬀersadditionalimprovementstogeneralizationerrorbeyondthoseobtainedbyensemblesofindependentmodels.Itisimportanttounderstandthatalargeportionofthepowerofdropoutarisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.Thiscanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformationcontentoftheinputratherthandestructionoftherawvaluesoftheinput.Forexample,ifthemodellearnsahiddenunithithatdetectsafacebyﬁndingthenose,thendroppinghicorrespondstoerasingtheinformationthatthereisanoseintheimage.Themodelmustlearnanotherhi,eitherthatredundantlyencodesthepresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputarenotabletorandomlyerasetheinformationaboutanosefromanimageofafaceunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin267'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 282}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvaluesallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinputdistributionthatthemodelhasacquiredsofar.Anotherimportantaspectofdropoutisthatthenoiseismultiplicative.Ifthenoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunithiwithaddednoise\\ue00fcouldsimplylearntohavehibecomeverylargeinordertomaketheaddednoise\\ue00finsigniﬁcantbycomparison.Multiplicativenoisedoesnotallowsuchapathologicalsolutiontothenoiserobustnessproblem.Anotherdeeplearningalgorithm,batchnormalization,reparametrizesthemodelinawaythatintroducesbothadditiveandmultiplicativenoiseonthehiddenunitsattrainingtime.Theprimarypurposeofbatchnormalizationistoimproveoptimization,butthenoisecanhavearegularizingeﬀect,andsometimesmakesdropoutunnecessary.BatchnormalizationisdescribedfurtherinSec..8.7.17.13AdversarialTrainingInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhenevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthesemodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inordertoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecansearchforexamplesthatthemodelmisclassiﬁes.()foundthatSzegedyetal.2014bevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%errorrateonexamplesthatareintentionallyconstructedbyusinganoptimizationproceduretosearchforaninputx\\ue030nearadatapointxsuchthatthemodeloutputisverydiﬀerentatx\\ue030.Inmanycases,x\\ue030canbesosimilartoxthatahumanobservercannottellthediﬀerencebetweentheoriginalexampleandtheadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.SeeFig.foran7.8example.Adversarialexampleshavemanyimplications,forexample,incomputersecurity,thatarebeyondthescopeofthischapter. However,theyareinterestinginthecontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.testsetviaadversarialtraining—trainingonadversariallyperturbedexamplesfromthetrainingset(,;Szegedyetal.2014bGoodfellow2014betal.,).Goodfellow2014betal.()showedthatoneoftheprimarycausesoftheseadversarial examplesis excessive linearity.Neural networks arebuilt out ofprimarilylinearbuildingblocks. Insomeexperimentstheoverallfunctiontheyimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy268'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 283}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING+.007×=xsign(∇xJ(θx,,y))x+\\ue00fsign(∇xJ(θx,,y))y=“panda”“nematode”“gibbon”w/57.7%conﬁdencew/8.2%conﬁdencew/99.3%conﬁdenceFigure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet(,)onImageNet.ByaddinganimperceptiblysmallvectorwhoseSzegedyetal.2014aelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwithrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproducedwithpermissionfrom().Goodfellowetal.2014btooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidlyifithasnumerousinputs.Ifwechangeeachinputby\\ue00f,thenalinearfunctionwithweightswcanchangebyasmuchas\\ue00f||||w1,whichcanbeaverylargeamountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighlysensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstantintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitlyintroducingalocalconstancypriorintosupervisedneuralnets.Adversarialtraininghelpstoillustratethepowerofusingalargefunctionfamilyincombinationwithaggressiveregularization.Purelylinearmodels,likelogisticregression,arenotabletoresistadversarialexamplesbecausetheyareforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrangefromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapturelineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervisedlearning.Atapointxthatisnotassociatedwithalabelinthedataset,themodelitselfassignssomelabelˆy.Themodel’slabelˆymaynotbethetruelabel,butifthemodelishighquality,thenˆyhasahighprobabilityofprovidingthetruelabel.Wecanseekanadversarialexamplex\\ue030thatcausestheclassiﬁertooutputalabely\\ue030withy\\ue030\\ue036=ˆy.Adversarialexamplesgeneratedusingnotthetruelabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarialexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthesamelabeltoxandx\\ue030.Thisencouragestheclassiﬁertolearnafunctionthatis269'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 284}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddatalies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylieondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojumpfromoneclassmanifoldtoanotherclassmanifold.7.14Tangent Distance, TangentProp,and ManifoldTangentClassiﬁerManymachinelearningalgorithmsaimtoovercomethecurseofdimensionalitybyassumingthatthedataliesnearalow-dimensionalmanifold,asdescribedinSec..5.11.3Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthetangentdistancealgorithm(,,).Itisanon-parametricSimardetal.19931998nearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclideandistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhichprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesandthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁershouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovementonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetweenpointsx1andx2thedistancebetweenthemanifoldsM1andM2towhichtheyrespectivelybelong.Althoughthatmaybecomputationallydiﬃcult(itwouldrequiresolvinganoptimizationproblem,toﬁndthenearestpairofpointsonM1andM2),acheapalternativethatmakessenselocallyistoapproximateMibyitstangentplaneatxiandmeasurethedistancebetweenthetwotangents,orbetweenatangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensionallinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequiresonetospecifythetangentvectors.Inarelatedspirit,thetangentpropalgorithm(,)(Fig.)Simardetal.19927.9trainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutputf(x)oftheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsofvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthesameclassconcentrate.Localinvarianceisachievedbyrequiring∇xf(x)tobeorthogonaltotheknownmanifoldtangentvectorsv()iatx,orequivalentlythatthedirectionalderivativeoffatxinthedirectionsv()ibesmallbyaddingaregularizationpenalty:ΩΩ() =f\\ue058i\\ue010(∇xf())x\\ue03ev()i\\ue0112.(7.67)270'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 285}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGThisregularizercanofcoursebyscaledbyanappropriatehyperparameter,and,formostneuralnetworks,wewouldneedtosumovermanyoutputsratherthantheloneoutputf(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeoftheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.Tangentprophasbeenusednotjustforsupervisedlearning(,)Simardetal.1992butalsointhecontextofreinforcementlearning(,).Thrun1995Tangentpropagationis closelyrelated todataset augmentation.Inbothcases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetaskbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthenetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation,thenetworkisexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplyingmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagationdoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalyticallyregularizesthemodeltoresistperturbationinthedirectionscorrespondingtothe speciﬁed transformation.While thisanalytical approach isintellectuallyelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresistinﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistancetolargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodelsbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivativesbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheirderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanhunitscan.Datasetaugmentationworkswellwithrectiﬁedlinearunitsbecausediﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsofeachoriginalinput.Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,1992)andadversarialtraining(,;,).Szegedyetal.2014bGoodfellowetal.2014bDoublebackpropregularizestheJacobiantobesmall,whileadversarialtrainingﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesameoutputontheseasontheoriginalinputs.Tangentpropagationanddatasetaugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthemodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbeinvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Justallasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,adversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.Themanifoldtangentclassiﬁer(,),eliminatestheneedtoRifaietal.2011cknowthetangentvectorsapriori.AswewillseeinChapter,autoencoderscan14271'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 286}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nx1x2NormalTangent\\nFigure7.9: Illustrationofthemainideaofthetangentpropalgorithm(,Simardetal.1992Rifai2011c)andmanifoldtangentclassiﬁer(etal.,),whichbothregularizetheclassiﬁeroutputfunctionf(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttotheclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltotheclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemanytangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctiontochangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeasitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangentclassiﬁerregularizef(x) tonotchangeverymuchasxmovesalongthemanifold.Tangentpropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangentdirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclassmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirectionsbytraininganautoencodertoﬁtthetrainingdata.TheuseofautoencoderstoestimatemanifoldswillbedescribedinChapter.14estimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuseofthistechniquetoavoidneedinguser-speciﬁedtangentvectors. AsillustratedinFig.,theseestimatedtangentvectorsgobeyondtheclassicalinvariants14.10thatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)andincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchasmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁeristhereforesimple:(1)useanautoencodertolearnthemanifoldstructurebyunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁerasintangentprop(Eq.).7.67Thischapterhasdescribedmostofthegeneralstrategiesusedtoregularizeneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch272'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 287}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentralthemeofmachinelearningisoptimization,describednext.\\n273'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 288}, page_content='Chapter8OptimizationforTrainingDeepModelsDeeplearningalgorithmsinvolveoptimizationinmanycontexts.Forexample,performinginferenceinmodelssuchasPCAinvolvessolvinganoptimizationproblem.Weoftenuseanalyticaloptimizationtowriteproofsordesignalgorithms.Ofallofthemanyoptimizationproblemsinvolvedindeeplearning,themostdiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsoftimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneuralnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,aspecializedsetofoptimizationtechniqueshavebeendevelopedforsolvingit.Thischapterpresentstheseoptimizationtechniquesforneuralnetworktraining.Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,wesuggestreviewingChapter.Thatchapterincludesabriefoverviewofnumerical4optimizationingeneral.Thischapterfocusesononeparticularcaseofoptimization:ﬁndingtheparam-etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunctionJ(θ),whichtypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetaswellasadditionalregularizationterms.Webeginwithadescriptionofhowoptimizationusedasatrainingalgorithmforamachinelearningtaskdiﬀersfrompureoptimization.Next,wepresentseveraloftheconcretechallengesthatmakeoptimizationofneuralnetworksdiﬃcult.Wethendeﬁneseveralpracticalalgorithms,includingbothoptimizationalgorithmsthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithmsadapttheirlearningratesduringtrainingorleverageinformationcontainedin274'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 289}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthesecondderivativesofthecostfunction.Finally,weconcludewithareviewofseveraloptimizationstrategiesthatareformedbycombiningsimpleoptimizationalgorithmsintohigher-levelprocedures.8.1HowLearningDiﬀersfromPureOptimizationOptimizationalgorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditionaloptimizationalgorithmsinseveralways.Machinelearningusuallyactsindirectly.Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasureP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable. WethereforeoptimizePonlyindirectly.WereduceadiﬀerentcostfunctionJ(θ)inthehopethatdoingsowillimproveP.Thisisincontrasttopureoptimization,whereminimizingJisagoalinandofitself.Optimizationalgorithmsfortrainingdeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureofmachinelearningobjectivefunctions.Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,suchasJ() = θE()ˆx,y∼pdataLf,y,((;)xθ)(8.1)whereListheper-examplelossfunction,f(x;θ)isthepredictedoutputwhentheinputisx,ˆpdataistheempiricaldistribution.Inthesupervisedlearningcase,yisthetargetoutput.Throughoutthischapter,wedeveloptheunregularizedsupervisedcase,wheretheargumentstoLaref(x;θ)andy.However,itistrivialtoextendthisdevelopment,forexample,toincludeθorxasarguments,ortoexcludeyasarguments,inordertodevelopvariousformsofregularizationorunsupervisedlearning.Eq.deﬁnesanobjectivefunctionwithrespecttothetrainingset.We8.1wouldusuallyprefertominimizethecorrespondingobjectivefunctionwheretheexpectationistakenacrossthedatageneratingdistributionpdataratherthanjustovertheﬁnitetrainingset:J∗() = θE()x,y∼pdataLf,y.((;)xθ)(8.2)8.1.1EmpiricalRiskMinimizationThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralizationerrorgivenbyEq..Thisquantityisknownasthe.Weemphasizeherethat8.2risktheexpectationistakenoverthetrueunderlyingdistributionpdata.Ifweknewthetruedistributionpdata(x,y),riskminimizationwouldbeanoptimizationtask275'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 290}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSsolvablebyanoptimizationalgorithm.However,whenwedonotknowpdata(x,y)butonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.Thesimplestwaytoconvertamachinelearningproblembackintoanop-timizationproblemistominimizetheexpectedlossonthetrainingset.Thismeansreplacingthetruedistributionp(x,y) withtheempiricaldistributionˆp(x,y)deﬁnedbythetrainingset.WenowminimizetheempiricalriskEx,y∼ˆpdata()x,y[((;))] =Lfxθ,y1mm\\ue058i=1Lf((x()i;)θ,y()i)(8.3)whereisthenumberoftrainingexamples.mThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknownasempiricalriskminimization.Inthissetting,machinelearningisstillverysimilartostraightforwardoptimization.Ratherthanoptimizingtheriskdirectly,weoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyaswell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetrueriskcanbeexpectedtodecreasebyvariousamounts.However,empiricalriskminimizationispronetooverﬁtting.Modelswithhighcapacitycansimplymemorizethetrainingset.Inmanycases,empiricalriskminimizationisnotreallyfeasible.Themosteﬀectivemodernoptimizationalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,suchas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁnedeverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,werarelyuseempiricalriskminimization.Instead,wemustuseaslightlydiﬀerentapproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerentfromthequantitythatwetrulywanttooptimize.8.1.2SurrogateLossFunctionsandEarlyStoppingSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnotonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1lossistypicallyintractable(exponentialintheinputdimension),evenforalinearclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizesasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasasurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimatetheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcandothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorinexpectation.276'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 291}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearnmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalongtimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthelog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,onecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapartfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextractingmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimplyminimizingtheaverage0-1lossonthetrainingset.Averyimportantdiﬀerencebetweenoptimizationingeneralandoptimizationasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhaltatalocalminimum.Instead,amachinelearningalgorithmusuallyminimizesasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearlystopping(Sec.)issatisﬁed.Typicallytheearlystoppingcriterionisbasedon7.8thetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,andisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,whichisverydiﬀerentfromthepureoptimizationsetting,whereanoptimizationalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.8.1.3BatchandMinibatchAlgorithmsOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneraloptimizationalgorithmsisthattheobjectivefunctionusuallydecomposesasasumoverthetrainingexamples.Optimizationalgorithmsformachinelearningtypicallycomputeeachupdatetotheparametersbasedonanexpectedvalueofthecostfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.Forexample,maximumlikelihoodestimationproblems,whenviewedinlogspace,decomposeintoasumovereachexample:θML= argmaxθm\\ue058i=1logpmodel(x()i,y()i;)θ.(8.4)Maximizingthissumisequivalenttomaximizingtheexpectationovertheempiricaldistributiondeﬁnedbythetrainingset:J() = θEx,y∼ˆpdatalogpmodel(;)x,yθ.(8.5)MostofthepropertiesoftheobjectivefunctionJusedbymostofouropti-mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the277'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 292}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmostcommonlyusedpropertyisthegradient:∇θJ() = θEx,y∼ˆpdata∇θlogpmodel(;)x,yθ.(8.6)Computing thisexpectation exactly isvery expensive because it requiresevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecancomputetheseexpectationsbyrandomlysamplingasmallnumberofexamplesfromthedataset,thentakingtheaverageoveronlythoseexamples.Recallthatthestandarderrorofthemean(Eq.)estimatedfrom5.46nsamplesisgivenbyσ/√n,whereσisthetruestandarddeviationofthevalueofthesamples.Thedenominatorof√nshowsthattherearelessthanlinearreturnstousingmoreexamplestoestimatethegradient.Comparetwohypotheticalestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000examples.Thelatterrequires100timesmorecomputationthantheformer,butreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimizationalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsofnumberofupdates)iftheyareallowedtorapidlycomputeapproximateestimatesofthegradientratherthanslowlycomputingtheexactgradient.Anotherconsiderationmotivatingstatisticalestimationofthegradientfromasmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,allmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-basedestimateofthegradientcouldcomputethecorrectgradientwithasinglesample,usingmtimeslesscomputationthanthenaiveapproach.Inpractice,weareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlargenumbersofexamplesthatallmakeverysimilarcontributionstothegradient.Optimizationalgorithmsthatusetheentiretrainingsetarecalledbatchordeterministicgradientmethods,becausetheyprocessallofthetrainingexamplessimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusingbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedbyminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribeagroupofexamplesdoesnot. Forexample,itisverycommontousetheterm“batchsize”todescribethesizeofaminibatch.Optimizationalgorithmsthatuseonlyasingleexampleatatimearesometimescalledstochasticonlineorsometimesmethods.Thetermonlineisusuallyreservedforthecasewheretheexamplesaredrawnfromastreamofcontinuallycreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveralpassesaremade.Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore278'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 293}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalledminibatchminibatchstochasticormethodsanditisnowcommontosimplycallthemstochasticmethods.Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,presentedindetailinSec..8.3.1Minibatchsizesaregenerallydrivenbythefollowingfactors:•Largerbatchesprovideamoreaccurateestimateofthegradient,butwithlessthanlinearreturns.•Multicorearchitecturesareusuallyunderutilizedbyextremelysmallbatches.Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthereisnoreductioninthetimetoprocessaminibatch.•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypicallythecase),thentheamountofmemoryscaleswiththebatchsize.Formanyhardwaresetupsthisisthelimitingfactorinbatchsize.•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀerbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16sometimesbeingattemptedforlargemodels.•Smallbatchescanoﬀeraregularizingeﬀect(,),WilsonandMartinez2003perhapsduetothenoisetheyaddtothelearningprocess.Generalizationerrorisoftenbestforabatchsizeof1.Trainingwithsuchasmallbatchsizemightrequireasmalllearningratetomaintainstabilityduetothehighvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhighduetotheneedtomakemoresteps,bothbecauseofthereducedlearningrateandbecauseittakesmorestepstoobservetheentiretrainingset.Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-batchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthanothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccuratelywithfewsamples,orbecausetheyuseinformationinwaysthatamplifysamplingerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgareusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-ordermethods,whichusealsotheHessianmatrixHandcomputeupdatessuchasH−1g,typicallyrequiremuchlargerbatchsizeslike10,000.TheselargebatchsizesarerequiredtominimizeﬂuctuationsintheestimatesofH−1g.SupposethatHisestimatedperfectlybuthasapoorconditionnumber.Multiplicationby279'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 294}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.VerysmallchangesintheestimateofgcanthuscauselargechangesintheupdateH−1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonlyapproximately,sotheupdateH−1gwillcontainevenmoreerrorthanwewouldpredictfromapplyingapoorlyconditionedoperationtotheestimateof.gItisalsocrucialthattheminibatchesbeselectedrandomly.Computinganunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthosesamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobeindependentfromeachother,sotwosubsequentminibatchesofexamplesshouldalsobeindependentfromeachother.Manydatasetsaremostnaturallyarrangedinawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemighthaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.Thislistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerenttimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthesecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifweweretodrawexamplesinorderfromthislist,theneachofourminibatcheswouldbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthemanypatientsinthedataset.Incasessuchasthesewheretheorderofthedatasetholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselectingminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsofexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformlyatrandomeverytimewewanttoconstructaminibatch.Fortunately,inpracticeitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitinshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutiveexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodelwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetrainingdata.However,thisdeviationfromtruerandomselectiondoesnotseemtohaveasigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycanseriouslyreducetheeﬀectivenessofthealgorithm.Manyoptimizationproblemsinmachinelearningdecomposeoverexampleswellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamplesinparallel.Inotherwords,wecancomputetheupdatethatminimizesJ(X)foroneminibatchofexamplesXatthesametimethatwecomputetheupdateforseveralotherminibatches.SuchasynchronousparalleldistributedapproachesarediscussedfurtherinSec..12.1.3Aninterestingmotivationforminibatchstochasticgradientdescentisthatitfollowsthegradientofthetruegeneralizationerror(Eq.)solongasno8.2examplesarerepeated.Mostimplementationsofminibatchstochasticgradient280'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 295}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Ontheﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetruegeneralizationerror.Onthesecondpass,theestimatebecomesbiasedbecauseitisformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtainingnewfairsamplesfromthedatageneratingdistribution.Thefactthatstochasticgradientdescentminimizesgeneralizationerroriseasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawnfromastreamofdata. Inotherwords,insteadofreceivingaﬁxed-sizetrainingset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,witheveryexample(x,y)comingfromthedatageneratingdistributionpdata(x,y).Inthisscenario,examplesareneverrepeated;everyexperienceisafairsamplefrompdata.Theequivalenceiseasiesttoderivewhenbothxandyarediscrete. Inthiscase,thegeneralizationerror(Eq.)canbewrittenasasum8.2J∗() =θ\\ue058x\\ue058ypdata()((;))x,yLfxθ,y,(8.7)withtheexactgradientg= ∇θJ∗() =θ\\ue058x\\ue058ypdata()x,y∇θLf,y.((;)xθ)(8.8)Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinEq.8.5andEq.;weobservenowthatthisholdsforotherfunctions8.6Lbesidesthelikelihood.Asimilarresultcanbederivedwhenxandyarecontinuous,undermildassumptionsregardingpdataand.LHence, wecanobtainanunbiasedestimatoroftheexactgradientof thegeneralizationerrorbysamplingaminibatchofexamples{x(1),...x()m}withcor-respondingtargetsy()ifromthedatageneratingdistributionpdata,andcomputingthegradientofthelosswithrespecttotheparametersforthatminibatch:ˆg=1m∇θ\\ue058iLf((x()i;)θ,y()i).(8.9)UpdatinginthedirectionofθˆgperformsSGDonthegeneralizationerror.Ofcourse, thisinterpretationonly applieswhenexamplesarenotreused.Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,unlessthetrainingsetisextremelylarge. Whenmultiplesuchepochsareused,onlytheﬁrstepochfollowstheunbiasedgradientofthegeneralizationerror,but281'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 296}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreasedtrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentrainingerrorandtesterror.Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,itisbecomingmorecommonformachinelearningapplicationstouseeachtrainingexampleonlyonceoreventomakeanincompletepassthroughthetrainingset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,sounderﬁttingandcomputationaleﬃciencybecomethepredominantconcerns.Seealso()foradiscussionoftheeﬀectofcomputationalBottouandBousquet2008bottlenecksongeneralizationerror,asthenumberoftrainingexamplesgrows.8.2ChallengesinNeuralNetworkOptimizationOptimizationingeneralisanextremelydiﬃculttask.Traditionally,machinelearninghasavoidedthediﬃcultyofgeneraloptimizationbycarefullydesigningtheobjectivefunctionandconstraintstoensurethattheoptimizationproblemisconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convexcase.Evenconvexoptimizationisnotwithoutitscomplications.Inthissection,wesummarizeseveralofthemostprominentchallengesinvolvedinoptimizationfortrainingdeepmodels.8.2.1Ill-ConditioningSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themostprominentisill-conditioningoftheHessianmatrixH.Thisisaverygeneralprobleminmostnumericaloptimization,convexorotherwise,andisdescribedinmoredetailinSec..4.3.1Theill-conditioningproblemisgenerallybelievedtobepresentinneuralnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.RecallfromEq.thatasecond-orderTaylorseriesexpansionofthecost4.9functionpredictsthatagradientdescentstepofwilladd−\\ue00fg12\\ue00f2g\\ue03eHgg−\\ue00f\\ue03eg(8.10)tothecost.Ill-conditioningofthegradientbecomesaproblemwhen12\\ue00f2g\\ue03eHgexceeds\\ue00fg\\ue03eg. Todeterminewhetherill-conditioningisdetrimentaltoaneuralnetworktrainingtask,onecanmonitorthesquaredgradientnormg\\ue03egandthe282'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 297}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n−50050100150200250Trainingtime(epochs)−20246810121416Gradient norm050100150200250Trainingtime(epochs)01.02.03.04.05.06.07.08.09.10.Classiﬁcationerrorrate\\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthisexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkusedforobjectdetection.(Left)Ascatterplotshowinghowthenormsofindividualgradientevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnormisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolidcurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewouldexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing(Right)gradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcationerrordecreasestoalowlevel.g\\ue03eHgterm.Inmanycases, thegradientnormdoesnotshrinksigniﬁcantlythroughoutlearning,buttheg\\ue03eHgtermgrowsbymorethanorderofmagnitude.Theresultisthatlearningbecomesveryslowdespitethepresenceofastronggradientbecausethelearningratemustbeshrunktocompensateforevenstrongercurvature.Fig.showsanexampleofthegradientincreasingsigniﬁcantlyduring8.1thesuccessfultrainingofaneuralnetwork.Thoughill-conditioningispresentinothersettingsbesidesneuralnetworktraining,someofthetechniquesusedtocombatitinothercontextsarelessapplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttoolforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butinthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcantmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.8.2.2LocalMinimaOneofthemostprominentfeaturesofaconvexoptimizationproblemisthatitcanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis283'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 298}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionatthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuchaﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,weknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.Withnon-convexfunctions,suchasneuralnets,itispossibletohavemanylocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohaveanextremelylargenumberoflocalminima.However,aswewillsee,thisisnotnecessarilyamajorproblem.Neuralnetworksandanymodelswithmultipleequivalentlyparametrizedlatentvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁabilityproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcanruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariablesareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanginglatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkandmodifylayer1byswappingtheincomingweightvectorforunitiwiththeincomingweightvectorforunitj,thendoingthesamefortheoutgoingweightvectors.Ifwehavemlayerswithnunitseach,thentherearen!mwaysofarrangingthehiddenunits.Thiskindofnon-identiﬁabilityisknownasweightspacesymmetry.Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshaveadditionalcausesofnon-identiﬁability.Forexample,inanyrectiﬁedlinearormaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitbyαifwealsoscaleallofitsoutgoingweightsby1α.Thismeansthat—ifthecostfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyontheweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinearormaxoutnetworkliesonan(mn×)-dimensionalhyperbolaofequivalentlocalminima.Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylargeorevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcostfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilityareequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaarenotaproblematicformofnon-convexity.Localminimacanbeproblematiciftheyhavehighcostincomparisontotheglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthiddenunits,thathavelocalminimawithhighercostthantheglobalminimum(SontagandSussman1989Brady1989GoriandTesi1992,;etal.,;,).Iflocalminimawithhighcostarecommon,thiscouldposeaseriousproblemforgradient-basedoptimizationalgorithms.Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost284'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 299}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSfornetworksofpracticalinterestandwhetheroptimizationalgorithmsencounterthem.Formanyyears,mostpractitionersbelievedthatlocalminimawereacommonproblemplaguingneuralnetworkoptimization.Today,thatdoesnotappeartobethecase.Theproblemremainsanactiveareaofresearch,butexpertsnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavealowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimumratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost(,;,;,;Saxeetal.2013Dauphinetal.2014Goodfellowetal.2015Choromanskaetal.,).2014Manypractitionersattributenearlyalldiﬃcultywithneuralnetworkoptimiza-tiontolocalminima.Weencouragepractitionerstocarefullytestforspeciﬁcproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthenormofthegradientovertime.Ifthenormofthegradientdoesnotshrinktoinsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcriticalpoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensionalspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaaretheproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.8.2.3Plateaus,SaddlePointsandOtherFlatRegionsFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddlepoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,whileothershavealowercost.Atasaddlepoint,theHessianmatrixhasbothpositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwithpositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslyingalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointasbeingalocalminimumalongonecross-sectionofthecostfunctionandalocalmaximumalonganothercross-section.SeeFig.foranillustration.4.5Manyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow-dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,localminimaarerareandsaddlepointsaremorecommon.Forafunctionf:Rn→Rofthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrowsexponentiallywithn.Tounderstandtheintuitionbehindthisbehavior,observethattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.TheHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingledimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheadsonce.Inn-dimensionalspace,itisexponentiallyunlikelythatallncointosseswill285'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSbeheads.See()forareviewoftherelevanttheoreticalwork.Dauphinetal.2014AnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesoftheHessianbecomemorelikelytobepositiveaswereachregionsoflowercost. Inourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeupheadsntimesifweareatacriticalpointwithlowcost. Thismeansthatlocalminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswithhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremelyhighcostaremorelikelytobelocalmaxima.Thishappensformanyclassesofrandomfunctions.Doesithappenforneuralnetworks?()showedtheoreticallythatshallowautoencodersBaldiandHornik1989(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedinChapter)withnononlinearitieshaveglobalminimaandsaddlepointsbutno14localminimawithhighercostthantheglobalminimum.Theyobservedwithoutproofthattheseresultsextendtodeepernetworkswithoutnonlinearities.Theoutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareusefultostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionisanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjustmultiplematricescomposedtogether.()providedexactsolutionsSaxeetal.2013tothecompletelearningdynamicsinsuchnetworksandshowedthatlearninginthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingofdeepmodelswithnonlinearactivationfunctions.()showedDauphinetal.2014experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainverymanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditionaltheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandomfunctionsrelatedtoneuralnetworksdoessoaswell.Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-rithms?Forﬁrst-orderoptimizationalgorithmsthatuseonlygradientinformation,thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddlepoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescapesaddlepointsinmanycases.()providedvisualizationsofGoodfellowetal.2015severallearningtrajectoriesofstate-of-the-artneuralnetworks,withanexamplegiveninFig..Thesevisualizationsshowaﬂatteningofthecostfunctionnear8.2aprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthegradientdescenttrajectoryrapidlyescapingthisregion.()Goodfellowetal.2015alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytoberepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituationmaybediﬀerentformorerealisticusesofgradientdescent.ForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem.286'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 301}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\nProjection2ofθProjection1ofθJ()θFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.ImageadaptedwithpermissionfromGoodfellow2015etal.(). Thesevisualizationsappearsimilarforfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksappliedtorealobjectrecognitionandnaturallanguageprocessingtasks.Surprisingly,thesevisualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessofstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convexstructurethanisrevealedbytheseprojections. Theprimaryobstaclerevealedbythisprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,asindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrixinthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleintheﬁgureviaanindirectarcingpath.\\n287'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 302}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesignedtoseekacriticalpoint.Newton’smethod,however,isdesignedtosolveforapointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjumptoasaddlepoint.Theproliferationofsaddlepointsinhighdimensionalspacespresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacinggradientdescentforneuralnetworktraining.()introducedDauphinetal.2014asaddle-freeNewtonmethodforsecond-orderoptimizationandshowedthatitimprovessigniﬁcantlyoverthetraditionalversion.Second-ordermethodsremaindiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholdspromiseifitcouldbescaled.Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddlepoints.Therearealsomaxima, whicharemuchlikesaddlepointsfromtheperspectiveofoptimization—manyalgorithmsarenotattractedtothem, butunmodiﬁedNewton’smethod is.Maximabecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,thegradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajorproblemsforallnumericaloptimizationalgorithms.Inaconvexproblem,awide,ﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimizationproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.8.2.4CliﬀsandExplodingGradientsNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresemblingcliﬀs,asillustratedinFig..Theseresultfromthemultiplicationofseverallarge8.3weightstogether.Onthefaceofanextremelysteepcliﬀstructure,thegradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀofthecliﬀstructurealtogether.\\n288'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 303}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n\\ue077\\ue062\\ue04a\\ue077\\ue03b\\ue062\\ue028\\ue029Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorforrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresultingfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetoveryhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,agradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostoftheoptimizationworkthathadbeendone. FigureadaptedwithpermissionfromPascanuetal.().2013aThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradientclippingheuristicdescribedinSec.. Thebasicideaistorecallthatthe10.11.1gradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirectionwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithmproposestomakeaverylargestep,thegradientclippingheuristicintervenestoreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregionwherethegradientindicatesthedirectionofapproximatelysteepestdescent.Cliﬀstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,becausesuchmodelsinvolveamultiplicationofmanyfactors,withonefactorforeachtimestep.Longtemporalsequencesthusincuranextremeamountofmultiplication.8.2.5Long-TermDependenciesAnotherdiﬃcultythatneuralnetworkoptimizationalgorithmsmustovercomeariseswhenthecomputationalgraphbecomesextremelydeep. Feedforwardnetworkswithmanylayershavesuchdeepcomputationalgraphs.Sodorecurrentnetworks,describedinChapter, whichconstructverydeepcomputationalgraphsby10289'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 304}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSrepeatedlyapplyingthesameoperationateachtimestepofalongtemporalsequence.Repeatedapplicationofthesameparametersgivesrisetoespeciallypronounceddiﬃculties.Forexample,supposethatacomputationalgraphcontainsapaththatconsistsofrepeatedlymultiplyingbyamatrixW.Aftertsteps,thisisequivalenttomul-tiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V−1.Inthissimplecase,itisstraightforwardtoseethatWt=\\ue000VλVdiag()−1\\ue001t= ()VdiagλtV−1.(8.11)Anyeigenvaluesλithatarenotnearanabsolutevalueofwilleitherexplodeif1theyaregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.11Thevanishingandexplodinggradientproblemreferstothefactthatgradientsthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradientsmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprovethecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀstructuresdescribedearlierthatmotivategradientclippingareanexampleoftheexplodinggradientphenomenon.TherepeatedmultiplicationbyWateachtimestepdescribedhereisverysimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueofamatrixWandthecorrespondingeigenvector.Fromthispointofviewitisnotsurprisingthatx\\ue03eWtwilleventuallydiscardallcomponentsofxthatareorthogonaltotheprincipaleigenvectorof.WRecurrentnetworksusethesamematrixWateachtimestep,butfeedforwardnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthevanishingandexplodinggradientproblem(,).Sussillo2014WedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworksuntilSec.,afterrecurrentnetworkshavebeendescribedinmoredetail.10.78.2.6InexactGradientsMostoptimizationalgorithmsareprimarilymotivatedbythecasewherewehaveexactknowledgeofthegradientorHessianmatrix.Inpractice,weusuallyonlyhaveanoisyorevenbiasedestimateofthesequantities.Nearlyeverydeeplearningalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatchoftrainingexamplestocomputethegradient.Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableaswell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise290'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 305}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSwiththemoreadvancedmodelsinPart.Forexample,contrastivedivergenceIIIgivesatechniqueforapproximatingthegradientoftheintractablelog-likelihoodofaBoltzmannmachine.Variousneuralnetworkoptimizationalgorithmsaredesignedtoaccountforimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosingasurrogatelossfunctionthatiseasiertoapproximatethanthetrueloss.8.2.7PoorCorrespondencebetweenLocalandGlobalStructureManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthelossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepifJ(θ)ispoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddlepointhidingtheopportunitytomakeprogressdownhillfromthegradient.Itispossibletoovercomealloftheseproblemsatasinglepointandstillperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoesnotpointtowarddistantregionsofmuchlowercost.Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisduetothelengthofthetrajectoryneededtoarriveatthesolution.Fig.showsthat8.2thelearningtrajectoryspendsmostofitstimetracingoutawidearcaroundamountain-shapedstructure.Muchofresearchintothediﬃcultiesofoptimizationhasfocusedonwhethertrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butinpracticeneuralnetworksdonotarriveatacriticalpointofanykind.Fig.8.1showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction−logp(y|x;θ)canlackaglobalminimumpointandinsteadasymptoticallyapproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwithdiscreteyandp(y|x)providedbyasoftmax,thenegativelog-likelihoodcanbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyeveryexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueofzero.Likewise,amodelofrealvaluesp(y|x) =N(y;f(θ),β−1)canhavenegativelog-likelihoodthatasymptotestonegativeinﬁnity—iff(θ)isabletocorrectlypredictthevalueofalltrainingsetytargets,thelearningalgorithmwillincreaseβwithoutbound.SeeFig.foranexampleofafailureoflocaloptimizationto8.4ﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddlepoints.Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthatinﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome291'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 306}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\nθJ()θ\\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoesnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,eveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunctioncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyinthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingabletotraverseit. Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigatesuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultinexcessivetrainingtime,asillustratedinFig..8.2oftheprocess.Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsforproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithmsthatusenon-localmoves.Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefortrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevioussectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmovescanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesoftheobjectivefunction,suchasitsgradient,onlyapproximately,withbiasorvarianceinourestimateofthecorrectdirection.Inthesecases,localdescentmayormaynotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactuallyabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissuessuchaspoorconditioningordiscontinuousgradients,causingtheregionwherethegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.Inthesecases,localdescentwithstepsofsize\\ue00fmaydeﬁneareasonablyshortpathtothesolution,butweareonlyabletocomputethelocaldescentdirectionwithstepsofsizeδ\\ue00f\\ue01c.Inthesecases,localdescentmayormaynotdeﬁneapathtothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa292'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 307}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELShighcomputationalcost.Sometimeslocalinformationprovidesusnoguide,whenthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacriticalpoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitlyforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoesnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedyandleadusalongapaththatmovesdownhillbutawayfromanysolution,asinFig.,oralonganunnecessarilylongtrajectorytothesolution,asinFig..8.48.2Currently,wedonotunderstandwhichoftheseproblemsaremostrelevanttomakingneuralnetworkoptimizationdiﬃcult,andthisisanactiveareaofresearch.Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbeavoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolutionbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearningwithinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosinggoodinitialpointsfortraditionaloptimizationalgorithmstouse.8.2.8TheoreticalLimitsofOptimizationSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofanyoptimizationalgorithmwemightdesignforneuralnetworks(,BlumandRivest1992Judd1989WolpertandMacReady1997;,;,).Typicallytheseresultshavelittlebearingontheuseofneuralnetworksinpractice.Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneuralnetworkoutput discretevalues.However, most neuralnetworkunitsoutputsmoothlyincreasingvaluesthatmakeoptimizationvialocalsearchfeasible.Sometheoreticalresultsshowthatthereexistproblemclassesthatareintractable,butitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Otherresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,butinpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmanymoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthecontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexactminimumofafunction,butonlyinreducingitsvaluesuﬃcientlytoobtaingoodgeneralizationerror. Theoreticalanalysisofwhetheranoptimizationalgorithmcanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticboundsontheperformanceofoptimizationalgorithmsthereforeremainsanimportantgoalformachinelearningresearch.293'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 308}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.3BasicAlgorithmsWehavepreviouslyintroducedthegradientdescent(Sec.)algorithmthat4.3followsthegradientofanentiretrainingsetdownhill.Thismaybeacceleratedconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomlyselectedminibatchesdownhill,asdiscussedinSec.andSec..5.98.1.38.3.1StochasticGradientDescentStochasticgradientdescent(SGD)anditsvariantsareprobablythemostusedoptimizationalgorithmsformachinelearningingeneralandfordeeplearninginparticular.AsdiscussedinSec.,itispossibletoobtainanunbiasedestimate8.1.3ofthegradientbytakingtheaveragegradientonaminibatchofmexamplesdrawni.i.dfromthedatageneratingdistribution.Algorithmshowshowtofollowthisestimateofthegradientdownhill.8.1Algorithm8.1Stochasticgradientdescent(SGD)updateattrainingiterationkRequire:Learningrate\\ue00fk.Require:InitialparameterθwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:ˆg←+1m∇θ\\ue050iLf((x()i;)θ,y()i)Applyupdate:θθ←−\\ue00fˆgendwhileAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,wehavedescribedSGDasusingaﬁxedlearningrate\\ue00f.Inpractice,itisnecessarytograduallydecreasethelearningrateovertime,sowenowdenotethelearningrateatiterationask\\ue00fk.ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(therandomsamplingofmtrainingexamples)thatdoesnotvanishevenwhenwearriveataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomessmallandthen0whenweapproachandreachaminimumusingbatchgradientdescent,sobatchgradientdescentcanuseaﬁxedlearningrate.AsuﬃcientconditiontoguaranteeconvergenceofSGDisthat∞\\ue058k=1\\ue00fk= and∞,(8.12)294'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 309}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS∞\\ue058k=1\\ue00f2k<.∞(8.13)Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration:τ\\ue00fk= (1)−α\\ue00f0+α\\ue00fτ(8.14)withα=kτ.Afteriteration,itiscommontoleaveconstant.τ\\ue00fThelearningratemaybechosenbytrialanderror,butitisusuallybesttochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasafunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthissubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,theparameterstochooseare\\ue00f0,\\ue00fτ,andτ.Usuallyτmaybesettothenumberofiterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\\ue00fτshouldbesettoroughlythevalueof1%\\ue00f0.Themainquestionishowtoset\\ue00f0.Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecostfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyiftrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromtheuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andiftheinitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandtheﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformanceaftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrstseveraliterationsandusealearningratethatishigherthanthebest-performinglearningrateatthistime,butnotsohighthatitcausessevereinstability.ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-basedoptimizationisthatcomputationtimeperupdatedoesnotgrowwiththenumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumberoftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmayconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithasprocessedtheentiretrainingset.TostudytheconvergencerateofanoptimizationalgorithmitiscommontomeasuretheexcesserrorJ(θ)−minθJ(θ),whichistheamountthatthecurrentcostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvexproblem,theexcesserrorisO(1√k)afterkiterations,whileinthestronglyconvexcaseitisO(1k).Theseboundscannotbeimprovedunlessextraconditionsareassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochasticgradientdescentintheory.However,theCramér-Raobound(,;,Cramér1946Rao1945)statesthatgeneralizationerrorcannotdecreasefasterthanO(1k).Bottou295'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 310}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursueanoptimizationalgorithmthatconvergesfasterthanO(1k)formachinelearningtasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,theasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescenthasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomakerapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamplesoutweighsitsslowasymptoticconvergence.MostofthealgorithmsdescribedintheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelostintheconstantfactorsobscuredbytheO(1k)asymptoticanalysis.Onecanalsotradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygraduallyincreasingtheminibatchsizeduringthecourseoflearning.FormoreinformationonSGD,see().Bottou19988.3.2MomentumWhilestochasticgradientdescentremainsaverypopularoptimizationstrategy,learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)isdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbutconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulatesanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomoveintheirdirection.TheeﬀectofmomentumisillustratedinFig..8.5Formally,themomentumalgorithmintroducesavariablevthatplaystheroleofvelocity—itisthedirectionandspeedatwhichtheparametersmovethroughparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthenegativegradient.Thenamederivesfromaphysicalanalogy,inmomentumwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,accordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorvmayalsoberegardedasthemomentumoftheparticle.Ahyperparameterα∈[0,1)determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.Theupdateruleisgivenby:vv←α−∇\\ue00fθ\\ue0201mm\\ue058i=1L((fx()i;)θ,y()i)\\ue021,(8.15)θθv←+.(8.16)Thevelocityvaccumulatesthegradientelements∇θ\\ue0001m\\ue050mi=1L((fx()i;)θ,y()i)\\ue001.Thelargerαisrelativeto\\ue00f,themorepreviousgradientsaﬀectthecurrentdirection.TheSGDalgorithmwithmomentumisgiveninAlgorithm.8.2296'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 311}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n−−−30201001020−30−20−1001020\\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningoftheHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentumovercomestheﬁrstofthesetwoproblems.ThecontourlinesdepictaquadraticlossfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthecontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthisfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradientdescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjectivelookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraversesthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthenarrowaxisofthecanyon.ComparealsoFig.,whichshowsthebehaviorofgradient4.6descentwithoutmomentum.\\n297'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 312}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSPreviously,thesizeofthestepwassimplythenormofthegradientmultipliedbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhowalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessivegradientspointinexactlythesamedirection.Ifthemomentumalgorithmalwaysobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachingaterminalvelocitywherethesizeofeachstepis\\ue00f||||g1−α.(8.17)Itisthushelpfultothinkofthemomentumhyperparameterintermsof11−α.Forexample,α=.9correspondstomultiplyingthemaximumspeedbyrelativeto10thegradientdescentalgorithm.Commonvaluesofαusedinpracticeinclude.5,.9,and.99.Likethelearningrate,αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueandislaterraised.Itislessimportanttoadaptαovertimethantoshrink\\ue00fovertime.Algorithm8.2Stochasticgradientdescent(SGD)withmomentumRequire:Learningrate,momentumparameter.\\ue00fαRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Computevelocityupdate:vvg←α−\\ue00fApplyupdate:θθv←+endwhileWecanviewthemomentumalgorithmassimulatingaparticlesubjecttocontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuildintuitionforhowthemomentumandgradientdescentalgorithmsbehave.Thepositionoftheparticleatanypointintimeisgivenbyθ(t).Theparticleexperiencesnetforce.Thisforcecausestheparticletoaccelerate:f()tf() =t∂2∂t2θ()t.(8.18)Ratherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,wecanintroducethevariablev(t)representingthevelocityoftheparticleattimetandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:v() =t∂∂tθ()t,(8.19)298'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSf() =t∂∂tv()t.(8.20)Themomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvianumericalsimulation.AsimplenumericalmethodforsolvingdiﬀerentialequationsisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedbytheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyaretheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:−∇θJ(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneachgradient,buttheNewtonianscenariousedbythemomentumalgorithminsteadusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticleasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsasteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirectionuntilitbeginstogouphillagain.Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,thentheparticlemightnevercometorest.Imagineahockeypuckslidingdownonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddoneotherforce,proportionalto−v(t).Inphysicsterminology,thisforcecorrespondstoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchassyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventuallyconvergetoalocalminimum.Whydoweuse−v(t)andviscousdraginparticular? Partofthereasontouse−v(t)ismathematicalconvenience—anintegerpowerofthevelocityiseasytoworkwith.However,otherphysicalsystemshaveotherkindsofdragbasedonotherintegerpowersofthevelocity.Forexample,aparticletravelingthroughtheairexperiencesturbulentdrag,withforceproportionaltothesquareofthevelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,withaforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityissmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticlewithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdragwillmoveawayfromitsinitialpositionforever,withthedistancefromthestartingpointgrowinglikeO(logt).Wemustthereforeusealowerpowerofthevelocity.Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,theconstantforceduetofrictioncancausetheparticletocometorestbeforereachingalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough299'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 314}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthatthegradientcancontinuetocausemotionuntilaminimumisreached,butstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.8.3.3NesterovMomentumSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwasinspiredbyNesterov’sacceleratedgradientmethod(,,).TheNesterov19832004updaterulesinthiscasearegivenby:vv←α−∇\\ue00fθ\\ue0221mm\\ue058i=1L\\ue010fx(()i;+)θαv,y()i\\ue011\\ue023,(8.21)θθv←+,(8.22)wheretheparametersαand\\ue00fplayasimilarroleasinthestandardmomentummethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumiswherethegradientisevaluated.WithNesterovmomentumthegradientisevaluatedafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentumasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.ThecompleteNesterovmomentumalgorithmispresentedinAlgorithm.8.3Intheconvexbatchgradientcase,NesterovmomentumbringstherateofconvergenceoftheexcesserrorfromO(1/k)(afterksteps)toO(1/k2)asshownbyNesterov1983().Unfortunately, inthestochasticgradientcase, Nesterovmomentumdoesnotimprovetherateofconvergence.Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentumRequire:Learningrate,momentumparameter.\\ue00fαRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondinglabelsy()i.Applyinterimupdate:˜θθv←+αComputegradient(atinterimpoint):g←1m∇˜θ\\ue050iLf((x()i;˜θy),()i)Computevelocityupdate:vvg←α−\\ue00fApplyupdate:θθv←+endwhile300'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 315}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.4ParameterInitializationStrategiesSomeoptimizationalgorithmsarenotiterativebynatureandsimplysolveforasolutionpoint.Otheroptimizationalgorithmsareiterativebynaturebut,whenappliedtotherightclassofoptimizationproblems,convergetoacceptablesolutionsinanacceptableamountoftimeregardlessofinitialization.Deeplearningtrainingalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeeplearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecifysomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeepmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedbythechoiceofinitialization.Theinitialpointcandeterminewhetherthealgorithmconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithmencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,theinitialpointcandeterminehowquicklylearningconvergesandwhetheritconvergestoapointwithhigh orlowcost.Also, pointsofcomparablecostcanhavewildlyvaryinggeneralizationerror,andtheinitialpointcanaﬀectthegeneralizationaswell.Moderninitializationstrategiesaresimpleandheuristic.Designingimprovedinitializationstrategiesisadiﬃculttaskbecauseneuralnetworkoptimizationisnotyetwellunderstood.Mostinitializationstrategiesarebasedonachievingsomenicepropertieswhenthenetworkisinitialized.However,wedonothaveagoodunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstancesafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpointsmaybebeneﬁcialfromtheviewpointofoptimizationbutdetrimentalfromtheviewpointofgeneralization.Ourunderstandingofhowtheinitialpointaﬀectsgeneralizationisespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselecttheinitialpoint.Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitialparametersneedto“breaksymmetry” betweendiﬀerentunits.Iftwohiddenunitswiththesameactivationfunctionareconnectedtothesameinputs,thentheseunitsmusthavediﬀerentinitialparameters. Iftheyhavethesameinitialparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccostandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthemodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerentupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusuallybesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheotherunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenullspaceofforwardpropagationandnogradientpatternsarelostinthenullspaceofback-propagation.Thegoalofhavingeachunitcomputeadiﬀerentfunction301'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 316}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmotivatesrandominitializationoftheparameters.Wecouldexplicitlysearchforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,butthisoftenincursanoticeablecomputationalcost.Forexample,ifwehaveatmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalizationonaninitialweightmatrix,andbeguaranteedthateachunitcomputesaverydiﬀerentfunctionfromeachotherunit.Randominitializationfromahigh-entropydistributionoverahigh-dimensionalspaceiscomputationallycheaperandunlikelytoassignanyunitstocomputethesamefunctionaseachother.Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,andinitializeonlytheweightsrandomly.Extraparameters,forexample,parametersencodingtheconditionalvarianceofaprediction,areusuallysettoheuristicallychosenconstantsmuchlikethebiasesare.Wealmostalwaysinitializealltheweightsin themodel tovalues drawnrandomly froma Gaussian oruniform distribution.The choice of Gaussianoruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeenexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavealargeeﬀectonboththeoutcomeoftheoptimizationprocedureandontheabilityofthenetworktogeneralize.Largerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helpingtoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardorback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthematrixresultinlargeroutputsofmatrixmultiplication.Initialweightsthataretoolargemay,however,resultinexplodingvaluesduringforwardpropagationorback-propagation. Inrecurrentnetworks,largeweightscanalsoresultinchaos(suchextremesensitivitytosmallperturbationsoftheinputthatthebehaviorofthedeterministicforwardpropagationprocedureappearsrandom). Tosomeextent,theexplodinggradientproblemcanbemitigatedbygradientclipping(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunctiontosaturate,causingcompletelossofgradientthroughsaturatedunits.Thesecompetingfactorsdeterminetheidealinitialscaleoftheweights.Theperspectivesofregularizationandoptimizationcangiveverydiﬀerentinsightsintohowweshouldinitializeanetwork.Theoptimizationperspectivesuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuseofanoptimizationalgorithmsuchasstochasticgradientdescentthatmakessmallincrementalchangestotheweightsandtendstohaltinareasthatarenearertotheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or302'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 317}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesapriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.RecallfromSec.thatgradientdescentwithearlystoppingisequivalenttoweight7.8decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingisnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabouttheeﬀectofinitialization.Wecanthinkofinitializingtheparametersθtoθ0asbeingsimilartoimposingaGaussianpriorp(θ)withmeanθ0.Fromthispointofview,itmakessensetochooseθ0tobenear0.Thispriorsaysthatitismorelikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Unitsinteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrongpreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ0tolargevalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,andhowtheyshouldinteract.Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.OneheuristicistoinitializetheweightsofafullyconnectedlayerwithminputsandnoutputsbysamplingeachweightfromU(−1√m,1√m),whileGlorotandBengio()suggestusingthe2010normalizedinitializationWi,j∼−U(6√mn+,6√mn+).(8.23)Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializingalllayerstohavethesameactivationvarianceandthegoalofinitializingalllayerstohavethesamegradientvariance.Theformulaisderivedusingtheassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,withnononlinearities.Realneuralnetworksobviouslyviolatethisassumption,butmanystrategiesdesignedforthelinearmodelperformreasonablywellonitsnonlinearcounterparts.Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,withacarefullychosenscalingorfactorgaingthataccountsforthenonlinearityappliedateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesofnonlinearactivationfunctions.Thisinitializationschemeisalsomotivatedbyamodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.Undersuchamodel,thisinitializationschemeguaranteesthatthetotalnumberoftrainingiterationsrequiredtoreachconvergenceisindependentofdepth.Increasingthescalingfactorgpushesthenetworktowardtheregimewhereactivationsincreaseinnormastheypropagateforwardthroughthenetworkandgradientsincreaseinnormastheypropagatebackward.()showedSussillo2014thatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas1,000layers,withoutneedingtouseorthogonalinitializations. Akeyinsightof303'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 318}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthisapproachisthatinfeedforwardnetworks,activationsandgradientscangroworshrinkoneachstepofforwardorback-propagation,followingarandomwalkbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixateachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforwardnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthatariseswhenthesameweightmatrixisusedateachstep,describedinSec..8.2.5Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadtooptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemaybeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethenormofasignalthroughouttheentirenetwork.Second,thepropertiesimposedatinitializationmaynotpersistafterlearninghasbeguntoproceed.Third,thecriteriamightsucceedatimprovingthespeedofoptimizationbutinadvertentlyincreasegeneralizationerror.Inpractice,weusuallyneedtotreatthescaleoftheweightsasahyperparameterwhoseoptimalvalueliessomewhereroughlynearbutnotexactlyequaltothetheoreticalpredictions.Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethesamestandarddeviation,suchas1√m,isthateveryindividualweightbecomesextremelysmallwhenthelayersbecomelarge.()introducedanalternativeMartens2010initializationschemecalledsparseinitializationinwhicheachunitisinitializedtohaveexactlyknon-zeroweights.Theideaistokeepthetotalamountofinputtotheunitindependentfromthenumberofinputsmwithoutmakingthemagnitudeofindividualweightelementsshrinkwithm.Sparseinitializationhelpstoachievemorediversityamongtheunitsatinitializationtime. However,italsoimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussianvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”largevalues,thisinitializationschemecancauseproblemsforunitssuchasmaxoutunitsthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.Whencomputationalresourcesallowit,itisusuallyagoodideatotreattheinitialscaleoftheweightsforeachlayerasahyperparameter,andtochoosethesescalesusingahyperparametersearchalgorithmdescribedinSec.,such11.4.2asrandomsearch.Thechoiceofwhethertousedenseorsparseinitializationcanalsobemadeahyperparameter.Alternately,onecanmanuallysearchforthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesistolookattherangeorstandarddeviationofactivationsorgradientsonasingleminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrosstheminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptablysmallactivationsandincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable304'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 319}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSinitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbeusefultolookattherangeorstandarddeviationofthegradientsaswellastheactivations. Thisprocedurecaninprinciplebeautomatedandisgenerallylesscomputationallycostlythanhyperparameteroptimizationbasedonvalidationseterrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelonasinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidationset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmoreformallyandstudiedby().MishkinandMatas2015So far we have focused on theinitialization of the weights.Fortunately,initializationofotherparametersistypicallyeasier.Theapproachforsettingthebiasesmustbecoordinatedwiththeapproachforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweightinitializationschemes.Thereareafewsituationswherewemaysetsomebiasestonon-zerovalues:•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiastoobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethattheinitialweightsaresmallenoughthattheoutputoftheunitisdeterminedonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivationfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.Forexample,iftheoutputisadistributionoverclassesandthisdistributionisahighlyskeweddistributionwiththemarginalprobabilityofclassigivenbyelementciofsomevectorc,thenwecansetthebiasvectorbbysolvingtheequationsoftmax(b) =c.ThisappliesnotonlytoclassiﬁersbutalsotomodelswewillencounterinPart,suchasautoencodersandBoltzmannIIImachines.Thesemodelshavelayerswhoseoutputshouldresembletheinputdatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayerstomatchthemarginaldistributionover.x•Sometimeswe maywanttochoose thebiastoavoidcausingtoo muchsaturationatinitialization.Forexample,wemaysetthebiasofaReLUhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.Thisapproachisnotcompatiblewithweightinitializationschemesthatdonotexpectstronginputfromthebiasesthough.Forexample,itisnotrecommendedforusewithrandomwalkinitialization(,).Sussillo2014•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateinafunction.Insuchsituations,wehaveaunitwithoutputuandanotherunith∈[0,1],thenwecanviewhasagatethatdetermineswhetheruh≈1oruh≈0.Inthesesituations,wewanttosetthebiasforhsothath≈1most305'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 320}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofthetimeatinitialization. Otherwiseudoesnothaveachancetolearn.Forexample,()advocatesettingthebiastofortheJozefowiczetal.20151forgetgateoftheLSTMmodel,describedinSec..10.10Anothercommontypeofparameterisavarianceorprecisionparameter.Forexample,wecanperformlinearregressionwithaconditionalvarianceestimateusingthemodelpyy(|Nx) = (|wTx+1)b,/β(8.24)whereβisaprecisionparameter.Wecanusuallyinitializevarianceorprecisionparametersto1safely.Anotherapproachistoassumetheinitialweightsarecloseenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andsetthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-ters,itispossibletoinitializemodelparametersusingmachinelearning.AcommonstrategydiscussedinPartofthisbookistoinitializeasupervisedmodelwithIIItheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperformingsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitializationthatoﬀersfasterconvergencethanarandominitialization.Someoftheseinitializationstrategiesmayyieldfasterconvergenceandbettergeneralizationbecausetheyencodeinformationaboutthedistributionintheinitialparametersofthemodel.Othersapparentlyperformwellprimarilybecausetheysettheparameterstohavetherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.8.5AlgorithmswithAdaptiveLearningRatesNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyoneofthehyperparametersthatisthemostdiﬃculttosetbecauseithasasigniﬁcantimpactonmodelperformance.AswehavediscussedinSec.andSec.,the4.38.2costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitivetoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,butdoessoattheexpenseofintroducinganotherhyperparameter.Inthefaceofthis,itisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsofsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearningrateforeachparameter,andautomaticallyadapttheselearningratesthroughoutthecourseoflearning.306'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 321}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSThedelta-bar-deltaalgorithm(,)isanearlyheuristicapproachJacobs1988toadaptingindividuallearningratesformodelparametersduringtraining.Theapproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespecttoagivenmodelparameter,remainsthesamesign,thenthelearningrateshouldincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,thenthelearningrateshoulddecrease. Ofcourse,thiskindofrulecanonlybeappliedtofullbatchoptimization.Morerecently,anumberofincremental(ormini-batch-based)methodshavebeenintroducedthatadaptthelearningratesofmodelparameters.Thissectionwillbrieﬂyreviewafewofthesealgorithms.8.5.1AdaGradTheAdaGradalgorithm,showninAlgorithm,individuallyadaptsthelearning8.4ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquarerootofthesumofalloftheirhistoricalsquaredvalues(Duchi2011etal.,).Theparameterswiththelargestpartialderivativeofthelosshaveacorrespondinglyrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivativeshavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreaterprogressinthemoregentlyslopeddirectionsofparameterspace.Inthecontextofconvexoptimization,theAdaGradalgorithmenjoyssomedesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—fortrainingdeepneuralnetworkmodels—theaccumulationofsquaredgradientsfromthebeginningoftrainingcanresultinaprematureandexcessivedecreaseintheeﬀectivelearningrate. AdaGradperformswellforsomebutnotalldeeplearningmodels.8.5.2RMSPropTheRMSPropalgorithm(Hinton2012,)modiﬁesAdaGradtoperformbetterinthenon-convexsettingbychangingthegradientaccumulationintoanexponentiallyweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenappliedtoaconvexfunction.Whenappliedtoanon-convexfunctiontotrainaneuralnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresandeventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthelearningrateaccordingtotheentirehistoryofthesquaredgradientandmayhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe307'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 322}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.4TheAdaGradalgorithmRequire:Globallearningrate\\ue00fRequire:InitialparameterθRequire:Smallconstant,perhapsδ10−7,fornumericalstabilityInitializegradientaccumulationvariabler= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←+\\ue00cComputeupdate:∆θ←−\\ue00fδ+√r\\ue00cg.(Divisionandsquarerootappliedelement-wise)Applyupdate:θθθ←+∆endwhileextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifitwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.RMSPropisshowninitsstandardforminAlgorithmandcombinedwith8.5NesterovmomentuminAlgorithm.ComparedtoAdaGrad,theuseofthe8.6movingaverageintroducesanewhyperparameter,ρ,thatcontrolsthelengthscaleofthemovingaverage.Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-tooptimizationmethodsbeingemployedroutinelybydeeplearningpractitioners.8.5.3AdamAdam(,)isyetanotheradaptivelearningrateoptimizationKingmaandBa2014algorithmandispresentedinAlgorithm.Thename“Adam”derivesfrom8.7thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itisperhapsbestseenasavariantonthecombinationofRMSPropandmomentumwithafewimportantdistinctions.First,inAdam,momentumisincorporateddirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)ofthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropistoapplymomentumtotherescaledgradients.Theuseofmomentumincombinationwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludesbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentumterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization308'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 323}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.5TheRMSPropalgorithmRequire:Globallearningrate,decayrate.\\ue00fρRequire:InitialparameterθRequire:Smallconstantδ, usually10−6, usedtostabilizedivision bysmallnumbers.Initializeaccumulationvariablesr= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←ρ+(1)−ρ\\ue00cComputeparameterupdate:∆θ=−\\ue00f√δ+r\\ue00cg.(1√δ+rappliedelement-wise)Applyupdate:θθθ←+∆endwhileattheorigin(seeAlgorithm).RMSPropalsoincorporatesanestimateofthe8.7(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbiasearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoiceofhyperparameters,thoughthelearningratesometimesneedstobechangedfromthesuggesteddefault.8.5.4ChoosingtheRightOptimizationAlgorithmInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddressthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeachmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldonechoose?Unfortunately,thereiscurrentlynoconsensusonthispoint.()Schauletal.2014presentedavaluablecomparisonofalargenumberofoptimizationalgorithmsacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyofalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)performedfairlyrobustly,nosinglebestalgorithmhasemerged.Currently,themostpopularoptimizationalgorithmsactivelyinuseincludeSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDeltaandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodependlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparametertuning).309'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 324}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.6RMSPropalgorithmwithNesterovmomentumRequire:Globallearningrate,decayrate,momentumcoeﬃcient.\\ue00fραRequire:Initialparameter,initialvelocity.θvInitializeaccumulationvariabler= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computeinterimupdate:˜θθv←+αComputegradient:g←1m∇˜θ\\ue050iLf((x()i;˜θy),()i)Accumulategradient:rrgg←ρ+(1)−ρ\\ue00cComputevelocityupdate:vv←α−\\ue00f√r\\ue00cg.(1√rappliedelement-wise)Applyupdate:θθv←+endwhile8.6ApproximateSecond-OrderMethodsInthissectionwediscusstheapplicationofsecond-ordermethodstothetrainingofdeepnetworks.See()foranearliertreatmentofthissubject.LeCunetal.1998aForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempiricalrisk:J() = θEx,y∼ˆpdata()x,y[((;))] =Lfxθ,y1mm\\ue058i=1Lf((x()i;)θ,y()i).(8.25)Howeverthemethodswediscusshereextendreadilytomoregeneralobjectivefunctionsthat,forinstance,includeparameterregularizationtermssuchasthosediscussedinChapter.78.6.1Newton’sMethodInSec.,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst-4.3ordermethods,second-ordermethodsmakeuseofsecondderivativestoimproveoptimization.Themostwidelyusedsecond-ordermethodisNewton’smethod.WenowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationtoneuralnetworktraining.Newton’smethodisanoptimizationschemebasedonusingasecond-orderTay-lorseriesexpansiontoapproximateJ(θ)nearsomepointθ0,ignoringderivatives310'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 325}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.7TheAdamalgorithmRequire:Stepsize(Suggesteddefault:)\\ue00f0001.Require:Exponentialdecayratesformoment estimates,ρ1andρ2in[0,1).(Suggesteddefaults:andrespectively)09.0999.Require:Smallconstantδusedfornumericalstabilization.(Suggesteddefault:10−8)Require:InitialparametersθInitialize1stand2ndmomentvariables,s= 0r= 0Initializetimestept= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)tt←+1Updatebiasedﬁrstmomentestimate:s←ρ1s+(1−ρ1)gUpdatebiasedsecondmomentestimate:r←ρ2r+(1−ρ2)gg\\ue00cCorrectbiasinﬁrstmoment:ˆs←s1−ρt1Correctbiasinsecondmoment:ˆr←r1−ρt2Computeupdate:∆= θ−\\ue00fˆs√ˆr+δ(operationsappliedelement-wise)Applyupdate:θθθ←+∆endwhileofhigherorder:JJ() θ≈(θ0)+(θθ−0)\\ue03e∇θJ(θ0)+12(θθ−0)\\ue03eHθθ(−0),(8.26)whereHistheHessianofJwithrespecttoθevaluatedatθ0.Ifwethensolveforthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:θ∗= θ0−H−1∇θJ(θ0)(8.27)Thusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescalingthegradientbyH−1,Newton’smethodjumpsdirectlytotheminimum. Iftheobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),thisupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’smethod,giveninAlgorithm.8.8Forsurfacesthatarenotquadratic,aslongastheHessianremainspositivedeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step311'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 326}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.8Newton’smethodwithobjectiveJ(θ)=1m\\ue050mi=1Lf((x()i;)θ,y()i).Require:Initialparameterθ0Require:TrainingsetofexamplesmwhiledostoppingcriterionnotmetComputegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)ComputeHessian:H←1m∇2θ\\ue050iLf((x()i;)θ,y()i)ComputeHessianinverse:H−1Computeupdate:∆= θ−H−1gApplyupdate:θθθ= +∆endwhileiterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdatingthequadraticapproximation).Second,updatetheparametersaccordingtoEq.8.27.InSec.,wediscussedhowNewton’smethodisappropriateonlywhen8.2.3theHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjectivefunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,thatareproblematicforNewton’smethod. IftheeigenvaluesoftheHessianarenotallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactuallycauseupdatestomoveinthewrongdirection.ThissituationcanbeavoidedbyregularizingtheHessian.Commonregularizationstrategiesincludeaddingaconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomesαθ∗= θ0−[((Hfθ0))+]αI−1∇θf(θ0).(8.28)ThisregularizationstrategyisusedinapproximationstoNewton’smethod,suchastheLevenberg–Marquardtalgorithm(Levenberg1944Marquardt1963,;,),andworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelativelyclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,thevalueofαwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.However,asαincreasesinsize,theHessianbecomesdominatedbytheαIdiagonalandthedirectionchosenbyNewton’smethodconvergestothestandardgradientdividedbyα. Whenstrongnegativecurvatureispresent,αmayneedtobesolargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwithaproperlychosenlearningrate.Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,suchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneuralnetworksislimitedbythesigniﬁcantcomputationalburdenitimposes.The312'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 327}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSnumberofelementsintheHessianissquaredinthenumberofparameters,sowithkparameters(andforevenverysmallneuralnetworksthenumberofparameterskcanbeinthemillions),Newton’smethodwouldrequiretheinversionofakk×matrix—withcomputationalcomplexityofO(k3).Also,sincetheparameterswillchangewitheveryupdate,theinverseHessianhastobecomputedateverytrainingiteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameterscanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection, wewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’smethodwhileside-steppingthecomputationalhurdles.8.6.2ConjugateGradientsConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverseHessianbyiterativelydescendingconjugatedirections.Theinspirationforthisapproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepestdescent(seeSec.fordetails),wherelinesearchesareappliediterativelyin4.3thedirectionassociatedwiththegradient.Fig.illustrateshowthemethodof8.6steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀectiveback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,whengivenbythegradient,isguaranteedtobeorthogonaltothepreviouslinesearchdirection.Lettheprevioussearchdirectionbedt−1.Attheminimum,wherethelinesearchterminates,thedirectionalderivativeiszeroindirectiondt−1:∇θJ(θ)·dt−1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,dt=∇θJ(θ) willhavenocontributioninthedirectiondt−1.Thusdtisorthogonaltodt−1.This relationshipbetweendt−1anddtisillustrated inFig.for8.6multipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceoforthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevioussearchdirections.Thisgivesrisetothezig-zagpatternofprogress,wherebydescendingtotheminimuminthecurrentgradientdirection,wemustre-minimizetheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientattheendofeachlinesearchweare,inasense,undoingprogresswehavealreadymadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradientsseekstoaddressthisproblem.Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthatisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogressmadeinthatdirection. Attrainingiterationt,thenextsearchdirectiondttakestheform:dt= ∇θJβ()+θtdt−1(8.29)313'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 328}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n\\U000f0913\\ue033\\ue030\\U000f0913\\ue032\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue031\\ue030\\ue032\\ue030\\U000f0913\\ue033\\ue030\\U000f0913\\ue032\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue031\\ue030\\ue032\\ue030\\nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.Themethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongthelinedeﬁnedbythegradientattheinitialpointoneachstep.ThisresolvessomeoftheproblemsseenwithusingaﬁxedlearningrateinFig.,butevenwiththeoptimalstepsizethe4.6algorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,attheminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointisorthogonaltothatdirection.wereβtisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,dt−1,weshouldaddbacktothecurrentsearchdirection.Twodirections,dtanddt−1,aredeﬁnedasconjugateifd\\ue03etHd()Jt−1= 0.d\\ue03etHdt−1= 0(8.30)ThestraightforwardwaytoimposeconjugacywouldinvolvecalculationoftheeigenvectorsofHtochooseβt,whichwouldnotsatisfyourgoalofdevelopingamethodthatismorecomputationallyviablethanNewton’smethodforlargeproblems. Canwecalculatetheconjugatedirectionswithoutresortingtothesecalculations?Fortunatelytheanswertothatisyes.Twopopularmethodsforcomputingtheβtare:1. Fletcher-Reeves:βt=∇θJ(θt)\\ue03e∇θJ(θt)∇θJ(θt−1)\\ue03e∇θJ(θt−1)(8.31)2. Polak-Ribière:βt=(∇θJ(θt)−∇θJ(θt−1))\\ue03e∇θJ(θt)∇θJ(θt−1)\\ue03e∇θJ(θt−1)(8.32)314'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 329}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSForaquadraticsurface,theconjugatedirectionsensurethatthegradientalongthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayattheminimumalongthepreviousdirections.Asaconsequence,inak-dimensionalparameterspace,conjugategradientsonlyrequiresklinesearchestoachievetheminimum.TheconjugategradientalgorithmisgiveninAlgorithm.8.9Algorithm8.9ConjugategradientmethodRequire:Initialparametersθ0Require:TrainingsetofexamplesmInitializeρ0= 0Initializeg0= 0Initializet= 1whiledostoppingcriterionnotmetInitializethegradientgt= 0Computegradient:gt←1m∇θ\\ue050iLf((x()i;)θ,y()i)Computeβt=(gt−gt−1)\\ue03egtg\\ue03et−1gt−1(Polak-Ribière)(Nonlinearconjugategradient:optionallyresetβttozero,forexampleiftisamultipleofsomeconstant,suchas)kk= 5Computesearchdirection:ρt= −gt+βtρt−1Performlinesearchtoﬁnd:\\ue00f∗= argmin\\ue00f1m\\ue050mi=1Lf((x()i;θt+\\ue00fρt),y()i)(Onatrulyquadraticcostfunction,analyticallysolvefor\\ue00f∗ratherthanexplicitlysearchingforit)Applyupdate:θt+1= θt+\\ue00f∗ρttt←+1endwhileNonlinearConjugateGradients:Sofarwehavediscussedthemethodofconjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse,ourprimaryinterestinthischapteristoexploreoptimizationmethodsfortrainingneuralnetworksandotherrelateddeeplearningmodelswherethecorrespondingobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodofconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirectionsarenolongerassuredtoremainattheminimumoftheobjectiveforpreviousdirections.Asaresult,thenonlinearconjugategradientsalgorithmincludesoccasionalresetswherethemethodofconjugategradientsisrestartedwithlinesearchalongtheunalteredgradient.Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate315'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 330}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSgradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialtoinitializetheoptimizationwithafewiterationsofstochasticgradientdescentbeforecommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugategradientsalgorithmhastraditionallybeencastasabatchmethod,minibatchversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.2011). Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshavebeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller1993).8.6.3BFGSTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptstobringsomeoftheadvantagesofNewton’smethodwithoutthecomputationalburden.Inthatrespect,BFGSissimilartoCG.However,BFGStakesamoredirectapproachtotheapproximationofNewton’supdate.RecallthatNewton’supdateisgivenbyθ∗= θ0−H−1∇θJ(θ0),(8.33)whereHistheHessianofJwithrespecttoθevaluatedatθ0.TheprimarycomputationaldiﬃcultyinapplyingNewton’supdateisthecalculationoftheinverseHessianH−1.Theapproachadoptedbyquasi-Newtonmethods(ofwhichtheBFGSalgorithmisthemostprominent)istoapproximatetheinversewithamatrixMtthatisiterativelyreﬁnedbylowrankupdatestobecomeabetterapproximationofH−1.ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmanytextbooksonoptimization,includingLuenberger1984().OncetheinverseHessianapproximationMtisupdated,thedirectionofdescentρtisdeterminedbyρt=Mtgt.Alinesearchisperformedinthisdirectiontodeterminethesizeofthestep,\\ue00f∗,takeninthisdirection.Theﬁnalupdatetotheparametersisgivenby:θt+1= θt+\\ue00f∗ρt.(8.34)Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesoflinesearcheswiththedirectionincorporatingsecond-orderinformation.Howeverunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependentonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspendlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmuststoretheinverseHessianmatrix,M,thatrequiresO(n2)memory,makingBFGS316'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 331}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsofparameters.Limited Memory BFGS (or L-BFGS)The memorycosts ofthe BFGSalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverseHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationMusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumptionthatM(1)t−istheidentitymatrix,ratherthanstoringtheapproximationfromonesteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGSaremutuallyconjugate.However,unlikethemethodofconjugategradients,thisprocedureremainswellbehavedwhentheminimumofthelinesearchisreachedonlyapproximately.ThL-BFGSstrategywithnostoragedescribedherecanbegeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthevectorsusedtoupdateateachtimestep,whichcostsonlyperstep.MOn()8.7OptimizationStrategiesandMeta-AlgorithmsManyoptimizationtechniquesarenotexactlyalgorithms, butrathergeneraltemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbeincorporatedintomanydiﬀerentalgorithms.8.7.1BatchNormalizationBatchnormalization(,)isoneofthemostexcitingrecentIoﬀeandSzegedy2015innovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimizationalgorithmatall.Instead,itisamethodofadaptivereparametrization,motivatedbythediﬃcultyoftrainingverydeepmodels.Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.Thegradienttellshowtoupdateeachparameter,undertheassumptionthattheotherlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctionscomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputedundertheassumptionthattheotherfunctionsremainconstant.Asasimpleexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayeranddoesnotuseanactivationfunctionateachhiddenlayer:ˆy=xw1w2w3...wl.Here,wiprovidestheweightusedbylayeri.Theoutputoflayeriishi=hi−1wi.Theoutputˆyisalinearfunctionoftheinputx,butanonlinearfunctionoftheweightswi.Supposeourcostfunctionhasputagradientofon1ˆy,sowewishto317'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 332}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdecreaseˆyslightly.Theback-propagationalgorithmcanthencomputeagradientg=∇wˆy.Considerwhathappenswhenwemakeanupdatewwg←−\\ue00f.Theﬁrst-orderTaylorseriesapproximationofˆypredictsthatthevalueofˆywilldecreaseby\\ue00fg\\ue03eg.Ifwewantedtodecreaseˆyby.1,thisﬁrst-orderinformationavailableinthegradientsuggestswecouldsetthelearningrate\\ue00fto.1g\\ue03eg.However,theactualupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforderl.Thenewvalueofˆyisgivenbyxw(1−\\ue00fg1)(w2−\\ue00fg2)(...wl−\\ue00fgl).(8.35)Anexampleofonesecond-ordertermarisingfromthisupdateis\\ue00f2g1g2\\ue051li=3wi.Thistermmightbenegligibleif\\ue051li=3wiissmall,ormightbeexponentiallylargeiftheweightsonlayersthrough3laregreaterthan.Thismakesitveryhard1tochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetotheparametersforonelayerdependssostronglyonalloftheotherlayers.Second-orderoptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthesesecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,evenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimizationalgorithmsareexpensiveandusuallyrequirenumerousapproximationsthatpreventthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions.Buildingann-thorderoptimizationalgorithmforn>2thusseemshopeless.Whatcanwedoinstead?Batchnormalizationprovidesanelegantwayofreparametrizingalmostanydeepnetwork.Thereparametrizationsigniﬁcantlyreducestheproblemofcoordinatingupdatesacrossmanylayers.Batchnormalizationcanbeappliedtoanyinputorhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayertonormalize,arrangedasadesignmatrix,withtheactivationsforeachexampleappearinginarowofthematrix.Tonormalize,wereplaceitwithHH\\ue030=Hµ−σ,(8.36)whereµisavectorcontainingthemeanofeachunitandσisavectorcontainingthestandarddeviationofeachunit.ThearithmetichereisbasedonbroadcastingthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Withineachrow,thearithmeticiselement-wise,soHi,jisnormalizedbysubtractingµjanddividingbyσj.TherestofthenetworkthenoperatesonH\\ue030inexactlythesamewaythattheoriginalnetworkoperatedon.HAttrainingtime,µ=1m\\ue058iHi,:(8.37)318'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 333}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSandσ=\\ue073δ+1m\\ue058i()Hµ−2i,(8.38)whereδisasmallpositivevaluesuchas10−8imposedtoavoidencounteringtheundeﬁnedgradientof√zatz=0.Crucially,weback-propagatethroughtheseoperationsforcomputingthemeanandthestandarddeviation,andforapplyingthemtonormalizeH.Thismeansthatthegradientwillneverproposeanoperation thatactssimplytoincreasethestandard deviationormeanofhi;thenormalizationoperationsremovetheeﬀectofsuchanactionandzerooutitscomponentinthegradient.Thiswasamajorinnovationofthebatchnormalizationapproach. Previousapproacheshadinvolvedaddingpenaltiestothecostfunctiontoencourageunitstohavenormalizedactivationstatisticsorinvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.Theformerapproachusuallyresultedinimperfectnormalizationandthelatterusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedlyproposedchangingthemeanandvarianceandthenormalizationsteprepeatedlyundidthischange.Batchnormalizationreparametrizesthemodeltomakesomeunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.Attesttime,µandσmaybereplacedbyrunningaveragesthatwerecollectedduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,withoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.Revisitingtheˆy=xw1w2...wlexample,weseethatwecanmostlyresolvethediﬃcultiesinlearningthismodelbynormalizinghl−1.SupposethatxisdrawnfromaunitGaussian.Thenhl−1willalsocomefromaGaussian,becausethetransformationfromxtohlislinear.However,hl−1willnolongerhavezeromeanandunitvariance.Afterapplyingbatchnormalization,weobtainthenormalizedˆhl−1thatrestoresthezeromeanandunitvarianceproperties.Foralmostanyupdatetothelowerlayers,ˆhl−1willremainaunitGaussian.Theoutputˆymaythenbelearnedasasimplelinearfunctionˆy=wlˆhl−1.Learninginthismodelisnowverysimplebecausetheparametersatthelowerlayerssimplydonothaveaneﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. Insomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelowerlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign0ofoneofthelowerweightscanﬂiptherelationshipbetweenˆhl−1andy. Thesesituationsareveryrare.Withoutnormalization,nearlyeveryupdatewouldhaveanextremeeﬀectonthestatisticsofhl−1.Batchnormalizationhasthusmadethismodelsigniﬁcantlyeasiertolearn.Inthisexample,theeaseoflearningofcoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,319'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhaveanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecondorderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneuralnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlineartransformationsofthedata,sotheyremainuseful.Batchnormalizationactstostandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,butallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingleunittochange.Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithinalayer.Indeed,thisistheapproachtakenby(),whoprovidedDesjardinsetal.2015theinspirationforbatchnormalization.Unfortunately, eliminatingalllinearinteractionsismuchmoreexpensivethanstandardizingthemeanandstandarddeviationofeachindividualunit,andsofarbatchnormalizationremainsthemostpracticalapproach.Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressivepowerofthe neuralnetworkcontaining thatunit.In ordertomaintain theexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunitactivationsHwithγH\\ue030+βratherthansimplythenormalizedH\\ue030.Thevariablesγandβarelearnedparametersthatallowthenewvariabletohaveanymeanandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidwesetthemeanto0,andthenintroduceaparameterthatallowsittobesetbacktoanyarbitraryvalueβ?Theansweristhatthenewparametrizationcanrepresentthesamefamilyoffunctionsoftheinputastheoldparametrization,butthenewparametrizationhasdiﬀerentlearningdynamics.Intheoldparametrization,themeanofHwasdeterminedbyacomplicatedinteractionbetweentheparametersinthelayersbelowH.Inthenewparametrization,themeanofγH\\ue030+βisdeterminedsolelybyβ.Thenewparametrizationismucheasiertolearnwithgradientdescent.Mostneuralnetworklayerstaketheformofφ(XW+b)whereφissomeﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.ItisnaturaltowonderwhetherweshouldapplybatchnormalizationtotheinputX,ortothetransformedvalueXW+b.()recommendIoﬀeandSzegedy2015thelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversionofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwiththeβparameterappliedbythebatchnormalizationreparametrization.Theinputtoalayerisusuallytheoutputofanonlinearactivationfunctionsuchastherectiﬁedlinearfunctioninapreviouslayer. Thestatisticsoftheinputarethus320'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 335}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.Inconvolutionalnetworks,describedinChapter,itisimportanttoapplythe9samenormalizingµandσateveryspatiallocationwithinafeaturemap,sothatthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.8.7.2CoordinateDescentInsomecases,itmaybepossibletosolveanoptimizationproblemquicklybybreakingitintoseparatepieces.Ifweminimizef(x)withrespecttoasinglevariablexi,thenminimizeitwithrespecttoanothervariablexjandsoon,repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)minimum.Thispracticeisknownascoordinatedescent,becauseweoptimizeonecoordinateatatime.Moregenerally,blockcoordinatedescentreferstominimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellasthestrictlyindividualcoordinatedescent.Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesintheoptimizationproblemcanbeclearlyseparatedintogroupsthatplayrelativelyisolatedroles,orwhenoptimizationwithrespecttoonegroupofvariablesissigniﬁcantlymoreeﬃcientthanoptimizationwithrespecttoallofthevariables.Forexample,considerthecostfunctionJ,(HW) =\\ue058i,j|Hi,j|+\\ue058i,j\\ue010XW−\\ue03eH\\ue0112i,j.(8.39)Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalistoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvaluesHtoreconstructthetrainingsetX.MostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inordertopreventthepathologicalsolutionwithextremelysmallandlarge.HWThefunctionJisnotconvex.However, wecandividetheinputstothetrainingalgorithmintotwosets:thedictionaryparametersWandthecoderepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneofthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgivesusanoptimizationstrategythatallowsustouseeﬃcientconvexoptimizationalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizingHWwithﬁxed.Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariablestronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunctionf(x) =321'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 336}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS(x1−x2)2+α\\ue000x21+x22\\ue001whereαisapositiveconstant.Theﬁrsttermencouragesthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthemtobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolvetheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.However,forsmallα,coordinatedescentwillmakeveryslowprogressbecausetheﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀerssigniﬁcantlyfromthecurrentvalueoftheothervariable.8.7.3PolyakAveragingPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveralpoints inthe trajectory throughparameter spacevisited by anoptimizationalgorithm. Iftiterationsofgradientdescentvisitpointsθ(1),...,θ()t,thentheoutputofthePolyakaveragingalgorithmisˆθ()t=1t\\ue050iθ()i. Onsomeproblemclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhasstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcationismoreheuristic,butitperformswellinpractice.Thebasicideaisthattheoptimizationalgorithmmayleapbackandforthacrossavalleyseveraltimeswithoutevervisitingapointnearthebottomofthevalley.Theaverageofallofthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.Innon-convexproblems,thepathtakenbytheoptimizationtrajectorycanbeverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameterspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylargebarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,whenapplyingPolyakaveragingtonon-convexproblems,itistypicaltouseanexponentiallydecayingrunningaverage:ˆθ()t= αˆθ(1)t−+(1)−αθ()t.(8.40)Therunningaverageapproachisusedinnumerousapplications.SeeSzegedyetal.()forarecentexample.20158.7.4SupervisedPretrainingSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitiousifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itissometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmakethemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolveasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthatinvolve322'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 337}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELStrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeoftrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownaspretraining.Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefortheoptimalversionofeachcomponentinisolation.Unfortunately,combiningtheindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcompletesolution.However,greedyalgorithmscanbecomputationallymuchcheaperthanalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolutionisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbyaﬁne-tuningstageinwhichajointoptimizationalgorithmsearchesforanoptimalsolutiontothefullproblem.Initializingthejointoptimizationalgorithmwithagreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionitﬁnds.Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousindeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithmsthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearningproblems.Thisapproachisknownasgreedysupervisedpretraining.Intheoriginal(,)versionofgreedysupervisedpretraining,Bengioetal.2007eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetofthelayersintheﬁnalneuralnetwork.AnexampleofgreedysupervisedpretrainingisillustratedinFig.,inwhicheachaddedhiddenlayerispretrainedaspartof8.7ashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrainedhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse2015theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeepernetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.Anotheroption,exploredbyYu2010etal.()istousetheofthepreviouslyoutputstrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.Why would greedy supervised pretraining help?The hypothesis initiallydiscussedby()isthatithelpstoprovidebetterguidancetotheBengioetal.2007intermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothintermsofoptimizationandintermsofgeneralization.Anapproachrelatedtosupervisedpretrainingextendstheideatothecontextoftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8layersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)andtheninitializeasame-sizenetworkwiththeﬁrstklayersoftheﬁrstnet.Allthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are323'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 338}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\ny yh(1)h(1)xx(a)U(1)U(1)W(1)W(1)y yh(1)h(1)x x(b)U(1)U(1)W(1)W(1)\\ny yh(1)h(1)x x(c)U(1)U(1)W(1)W(1)h(2)h(2)y yU(2)U(2)W(2)W(2)y yh(1)h(1)x x(d)U(1)U(1)W(1)W(1)h(2)h(2)yU(2)U(2)W(2)W(2)\\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining(,).Bengioetal.2007(a)Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe(b)samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand(c)discardthehidden-to-outputlayer.WesendtheoutputoftheﬁrsthiddenlayerasinputtoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjectiveastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforasmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforward(d)network.Tofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyattheendorateachstageofthisprocess.324'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000ImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetoftasks.OtherapproachestotransferlearningwithneuralnetworksarediscussedinSec..15.2Anotherrelatedlineofworkisthe(,)approach.ThisFitNetsRomeroetal.2015approachbeginsbytraininganetworkthathaslowenoughdepthandgreatenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthenbecomesateacherforasecondnetwork,designatedthe. Thestudentnetworkisstudentmuchdeeperandthinner(eleventonineteenlayers)andwouldbediﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthestudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredicttheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayeroftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthehiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additionalparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetworkfromthemiddlelayerofthedeeperstudentnetwork. However,insteadofpredictingtheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayeroftheteachernetwork.Thelowerlayersofthestudentnetworksthushavetwoobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,aswellastopredicttheintermediatelayeroftheteachernetwork.Althoughathinanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallownetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslowercomputationalcostifitisthinenoughtohavefarfewerparameters.Withoutthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyintheexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythusbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃculttotrain,butotheroptimizationtechniquesorchangesinthearchitecturemayalsosolvetheproblem.8.7.5DesigningModelstoAidOptimizationToimproveoptimization,thebeststrategyisnotalwaystoimprovetheoptimizationalgorithm.Instead,manyimprovementsintheoptimizationofdeepmodelshavecomefromdesigningthemodelstobeeasiertooptimize.Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreaseinjaggednon-monotonicpatterns.However,thiswouldmakeoptimizationextremelydiﬃcult. Inpractice,itismoreimportanttochooseamodelfamilythatiseasytooptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesinneuralnetworklearningoverthepast30yearshavebeen325'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 340}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSobtainedbychangingthemodelfamilyratherthanchangingtheoptimizationprocedure.Stochasticgradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-formationsbetweenlayersandactivationfunctionsthatarediﬀerentiablealmosteverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain. Inpar-ticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunitshaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeepnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmakeoptimizationeasier.ThegradientﬂowsthroughmanylayersprovidedthattheJacobianofthelineartransformationhasreasonablesingularvalues. Moreover,linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’soutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradientwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,modernneuralnetshavebeendesignedsothattheirlocalgradientinformationcorrespondsreasonablywelltomovingtowardadistantsolution.Othermodeldesignstrategiescanhelptomakeoptimizationeasier.Forexample,linearpathsorskipconnectionsbetweenlayersreducethelengthoftheshortestpathfromthelowerlayer’sparameterstotheoutput, and thusmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedideatoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtotheintermediatehiddenlayersofthenetwork,asinGoogLeNet(,)Szegedyetal.2014aanddeeply-supervisednets(,).These“auxiliaryheads”aretrainedLeeetal.2014toperformthesametaskastheprimaryoutputatthetopofthenetworkinordertoensurethatthelowerlayersreceivealargegradient.Whentrainingiscompletetheauxiliaryheadsmaybediscarded. Thisisanalternativetothepretrainingstrategies,whichwereintroducedintheprevioussection.Inthisway,onecantrainjointlyallthelayersinasinglephasebutchangethearchitecture,sothatintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhattheyshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.8.7.6ContinuationMethodsandCurriculumLearningAsarguedinSec.,manyofthechallengesinoptimizationarisefromtheglobal8.2.7structureofthecostfunctionandcannotberesolvedmerelybymakingbetterestimatesoflocalupdatedirections.Thepredominantstrategyforovercomingthisproblemistoattempttoinitializetheparametersinaregionthatisconnectedtothesolutionbyashortpaththroughparameterspacethatlocaldescentcan326'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 341}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdiscover.Continuationmethodsareafamilyofstrategiesthatcanmakeoptimizationeasierbychoosinginitialpointstoensurethatlocaloptimizationspendsmostofitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsistoconstructaseriesofobjectivefunctionsoverthesameparameters.InordertominimizeacostfunctionJ(θ),wewillconstructnewcostfunctions{J(0),...,J()n}.Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,withJ(0)beingfairlyeasytominimize,andJ()n,themostdiﬃcult,beingJ(θ),thetruecostfunctionmotivatingtheentireprocess.WhenwesaythatJ()iiseasierthanJ(+1)i,wemeanthatitiswellbehavedovermoreofθspace.Arandominitializationismorelikelytolandintheregionwherelocaldescentcanminimizethecostfunctionsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesignedsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginbysolvinganeasyproblemthenreﬁnethesolutiontosolveincrementallyharderproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.Traditionalcontinuationmethods(predatingtheuseofcontinuationmethodsforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.SeeWu1997()foranexampleofsuchamethodandareviewofsomerelatedmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,whichaddsnoisetotheparameters(Kirkpatrick1983etal., ).Continuationmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher()foranoverviewofrecentliterature,especiallyforAIapplications.2015Continuationmethodstraditionallyweremostlydesignedwiththegoalofovercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedtoreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,thesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”theoriginalcostfunction.ThisblurringoperationcanbedonebyapproximatingJ()i() = θEθ\\ue030∼N(θ\\ue030;θ,σ()2i)J(θ\\ue030)(8.41)viasampling.Theintuitionforthisapproachisthatsomenon-convexfunctionsbecomeapproximatelyconvexwhenblurred.Inmanycases,thisblurringpreservesenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndtheglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.Thisapproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁneaseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfromonefunctiontothenextarrivingattheglobalminimum,butitmightrequiresomanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.NP-hardoptimizationproblemsremainNP-hard,evenwhencontinuationmethods327'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 342}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespondtothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,nomatterhowmuchitisblurred.ConsiderforexamplethefunctionJ(θ) =−θ\\ue03eθ.Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimumofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumoftheoriginalcostfunction.Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththeproblemoflocalminima,localminimaarenolongerbelievedtobetheprimaryproblemforneuralnetworkoptimization.Fortunately,continuationmethodscanstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcaneliminateﬂatregions,decreasevarianceingradientestimates,improveconditioningoftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdateseasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirectionsandprogresstowardaglobalsolution.Bengio2009etal.()observedthatanapproachcalledcurriculumlearningorshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningisbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconceptsandprogresstolearningmorecomplexconceptsthatdependonthesesimplerconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimaltraining(,;,;Skinner1958Peterson2004KruegerandDayan2009,)andmachinelearning(,;,;,).()Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009justiﬁedthisstrategyasacontinuationmethod,whereearlierJ()iaremadeeasierbyincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributionstothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),andexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowingacurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearninghasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;Collobert2011aMikolov2011bTuandHonavar2011etal.,;etal.,;,)andcomputervision(,;,;,)Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayinwhichhumansteach(,):teachersstartbyshowingeasierandKhanetal.2011moreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurfacewiththelessobviouscases.Curriculum-basedstrategiesaremoreeﬀectiveforteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcanalsoincreasetheeﬀectivenessofotherteachingstrategies(,BasuandChristensen2013).Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthecontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:328'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 343}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwithastochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalwayspresentedtothelearner,butwheretheaverageproportionofthemorediﬃcultexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.Withadeterministiccurriculum,noimprovementoverthebaseline(ordinarytrainingfromthefulltrainingset)wasobserved.Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowtoregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsoftheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesandprocessinputdatathathasspecialstructure.Theoptimizationmethodsdiscussedinthischapterareoftendirectlyapplicabletothesespecializedarchitectureswithlittleornomodiﬁcation.\\n329'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 344}, page_content='Chapter9ConvolutionalNetworksConvolutionalnetworksconvolutionalneuralnetworks(,),alsoknownasLeCun1989or,areaspecializedkindofneuralnetworkforprocessingdatathathasCNNsaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcanbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,whichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeentremendouslysuccessfulinpracticalapplications.Thename“convolutionalneuralnetwork” indicatesthatthenetworkemploysamathematicaloperationcalledconvolution.Convolutionisaspecializedkindoflinearoperation.Convolutionalnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrixmultiplicationinatleastoneoftheirlayers.Inthischapter, we willﬁrstdescribewhatconvolution is.Next, wewillexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthendescribeanoperationcalledpooling,whichalmostallconvolutionalnetworksemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnotcorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuchasengineeringorpuremathematics.Wewilldescribeseveralvariantsontheconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.Wewillalsoshowhowconvolutionmaybeappliedto many kindsofdata, withdiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolutionmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁcprinciplesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,thenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayedinthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowtochoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteristodescribethekindsoftoolsthatconvolutionalnetworksprovide,whileChapter11330'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 345}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSdescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.Researchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanewbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,renderingitimpracticaltodescribethebestarchitectureinprint.However,thebestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribedhere.9.1TheConvolutionOperationInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-valuedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamplesoftwofunctionswemightuse.Supposewearetrackingthelocationofaspaceshipwithalasersensor.Ourlasersensorprovidesasingleoutputx(t),thepositionofthespaceshipattimet.Bothxandtarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelasersensoratanyinstantintime.Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisyestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveralmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewillwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.Wecandothiswithaweightingfunctionw(a),whereaistheageofameasurement.Ifweapplysuchaweightedaverageoperationateverymoment,weobtainanewfunctionprovidingasmoothedestimateofthepositionofthespaceship:sst() =\\ue05axawtada()(−)(9.1)Thisoperationiscalledconvolution.Theconvolutionoperationistypicallydenotedwithanasterisk:stxwt() = (∗)()(9.2)Inourexample,wneedstobeavalidprobabilitydensityfunction,ortheoutputisnotaweightedaverage.Also,wneedstobeforallnegativearguments,0oritwilllookintothefuture,whichispresumablybeyondourcapabilities.Theselimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁnedforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforotherpurposesbesidestakingweightedaverages.Inconvolutionalnetworkterminology,theﬁrstargument(inthisexample,thefunctionx)totheconvolutionisoftenreferredtoastheandthesecondinput331'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 346}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSargument(inthisexample,thefunctionw)asthe.Theoutputissometimeskernelreferredtoasthefeaturemap.Inourexample,theideaofalasersensorthatcanprovidemeasurementsateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataonacomputer,timewillbediscretized,andoursensorwillprovidedataatregularintervals.Inourexample,itmightbemorerealistictoassumethatourlaserprovidesameasurementoncepersecond.Thetimeindextcanthentakeononlyintegervalues.Ifwenowassumethatxandwaredeﬁnedonlyonintegert,wecandeﬁnethediscreteconvolution:stxwt() = (∗)() =∞\\ue058a=−∞xawta()(−)(9.3)Inmachinelearningapplications,theinputisusuallyamultidimensionalarrayofdataandthekernelisusuallyamultidimensionalarrayofparametersthatareadaptedbythelearningalgorithm.Wewillrefertothesemultidimensionalarraysastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystoredseparately,weusuallyassumethatthesefunctionsarezeroeverywherebuttheﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewecanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberofarrayelements.Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.Forexample,ifweuseatwo-dimensionalimageIasourinput,weprobablyalsowanttouseatwo-dimensionalkernel:KSi,jIKi,j() = (∗)() =\\ue058m\\ue058nIm,nKim,jn.()(−−)(9.4)Convolutioniscommutative,meaningwecanequivalentlywrite:Si,jKIi,j() = (∗)() =\\ue058m\\ue058nIim,jnKm,n.(−−)()(9.5)Usuallythelatterformulaismorestraightforwardtoimplementinamachinelearninglibrary,becausethereislessvariationintherangeofvalidvaluesofmand.nThecommutativepropertyofconvolutionarisesbecausewehaveﬂippedthekernelrelativetotheinput,inthesensethatasmincreases,theindexintotheinputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂipthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty332'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 347}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneuralnetworkimplementation.Instead,manyneuralnetworklibrariesimplementarelatedfunctioncalledthecross-correlation,whichisthesameasconvolutionbutwithoutﬂippingthekernel:Si,jIKi,j() = (∗)() =\\ue058m\\ue058nIim,jnKm,n.(++)()(9.6)Manymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.Inthistextwewillfollowthisconventionofcallingbothoperationsconvolution,andspecifywhetherwemeantoﬂipthekernelornotincontextswherekernelﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwilllearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithmbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelativetothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorareforconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisusedsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoesnotcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelornot.SeeFig.foranexampleofconvolution(withoutkernelﬂipping)appliedto9.1a2-Dtensor.Discreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,thematrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,forunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobeequaltotherowaboveshiftedbyoneelement.ThisisknownasaToeplitzmatrix.Intwodimensions,adoublyblockcirculantmatrixcorrespondstoconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequaltoeachother,convolutionusuallycorrespondstoaverysparsematrix(amatrixwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuchsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswithmatrixmultiplicationanddoesnotdependonspeciﬁcpropertiesofthematrixstructureshouldworkwithconvolution,withoutrequiringanyfurtherchangestotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseoffurtherspecializationsinordertodealwithlargeinputseﬃciently,butthesearenotstrictlynecessaryfromatheoreticalperspective.333'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 348}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nabcdefghijklwxyz\\naw+bx+ey+fzaw+bx+ey+fzbw+cx+fy+gzbw+cx+fy+gzcw+dx+gy+hzcw+dx+gy+hzew+fx+iy+jzew+fx+iy+jzfw+gx+jy+kzfw+gx+jy+kzgw+hx+ky+lzgw+hx+ky+lzInputKernel\\nOutput\\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestricttheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-leftelementoftheoutputtensorisformedbyapplyingthekerneltothecorrespondingupper-leftregionoftheinputtensor.\\n334'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 349}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS9.2MotivationConvolutionleveragesthreeimportantideasthatcanhelpimproveamachinelearningsystem:sparseinteractionsparametersharingequivariantrepresenta-,andtions.Moreover,convolutionprovidesameansforworkingwithinputsofvariablesize.Wenowdescribeeachoftheseideasinturn.Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixofparameterswithaseparateparameterdescribingtheinteractionbetweeneachinputunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinputunit.Convolutionalnetworks,however,typicallyhavesparseinteractions(alsoreferredtoassparseconnectivitysparseweightsor).Thisisaccomplishedbymakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,theinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,meaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsofpixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthememoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italsomeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovementsineﬃciencyareusuallyquitelarge.Ifthereareminputsandnoutputs,thenmatrixmultiplicationrequiresmn×parametersandthealgorithmsusedinpracticehaveO(mn×)runtime(perexample).Ifwelimitthenumberofconnectionseachoutputmayhavetok,thenthesparselyconnectedapproachrequiresonlykn×parametersandO(kn×)runtime.Formanypracticalapplications,itispossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeepingkseveralordersofmagnitudesmallerthanm. Forgraphicaldemonstrationsofsparseconnectivity,seeFig.andFig..Inadeepconvolutionalnetwork,9.29.3unitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,asshowninFig..Thisallowsthenetworktoeﬃcientlydescribecomplicated9.4interactionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimplebuildingblocksthateachdescribeonlysparseinteractions.Parameter sharingreferstousing thesameparameterfor morethanonefunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrixisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedbyoneelementoftheinputandthenneverrevisited.Asasynonymforparametersharing,onecansaythatanetworkhastiedweights,becausethevalueoftheweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.Inaconvolutionalneuralnet,eachmemberofthekernelisusedateverypositionoftheinput(exceptperhapssomeoftheboundarypixels,dependingonthedesigndecisionsregardingtheboundary).Theparametersharingusedbytheconvolutionoperationmeansthatratherthanlearningaseparatesetofparametersforeverylocation,welearn335'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 350}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nFigure9.2:Sparseconnectivity,viewedfrombelow:Wehighlightoneinputunit,x3,andalsohighlighttheoutputunitsinsthatareaﬀectedbythisunit.(Top)Whensisformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby3x.(Bottom)Whensisformedbymatrixmultiplication,connectivityisnolongersparse,soalloftheoutputsareaﬀectedbyx3.\\n336'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 351}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nFigure9.3:Sparseconnectivity,viewedfromabove:Wehighlightoneoutputunit,s3,andalsohighlighttheinputunitsinxthataﬀectthisunit.Theseunitsareknownasthereceptiveﬁeldofs3.(Top)Whensisformedbyconvolutionwithakernelofwidth,only3threeinputsaﬀects3.When(Bottom)sisformedbymatrixmultiplication,connectivityisnolongersparse,soalloftheinputsaﬀects3.\\nx1x1x2x2x3x3h2h2h1h1h3h3x4x4h4h4x5x5h5h5g2g2g1g1g3g3g4g4g5g5\\nFigure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetworkislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesifthenetworkincludesarchitecturalfeatureslikestridedconvolution(Fig.)orpooling9.12(Sec.).Thismeansthateventhough9.3directconnectionsinaconvolutionalnetareverysparse,unitsinthedeeperlayerscanbeindirectlyconnectedtoallormostoftheinputimage.337'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 352}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nx1x1x2x2x3x3x4x4x5x5s2s2s1s1s3s3s4s4s5s5Figure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticularparameterintwodiﬀerentmodels.(Top)Theblackarrowsindicateusesofthecentralelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,thissingleparameterisusedatallinputlocations.Thesingleblackarrowindicates(Bottom)theuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodelhasnoparametersharingsotheparameterisusedonlyonce.onlyoneset.Thisdoesnotaﬀecttheruntimeofforwardpropagation—itisstillO(kn×)—butitdoesfurtherreducethestoragerequirementsofthemodeltokparameters. Recallthatkisusuallyseveralordersofmagnitudelessthanm.Sincemandnareusuallyroughlythesamesize,kispracticallyinsigniﬁcantcomparedtomn×.Convolutionisthusdramaticallymoreeﬃcientthandensematrixmultiplicationintermsofthememoryrequirementsandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,seeFig..9.5Asanexampleofbothoftheseﬁrsttwoprinciplesinaction,Fig.showshow9.6sparseconnectivityandparametersharingcandramaticallyimprovetheeﬃciencyofalinearfunctionfordetectingedgesinanimage.Inthecaseofconvolution,theparticularformofparametersharingcausesthelayertohaveapropertycalledequivariancetotranslation.Tosayafunctionisequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.Speciﬁcally,afunctionf(x)isequivarianttoafunctiongiff(g(x))=g(f(x)).Inthecaseofconvolution,ifweletgbeanyfunctionthattranslatestheinput,i.e.,shiftsit,thentheconvolutionfunctionisequivarianttog.Forexample,letIbeafunctiongivingimagebrightnessatintegercoordinates.Letgbeafunctionmappingoneimagefunctiontoanotherimagefunction,suchthatI\\ue030=g(I)is338'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 353}, page_content='CHAPTER9.CONVOLUTIONALNETWORKStheimagefunctionwithI\\ue030(x,y)=I(x−1,y).ThisshiftseverypixelofIoneunittotheright. IfweapplythistransformationtoI,thenapplyconvolution,theresultwillbethesameasifweappliedconvolutiontoI\\ue030,thenappliedthetransformationgtotheoutput.Whenprocessingtimeseriesdata,thismeansthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeaturesappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexactsamerepresentationofitwillappearintheoutput,justlaterintime.Similarlywithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearintheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethesameamountintheoutput.Thisisusefulforwhenweknowthatsomefunctionofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinputlocations.Forexample,whenprocessingimages,itisusefultodetectedgesintheﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorlesseverywhereintheimage,soitispracticaltoshareparametersacrosstheentireimage.Insomecases,wemaynotwishtoshareparametersacrosstheentireimage.Forexample,ifweareprocessingimagesthatarecroppedtobecenteredonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerentlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookforeyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedstolookforachin.Convolutionisnotnaturallyequivarianttosomeothertransformations,suchaschangesinthescaleorrotationofanimage.Othermechanismsarenecessaryforhandlingthesekindsoftransformations.Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedbymatrixmultiplicationwithaﬁxed-shapematrix.Convolutionenablesprocessingofsomeofthesekindsofdata.WediscussthisfurtherinSec..9.79.3PoolingAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeFig.).In9.7theﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproduceasetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthroughanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.Thisstageissometimescalledthedetectorstage.Inthethirdstage,weuseapoolingfunctiontomodifytheoutputofthelayerfurther.Apoolingfunctionreplacestheoutputofthenetatacertainlocationwithasummarystatisticofthenearbyoutputs.Forexample,themaxpooling(ZhouandChellappa1988,)operationreportsthemaximumoutputwithinarectangular339'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 354}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.6:Eﬃciencyofedgedetection. Theimageontherightwasformedbytakingeachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelontheleft. Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,whichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.Theinputimageis320pixelswidewhiletheoutputimageis319pixelswide.Thistransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,andrequires319×280×3=267,960ﬂoatingpointoperations(twomultiplicationsandoneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesametransformationwithamatrixmultiplicationwouldtake320×280×319×280,orovereightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientforrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithmperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000timesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbezero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplicationandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.Thematrixwouldstillneedtocontain2×319×280=178,640entries.Convolutionisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelineartransformationofasmall,localregionacrosstheentireinput.(Photocredit:PaulaGoodfellow)\\n340'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 355}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nConvolutional Layer\\nInput to layerConvolution stage:Ane transformﬃDetector stage:Nonlinearitye.g., rectiﬁed linearPooling stageNext layer\\nInput to layersConvolution layer:Ane transform ﬃDetector layer: Nonlinearitye.g., rectiﬁed linearPooling layerNext layerComplex layer terminologySimple layer terminology\\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwocommonlyusedsetsofterminologyfordescribingtheselayers.(Left)Inthisterminology,theconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,witheachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemappingbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.(Right)Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimplelayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthatnotevery“layer”hasparameters.\\n341'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 356}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangularneighborhood,theL2normofarectangularneighborhood,oraweightedaveragebasedonthedistancefromthecentralpixel.Inallcases,poolinghelpstomaketherepresentationbecomeapproximatelyinvarianttosmalltranslationsoftheinput.Invariancetotranslationmeansthatifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooledoutputsdonotchange.SeeFig.foranexampleofhowthisworks.9.8Invariancetolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhethersomefeatureispresentthanexactlywhereitis.Forexample,whendeterminingwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswithpixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftsideofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismoreimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁndacornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreservethelocationoftheedgeswellenoughtotestwhethertheymeet.Theuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthatthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthisassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.Poolingoverspatialregionsproducesinvariancetotranslation,butifwepoolovertheoutputsofseparatelyparametrizedconvolutions,thefeaturescanlearnwhichtransformationstobecomeinvariantto(seeFig.).9.9Becausepoolingsummarizestheresponsesoverawholeneighborhood,itispossibletousefewerpoolingunitsthandetectorunits,byreportingsummarystatisticsforpoolingregionsspacedkpixelsapartratherthan1pixelapart.SeeFig.foranexample.Thisimprovesthecomputationaleﬃciencyofthenetwork9.10becausethenextlayerhasroughlyktimesfewerinputstoprocess.Whenthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchaswhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication)thisreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyandreducedmemoryrequirementsforstoringtheparameters.Formanytasks,poolingisessentialforhandlinginputsofvaryingsize. Forexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcationlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofanoﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthesamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,theﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummarystatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.Sometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould342'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 357}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\n0.11.0.21.1.1.0.10.2............\\n0.30.11.1.0.31.0.21.............DETECTOR STAGEPOOLING STAGE\\nPOOLING STAGE\\nDETECTOR STAGEFigure9.8:Maxpoolingintroducesinvariance.(Top)Aviewofthemiddleoftheoutputofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetoprowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregionsandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after(Bottom)theinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhaschanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpoolingunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\\n343'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 358}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSLarge responsein pooling unitLarge responsein pooling unitLargeresponsein detectorunit 1Largeresponsein detectorunit 3\\nFigure9.9:Exampleoflearnedinvariances:Apoolingunitthatpoolsovermultiplefeaturesthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsoftheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearntobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.Eachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsintheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetectorunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichpoolingunitwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resultingintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughlythesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(,Goodfellowetal.2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturallyinvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningothertransformations.\\n0.11.0.21.0.20.10.10.00.1Figure9.10:Poolingwithdownsampling.Hereweusemax-poolingwithapoolwidthofthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactoroftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Notethattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonotwanttoignoresomeofthedetectorunits.344'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 359}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSuseinvarioussituations(,).ItisalsopossibletodynamicallyBoureauetal.2010poolfeaturestogether,forexample,byrunningaclusteringalgorithmonthelocationsofinterestingfeatures(,).ThisapproachyieldsaBoureauetal.2011diﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearnasinglepoolingstructurethatisthenappliedtoallimages(,).Jiaetal.2012Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatusetop-downinformation,suchasBoltzmannmachinesandautoencoders.TheseissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinPartIII.PoolinginconvolutionalBoltzmannmachinesispresentedinSec..The20.6inverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswillbecoveredinSec..20.10.6SomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcationusingconvolutionandpoolingareshowninFig..9.119.4ConvolutionandPooling as anInﬁnitely StrongPriorRecalltheconceptofapriorprobabilitydistributionfromSec..Thisisa5.2probabilitydistributionovertheparametersofamodelthatencodesourbeliefsaboutwhatmodelsarereasonable,beforewehaveseenanydata.Priorscanbeconsideredweakorstrongdependingonhowconcentratedtheprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhighentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallowsthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylowentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysamoreactiveroleindeterminingwheretheparametersendup.Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsaysthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuchsupportthedatagivestothosevalues.Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,butwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongpriorsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofitsneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,exceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathiddenunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitelystrongpriorprobabilitydistributionovertheparametersofalayer.Thispriorsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis345'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 360}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nInput image: 256x256x3Output of convolution+ ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution+ReLU: 64x64x64Output of pooling with stride 4: 16x16x64Output of reshape to vector:16,384 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities\\nInput image: 256x256x3Output of convolution+ ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution+ReLU: 64x64x64Output of pooling to 3x3 grid: 3x3x64Output of reshape to vector:576 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities\\nInput image: 256x256x3Output of convolution+ ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution+ReLU: 64x64x64Output of convolution:16x16x1,000Output of average pooling: 1x1x1,000Output of softmax: 1,000 class probabilities\\nOutput of pooling with stride 4: 16x16x64\\nFigure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.Thespeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyaredesignedtobeveryshallowinordertoﬁtontothepage. Realconvolutionalnetworksalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresusedhereforsimplicity.(Left)Aconvolutionalnetworkthatprocessesaﬁxedimagesize.Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorfortheconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therestofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinChapter.6(Center)Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintainsafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpoolsbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothefullyconnectedportionofthenetwork.Aconvolutionalnetworkthatdoesnot(Right)haveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsonefeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassistooccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovidestheargumenttothesoftmaxclassiﬁeratthetop.346'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 361}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongpriorthateachunitshouldbeinvarianttosmalltranslations.Ofcourse,implementingaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorwouldbeextremelycomputationallywasteful.Butthinkingofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcangiveussomeinsightsintohowconvolutionalnetswork.Onekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting.Likeanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmadebythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatialinformation,thenusingpoolingonallfeaturescanincreasethetrainingerror.Someconvolutionalnetworkarchitectures(,)aredesignedtoSzegedyetal.2014ausepoolingonsomechannelsbutnotonotherchannels,inordertogetbothhighlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslationinvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfromverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybeinappropriate.Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-tionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearningperformance.Modelsthatdonotuseconvolutionwouldbeabletolearnevenifwepermutedallofthepixelsintheimage. Formanyimagedatasets,thereareseparatebenchmarksformodelsthatarepermutationinvariantandmustdiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledgeofspatialrelationshipshard-codedintothembytheirdesigner.9.5VariantsoftheBasicConvolutionFunctionWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydonotreferexactlytothestandarddiscreteconvolutionoperationasitisusuallyunderstoodinthemathematicalliterature.Thefunctionsusedinpracticediﬀerslightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeusefulpropertiesofthefunctionsusedinneuralnetworks.First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusuallyactuallymeananoperationthatconsistsofmanyapplicationsofconvolutioninparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekindoffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofournetworktoextractmanykindsoffeatures,atmanylocations.Additionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa347'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 362}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSgridofvector-valuedobservations. Forexample,acolorimagehasared,greenandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinputtothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutputofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,weusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,withoneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinatesofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sotheywillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesinthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.Becauseconvolutionalnetworksusuallyusemulti-channelconvolution,thelinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenifkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeifeachoperationhasthesamenumberofoutputchannelsasinputchannels.Assumewehavea4-DkerneltensorKwithelementKi,j,k,lgivingtheconnectionstrengthbetweenaunitinchannelioftheoutputandaunitinchanneljoftheinput,withanoﬀsetofkrowsandlcolumnsbetweentheoutputunitandtheinputunit.AssumeourinputconsistsofobserveddataVwithelementVi,j,kgivingthevalueoftheinputunitwithinchanneliatrowjandcolumnk.AssumeouroutputconsistsofZwiththesameformatasV.IfZisproducedbyconvolvingKacrosswithoutﬂipping,thenVKZi,j,k=\\ue058l,m,nVl,jm,kn+−1+−1Ki,l,m,n(9.7)wherethesummationoverl,mandnisoverallvaluesforwhichthetensorindexingoperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexintoarraysusingafortheﬁrstentry.Thisnecessitatesthe1−1intheaboveformula.ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0theaboveexpressionevensimpler.Wemaywanttoskipoversomepositionsofthekernelinordertoreducethecomputationalcost(attheexpenseofnotextractingourfeaturesasﬁnely).Wecanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.Ifwewanttosampleonlyeveryspixelsineachdirectionintheoutput,thenwecandeﬁneadownsampledconvolutionfunctionsuchthatcZi,j,k= ()cKV,,si,j,k=\\ue058l,m,n\\ue002Vl,jsm,ksn(−×1)+(−×1)+Ki,l,m,n\\ue003.(9.8)Werefertosastheofthisdownsampledconvolution.Itisalsopossiblestridetodeﬁneaseparatestrideforeachdirectionofmotion.SeeFig.foran9.12illustration.348'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 363}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s1s1s2s2x4x4x5x5s3s3\\nx1x1x2x2x3x3z2z2z1z1z3z3x4x4z4z4x5x5z5z5s1s1s2s2s3s3Stridedconvolution\\nDownsampling\\nConvolutionFigure9.12:Convolutionwithastride. Inthisexample,weuseastrideoftwo.(Top)Convolutionwithastridelengthoftwoimplementedinasingleoperation.(Bottom)Convolutionwithastridegreaterthanonepixelismathematicallyequivalenttoconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproachinvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvaluesthatarethendiscarded.\\n349'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 364}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSOneessentialfeatureofanyconvolutionalnetworkimplementationistheabilitytoimplicitlyzero-padtheinputVinordertomakeitwider.Withoutthisfeature,thewidthoftherepresentationshrinksbyonepixellessthanthekernelwidthateachlayer.Zeropaddingtheinputallowsustocontrolthekernelwidthandthesizeoftheoutputindependently.Withoutzeropadding,weareforcedtochoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmallkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.SeeFig.foranexample.9.13Threespecialcasesofthezero-paddingsettingareworthmentioning.Oneistheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolutionkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirelywithintheimage. InMATLABterminology,thisiscalledconvolution.Invalidthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsintheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,thesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidthmandthekernelhaswidthk,theoutputwillbeofwidthmk−+1. Therateofthisshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageisgreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincludedinthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwilleventuallydropto1×1,atwhichpointadditionallayerscannotmeaningfullybeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingiswhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequaltothesizeoftheinput.MATLABcallsthisconvolution.Inthiscase,thenetworksamecancontainasmanyconvolutionallayersastheavailablehardwarecansupport,sincetheoperationofconvolutiondoesnotmodifythearchitecturalpossibilitiesavailabletothenextlayer. However,theinputpixelsneartheborderinﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmaketheborderpixelssomewhatunderrepresentedinthemodel.Thismotivatestheotherextremecase,whichMATLABreferstoasfullconvolution,inwhichenoughzeroesareaddedforeverypixeltobevisitedktimesineachdirection,resultinginanoutputimageofwidth.Inthiscase,theoutputpixelsnearthebordermk+−1areafunctionoffewerpixelsthantheoutputpixelsnearthecenter.Thiscanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsintheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(intermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”convolution.Insomecases,wedonotactuallywanttouseconvolution,butratherlocallyconnectedlayers(,,).Inthiscase,theadjacencymatrixintheLeCun19861989graphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed350'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 365}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\n.........\\n..................\\nFigure9.13:Theeﬀectofzeropaddingonnetworksize:Consideraconvolutionalnetworkwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,soonlytheconvolutionoperationitselfshrinksthenetworksize.(Top)Inthisconvolutionalnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationtoshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonlyabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,soarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcanbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsomeshrinkingisinevitableinthiskindofarchitecture.Byaddingﬁveimplicitzeroes(Bottom)toeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsustomakeanarbitrarilydeepconvolutionalnetwork.351'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 366}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSbya6-DtensorW. TheindicesintoWarerespectively:i,theoutputchannel,j,theoutputrow,k,theoutputcolumn,l,theinputchannel,m,therowoﬀsetwithintheinput,andn,thecolumnoﬀsetwithintheinput.ThelinearpartofalocallyconnectedlayeristhengivenbyZi,j,k=\\ue058l,m,n[Vl,jm,kn+−1+−1wi,j,k,l,m,n].(9.9)Thisissometimesalsocalledunsharedconvolution,becauseitisasimilaroperationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparametersacrosslocations.Fig.compareslocalconnections,convolution,andfullconnections.9.14Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbeafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesamefeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimageisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfoftheimage.Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayersinwhichtheconnectivityisfurtherrestricted,forexampletoconstrainthateachoutputchannelibeafunctionofonlyasubsetoftheinputchannelsl.Acommonwaytodothisistomaketheﬁrstmoutputchannelsconnecttoonlytheﬁrstninputchannels,thesecondmoutputchannelsconnecttoonlythesecondninputchannels,andsoon. SeeFig.foranexample. Modelinginteractions9.15betweenfewchannelsallowsthenetworktohavefewerparametersinordertoreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreducestheamountofcomputationneededtoperformforwardandback-propagation.Itaccomplishesthesegoalswithoutreducingthenumberofhiddenunits.Tiledconvolution(,;,)oﬀersacompromiseGregorandLeCun2010aLeetal.2010betweenaconvolutionallayerandalocallyconnectedlayer.Ratherthanlearningaseparatesetofweightsatspatiallocation,welearnasetofkernelsthateverywerotate throughaswemove throughspace.Thismeansthatimmediatelyneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,butthememoryrequirementsforstoringtheparameterswillincreaseonlybyafactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutputfeaturemap.SeeFig.foracomparisonoflocallyconnectedlayers,tiledconvolution,9.16andstandardconvolution.Todeﬁnetiledconvolutionalgebraically,letkbea6-Dtensor,wheretwoofthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthanhavingaseparateindexforeachlocationintheoutputmap,outputlocationscyclethroughasetoftdiﬀerentchoicesofkernelstackineachdirection.Iftisequalto352'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 367}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\nx1x1x2x2s1s1s3s3x5x5s5s5x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\na        ba        ba        ba        ba        a        bc      de     fg      h i  \\nx4x4x3x3s4s4s2s2Figure9.14:Comparisonoflocalconnections,convolution,andfullconnections.(Top)Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwithauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.(Center)Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactlythesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunitsinteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayerhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedlyacrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.(Bottom)Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateachedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthisdiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnectedlayer.353'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 368}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nInput TensorOutput Tensor\\nSpatial coordinatesChannel coordinates\\nFigure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedtoonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonlythesecondtwoinputchannels.354'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 369}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5\\na        ba        ba        ba        ba       a        bc      de     fg      h i  \\nx1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5a        bc        da        bc        da        \\nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandardconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesamesizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.Thediﬀerencesbetweenthemethodsliesinhowtheyshareparameters.(Top)Alocallyconnectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweightbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof(Center)tdiﬀerentkernels.Hereweillustratethecaseoft= 2. Oneofthesekernelshasedgeslabeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.”Eachtimewemoveonepixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.Thismeansthat,likethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerentparameters.Unlikethelocallyconnectedlayer,afterwehavegonethroughalltavailablekernels,wecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultipleoftsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled(Bottom)convolutionwitht= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicatedinthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere.355'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 370}, page_content='CHAPTER9.CONVOLUTIONALNETWORKStheoutputwidth,thisisthesameasalocallyconnectedlayer.Zi,j,k=\\ue058l,m,nVl,jm,kn+−1+−1Ki,l,m,n,jt,kt%+1%+1,(9.10)whereis themodulooperation, with%t%t=0(, t+1)%t=1, etc.Itisstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeachdimension.Bothlocallyconnectedlayersandtiledconvolutionallayershaveaninterestinginteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenbydiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsofthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothelearnedtransformation(seeFig.).Convolutionallayersarehard-codedtobe9.9invariantspeciﬁcallytotranslation.Otheroperationsbesidesconvolutionareusuallynecessarytoimplementaconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethegradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.Insomesimplecases, thisoperationcanbeperformedusingtheconvolutionoperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,donothavethisproperty.Recallthatconvolutionisalinearoperationandcanthusbedescribedasamatrixmultiplication(ifweﬁrstreshapetheinputtensorintoaﬂatvector).Thematrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseandeachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisviewhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutionalnetwork.Multiplicationbythetransposeofthematrixdeﬁnedbyconvolutionisonesuchoperation.Thisistheoperationneededtoback-propagateerrorderivativesthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworksthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwewishtoreconstructthevisibleunitsfromthehiddenunits(,).Simardetal.1992ReconstructingthevisibleunitsisanoperationcommonlyusedinthemodelsdescribedinPartofthisbook,suchasautoencoders,RBMs,andsparsecoding.IIITransposeconvolutionisnecessarytoconstructconvolutionalversionsofthosemodels.Likethekernelgradientoperation,thisinputgradientoperationcanbeimplementedusingaconvolutioninsomecases,butinthegeneralcaserequiresathirdoperationtobeimplemented.Caremustbetakentocoordinatethistransposeoperationwiththeforwardpropagation.Thesizeoftheoutputthatthetransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof356'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 371}, page_content='CHAPTER9.CONVOLUTIONALNETWORKStheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’soutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcanresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitlytoldwhatthesizeoftheoriginalinputwas.Thesethreeoperations—convolution,backpropfromoutputtoweights,andbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradientsneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrainconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeofconvolution. See()forafullderivationoftheequationsintheGoodfellow2010fullygeneralmulti-dimensional,multi-examplecase.Togiveasenseofhowtheseequationswork,wepresentthetwodimensional,singleexampleversionhere.SupposewewanttotrainaconvolutionalnetworkthatincorporatesstridedconvolutionofkernelstackKappliedtomulti-channelimageVwithstridesasdeﬁnedbyc(KV,,s) asinEq..Supposewewanttominimizesomelossfunction9.8J(VK,).Duringforwardpropagation,wewillneedtousecitselftooutputZ,whichisthenpropagatedthroughtherestofthenetworkandusedtocomputethecostfunctionJ.Duringback-propagation,wewillreceiveatensorGsuchthatGi,j,k=∂∂Zi,j,kJ,.(VK)Totrainthenetwork,weneedtocomputethederivativeswithrespecttotheweightsinthekernel.Todoso,wecanuseafunctiong,,s(GV)i,j,k,l=∂∂Ki,j,k,lJ,(VK) =\\ue058m,nGi,m,nVj,msk,nsl(−×1)+(−×1)+.(9.11)Ifthislayerisnotthebottomlayerofthenetwork,wewillneedtocomputethegradientwithrespecttoVinordertoback-propagatetheerrorfartherdown.Todoso,wecanuseafunctionh,,s(KG)i,j,k=∂∂Vi,j,kJ,(VK)(9.12)=\\ue058l,ms.t.(1)+=l−×smj\\ue058n,ps.t.(1)+=n−×spk\\ue058qKq,i,m,pGq,l,n.(9.13)Autoencodernetworks,describedinChapter,arefeedforwardnetworks14trainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,thatcopiesitsinputxtoanapproximatereconstructionrusingthefunctionW\\ue03eWx.Itiscommonformoregeneralautoencodersto usemultiplicationbythetransposeoftheweightmatrixjustasPCAdoes. Tomakesuchmodels357'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 372}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSconvolutional,wecanusethefunctionhtoperformthetransposeoftheconvolutionoperation.SupposewehavehiddenunitsHinthesameformatasZandwedeﬁneareconstructionRKH= (h,,s.)(9.14)Inordertotraintheautoencoder,wewillreceivethegradientwithrespecttoRasatensorE.Totrainthedecoder,weneedtoobtainthegradientwithrespecttoK.Thisisgivenbyg(HE,,s).Totraintheencoder,weneedtoobtainthegradientwithrespecttoH.Thisisgivenbyc(KE,,s).Itisalsopossibletodiﬀerentiatethroughgusingcandh,buttheseoperationsarenotneededfortheback-propagationalgorithmonanystandardnetworkarchitectures.Generally,wedonotuseonlyalinearoperationinordertotransformfromtheinputstotheoutputsinaconvolutionallayer. Wegenerallyalsoaddsomebiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestionofhowtoshareparametersamongthebiases. Forlocallyconnectedlayersitisnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturaltosharethebiaseswiththesametilingpatternasthekernels.Forconvolutionallayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacrossalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxedsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.Separatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,butalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerentlocations.Forexample,whenusingimplicitzeropadding,detectorunitsattheedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.9.6StructuredOutputsConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structuredobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorarealvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbyastandardconvolutionallayer.Forexample,themodelmightemitatensorS,whereSi,j,kistheprobabilitythatpixel(j,k)oftheinputtothenetworkbelongstoclassi.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasksthatfollowtheoutlinesofindividualobjects.Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthantheinputplane,asshowninFig..Inthekindsofarchitecturestypicallyusedfor9.13classiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatialdimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In358'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 373}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSˆY(1)ˆY(1)ˆY(2)ˆY(2)ˆY(3)ˆY(3)H(1)H(1)H(2)H(2)H(3)H(3)X XUUUVVVWW\\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.Theinputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,andXchannels(red,green,blue).ThegoalistooutputatensoroflabelsˆY,withaprobabilitydistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,imagecolumns,andthediﬀerentclasses.RatherthanoutputtingˆYinasingleshot,therecurrentnetworkiterativelyreﬁnesitsestimateˆYbyusingapreviousestimateofˆYasinputforcreatinganewestimate. Thesameparametersareusedforeachupdatedestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.ThetensorofconvolutionkernelsUisusedoneachsteptocomputethehiddenrepresentationgiventheinputimage.ThekerneltensorVisusedtoproduceanestimateofthelabelsgiventhehiddenvalues.Onallbuttheﬁrststep,thekernelsWareconvolvedoverˆYtoprovideinputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Becausethesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,asdescribedinChapter.10ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpoolingaltogether(,).Anotherstrategyistosimplyemitalower-resolutionJainetal.2007gridoflabels(,,).Finally,inprinciple,onecouldPinheiroandCollobert20142015useapoolingoperatorwithunitstride.Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguessoftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetweenneighboringpixels. Repeatingthisreﬁnementstepseveraltimescorrespondstousingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersofthedeepnet(,).ThismakesthesequenceofcomputationsJainetal.2007performedbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticularkindofrecurrentnetwork(,,).Fig.PinheiroandCollobert201420159.17showsthearchitectureofsucharecurrentconvolutionalnetwork.Onceapredictionforeachpixelismade,variousmethodscanbeusedtofurtherprocessthesepredictionsinordertoobtainasegmentationoftheimageintoregions(,;Briggmanetal.2009Turaga2010Farabet2013etal.,;etal.,).359'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 374}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobeassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilisticrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetworkcanbetrainedtomaximizeanapproximationofthegraphicalmodeltrainingobjective(,;,).Ningetal.2005Thompsonetal.20149.7DataTypesThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,eachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspaceortime.SeeTableforexamplesofdatatypeswithdiﬀerentdimensionalities9.1andnumberofchannels.Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal.().2010Sofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtestdatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworksisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsofinputsimplycannotberepresentedbytraditional,matrixmultiplication-basedneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworksevenwhencomputationalcostandoverﬁttingarenotsigniﬁcantissues.Forexample,consideracollectionofimages,whereeachimagehasadiﬀerentwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixofﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyappliedadiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputoftheconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrixmultiplication;thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblockcirculantmatrixforeachsizeofinput. Sometimestheoutputofthenetworkisallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassignaclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkisnecessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,forexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscasewemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhosepoolingregionsscaleinsizeproportionaltothesizeoftheinput,inordertomaintainaﬁxednumberofpooledoutputs.SomeexamplesofthiskindofstrategyareshowninFig..9.11Notethattheuseofconvolutionforprocessingvariablesizedinputsonlymakessenseforinputsthathavevariablesizebecausetheycontainvaryingamounts360'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 375}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSSinglechannelMulti-channel1-DAudio waveform:The axis weconvolveovercorrespondstotime.Wediscretize timeandmeasuretheamplitudeofthewaveformoncepertimestep.Skeletonanimationdata:Anima-tionsof3-Dcomputer-renderedcharactersaregeneratedbyalter-ingtheposeofa“skeleton”overtime.Ateachpointintime,theposeofthecharacterisdescribedbyaspeciﬁcationoftheanglesofeachofthejointsinthecharac-ter’sskeleton.Eachchannelinthedatawefeedtotheconvolu-tionalmodelrepresentstheangleaboutoneaxisofonejoint.2-DAudiodatathathasbeenprepro-cessedwithaFouriertransform:Wecantransformtheaudiowave-formintoa2Dtensorwithdif-ferentrowscorrespondingtodif-ferentfrequencies anddiﬀerentcolumnscorrespondingtodiﬀer-entpointsintime.Usingconvolu-tioninthetimemakesthemodelequivarianttoshiftsintime.Us-ingconvolutionacrossthefre-quencyaxismakesthemodelequivarianttofrequency,sothatthesamemelodyplayedinadif-ferentoctaveproducesthesamerepresentationbutatadiﬀerentheightinthenetwork’soutput.Colorimagedata:Onechannelcontainstheredpixels,onethegreen pixels, and one thebluepixels.Theconvolutionkernelmovesoverboththehorizontalandverticalaxes oftheimage,conferringtranslationequivari-anceinbothdirections.\\n3-DVolumetricdata:Acommonsourceofthiskindofdataismed-icalimagingtechnology,suchasCTscans.Colorvideodata:Oneaxiscorre-spondstotime,onetotheheightofthevideoframe,andonetothewidthofthevideoframe.Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutionalnetworks.361'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 376}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSofobservationofthesamekindofthing—diﬀerentlengthsofrecordingsovertime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmakesenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerentkindsofobservations.Forexample,ifweareprocessingcollegeapplications,andourfeaturesconsistofbothgradesandstandardizedtestscores,butnoteveryapplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethesameweightsoverboththefeaturescorrespondingtothegradesandthefeaturescorrespondingtothetestscores.9.8EﬃcientConvolutionAlgorithmsModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmorethanonemillionunits.Powerfulimplementationsexploitingparallelcomputationresources,asdiscussedinSec.,areessential.However,inmanycasesitisalso12.1possibletospeedupconvolutionbyselectinganappropriateconvolutionalgorithm.ConvolutionisequivalenttoconvertingboththeinputandthekerneltothefrequencydomainusingaFouriertransform,performingpoint-wisemultiplicationofthetwosignals, andconvertingbacktothetimedomainusinganinverseFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaiveimplementationofdiscreteconvolution.Whenad-dimensionalkernelcan beexpressedas theouterproductofdvectors,onevectorperdimension,thekerneliscalledseparable.Whenthekernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocomposedone-dimensionalconvolutionswitheachofthesevectors.Thecomposedapproachissigniﬁcantlyfasterthanperformingoned-dimensionalconvolutionwiththeirouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.Ifthekerneliswelementswideineachdimension,thennaivemultidimensionalconvolutionrequiresO(wd)runtimeandparameterstoragespace,whileseparableconvolutionrequiresO(wd×)runtimeandparameterstoragespace.Ofcourse,noteveryconvolutioncanberepresentedinthisway.Devisingfasterwaysofperformingconvolutionorapproximateconvolutionwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-niquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecauseinthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentofanetworkthantoitstraining.362'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 377}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS9.9RandomorUnsupervisedFeaturesTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthefeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumberoffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersofpooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradientsteprequiresacompleterunofforwardpropagationandbackwardpropagationthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetworktrainingistousefeaturesthatarenottrainedinasupervisedfashion.Therearethreebasicstrategiesforobtainingconvolutionkernelswithoutsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristodesignthembyhand,forexamplebysettingeachkerneltodetectedgesatacertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervisedcriterion.Forexample,()applyCoatesetal.2011k-meansclusteringtosmallimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel. PartIIIdescribesmanymoreunsupervisedlearningapproaches.Learningthefeatureswithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromtheclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfortheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthelastlayer.Learningthelastlayeristhentypicallyaconvexoptimizationproblem,assumingthelastlayerissomethinglikelogisticregressionoranSVM.Randomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrettetal.etal.etal.etal.,;2009Saxe,;2011Pinto,;,).2011CoxandPinto2011Saxe()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally2011becomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureofaconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutionalnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthesearchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.Anintermediateapproachistolearnthefeatures,butusingmethodsthatdonotrequirefullforwardandback-propagationateverygradientstep.Aswithmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayerinisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthesecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed8howtoperformsupervisedgreedylayer-wisepretraining,andPartextendsthisIIItogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.Thecanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelistheconvolutionaldeepbeliefnetwork(,).ConvolutionalnetworksoﬀerLeeetal.2009363'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 378}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossiblewithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayeratatime,wecantrainamodelofasmallpatch,as()dowithCoatesetal.2011k-means.Wecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernelsofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearningtotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetrainingprocess.Usingthisapproach,wecantrainverylargemodelsandincurahighcomputationalcostonlyatinferencetime(,;,Ranzatoetal.2007bJarrettetal.2009Kavukcuoglu2010Coates2013;etal.,;etal.,).Thisapproachwaspopularfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputationalpowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedinapurelysupervisedfashion,usingfullforwardandback-propagationthroughtheentirenetworkoneachtrainingiteration.Aswithotherapproachestounsupervisedpretraining,itremainsdiﬃculttoteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervisedpretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmaysimplyallowustotrainmuchlargerarchitecturesduetothereducedcomputationalcostofthelearningrule.9.10TheNeuroscientiﬁcBasisforConvolutionalNet-worksConvolutionalnetworks areperhaps thegreatest successstory ofbiologicallyinspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguidedbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworksweredrawnfromneuroscience.Thehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperimentslongbeforetherelevantcomputationalmodelsweredeveloped.NeurophysiologistsDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemanyofthemostbasicfactsabouthowthemammalianvisionsystemworks(HubelandWiesel195919621968,,,).TheiraccomplishmentswereeventuallyrecognizedwithaNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporarydeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsincats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojectedinpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywasthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁcpatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatalltootherpatterns.364'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 379}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatarebeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecanfocusonasimpliﬁed,cartoonviewofbrainfunction.Inthissimpliﬁedview,wefocusonapartofthebraincalled,alsoknownasV1theprimaryvisualcortex.V1istheﬁrstareaofthebrainthatbeginstoperformsigniﬁcantlyadvancedprocessingofvisualinput.Inthiscartoonview,imagesareformedbylightarrivingintheeyeandstimulatingtheretina,thelight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperformsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitisrepresented.Theimagethenpassesthroughtheopticnerveandabrainregioncalledthelateralgeniculatenucleus.Themainrole,asfarasweareconcernedhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfromtheeyetoV1,whichislocatedatthebackofthehead.AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructuremirroring thestructure ofthe image inthe retina.Forexample,lightarrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfofV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeaturesdeﬁnedintermsoftwodimensionalmaps.2.V1containsmanysimplecells.Asimplecell’sactivitycantosomeextentbecharacterizedbyalinearfunctionoftheimageinasmall,spatiallylocalizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkaredesignedtoemulatethesepropertiesofsimplecells.3.V1alsocontainsmanycomplexcells. Thesecellsrespondtofeaturesthataresimilartothosedetectedbysimplecells,butcomplexcellsareinvarianttosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunitsofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechangesinlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategiesinconvolutionalnetworks,suchasmaxoutunits(,).Goodfellowetal.2013aThoughweknowthemostaboutV1,itisgenerallybelievedthatthesamebasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewofthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedlyappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomicallayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconceptandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen365'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 380}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSnicknamed“grandmothercells”—theideaisthatapersoncouldhaveaneuronthatactivateswhenseeinganimageoftheirgrandmother,regardlessofwhethersheappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upofherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orinshadow,etc.Thesegrandmothercellshavebeenshowntoactuallyexistinthehumanbrain,inaregioncalledthemedialtemporallobe(,).ResearchersQuirogaetal.2005testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.Theyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividualneuronthatisactivatedbytheconceptofHalleBerry.ThisneuronﬁreswhenapersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontainingthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;otherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodernconvolutionalnetworks,whichwouldnotautomaticallygeneralizetoidentifyingapersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutionalnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporalcortex(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughtheLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst100msofglimpsinganobject. Ifapersonisallowedtocontinuelookingattheobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrainusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.However,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthatresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobeverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictITﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobjectrecognitiontasks(,).DiCarlo2013Thatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworksandthemammalianvisionsystem.Someofthesediﬀerencesarewellknowntocomputationalneuroscientists,butoutsidethescopeofthisbook.Someofthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthemammalianvisionsystemworksremainunanswered.Asabrieflist:•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthefovea.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldatarmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitchestogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactuallyreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes366'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 381}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSseveraleyemovementscalledsaccadestoglimpsethemostvisuallysalientortask-relevantpartsofascene.Incorporatingsimilarattentionmechanismsintodeeplearningmodelsisanactiveresearchdirection.Inthecontextofdeeplearning,attentionmechanismshavebeenmostsuccessfulfornaturallanguageprocessing,asdescribedinSec..Severalvisualmodels12.4.5.1withfoveationmechanismshavebeendevelopedbutsofarhavenotbecomethedominantapproach(LarochelleandHinton2010Denil2012,;etal.,).•Thehumanvisualsystemisintegratedwithmanyothersenses,suchashearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworkssofararepurelyvisual.•Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itisabletounderstandentirescenesincludingmanyobjectsandrelationshipsbetweenobjects,andprocessesrich3-Dgeometricinformationneededforourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeenappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigherlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbuthasnotyetbeenshowntooﬀeracompellingimprovement.•WhilefeedforwardITﬁringratescapturemuchofthesameinformationasconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediatecomputationsare.Thebrainprobablyusesverydiﬀerentactivationandpoolingfunctions. Anindividualneuron’sactivationprobablyisnotwell-characterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involvesmultiplequadraticﬁltersforeachneuron(,).IndeedourRustetal.2005cartoonpictureof“simplecells” and“complexcells” mightcreateanon-existentdistinction;simplecellsandcomplexcellsmightbothbethesamekindofcellbutwiththeir“parameters”enablingacontinuumofbehaviorsrangingfromwhatwecall“simple”towhatwecall“complex.”Itis alsoworthmentioningthat neurosciencehas toldus relativelylittleabouthowtotrainconvolutionalnetworks.Modelstructureswithparametersharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodelsofvision(,),butthesemodelsdidnotusethemodernMarrandPoggio1976back-propagationalgorithmandgradientdescent.Forexample,theNeocognitron(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsofthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclusteringalgorithm.367'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 382}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSLangandHinton1988()introducedtheuseofback-propagationtotraintime-delayneuralnetworks(TDNNs).Tousecontemporaryterminology,TDNNsareone-dimensionalconvolutionalnetworksappliedtotimeseries.Back-propagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobservationandisconsideredbysometobebiologicallyimplausible.Followingthesuccessofback-propagation-basedtrainingofTDNNs,(,)developedtheLeCunetal.1989modernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-Dconvolutionappliedtoimages.Sofarwehavedescribedhowsimplecellsareroughlylinearandselectiveforcertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosometransformationsofthesesimplecellfeatures,andstacksoflayersthatalternatebetweenselectivityandinvariancecanyieldgrandmothercellsforveryspeciﬁcphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.Inadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionofindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheirresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecanjustdisplayanimageoftheconvolutionkerneltoseewhatthecorrespondingchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,wedonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeintheneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’sretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecanthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximationoftheneuron’sweights.Thisapproachisknownasreversecorrelation(RingachandShapley2004,).ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribedbyGaborfunctions.TheGaborfunctiondescribestheweightata2-Dpointintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,I(x,y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetoflocations,deﬁnedbyasetofxcoordinatesXandasetofycoordinates,Y,andapplyingweightsthatarealsoafunctionofthelocation,w(x,y).Fromthispointofview,theresponseofasimplecelltoanimageisgivenbysI() =\\ue058x∈X\\ue058y∈Ywx,yIx,y.()()(9.15)Speciﬁcally,takestheformofaGaborfunction:wx,y()wx,yα,β(;x,βy,f,φ,x0,y0,τα) = exp\\ue000−βxx\\ue0302−βyy\\ue0302\\ue001cos(fx\\ue030+)φ,(9.16)wherex\\ue030= (xx−0)cos()+(τyy−0)sin()τ(9.17)368'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 383}, page_content='CHAPTER9.CONVOLUTIONALNETWORKSandy\\ue030= (−xx−0)sin()+(τyy−0)cos()τ.(9.18)Here,α,βx,βy,f,φ,x0,y0,andτareparametersthatcontrolthepropertiesoftheGaborfunction. Fig.showssomeexamplesofGaborfunctionswith9.18diﬀerentsettingsoftheseparameters.Theparametersx0,y0,andτdeﬁneacoordinatesystem. Wetranslateandrotatexandytoformx\\ue030andy\\ue030.Speciﬁcally,thesimplecellwillrespondtoimagefeaturescenteredatthepoint(x0,y0),anditwillrespondtochangesinbrightnessaswemovealongalinerotatedradiansfromthehorizontal.τViewedasafunctionofx\\ue030andy\\ue030,thefunctionwthenrespondstochangesinbrightnessaswemovealongthex\\ue030axis. Ithastwoimportantfactors: oneisaGaussianfunctionandtheotherisacosinefunction.TheGaussianfactorαexp\\ue000−βxx\\ue0302−βyy\\ue0302\\ue001canbeseenasagatingtermthatensuresthesimplecellwillonlyrespondtovaluesnearwherex\\ue030andy\\ue030arebothzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactorαadjuststhetotalmagnitudeofthesimplecell’sresponse,whileβxandβycontrolhowquicklyitsreceptiveﬁeldfallsoﬀ.Thecosinefactorcos(fx\\ue030+φ) controlshowthesimplecellrespondstochangingbrightnessalongthex\\ue030axis.Theparameterfcontrolsthefrequencyofthecosineandcontrolsitsphaseoﬀset.φAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellrespondstoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁclocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimagehasthesamephaseastheweights.Thisoccurswhentheimageisbrightwheretheweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremostinhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—whentheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsarenegative.ThecartoonviewofacomplexcellisthatitcomputestheL2normofthe2-Dvectorcontainingtwosimplecells’responses:c(I)=\\ue070s0()I2+s1()I2. Animportantspecialcaseoccurswhens1hasallofthesameparametersass0exceptforφ,andφissetsuchthats1isonequartercycleoutofphasewiths0.Inthiscase,s0ands1formaquadraturepair.AcomplexcelldeﬁnedinthiswayrespondswhentheGaussianreweightedimageI(x,y)exp(−βxx\\ue0302−βyy\\ue0302) containsahighamplitudesinusoidalwavewithfrequencyfindirectionτnear(x0,y0),regardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellisinvarianttosmalltranslationsoftheimageindirectionτ,ortonegatingthe369'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 384}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicateslargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgraycorrespondstozeroweight.(Left)Gaborfunctionswithdiﬀerentvaluesoftheparametersthatcontrolthecoordinatesystem:x0,y0,andτ. EachGaborfunctioninthisgridisassignedavalueofx0andy0proportionaltoitspositioninitsgrid,andτischosensothateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.Fortheothertwoplots,x0,y0,andτareﬁxedtozero.Gaborfunctionswith(Center)diﬀerentGaussianscaleparametersβxandβy.Gaborfunctionsarearrangedinincreasingwidth(decreasingβx)aswemovelefttorightthroughthegrid,andincreasingheight(decreasingβy)aswemovetoptobottom.Fortheothertwoplots,theβvaluesareﬁxedto1.5×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters(Right)fandφ.Aswemovetoptobottom,fincreases,andaswemovelefttoright,φincreases.Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth.φf×image(replacingblackwithwhiteandviceversa).SomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachinelearningcomefromvisuallycomparingthefeatureslearnedbymachinelearningmodelswiththoseemployedbyV1.()showedthatOlshausenandField1996asimpleunsupervisedlearningalgorithm, sparsecoding, learnsfeatureswithreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthatanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswithGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeeplearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Fig.shows9.19someexamples.Becausesomanydiﬀerentlearningalgorithmslearnedgedetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcancertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetectorwhenappliedtonotnaturalimages).Thesefeaturesareanimportantpartofthestatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerentapproachestostatisticalmodeling.SeeHyvärinen2009etal.()forareviewoftheﬁeldofnaturalimagestatistics.370'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 385}, page_content='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁccolorsofedgeswhenappliedtonaturalimages.ThesefeaturedetectorsarereminiscentoftheGaborfunctionsknowntobepresentinprimaryvisualcortex.(Left)Weightslearnedbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmallimagepatches.Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised(Right)convolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.9.11ConvolutionalNetworksandtheHistoryofDeepLearningConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeeplearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtainedbystudyingthebraintomachinelearningapplications.Theywerealsosomeoftheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswereconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneuralnetworkstosolveimportantcommercialapplicationsandremainattheforefrontofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,theneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkforreadingchecks(,).Bytheendofthe1990s,thissystemdeployedLeCunetal.1998bbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCRandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedbyMicrosoft(,).SeeChapterformoredetailsonsuchSimardetal.200312applicationsandmoremodernapplicationsofconvolutionalnetworks.SeeLeCunetal.()foramorein-depthhistoryofconvolutionalnetworksupto2010.2010Convolutionalnetworkswerealsousedtowinmanycontests.ThecurrentintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks2012371'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 386}, page_content='CHAPTER9.CONVOLUTIONALNETWORKShadbeenusedtowinothermachinelearningandcomputervisioncontestswithlessimpactforyearsearlier.Convolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwithback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceededwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmaysimplybethatconvolutionalnetworksweremorecomputationallyeﬃcientthanfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththemandtunetheirimplementationandhyperparameters.Largernetworksalsoseemtobeeasiertotrain.Withmodernhardware,largefullyconnectednetworksappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwereavailableandactivationfunctionsthatwerepopularduringthetimeswhenfullyconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimarybarrierstothesuccessofneuralnetworkswerepsychological(practitionersdidnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouseneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworksperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestofdeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.Convolutionalnetworksprovideawaytospecializeneuralnetworkstoworkwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelstoverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,imagetopology.Toprocessone-dimensional,sequentialdata,weturnnexttoanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneuralnetworks.\\n372'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 387}, page_content='Chapter10SequenceModeling:RecurrentandRecursiveNetsRecurrentneuralnetworksRNNsor(,)areafamilyofRumelhartetal.1986aneuralnetworksforprocessingsequentialdata.MuchasaconvolutionalnetworkisaneuralnetworkthatisspecializedforprocessingagridofvaluesXsuchasanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedforprocessingasequenceofvaluesx(1),...,x()τ.Justasconvolutionalnetworkscanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutionalnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuchlongersequencesthanwouldbepracticalfornetworkswithoutsequence-basedspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariablelength.Togofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-tageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsofthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharingmakesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparametersforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnotseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengthsandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhenaspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.Forexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,IwenttoNepal.”IfweaskamachinelearningmodeltoreadeachsentenceandextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognizetheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth373'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 388}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwordorthesecondwordofthesentence.Supposethatwetrainedafeedforwardnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnectedfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soitwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositioninthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweightsacrossseveraltimesteps.Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.Thisconvolutionalapproachisthebasisfortime-delayneuralnetworks(LangandHinton1988Waibel1989Lang1990,;etal.,;etal.,).Theconvolutionoperationallowsanetworktoshareparametersacrosstime,butisshallow.Theoutputofconvolutionisasequencewhereeachmemberoftheoutputisafunctionofasmallnumberofneighboringmembersoftheinput.Theideaofparametersharingmanifestsintheapplicationofthesameconvolutionkernelateachtimestep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberoftheoutputisafunctionofthepreviousmembersoftheoutput.Eachmemberoftheoutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.Thisrecurrentformulationresultsinthesharingofparametersthroughaverydeepcomputationalgraph.Forthesimplicityofexposition,werefertoRNNsasoperatingonasequencethatcontainsvectorsx()twiththetimestepindextrangingfromto1τ.Inpractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,withadiﬀerentsequencelengthτforeachmemberoftheminibatch.Wehaveomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindexneednotliterallyrefertothepassageoftimeintherealworld,butonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensionsacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,thenetworkmayhaveconnectionsthatgobackwardsintime,providedthattheentiresequenceisobservedbeforeitisprovidedtothenetwork.Thischapterextendstheideaofacomputationalgraphtoincludecycles.Thesecyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalueatafuturetimestep.Suchcomputationalgraphsallowustodeﬁnerecurrentneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,anduserecurrentneuralnetworks.Formoreinformationonrecurrentneuralnetworksthanisavailableinthischapter,wereferthereadertothetextbookofGraves2012().374'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 389}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.1UnfoldingComputationalGraphsAcomputationalgraphisawaytoformalizethestructureofasetofcomputations,suchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.PleaserefertoSec.forageneralintroduction.Inthissectionweexplain6.5.1theideaofarecursiveorrecurrentcomputationintoacomputationalunfoldinggraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetworkstructure.Forexample,considertheclassicalformofadynamicalsystem:s()t= (fs(1)t−;)θ,(10.1)wheres()tiscalledthestateofthesystem.Eq. isrecurrentbecausethedeﬁnitionof10.1sattimetrefersbacktothesamedeﬁnitionattime.t−1Foraﬁnitenumberoftimestepsτ,thegraphcanbeunfoldedbyapplyingthedeﬁnitionτ−1times.Forexample,ifweunfoldEq.for10.1τ= 3timesteps,weobtains(3)=(fs(2);)θ(10.2)=((ffs(1););)θθ(10.3)Unfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhasyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncannowberepresentedbyatraditionaldirectedacycliccomputationalgraph. TheunfoldedcomputationalgraphofEq.andEq.isillustratedinFig..10.110.310.1s(t−1)s(t−1)s()ts()ts(+1)ts(+1)tf fs()...s()...s()...s()...f ff ff fFigure10.1:TheclassicaldynamicalsystemdescribedbyEq.,illustratedasan10.1unfoldedcomputationalgraph. Eachnoderepresentsthestateatsometimetandthefunctionfmapsthestateatttothestateatt+1.Thesameparameters(thesamevalueofusedtoparametrize)areusedforalltimesteps.θfAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternalsignalx()t,s()t= (fs(1)t−,x()t;)θ,(10.4)375'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 390}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.Recurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchasalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentiallyanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.ManyrecurrentneuralnetworksuseEq.orasimilarequationtodeﬁne10.5thevaluesoftheirhiddenunits.Toindicatethatthestateisthehiddenunitsofthenetwork,wenowrewriteEq.usingthevariabletorepresentthestate:10.4hh()t= (fh(1)t−,x()t;)θ,(10.5)illustratedinFig.,typicalRNNswilladdextraarchitecturalfeaturessuchas10.2outputlayersthatreadinformationoutofthestatetomakepredictions.hWhentherecurrentnetworkistrainedtoperformataskthatrequirespredictingthefuturefromthepast,thenetworktypicallylearnstouseh()tasakindoflossysummaryofthetask-relevantaspectsofthepastsequenceofinputsuptot.Thissummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence(x()t,x(1)t−,x(2)t−,...,x(2),x(1))toaﬁxedlengthvectorh()t.Dependingonthetrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepastsequencewithmoreprecisionthanotheraspects. Forexample,iftheRNNisusedinstatisticallanguagemodeling,typicallytopredictthenextwordgivenpreviouswords,itmaynotbenecessarytostorealloftheinformationintheinputsequenceuptotimet,butratheronlyenoughinformationtopredicttherestofthesentence.Themostdemandingsituationiswhenweaskh()ttoberichenoughtoallowonetoapproximatelyrecovertheinputsequence,asinautoencoderframeworks(Chapter).14f fh hx xh(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)th()...h()...h()...h()...f fUnfoldf ff ffFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocessesinformationfromtheinputxbyincorporatingitintothestatehthatispassedforwardthroughtime.(Left)Circuitdiagram.Theblacksquareindicatesadelayof1timestep.(Right)Thesamenetworkseenasanunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticulartimeinstance.Eq. canbedrawnintwodiﬀerentways. OnewaytodrawtheRNNis10.5withadiagramcontainingonenodeforeverycomponentthatmightexistina376'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 391}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthisview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalpartswhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofFig..10.2Throughoutthischapter,weuseablacksquareinacircuitdiagramtoindicatethataninteractiontakesplacewithadelayof1timestep,fromthestateattimettothestateattimet+1. TheotherwaytodrawtheRNNisasanunfoldedcomputationalgraph,inwhicheachcomponentisrepresentedbymanydiﬀerentvariables,withonevariablepertimestep,representingthestateofthecomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasaseparatenodeofthecomputationalgraph,asintherightofFig..Whatwecallis10.2unfoldingtheoperationthatmapsacircuitasintheleftsideoftheﬁguretoacomputationalgraphwithrepeatedpiecesasintherightside.Theunfoldedgraphnowhasasizethatdependsonthesequencelength.Wecanrepresenttheunfoldedrecurrenceafterstepswithafunctiontg()t:h()t=g()t(x()t,x(1)t−,x(2)t−,...,x(2),x(1))(10.6)=(fh(1)t−,x()t;)θ(10.7)Thefunctiong()ttakesthewholepastsequence(x()t,x(1)t−,x(2)t−,...,x(2),x(1))asinputandproducesthecurrentstate,buttheunfoldedrecurrentstructureallowsustofactorizeg()tintorepeatedapplicationofafunctionf.Theunfoldingprocessthusintroducestwomajoradvantages:1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesameinputsize,becauseitisspeciﬁedintermsoftransitionfromonestatetoanotherstate,ratherthanspeciﬁedintermsofavariable-lengthhistoryofstates.2.Itispossibletousethesametransitionfunctionfwiththesameparametersateverytimestep.Thesetwofactorsmakeitpossibletolearnasinglemodelfthatoperatesonalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparatemodelg()tforallpossibletimesteps.Learningasingle,sharedmodelallowsgeneralizationtosequencelengthsthatdidnotappearinthetrainingset,andallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldberequiredwithoutparametersharing.Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrentgraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhichcomputationstoperform.Theunfoldedgraphalsohelpstoillustratetheideaof377'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 392}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSinformationﬂowforwardintime(computingoutputsandlosses)andbackwardintime(computinggradients)byexplicitlyshowingthepathalongwhichthisinformationﬂows.10.2RecurrentNeuralNetworksArmedwiththegraphunrollingandparametersharingideasofSec.,wecan10.1designawidevarietyofrecurrentneuralnetworks.\\nU UV VW Wo(t−1)o(t−1)h ho oy yL L\\nx xo()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t\\nh(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tW WW WW WW Wh()...h()...h()...h()...V VV VV VU UU UU UUnfold\\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetworkthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.AlossLmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusingsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.ThelossLinternallycomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohiddenconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnectionsparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedbyaweightmatrixV.Eq.deﬁnesforwardpropagationinthismodel.10.8(Left)TheRNNanditslossdrawnwithrecurrentconnections.(Right)Thesameseenasantime-unfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticulartimeinstance.Someexamplesofimportantdesignpatternsforrecurrentneuralnetworksincludethefollowing:•Recurrentnetworksthatproduceanoutputateachtimestepandhave378'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 393}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSrecurrentconnectionsbetweenhiddenunits,illustratedinFig..10.3•Recurrentnetworksthatproduceanoutputateachtimestepandhaverecurrentconnectionsonlyfromtheoutputatonetimesteptothehiddenunitsatthenexttimestep,illustratedinFig.10.4•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,thatreadanentiresequenceandthenproduceasingleoutput,illustratedinFig.10.5.Fig.isareasonablyrepresentativeexamplethatwereturntothroughout10.3mostofthechapter.TherecurrentneuralnetworkofFig.andEq.isuniversalinthe10.310.8sensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysucharecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafteranumberoftimestepsthatisasymptoticallylinearinthenumberoftimestepsusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput(,;,;,;SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995Hyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,sotheseresultsregardexactimplementationofthefunction,notapproximations.TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputanditsoutputsmustbediscretizedtoprovideabinaryoutput.ItispossibletocomputeallfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(SiegelmannandSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcationofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuringmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproofcansimulateanunboundedstackbyrepresentingitsactivationsandweightswithrationalnumbersofunboundedprecision.WenowdeveloptheforwardpropagationequationsfortheRNNdepictedinFig..Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe10.3hiddenunits. Hereweassumethehyperbolictangentactivationfunction.Also,theﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwordsorcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutputoasgivingtheunnormalizedlogprobabilitiesofeachpossiblevalueofthediscretevariable.Wecanthenapplythesoftmaxoperationasapost-processingsteptoobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagationbeginswithaspeciﬁcationoftheinitialstateh(0).Then,foreachtimestepfromttτ= 1to= ,weapplythefollowingupdateequations:a()t=+bWh(1)t−+Ux()t(10.8)379'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 394}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nUVWo(t−1)o(t−1)h ho oy yL L\\nx xo()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t\\nh(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tWWWWo()...o()...h()...h()...VVVUUUUnfoldFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutputtothehiddenlayer.Ateachtimestept,theinputisxt,thehiddenlayeractivationsareh()t,theoutputsareo()t,thetargetsarey()tandthelossisL()t.(Left)Circuitdiagram.(Right)Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressasmallersetoffunctions)thanthoseinthefamilyrepresentedbyFig..TheRNN10.3inFig.canchoosetoputanyinformationitwantsaboutthepastintoitshidden10.3representationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedtoputaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosendtothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioushisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.Unlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformationfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasiertotrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreaterparallelizationduringtraining,asdescribedinSec..10.2.1\\n380'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 395}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSh()t=tanh(a()t)(10.9)o()t=+cVh()t(10.10)ˆy()t=softmax(o()t)(10.11)wheretheparametersarethebiasvectorsbandcalongwiththeweightmatricesU,VandW,respectivelyforinput-to-hidden,hidden-to-outputandhidden-to-hiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsaninputsequencetoanoutputsequenceofthesamelength.Thetotallossforagivensequenceofvaluespairedwithasequenceofvalueswouldthenbejustxythesumofthelossesoverallthetimesteps.Forexample,ifL()tisthenegativelog-likelihoodofy()tgivenx(1),...,x()t,thenL\\ue010{x(1),...,x()τ}{,y(1),...,y()τ}\\ue011(10.12)=\\ue058tL()t(10.13)=−\\ue058tlogpmodel\\ue010y()t|{x(1),...,x()t}\\ue011,(10.14)wherepmodel\\ue000y()t|{x(1),...,x()t}\\ue001isgivenbyreadingtheentryfory()tfromthemodel’soutputvectorˆy()t.Computingthegradientofthislossfunctionwithrespecttotheparametersisanexpensiveoperation.ThegradientcomputationinvolvesperformingaforwardpropagationpassmovinglefttorightthroughourillustrationoftheunrolledgraphinFig.,followedbyabackwardpropagation10.3passmovingrighttoleftthroughthegraph.TheruntimeisO(τ)andcannotbereducedbyparallelizationbecausetheforwardpropagationgraphisinherentlysequential;eachtimestepmayonlybecomputedafterthepreviousone.Statescomputedintheforwardpassmustbestoreduntiltheyarereusedduringthebackwardpass,sothememorycostisalsoO(τ).Theback-propagationalgorithmappliedtotheunrolledgraphwithO(τ)costiscalledback-propagationthroughtimeBPTTorandisdiscussedfurtherSec..Thenetworkwithrecurrence10.2.2betweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Isthereanalternative?10.2.1TeacherForcingandNetworkswithOutputRecurrenceThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimesteptothehiddenunitsatthenexttimestep(showninFig.)isstrictlylesspowerful10.4becauseitlackshidden-to-hiddenrecurrentconnections.Forexample,itcannotsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden381'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 396}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSrecurrence,itrequiresthattheoutputunitscapturealloftheinformationaboutthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunitsareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapturethenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuserknowshowtodescribethefullstateofthesystemandprovidesitaspartofthetrainingsettargets.Theadvantageofeliminatinghidden-to-hiddenrecurrenceisthat,foranylossfunctionbasedoncomparingthepredictionattimettothetrainingtargetattimet,allthetimestepsaredecoupled.Trainingcanthusbeparallelized,withthegradientforeachsteptcomputedinisolation.Thereisnoneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetrainingsetprovidestheidealvalueofthatoutput.\\nh(t−1)h(t−1)Wh()th()t... ...x(t−1)x(t−1)x()tx()tx()...x()...WWUUUh()τh()τx()τx()τWUo()τo()τy()τy()τL()τL()τV... ...Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheendofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproduceaﬁxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatargetrightattheend(asdepictedhere)orthegradientontheoutputo()tcanbeobtainedbyback-propagatingfromfurtherdownstreammodules.Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackintothemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedurethatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthemodelreceivesthegroundtruthoutputy()tasinputattimet+1. Wecanseethisbyexaminingasequencewithtwotimesteps.Theconditionalmaximumlikelihoodcriterionislogp\\ue010y(1),y(2)|x(1),x(2)\\ue011(10.15)382'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 397}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no(t−1)o(t−1)o()to()th(t−1)h(t−1)h()th()tx(t−1)x(t−1)x()tx()tWVVUUo(t−1)o(t−1)o()to()tL(t−1)L(t−1)L()tL()ty(t−1)y(t−1)y()ty()t\\nh(t−1)h(t−1)h()th()tx(t−1)x(t−1)x()tx()tWVVUUTrain timeTest timeFigure10.6:Illustrationofteacherforcing.TeacherforcingisatrainingtechniquethatisapplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthenexttimestep.(Left)correctoutputAttraintime,wefeedthey()tdrawnfromthetrainsetasinputtoh(+1)t.(Right)Whenthemodelisdeployed,thetrueoutputisgenerallynotknown.Inthiscase,weapproximatethecorrectoutputy()twiththemodel’soutputo()t,andfeedtheoutputbackintothemodel.\\n383'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 398}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS=logp\\ue010y(2)|y(1),x(1),x(2)\\ue011+logp\\ue010y(1)|x(1),x(2)\\ue011(10.16)Inthisexample,weseethatattimet= 2,themodelistrainedtomaximizetheconditionalprobabilityofy(2)givenboththexsequencesofarandthepreviousyvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,ratherthanfeedingthemodel’sownoutputbackintoitself,theseconnectionsshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.ThisisillustratedinFig..10.6Weoriginallymotivatedteacherforcingasallowingustoavoidback-propagationthroughtimeinmodelsthatlackhidden-to-hiddenconnections.Teacherforcingmaystillbeappliedtomodelsthathavehidden-to-hiddenconnectionssolongastheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthenexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearliertimesteps,theBPTTalgorithmisnecessary.SomemodelsmaythusbetrainedwithbothteacherforcingandBPTT.Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobelaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfromtheoutputdistribution)fedbackasinput.Inthiscase,thekindofinputsthatthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputsthatitwillseeattesttime. Onewaytomitigatethisproblemistotrainwithbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredictingthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrentoutput-to-inputpaths.Inthisway,thenetworkcanlearntotakeintoaccountinputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)notseenduringtrainingandhowtomapthestatebacktowardsonethatwillmakethenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengioetal.,)tomitigatethegapbetweentheinputsseenattraintimeandthe2015binputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdatavaluesasinput.Thisapproachexploitsacurriculumlearningstrategytograduallyusemoreofthegeneratedvaluesasinput.10.2.2ComputingtheGradientinaRecurrentNeuralNetworkComputingthegradientthrougharecurrentneuralnetworkisstraightforward.Onesimplyappliesthegeneralizedback-propagationalgorithmofSec.tothe6.5.6unrolledcomputationalgraph.Nospecializedalgorithmsarenecessary.Theuseofback-propagationontheunrolledgraphiscalledtheback-propagationthroughtime(BPTT)algorithm. Gradientsobtainedbyback-propagationmaythenbeusedwithanygeneral-purposegradient-basedtechniquestotrainanRNN.384'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 399}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovideanexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove(Eq.andEq.).Thenodesofourcomputationalgraphincludethe10.810.12parametersU,V,W,bandcaswellasthesequenceofnodesindexedbytforx()t,h()t,o()tandL()t.ForeachnodeNweneedtocomputethegradient∇NLrecursively,basedonthegradientcomputedatnodesthatfollowitinthegraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss∂L∂L()t= 1.(10.17)Inthisderivationweassumethattheoutputso()tareusedastheargumenttothesoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealsoassumethatthelossisthenegativelog-likelihoodofthetruetargety()tgiventheinputsofar.Thegradient∇o()tLontheoutputsattimestept,foralli,t,isasfollows:(∇o()tL)i=∂L∂o()ti=∂L∂L()t∂L()t∂o()ti=ˆy()ti−1i,y()t.(10.18)Weworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnaltimestep,τh()τonlyhaso()τasadescendent,soitsgradientissimple:∇h()τL= V\\ue03e∇o()τL.(10.19)Wecantheniteratebackwardsintimetoback-propagategradientsthroughtime,fromt=τ−1downtot= 1,notingthath()t(fort<τ)hasasdescendentsbotho()tandh(+1)t.Itsgradientisthusgivenby∇h()tL=\\ue020∂h(+1)t∂h()t\\ue021\\ue03e(∇h(+1)tL)+\\ue020∂o()t∂h()t\\ue021\\ue03e(∇o()tL)(10.20)= W\\ue03e(∇h(+1)tL)diag\\ue0121−\\ue010h(+1)t\\ue0112\\ue013+V\\ue03e(∇o()tL)(10.21)wherediag\\ue0101−\\ue000h(+1)t\\ue0012\\ue011indicatesthediagonalmatrixcontainingtheelements1−(h(+1)ti)2.ThisistheJacobianofthehyperbolictangentassociatedwiththehiddenunitattime.it+1Oncethegradientsonthe internal nodesofthecomputationalgraph areobtained, wecanobtainthegradientsontheparameternodes.Becausetheparametersaresharedacrossmanytimesteps,wemusttakesomecarewhendenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto385'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 400}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSimplementusethebpropmethodofSec.,thatcomputesthecontribution6.5.6ofasingleedgeinthecomputationalgraphtothegradient.However,the∇WfoperatorusedincalculustakesintoaccountthecontributionofWtothevalueoffduetoalledgesinthecomputationalgraph.Toresolvethisambiguity,weintroducedummyvariablesW()tthataredeﬁnedtobecopiesofWbutwitheachW()tusedonlyattimestept.Wemaythenuse∇W()ttodenotethecontributionoftheweightsattimesteptothegradient.tUsingthisnotation,thegradientontheremainingparametersisgivenby:∇cL=\\ue058t\\ue020∂o()t∂c\\ue021\\ue03e∇o()tL=\\ue058t∇o()tL(10.22)∇bL=\\ue058t\\ue020∂h()t∂b()t\\ue021\\ue03e∇h()tL=\\ue058tdiag\\ue0121−\\ue010h()t\\ue0112\\ue013∇h()tL(10.23)∇VL=\\ue058t\\ue058i\\ue020∂L∂o()ti\\ue021∇Vo()ti=\\ue058t(∇o()tL)h()t\\ue03e(10.24)∇WL=\\ue058t\\ue058i\\ue020∂L∂h()ti\\ue021∇W()th()ti(10.25)=\\ue058tdiag\\ue0121−\\ue010h()t\\ue0112\\ue013(∇h()tL)h(1)t−\\ue03e(10.26)∇UL=\\ue058t\\ue058i\\ue020∂L∂h()ti\\ue021∇U()th()ti(10.27)=\\ue058tdiag\\ue0121−\\ue010h()t\\ue0112\\ue013(∇h()tL)x()t\\ue03e(10.28)Wedonotneedtocomputethegradientwithrespecttox()tfortrainingbecauseitdoesnothaveanyparametersasancestorsinthecomputationalgraphdeﬁningtheloss.10.2.3RecurrentNetworksasDirectedGraphicalModelsIntheexamplerecurrentnetworkwehavedevelopedsofar,thelossesL()twerecross-entropiesbetweentrainingtargetsy()tandoutputso()t.Aswithafeedforwardnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,weusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and386'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 401}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSweusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.Meansquarederroristhecross-entropylossassociatedwithanoutputdistributionthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.Whenweuseapredictivelog-likelihoodtrainingobjective,suchasEq.,we10.12traintheRNNtoestimatetheconditionaldistributionofthenextsequenceelementy()tgiventhepastinputs.Thismaymeanthatwemaximizethelog-likelihoodlog(py()t|x(1),...,x()t),(10.29)or,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenexttimestep,log(py()t|x(1),...,x()t,y(1),...,y(1)t−).(10.30)Decomposingthejointprobabilityoverthesequenceofyvaluesasaseriesofone-stepprobabilisticpredictionsisonewaytocapturethefulljointdistributionacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthatconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedgesfromanyy()iinthepasttothecurrenty()t.Inthiscase,theoutputsyareconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedtheactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)backintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally()ivaluesinthepasttothecurrenty()tvalue.\\ny(1)y(1)y(2)y(2)y(3)y(3)y(4)y(4)y(5)y(5)y()...y()...Figure10.7:Fullyconnectedgraphicalmodelforasequencey(1),y(2),...,y()t,...:everypastobservationy()imayinﬂuencetheconditionaldistributionofsomey()t(fort>i),giventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothisgraph(asinEq.)mightbeveryineﬃcient,withanevergrowingnumberofinputs10.6andparametersforeachelementofthesequence.RNNsobtainthesamefullconnectivitybuteﬃcientparametrization,asillustratedinFig..10.8387'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 402}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlyasequenceofscalarrandomvariablesY={y(1),...,y()τ},withnoadditionalinputsx.Theinputattimesteptissimplytheoutputattimestept−1.TheRNNthendeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejointdistributionoftheseobservationsusingthechainrule(Eq.)forconditional3.6probabilities:PP() = Y(y(1),...,y()τ) =τ\\ue059t=1P(y()t|y(1)t−,y(2)t−,...,y(1))(10.31)wheretheright-handsideofthebarisemptyfort=1,ofcourse.Hencethenegativelog-likelihoodofasetofvalues{y(1),...,y()τ}accordingtosuchamodelisL=\\ue058tL()t(10.32)whereL()t= log(−Py()t= y()t|y(1)t−,y(2)t−,...,y(1)).(10.33)\\ny(1)y(1)y(2)y(2)y(3)y(3)y(4)y(4)y(5)y(5)y()...y()...h(1)h(1)h(2)h(2)h(3)h(3)h(4)h(4)h(5)h(5)h()...h()...Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,eventhoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainaveryeﬃcientparametrization,basedonEq.. Everystageinthesequence(for10.5h()tandy()t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcansharethesameparameterswiththeotherstages.Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonothervariables.Manygraphicalmodelsaimtoachievestatisticalandcomputationaleﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.Forexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodelshouldonlycontainedgesfrom{y()tk−,...,y(1)t−}toy()t,ratherthancontainingedgesfromtheentirepasthistory.However,insomecases,webelievethatallpastinputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsareusefulwhenwebelievethatthedistributionovery()tmaydependonavalueofy()ifromthedistantpastinawaythatisnotcapturedbytheeﬀectofy()iony(1)t−.388'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 403}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSOnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNasdeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresentdirectdependenciesbetweenanypairofyvalues.ThegraphicalmodelovertheyvalueswiththecompletegraphstructureisshowninFig..Thecomplete10.7graphinterpretationoftheRNNisbasedonignoringthehiddenunitsh()tbymarginalizingthemoutofthemodel.ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthatresultsfromregardingthehiddenunitsh()tasrandomvariables.1IncludingthehiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcientparametrizationofthejointdistributionovertheobservations.Supposethatwerepresentedanarbitraryjointdistributionoverdiscretevalueswithatabularrepresentation—anarraycontainingaseparateentryforeachpossibleassignmentofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignmentoccurring. Ifycantakeonkdiﬀerentvalues,thetabularrepresentationwouldhaveO(kτ)parameters. Bycomparison,duetoparametersharing,thenumberofparametersintheRNNisO(1)asafunctionofsequencelength.ThenumberofparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforcedtoscalewithsequencelength.Eq.showsthattheRNNparametrizes10.5long-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplicationsofthesamefunctionfandsameparametersθateachtimestep.Fig.10.8illustratesthegraphicalmodelinterpretation.Incorporatingtheh()tnodesinthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediatequantitybetweenthem.Avariabley()iinthedistantpastmayinﬂuenceavariabley()tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbeeﬃcientlyparametrizedbyusingthesameconditionalprobabilitydistributionsateachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthejointassignmentofallvariablescanbeevaluatedeﬃciently.Evenwiththeeﬃcientparametrizationofthegraphicalmodel,someoperationsremaincomputationallychallenging.Forexample,itisdiﬃculttopredictmissingvaluesinthemiddleofthesequence.Thepricerecurrentnetworkspayfortheirreducednumberofparametersisthattheparametersmaybediﬃcult.optimizingTheparametersharingusedinrecurrentnetworksreliesontheassumptionthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,theassumptionisthattheconditionalprobabilitydistributionoverthevariablesat1Theconditionaldistributionoverthesevariablesgiventheirparentsisdeterministic.Thisisperfectlylegitimate,thoughitissomewhatraretodesignagraphicalmodelwithsuchdeterministichiddenunits.389'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 404}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETStimet+1giventhevariablesattimetis,meaningthattherelationshipstationarybetweentheprevioustimestepandthenexttimestepdoesnotdependont.Inprinciple,itwouldbepossibletousetasanextrainputateachtimestepandletthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetweendiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerentconditionalprobabilitydistributionforeacht,butthenetworkwouldthenhavetoextrapolatewhenfacedwithnewvaluesof.tTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehowtodrawsamplesfromthemodel.Themainoperationthatweneedtoperformissimplytosamplefromtheconditionaldistributionateachtimestep. However,thereisoneadditionalcomplication.TheRNNmusthavesomemechanismfordeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.Inthecasewhentheoutputisasymboltakenfromavocabulary,onecanaddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,weinsertthissymbolasanextramemberofthesequence,immediatelyafterx()τineachtrainingexample.AnotheroptionistointroduceanextraBernoullioutputtothemodelthatrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateachtimestep.Thisapproachismoregeneralthantheapproachofaddinganextrasymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthanonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedtoanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallyasigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidistrainedtomaximizethelog-probabilityofthecorrectpredictionastowhetherthesequenceendsorcontinuesateachtimestep.Anotherwaytodeterminethesequencelengthτistoaddanextraoutputtothemodelthatpredictstheintegerτitself.Themodelcansampleavalueofτandthensampleτstepsworthofdata.Thisapproachrequiresaddinganextrainputtotherecurrentupdateateachtimestepsothattherecurrentupdateisawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainputcaneitherconsistofthevalueofτorcanconsistofτt−,thenumberofremainingtimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthatendabruptly,suchasasentencethatendsbeforeitiscomplete.ThisapproachisbasedonthedecompositionP(x(1),...,x()τ) = ()(PτPx(1),...,x()τ|τ.)(10.34)ThestrategyofpredictingτdirectlyisusedforexamplebyGoodfellowetal.390'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 405}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS().2014d10.2.4ModelingSequencesConditionedonContextwithRNNsIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirectedgraphicalmodeloverasequenceofrandomvariablesy()twithnoinputsx.Ofcourse,ourdevelopmentofRNNsasinEq.includedasequenceofinputs10.8x(1),x(2),...,x()τ.Ingeneral,RNNsallowtheextensionofthegraphicalmodelviewtorepresentnotonlyajointdistributionovertheyvariablesbutalsoaconditionaldistributionoverygivenx.AsdiscussedinthecontextoffeedforwardnetworksinSec.,anymodelrepresentingavariable6.2.1.1P(y;θ)canbereinter-pretedasamodelrepresentingaconditionaldistributionP(yω|)withω=θ.WecanextendsuchamodeltorepresentadistributionP(yx|)byusingthesameP(yω|)asbefore,butmakingωafunctionofx. InthecaseofanRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommonandobviouschoices.Previously,wehavediscussedRNNsthattakeasequenceofvectorsx()tfort=1,...,τasinput. Anotheroptionistotakeonlyasinglevectorxasinput.Whenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNNthatgeneratestheysequence.SomecommonwaysofprovidinganextrainputtoanRNNare:1. asanextrainputateachtimestep,or2. astheinitialstateh(0),or3. both.TheﬁrstandmostcommonapproachisillustratedinFig..Theinteraction10.9betweentheinputxandeachhiddenunitvectorh()tisparametrizedbyanewlyintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequenceofyvalues. Thesameproductx\\ue03eRisaddedasadditionalinputtothehiddenunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalueofx\\ue03eRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.Theweightsremainindependentoftheinput.Wecanthinkofthismodelastakingtheparametersθofthenon-conditionalmodelandturningthemintoω,wherethebiasparameterswithinarenowafunctionoftheinput.ωRatherthanreceivingonlyasinglevectorxasinput,theRNNmayreceiveasequenceofvectorsx()tasinput.TheRNNdescribedinEq.correspondstoa10.8391'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 406}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t\\nh(t−1)h(t−1)h()th()th(+1)th(+1)tWWWWs()...s()...h()...h()...VVVUUU\\nx xy()...y()...\\nRRRRRFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequencesY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageisusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.Eachelementy()toftheobservedoutputsequenceservesbothasinput(forthecurrenttimestep)and,duringtraining,astarget(fortheprevioustimestep).\\n392'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 407}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t\\nh(t−1)h(t−1)h()th()th(+1)th(+1)tWWWWh()...h()...h()...h()...VVVUUUx(t−1)x(t−1)R\\nx()tx()tx(+1)tx(+1)tRR\\nFigure10.10:Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequenceofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.ComparedtoFig.,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate.10.3TheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofygivensequencesofofthesamelength.TheRNNofFig.isonlyabletorepresentx10.3distributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiventhevalues.x\\n393'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 408}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSconditionaldistributionP(y(1),...,y()τ|x(1),...,x()τ)thatmakesaconditionalindependenceassumptionthatthisdistributionfactorizesas\\ue059tP(y()t|x(1),...,x()t).(10.35)Toremovetheconditionalindependenceassumption,wecanaddconnectionsfromtheoutputattimettothehiddenunitattimet+1,asshowninFig..The10.10modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.Thiskindofmodelrepresentingadistributionoverasequencegivenanothersequencestillhasonerestriction,whichisthatthelengthofbothsequencesmustbethesame.WedescribehowtoremovethisrestrictioninSec..10.4\\no(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t\\nh(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tg(t−1)g(t−1)g()tg()tg(+1)tg(+1)t\\nFigure10.11:Computationofatypicalbidirectionalrecurrentneuralnetwork,meanttolearntomapinputsequencesxtotargetsequencesy,withlossL()tateachstept.Thehrecurrencepropagatesinformationforwardintime(towardstheright)whilethegrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateachpointt,theoutputunitso()tcanbeneﬁtfromarelevantsummaryofthepastinitsh()tinputandfromarelevantsummaryofthefutureinitsg()tinput.394'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 409}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.3BidirectionalRNNsAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-ture,meaningthatthestateattimetonlycapturesinformationfromthepast,x(1),...,x(1)t−,andthepresentinputx()t.Someofthemodelswehavediscussedalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhentheyvaluesareavailable.However,inmanyapplicationswewanttooutputapredictionofy()twhichmaydependonthewholeinputsequence.Forexample,inspeechrecognition,thecorrectinterpretationofthecurrentsoundasaphonememaydependonthenextfewphonemesbecauseofco-articulationandpotentiallymayevendependonthenextfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:iftherearetwointerpretationsofthecurrentwordthatarebothacousticallyplausible,wemayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisisalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearningtasks,describedinthenextsection.Bidirectionalrecurrentneuralnetworks(orbidirectionalRNNs)wereinventedtoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-cessful(Graves2012,)inapplicationswherethatneedarises,suchashandwritingrecognition(Graves2008GravesandSchmidhuber2009etal.,;,),speechrecogni-tion(GravesandSchmidhuber2005Graves2013Baldi,;etal.,)andbioinformatics(etal.,).1999Asthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforwardthroughtimebeginningfromthestartofthesequencewithanotherRNNthatmovesbackwardthroughtimebeginningfromtheendofthesequence.Fig.10.11illustratesthetypicalbidirectionalRNN,withh()tstandingforthestateofthesub-RNNthatmovesforwardthroughtimeandg()tstandingforthestateofthesub-RNNthatmovesbackwardthroughtime.Thisallowstheoutputunitso()ttocomputearepresentationthatdependsonboththepastandthefuturebutismostsensitivetotheinputvaluesaroundtimet,withouthavingtospecifyaﬁxed-sizewindowaroundt(asonewouldhavetodowithafeedforwardnetwork,aconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).Thisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,byhavingfourRNNs,eachonegoinginoneofthefourdirections:up,down,left,right.Ateachpoint(i,j)ofa2-Dgrid,anoutputOi,jcouldthencomputearepresentationthatwouldcapturemostlylocalinformationbutcouldalsodependon long-rangeinputs, if the RNNis ableto learnto carry thatinformation.Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore395'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 410}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthesamefeaturemap(,;Visinetal.2015Kalchbrenner2015etal.,).Indeed,theforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshowstheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,priortotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateralinteractions.10.4Encoder-DecoderSequence-to-SequenceArchitec-turesWehaveseeninFig.howanRNNcanmapaninputsequencetoaﬁxed-size10.5vector.WehaveseeninFig.howanRNNcanmapaﬁxed-sizevectortoa10.9sequence.WehaveseeninFig.,Fig.,Fig.andFig.howan10.310.410.1010.11RNNcanmapaninputsequencetoanoutputsequenceofthesamelength.HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoanoutputsequencewhichisnotnecessarilyofthesamelength. Thiscomesupinmanyapplications,suchasspeechrecognition,machinetranslationorquestionanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerallynotofthesamelength(althoughtheirlengthsmightberelated).WeoftencalltheinputtotheRNNthe“context.”Wewanttoproducearepresentationofthiscontext,C.ThecontextCmightbeavectororsequenceofvectorsthatsummarizetheinputsequenceXx= ((1),...,x(nx)).ThesimplestRNNarchitectureformappingavariable-lengthsequencetoan-othervariable-lengthsequencewasﬁrstproposedby()andshortlyChoetal.2014aafterbySutskever2014etal.(),whoindependentlydevelopedthatarchitectureandweretheﬁrsttoobtainstate-of-the-arttranslationusingthisapproach.Theformersystemisbasedonscoringproposalsgeneratedbyanothermachinetranslationsystem,whilethelatterusesastandalonerecurrentnetworktogeneratethetrans-lations.Theseauthorsrespectivelycalledthisarchitecture,illustratedinFig.,10.12theencoder-decoderorsequence-to-sequencearchitecture.Theideaisverysimple:(1)anencoderreaderinputororRNNprocessestheinputsequence.TheencoderemitsthecontextC,usuallyasasimplefunctionofitsﬁnalhiddenstate.(2)adecoderwriteroutputororRNNisconditionedonthatﬁxed-lengthvector(justlikeinFig.)togeneratetheoutputsequence10.9Y= (y(1),...,y(ny)).Theinnovationofthiskindofarchitectureoverthosepresentedinearliersectionsofthischapteristhatthelengthsnxandnycanvaryfromeachother,whilepreviousarchitecturesconstrainednx=ny=τ.Inasequence-to-sequencearchitecture,thetwoRNNs396'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 411}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nEncoder…x(1)x(1)x(2)x(2)x()...x()...x(nx)x(nx)\\nDecoder…y(1)y(1)y(2)y(2)y()...y()...y(ny)y(ny)C C\\nFigure10.12:Exampleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,forlearningtogenerateanoutputsequence(y(1),...,y(ny))givenaninputsequence(x(1),x(2),...,x(nx)).ItiscomposedofanencoderRNNthatreadstheinputsequenceandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofagivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputeagenerallyﬁxed-sizecontextvariableCwhichrepresentsasemanticsummaryoftheinputsequenceandisgivenasinputtothedecoderRNN.\\n397'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 412}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSaretrainedjointlytomaximizetheaverageoflogP(y(1),...,y(ny)|x(1),...,x(nx))overallthepairsofxandysequencesinthetrainingset.ThelaststatehnxoftheencoderRNNistypicallyusedasarepresentationCoftheinputsequencethatisprovidedasinputtothedecoderRNN.IfthecontextCisavector,thenthedecoderRNNissimplyavector-to-sequenceRNNasdescribedinSec..Aswehaveseen,thereareatleasttwo10.2.4waysforavector-to-sequenceRNNtoreceiveinput.TheinputcanbeprovidedastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunitsateachtimestep.Thesetwowayscanalsobecombined.Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayerasthedecoder.OneclearlimitationofthisarchitectureiswhenthecontextCoutputbytheencoderRNNhasadimensionthatistoosmalltoproperlysummarizealongsequence.Thisphenomenonwasobservedby()inthecontextBahdanauetal.2015ofmachinetranslation.TheyproposedtomakeCavariable-lengthsequenceratherthanaﬁxed-sizevector. Additionally,theyintroducedanattentionmechanismthatlearnstoassociateelementsofthesequenceCtoelementsoftheoutputsequence.SeeSec.formoredetails.12.4.5.110.5DeepRecurrentNetworksThecomputationinmostRNNscanbedecomposedintothreeblocksofparametersandassociatedtransformations:1. fromtheinputtothehiddenstate,2. fromtheprevioushiddenstatetothenexthiddenstate,and3. fromthehiddenstatetotheoutput.WiththeRNNarchitectureofFig.,eachofthesethreeblocksisassociated10.3withasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,eachofthesecorrespondstoashallowtransformation. Byashallowtransformation,wemeanatransformationthatwouldberepresentedbyasinglelayerwithinadeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃnetransformationfollowedbyaﬁxednonlinearity.Woulditbeadvantageoustointroducedepthineachoftheseoperations?Experimentalevidence(Graves2013Pascanu2014aetal.,;etal.,)stronglysuggestsso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough398'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 413}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nhy\\nxz\\n(a)(b)(c)xhy\\nxhy\\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanuetal.,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized2014a(a)hierarchically. Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to-(b)hidden,hidden-to-hiddenandhidden-to-outputparts. Thismaylengthentheshortestpathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby(c)introducingskipconnections.\\n399'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 414}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSdepthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),ElHihiandBengio1996Jaeger2007a(),or()forearlierworkondeepRNNs.Graves2013etal.()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposingthestateofanRNNintomultiplelayersasinFig.(left).Wecanthink10.13ofthelowerlayersinthehierarchydepictedinFig. aasplayingarolein10.13transformingtherawinputintoarepresentationthatismoreappropriate,atthehigherlevelsofthehiddenstate.Pascanu2014aetal.()goastepfurtherandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocksenumeratedabove,asillustratedinFig.b.Considerationsofrepresentational10.13capacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoingsobyaddingdepthmayhurtlearningbymakingoptimizationdiﬃcult.Ingeneral,itiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthofFig.bmakestheshortestpathfromavariableintimestep10.13ttoavariableintimestept+1becomelonger.Forexample,ifanMLPwithasinglehiddenlayerisusedforthestate-to-statetransition,wehavedoubledthelengthoftheshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththeordinaryRNNofFig.. However,asarguedby10.3Pascanu2014aetal.(),thiscanbemitigatedbyintroducingskipconnectionsinthehidden-to-hiddenpath,asillustratedinFig.c.10.1310.6RecursiveNeuralNetworksRecursiveneuralnetworks2representyetanothergeneralizationofrecurrentnet-works,withadiﬀerentkindofcomputationalgraph,whichisstructuredasadeeptree,ratherthanthechain-likestructureofRNNs.ThetypicalcomputationalgraphforarecursivenetworkisillustratedinFig..Recursiveneuralnetworks10.14wereintroducedbyPollack1990()andtheirpotentialuseforlearningtoreasonwasdescribedbyby().RecursivenetworkshavebeensuccessfullyBottou2011appliedtoprocessingdatastructuresasinputtoneuralnets(Frasconietal.,19971998Socher2011ac2013a,),innaturallanguageprocessing(etal.,,,)aswellasincomputervision(,).Socheretal.2011bOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequenceofthesamelengthτ,thedepth(measuredasthenumberofcompositionsofnonlinearoperations)canbedrasticallyreducedfromτtoO(logτ),whichmighthelpdealwithlong-termdependencies.Anopenquestionishowtobeststructurethetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,2Wesuggesttonotabbreviate“recursiveneuralnetwork”as“RNN”toavoidconfusionwith“recurrentneuralnetwork.”400'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 415}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nx(1)x(1)x(2)x(2)x(3)x(3)VVVy yL L\\nx(4)x(4)Vo o\\nUWUWUW\\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatoftherecurrentnetworkfromachaintoatree.Avariable-sizesequencex(1),x(2),...,x()tcanbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhichsometargetisprovidedwhichisassociatedwiththewholesequence.y\\n401'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 416}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethodscansuggesttheappropriatetreestructure.Forexample,whenprocessingnaturallanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedtothestructureoftheparsetreeofthesentenceprovidedbyanaturallanguageparser(,,). Ideally,onewouldlikethelearneritselftoSocheretal.2011a2013adiscoverandinferthetreestructurethatisappropriateforanygiveninput,assuggestedby().Bottou2011Manyvariantsoftherecursivenetideaarepossible.Forexample,Frasconietal.()and1997Frasconi1998etal.()associatethedatawithatreestructure,andassociatetheinputsand targetswith individualnodesof thetree.Thecomputationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcialneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotonenonlinearity).Forexample,()proposeusingtensoroperationsSocheretal.2013aandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationshipsbetweenconcepts(Weston2010Bordes2012etal.,;etal.,)whentheconceptsarerepresentedbycontinuousvectors(embeddings).10.7TheChallengeofLong-TermDependenciesThemathematicalchallengeoflearninglong-termdependenciesinrecurrentnet-workswasintroducedinSec..Thebasicproblemisthatgradientspropagated8.2.5overmanystagestendtoeithervanish(mostofthetime)orexplode(rarely,butwithmuchdamagetotheoptimization).Evenifweassumethattheparametersaresuchthattherecurrentnetworkisstable(canstorememories,withgradientsnotexploding),thediﬃcultywithlong-termdependenciesarisesfromtheexponentiallysmallerweightsgiventolong-terminteractions(involvingthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyothersourcesprovideadeepertreatment(,;Hochreiter1991Doya1993Bengio1994Pascanu,;etal.,;etal.,2013a).Inthissection,wedescribetheprobleminmoredetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.Recurrentnetworksinvolvethecompositionofthesamefunctionmultipletimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinearbehavior,asillustratedinFig..10.15Inparticular,thefunctioncompositionemployedbyrecurrentneuralnetworkssomewhatresemblesmatrixmultiplication.Wecanthinkoftherecurrencerelationh()t= W\\ue03eh(1)t−(10.36)asaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,402'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 417}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n\\U000f0913\\ue036\\ue030\\U000f0913\\ue034\\ue030\\U000f0913\\ue032\\ue030\\ue030\\ue032\\ue030\\ue034\\ue030\\ue036\\ue030\\ue049\\ue06e\\ue070\\ue075\\ue074\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\U000f0913\\ue034\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue034\\ue050\\ue072\\ue06f\\ue06a\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue06f\\ue066\\ue020\\ue06f\\ue075\\ue074\\ue070\\ue075\\ue074\\ue052\\ue065\\ue070\\ue065\\ue061\\ue074\\ue065\\ue064\\ue020\\ue066\\ue075\\ue06e\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue063\\ue06f\\ue06d\\ue070\\ue06f\\ue073\\ue069\\ue074\\ue069\\ue06f\\ue06e\\ue030\\ue031\\ue032\\ue033\\ue034\\ue035Figure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershownhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatinyderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasinganddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstatedowntoasingledimension,plottedonthey-axis. Thex-axisisthecoordinateoftheinitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthisplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunctionaftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunctionhasbeencomposed.\\n403'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 418}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSandlackinginputsx.AsdescribedinSec.,thisrecurrencerelationessentially8.2.5describesthepowermethod.Itmaybesimpliﬁedtoh()t=\\ue000Wt\\ue001\\ue03eh(0),(10.37)andifadmitsaneigendecompositionoftheformWWQQ= Λ\\ue03e,(10.38)withorthogonal,therecurrencemaybesimpliﬁedfurthertoQh()t= Q\\ue03eΛtQh(0).(10.39)Theeigenvaluesareraisedtothepoweroftcausingeigenvalueswithmagnitudelessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanonetoexplode.Anycomponentofh(0)thatisnotalignedwiththelargesteigenvectorwilleventuallybediscarded.Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imaginemultiplyingaweightwbyitselfmanytimes.Theproductwtwilleithervanishorexplodedependingonthemagnitudeofw.However,ifwemakeanon-recurrentnetworkthathasadiﬀerentweightw()tateachtimestep,thesituationisdiﬀerent.Iftheinitialstateisgivenby,thenthestateattime1tisgivenby\\ue051tw()t.Supposethatthew()tvaluesaregeneratedrandomly,independentlyfromoneanother,withzeromeanandvariancev.ThevarianceoftheproductisO(vn).Toobtainsomedesiredvariancev∗wemaychoosetheindividualweightswithvariancev=n√v∗.Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthevanishingandexplodinggradientproblem,asarguedby().Sussillo2014ThevanishingandexplodinggradientproblemforRNNswasindependentlydiscoveredbyseparateresearchers(,;,,).Hochreiter1991Bengioetal.19931994Onemayhopethattheproblemcanbeavoidedsimplybystayinginaregionofparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,inordertostorememoriesinawaythatisrobusttosmallperturbations,theRNNmustenteraregionofparameterspacewheregradientsvanish(,,Bengioetal.19931994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,thegradientofalongterminteractionhasexponentiallysmallermagnitudethanthegradientofashortterminteraction. Itdoesnotmeanthatitisimpossibletolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,becausethesignalaboutthesedependencieswilltendtobehiddenbythesmallestﬂuctuationsarisingfromshort-termdependencies. Inpractice,theexperimentsin()showthatasweincreasethespanofthedependenciesthatBengioetal.1994404'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 419}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSneedtobecaptured,gradient-basedoptimizationbecomesincreasinglydiﬃcult,withtheprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidlyreaching0forsequencesofonlylength10or20.Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya(),()and1993Bengioetal.1994SiegelmannandSontag1995(),withareviewinPascanu2013aetal.().Theremainingsectionsofthischapterdiscussvariousapproachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-termdependencies(insomecasesallowinganRNNtolearndependenciesacrosshundredsofsteps),buttheproblemoflearninglong-termdependenciesremainsoneofthemainchallengesindeeplearning.10.8EchoStateNetworksTherecurrentweightsmappingfromh(1)t−toh()tandtheinputweightsmappingfromx()ttoh()taresomeofthemostdiﬃcultparameterstolearninarecurrentnetwork.Oneproposed(,;,;,;Jaeger2003Maassetal.2002JaegerandHaas2004Jaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweightssuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpastinputs,andonlylearntheoutputweights.ThisistheideathatwasindependentlyproposedforechostatenetworksorESNs(,;JaegerandHaas2004Jaeger2007bMaass2002,)and(liquidstatemachinesetal.,).Thelatterissimilar,exceptthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valuedhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermedreservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthatthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerentaspectsofthehistoryofinputs.Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthattheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(thehistoryofinputsuptotimet)intoaﬁxed-lengthvector(therecurrentstateh()t),onwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolvetheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobeconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsistsoflinearregressionfromthehiddenunitstotheoutputtargets,andthetrainingcriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywithsimplelearningalgorithms(,).Jaeger2003Theimportantquestionistherefore: howdowesettheinputandrecurrentweightssothatarichsetofhistoriescanberepresentedintherecurrentneuralnetworkstate? Theanswerproposedinthereservoircomputingliteratureisto405'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 420}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrentweightssuchthatthedynamicalsystemisneartheedgeofstability.TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-statetransitionfunctionbecloseto.AsexplainedinSec.,animportant18.2.5characteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobiansJ()t=∂s()t∂s(1)t−.OfparticularimportanceisthespectralradiusofJ()t,deﬁnedtobethemaximumoftheabsolutevaluesofitseigenvalues.Tounderstandtheeﬀectofthespectralradius,considerthesimplecaseofback-propagationwithaJacobianmatrixJthatdoesnotchangewitht.Thiscasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhasaneigenvectorvwithcorrespondingeigenvalueλ.Considerwhathappensaswepropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradientvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafternstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagateaperturbedversionofg.Ifwebeginwithg+δv,thenafteronestep,wewillhaveJ(g+δv).Afternsteps,wewillhaveJn(g+δv).Fromthiswecanseethatback-propagationstartingfromgandback-propagationstartingfromg+δvdivergebyδJnvafternstepsofback-propagation.IfvischosentobeauniteigenvectorofJwitheigenvalueλ,thenmultiplicationbytheJacobiansimplyscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationareseparatedbyadistanceofδλ||n.Whenvcorrespondstothelargestvalueof||λ,thisperturbationachievesthewidestpossibleseparationofaninitialperturbationofsize.δWhen||λ>1,thedeviationsizeδλ||ngrowsexponentiallylarge.When||λ<1,thedeviationsizebecomesexponentiallysmall.Ofcourse,thisexampleassumedthattheJacobianwasthesameateverytimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whenanonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroonmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectralradius. Indeed,themostrecentworkonechostatenetworksadvocatesusingaspectralradiusmuchlargerthanunity(,;,).Yildizetal.2012Jaeger2012Everythingwehavesaidaboutback-propagationviarepeatedmatrixmultipli-cationappliesequallytoforwardpropagationinanetworkwithnononlinearity,wherethestateh(+1)t= h()t\\ue03eW.WhenalinearmapW\\ue03ealwaysshrinkshasmeasuredbytheL2norm,thenwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,themappingfromh()ttoh(+1)tiscontractive,soasmallchangebecomessmalleraftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationaboutthe406'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 421}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSpastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostorethestatevector.TheJacobianmatrixtellsushowasmallchangeofh()tpropagatesonestepforward,orequivalently,howthegradientonh(+1)tpropagatesonestepbackward,duringback-propagation.NotethatneitherWnorJneedtobesymmetric(al-thoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesandeigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatorybehavior(ifthesameJacobianwasappliediteratively).Eventhoughh()torasmallvariationofh()tofinterestinback-propagationarereal-valued,theycanbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappenstothemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasiscoeﬃcients, whenwemultiplythematrixbythevector.Aneigenvaluewithmagnitudegreaterthanonecorrespondstomagniﬁcation(exponentialgrowth,ifappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).Withanonlinearmap, theJacobianisfreeto changeateachstep.Thedynamicsthereforebecomemorecomplicated. However,itremainstruethatasmallinitialvariationcanturnintoalargevariationafterseveralsteps.Onediﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseofasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecomebounded.Notethat itis possiblefor back-propagation toretainunboundeddynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,whenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandareconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis1rareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint.tanhThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesomespectralradiussuchas,whereinformationiscarriedforwardthroughtimebut3doesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearitiesliketanh.Morerecently,ithasbeenshownthatthetechniquesusedtosettheweightsinESNscouldbeusedtoinitializetheweightsinafullytrainablerecurrentnet-work(withthehidden-to-hiddenrecurrentweightstrainedusingback-propagationthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever,;etal.,).Inthissetting,aninitialspectralradiusof1.2performswell,combined2013withthesparseinitializationschemedescribedinSec..8.4\\n407'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 422}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.9LeakyUnitsandOtherStrategiesforMultipleTimeScalesOnewaytodealwithlong-termdependenciesistodesignamodelthatoperatesatmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grainedtimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetimescalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.Variousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.Theseincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegratesignalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnectionsusedtomodelﬁne-grainedtimescales.10.9.1AddingSkipConnectionsthroughTimeOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesinthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnectionsdatesbackto()andfollowsfromtheideaofincorporatingdelaysinLinetal.1996feedforwardneuralnetworks(,).InanordinaryrecurrentLangandHinton1988network,arecurrentconnectiongoesfromaunitattimettoaunitattimet+1.Itispossibletoconstructrecurrentnetworkswithlongerdelays(,).Bengio1991AswehaveseeninSec.,gradientsmayvanishorexplodeexponentially8.2.5withrespecttothenumberoftimesteps.()introducedLinetal.1996recurrentconnectionswithatime-delayofdtomitigatethisproblem.Gradientsnowdiminishexponentiallyasafunctionofτdratherthanτ.Sincetherearebothdelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyinτ.Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotalllong-termdependenciesmayberepresentedwellinthisway.10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScalesAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneistohaveunitswithlinearself-connectionsandaweightnearoneontheseconnections.Whenweaccumulatearunningaverageµ()tofsomevaluev()tbyapplyingtheupdateµ()t←αµ(1)t−+(1−α)v()ttheαparameterisanexampleofalinearself-connectionfromµ(1)t−toµ()t.Whenαisnearone,therunningaverageremembersinformationaboutthepastforalongtime,andwhenαisnearzero,informationaboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscanbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleakyunits.408'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 423}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSSkipconnectionsthroughdtimestepsareawayofensuringthataunitcanalwayslearntobeinﬂuencedbyavaluefromdtimestepsearlier.Theuseofalinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthattheunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallowsthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valuedαratherthanbyadjustingtheinteger-valuedskiplength.Theseideaswereproposedby()andby().Mozer1992ElHihiandBengio1996Leakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks(,).Jaegeretal.2007Therearetwobasicstrategiesforsettingthetimeconstantsusedbyleakyunits. Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,forexamplebysamplingtheirvaluesfromsomedistributiononceatinitializationtime.Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-termdependencies(,;Mozer1992Pascanu2013aetal.,).10.9.3RemovingConnectionsAnotherapproachtohandlelong-termdependenciesistheideaoforganizingthestateoftheRNNatmultipletime-scales(,),withElHihiandBengio1996informationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.Thisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlierbecauseitinvolvesactivelyremovinglength-oneconnectionsandreplacingthemwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateonalongtimescale.Skipconnectionsthroughtimeaddedges.Unitsreceivingsuchnewconnectionsmaylearntooperateonalongtimescalebutmayalsochoosetofocusontheirothershort-termconnections.Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedtooperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,buttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.Thiswastheproposalin()andhasbeensuccessfullyusedinMozer1992Pascanuetal.().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace2013aatdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisistheapproachof()and().ItworkedElHihiandBengio1996Koutniketal.2014wellonanumberofbenchmarkdatasets.409'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 424}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.10TheLongShort-TermMemoryandOtherGatedRNNsAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplicationsarecalledgatedRNNslongshort-termmemory.Theseincludetheandnetworksbasedonthegatedrecurrentunit.Likeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthroughtimethathavederivativesthatneithervanishnorexplode.Leakyunits didthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwereparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychangeateachtimestep.Leakyunitsallowthenetworktoaccumulateinformation(suchasevidenceforaparticularfeatureorcategory)overalongduration.However,oncethatinformationhasbeenused,itmightbeusefulfortheneuralnetworktoforgettheoldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleakyunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismtoforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhentoclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.ThisiswhatgatedRNNsdo.10.10.1LSTMThecleverideaofintroducingself-loopstoproducepathswherethegradientcanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-termmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialadditionhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthanﬁxed(,).Bymakingtheweightofthisself-loopgated(controlledbyGersetal.2000anotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.Inthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescaleofintegrationcanchangebasedontheinputsequence,becausethetimeconstantsareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessfulinmanyapplications,suchasunconstrainedhandwritingrecognition(Gravesetal.,),speechrecognition(2009Graves2013GravesandJaitly2014etal.,;,),handwritinggeneration(Graves2013Sutskever2014,),machinetranslation(etal.,),imagecaptioning(,;Kirosetal.2014bVinyals2014bXu2015etal.,;etal.,)andparsing(Vinyals2014aetal.,).TheLSTMblockdiagramisillustratedinFig..Thecorresponding10.16forwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent410'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 425}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n×inputinput gateforget gateoutput gateoutput\\nstateself-loop×+×\\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnectedrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.Aninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbeaccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasalinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcanbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whiletheinputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasanextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.411'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 426}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSnetworkarchitecture.Deeperarchitectureshavealsobeensuccessfullyused(Gravesetal.,;2013Pascanu2014aetal.,).Insteadofaunitthatsimplyappliesanelement-wisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTMrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),inadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputsandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersandasystemofgatingunitsthatcontrolstheﬂowofinformation.Themostimportantcomponentisthestateunits()tithathasalinearself-loopsimilartotheleakyunitsdescribedintheprevioussection.However,here,theself-loopweight(ortheassociatedtimeconstant)iscontrolledbyaforgetgateunitf()ti(fortimesteptandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit:if()ti= σ\\uf8eb\\uf8edbfi+\\ue058jUfi,jx()tj+\\ue058jWfi,jh(1)t−j\\uf8f6\\uf8f8,(10.40)wherex()tisthecurrentinputvectorandh()tisthecurrenthiddenlayervector,containingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectivelybiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcellinternalstateisthusupdatedasfollows,butwithaconditionalself-loopweightf()ti:s()ti= f()tis(1)t−i+g()tiσ\\uf8eb\\uf8edbi+\\ue058jUi,jx()tj+\\ue058jWi,jh(1)t−j\\uf8f6\\uf8f8,(10.41)whereb,UandWrespectivelydenotethebiases,inputweightsandrecurrentweightsintotheLSTMcell.Theunitexternalinputgateg()tiiscomputedsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween0and1),butwithitsownparameters:g()ti= σ\\uf8eb\\uf8edbgi+\\ue058jUgi,jx()tj+\\ue058jWgi,jh(1)t−j\\uf8f6\\uf8f8.(10.42)Theoutputh()tioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgateq()ti,whichalsousesasigmoidunitforgating:h()ti= tanh\\ue010s()ti\\ue011q()ti(10.43)q()ti= σ\\uf8eb\\uf8edboi+\\ue058jUoi,jx()tj+\\ue058jWoi,jh(1)t−j\\uf8f6\\uf8f8(10.44)412'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 427}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrentweights,respectively.Amongthevariants,onecanchoosetousethecellstates()tiasanextrainput(withitsweight)intothethreegatesofthei-thunit,asshowninFig..Thiswouldrequirethreeadditionalparameters.10.16LSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasilythanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfortestingtheabilitytolearnlong-termdependencies(,;Bengioetal.1994HochreiterandSchmidhuber1997Hochreiter2001,;etal.,),thenonchallengingsequenceprocessingtaskswherestate-of-the-artperformancewasobtained(Graves2012,;Graves2013Sutskever2014etal.,;etal.,).VariantsandalternativestotheLSTMhavebeenstudiedandusedandarediscussednext.10.10.2OtherGatedRNNsWhichpieces oftheLSTMarchitectureareactually necessary?What othersuccessfularchitecturescouldbedesignedthatallowthenetworktodynamicallycontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?SomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,whoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,;Choetal.2014bChung20142015aJozefowicz2015Chrupala2015etal.,,;etal.,;etal.,).ThemaindiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolstheforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequationsarethefollowing:h()ti= u(1)t−ih(1)t−i+(1−u(1)t−i)σ\\uf8eb\\uf8edbi+\\ue058jUi,jx(1)t−j+\\ue058jWi,jr(1)t−jh(1)t−j\\uf8f6\\uf8f8,(10.45)whereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedasusual:u()ti= σ\\uf8eb\\uf8edbui+\\ue058jUui,jx()tj+\\ue058jWui,jh()tj\\uf8f6\\uf8f8(10.46)andr()ti= σ\\uf8eb\\uf8edbri+\\ue058jUri,jx()tj+\\ue058jWri,jh()tj\\uf8f6\\uf8f8.(10.47)Theresetandupdatesgatescanindividually“ignore”partsofthestatevector.Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany413'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 428}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSdimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletelyignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrolwhichpartsofthestategetusedtocomputethenexttargetstate,introducinganadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.Manymorevariantsaroundthisthemecanbedesigned.Forexampletheresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchasanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrolandlocalcontrol.However,severalinvestigationsoverarchitecturalvariationsoftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothoftheseacrossawiderangeoftasks(,;Greﬀetal.2015Jozefowicz2015Greﬀetal.,).etal.()foundthatacrucialingredientistheforgetgate,while2015Jozefowiczetal.()foundthataddingabiasof1totheLSTMforgetgate,apractice2015advocatedby(),makestheLSTMasstrongasthebestoftheGersetal.2000exploredarchitecturalvariants.10.11OptimizationforLong-TermDependenciesSec.andSec.havedescribedthevanishingandexplodinggradient8.2.510.7problemsthatoccurwhenoptimizingRNNsovermanytimesteps.AninterestingideaproposedbyMartensandSutskever2011()isthatsecondderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-orderoptimizationalgorithmsmayroughlybeunderstoodasdividingtheﬁrstderivativebythesecondderivative(inhigherdimension,multiplyingthegradientbytheinverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrstderivative,thentheratioofﬁrstandsecondderivativesmayremainrelativelyconstant.Unfortunately,second-ordermethodshavemanydrawbacks,includinghighcomputationalcost,theneedforalargeminibatch,andatendencytobeattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresultsusingsecond-ordermethods.Later,Sutskever2013etal.()foundthatsimplermethodssuchasNesterovmomentumwithcarefulinitializationcouldachievesimilarresults.SeeSutskever2012()formoredetail.BothoftheseapproacheshavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)appliedtoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoftenmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamorepowerfuloptimizationalgorithm.414'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 429}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.11.1ClippingGradientsAsdiscussedinSec.,stronglynonlinearfunctionssuchasthosecomputedby8.2.4arecurrentnetovermanytimestepstendtohavederivativesthatcanbeeitherverylargeorverysmallinmagnitude.ThisisillustratedinFig.andFig.,8.310.17inwhichweseethattheobjectivefunction(asafunctionoftheparameters)hasa“landscape”inwhichoneﬁnds“cliﬀs”:wideandratherﬂatregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,formingakindofcliﬀ.Thediﬃcultythatarisesisthatwhentheparametergradientisverylarge,agradientdescentparameterupdatecouldthrowtheparametersveryfar,intoaregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathadbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthatcorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthecurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmaybegintocurvebackupwards.Theupdatemustbechosentobesmallenoughtoavoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthatdecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearningrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeisofteninappropriateandcausesuphillmotionifweenteramorecurvedpartofthelandscapeonthenextstep.\\n415'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 430}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n\\ue077\\ue062\\ue04a\\ue077\\ue03b\\ue062\\ue028\\ue029\\ue057\\ue069\\ue074\\ue068\\ue06f\\ue075\\ue074\\ue020\\ue063\\ue06c\\ue069\\ue070\\ue070\\ue069\\ue06e\\ue067\\n\\ue077\\ue062\\ue04a\\ue077\\ue03b\\ue062\\ue028\\ue029\\ue057\\ue069\\ue074\\ue068\\ue020\\ue063\\ue06c\\ue069\\ue070\\ue070\\ue069\\ue06e\\ue067\\nFigure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwithtwoparameterswandb.Gradientclippingcanmakegradientdescentperformmorereasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccurinrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.Thecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrixismultipliedbyitselfonceforeachtimestep.(Left)Gradientdescentwithoutgradientclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradientfromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidetheaxesoftheplot.(Right)Gradientdescentwithgradientclippinghasamoremoderatereactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothatitcannotbepropelledawayfromsteepregionnearthesolution.FigureadaptedwithpermissionfromPascanu2013aetal.().Asimpletypeofsolutionhasbeeninusebypractitionersformanyyears:clippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;Pascanu2013aetal.,).Oneoptionistocliptheparametergradientfromaminibatch(element-wiseMikolov2012,)justbeforetheparameterupdate.Anotheristoclipthenorm||||gofthegradientg(Pascanu2013aetal.,)justbeforetheparameterupdate:if||||g>v(10.48)g←gv||||g(10.49)wherevisthenormthresholdandgisusedtoupdateparameters.Becausethegradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchasweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelattermethodhastheadvantagethatitguaranteesthateachstepisstillinthegradientdirection,butexperimentssuggestthatbothformsworksimilarly.Although416'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 431}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETStheparameterupdatehasthesamedirectionasthetruegradient,withgradientnormclipping,theparameterupdatevectornormisnowbounded.Thisboundedgradientavoidsperformingadetrimentalstepwhenthegradientexplodes.Infact,evensimplytakingarandomstepwhenthegradientmagnitudeisaboveathresholdtendstoworkalmostaswell.IftheexplosionissoseverethatthegradientisnumericallyInforNan(consideredinﬁniteornot-a-number),thenarandomstepofsizevcanbetakenandwilltypicallymoveawayfromthenumericallyunstableconﬁguration.Clippingthegradientnormper-minibatchwillnotchangethedirectionofthegradientforanindividualminibatch.However,takingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnotequivalenttoclippingthenormofthetruegradient(thegradientformedfromusingallexamples).Examplesthathavelargegradientnorm,aswellasexamplesthatappearinthesameminibatchassuchexamples,willhavetheircontributiontotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatchgradientdescent,wherethetruegradientdirectionisequaltotheaverageoverallminibatchgradients.Putanotherway,traditionalstochasticgradientdescentusesanunbiasedestimateofthegradient,whilegradientdescentwithnormclippingintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-wiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradientortheminibatchgradient,butitisstilladescentdirection.Ithasalsobeenproposed(Graves2013,)tocliptheback-propagatedgradient(withrespecttohiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;weconjecturethatallthesemethodsbehavesimilarly.10.11.2RegularizingtoEncourageInformationFlowGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwithvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-termdependencies,wediscussedtheideaofcreatingpathsinthecomputationalgraphoftheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociatedwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-loopsandgatingmechanisms,describedaboveinSec..Anotherideaistoregularize10.10orconstraintheparameterssoastoencourage“informationﬂow.” Inparticular,wewouldlikethegradientvector∇h()tLbeingback-propagatedtomaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputattheendofthesequence.Formally,wewant(∇h()tL)∂h()t∂h(1)t−(10.50)417'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 432}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETStobeaslargeas∇h()tL.(10.51)Withthisobjective,Pascanu2013aetal.()proposethefollowingregularizer:Ω =\\ue058t\\uf8eb\\uf8ed\\ue00c\\ue00c\\ue00c|∇(h()tL)∂h()t∂h(1)t−\\ue00c\\ue00c\\ue00c|||∇h()tL||−1\\uf8f6\\uf8f82.(10.52)Computingthegradientofthisregularizermayappeardiﬃcult,butPascanuetal.()proposeanapproximationinwhichweconsidertheback-propagated2013avectors∇h()tLasiftheywereconstants(forthepurposeofthisregularizer,sothatthereisnoneedtoback-propagatethroughthem).Theexperimentswiththisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(whichhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanofthedependenciesthatanRNNcanlearn.BecauseitkeepstheRNNdynamicsontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfortaskswheredataisabundant,suchaslanguagemodeling.10.12ExplicitMemoryIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,whichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,therearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-conscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooksdiﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelativelystraightforwardtoputintowords—everydaycommonsenseknowledge,like“acatisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplishyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom141.”Neuralnetworksexcelatstoringimplicitknowledge.However,theystruggletomemorizefacts.Stochasticgradientdescentrequiresmanypresentationsofthesameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,thatinputwillnotbestoredespeciallyprecisely.Graves2014betal.()hypothesizedthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemorysystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesofinformationthatarerelevanttoachievingsomegoal.Suchexplicitmemory418'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 433}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nTask network,controlling the memoryMemory cellsWritingmechanismReadingmechanism\\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturingsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwedistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrentnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcanstorefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfromandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,indicatedbyboldarrowspointingatthereadingandwritingaddresses).\\n419'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 434}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETScomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”storeandretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem. Theneedforneuralnetworksthatcanprocessinformationinasequenceofsteps,changingthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognizedasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitiveresponsestotheinput(,).Hinton1990Toresolvethisdiﬃculty,Weston2014etal.()introducedthatmemorynetworksincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmechanism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthemhowtousetheirmemorycells.Graves2014betal.()introducedtheneuralTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontenttomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,andallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseofacontent-basedsoftattentionmechanism(see()andSec.).ThisBahdanauetal.201512.4.5.1softaddressingmechanismhasbecomestandardwithotherrelatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallowsgradient-basedopti-mization(,;Sukhbaataretal.2015JoulinandMikolov2015Kumar2015,;etal.,;Vinyals2015aGrefenstette2015etal.,;etal.,).EachmemorycellcanbethoughtofasanextensionofthememorycellsinLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstatethatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesinadigitalcomputerreadfromorwritetoaspeciﬁcaddress.Itisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.Toalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycellssimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,theymodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperationsarechosentobefocusedonasmallnumberofcells,forexample,byproducingthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallowsthefunctionscontrollingaccesstothememorytobeoptimizedusinggradientdescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshouldbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthosememoryaddressesreceivingalargecoeﬃcient.Thesememorycellsaretypicallyaugmentedtocontainavector,ratherthanthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasonstoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthecostofaccessingamemorycell.Wepaythecomputationalcostofproducingacoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmallnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan420'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 435}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSoﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthattheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoorwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrieveacompletevector-valuedmemoryifweareabletoproduceapatternthatmatchessomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecanrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-basedreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘Weallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwemaketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredinaseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,location-basedaddressingisnotallowedtorefertothecontentofthememory.Wecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsofthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensiblemechanismevenwhenthememorycellsaresmall.Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,thentheinformationitcontainscanbepropagatedforwardintimeandthegradientspropagatedbackwardintimewithouteithervanishingorexploding.TheexplicitmemoryapproachisillustratedinFig.,whereweseethat10.18a“taskneuralnetwork”iscoupledwithamemory.Althoughthattaskneuralnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.Thetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses.ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTMRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationandgradientscanbepropagated(forwardintimeorbackwardsintime,respectively)forverylongdurations.Asanalternativetoback-propagationthroughweightedaveragesofmemorycells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilitiesandstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodelsthatmakediscretedecisionsrequiresspecializedoptimizationalgorithms,describedinSec..Sofar,trainingthesestochasticarchitecturesthatmakediscrete20.9.1decisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoftdecisions.Whetheritissoft(allowingback-propagation)orstochasticandhard,themech-anismforchoosinganaddressisinitsformidenticaltotheattentionmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachinetranslation(Bah-danau2015etal.,)anddiscussedinSec..Theideaofattentionmechanisms12.4.5.1forneuralnetworkswasintroducedevenearlier,inthecontextofhandwritinggeneration(Graves2013,),withanattentionmechanismthatwasconstrainedto421'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 436}, page_content='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSmoveonlyforwardintimethroughthesequence.Inthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusofattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequentialdata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnowmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-worldtasks.\\n422'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 437}, page_content='Chapter11PracticalMethodologySuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagoodknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowtheywork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochooseanalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedbackobtainedfromexperimentsinordertoimproveamachinelearningsystem.Duringdaytodaydevelopmentofmachinelearningsystems,practitionersneedtodecidewhethertogathermoredata,increaseordecreasemodelcapacity,addorremoveregularizingfeatures,improvetheoptimizationofamodel,improveapproximateinferenceinamodel,ordebugthesoftwareimplementationofthemodel.Alloftheseoperationsareattheveryleasttime-consumingtotryout,soitisimportanttobeabletodeterminetherightcourseofactionratherthanblindlyguessing.Mostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-rithms,andobjectivefunctions.Thismaygivetheimpressionthatthemostimportantingredienttobeingamachinelearningexpertisknowingawidevarietyofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-tice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplacealgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationofanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyoftherecommendationsinthischapterareadaptedfrom().Ng2015Werecommendthefollowingpracticaldesignprocess:•Determineyourgoals—whaterrormetrictouse,andyourtargetvalueforthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbytheproblemthattheapplicationisintendedtosolve.•Establishaworkingend-to-endpipelineassoonaspossible,includingthe423'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 438}, page_content='CHAPTER11.PRACTICALMETHODOLOGYestimationoftheappropriateperformancemetrics.•Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-nosewhichcomponentsareperformingworsethanexpectedandwhetheritisduetooverﬁtting,underﬁtting,oradefectinthedataorsoftware.•Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjustinghyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfromyourinstrumentation.Asarunningexample,wewilluseStreetViewaddressnumbertranscriptionsystem(,).ThepurposeofthisapplicationistoaddGoodfellowetal.2014dbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecordtheGPScoordinatesassociatedwitheachphotograph.Aconvolutionalnetworkrecognizestheaddressnumberineachphotograph,allowingtheGoogleMapsdatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthiscommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesignmethodologyweadvocate.Wenowdescribeeachofthestepsinthisprocess.11.1PerformanceMetricsDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrststepbecauseyourerrormetricwillguideallofyourfutureactions. Youshouldalsohaveanideaofwhatlevelofperformanceyoudesire.Keepinmindthatformostapplications,itisimpossibletoachieveabsolutezeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopetoachieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobabilitydistribution.This isbecause your inputfeatures maynot contain completeinformationabouttheoutputvariable,orbecausethesystemmightbeintrinsicallystochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyourgoalistobuildthebestpossiblereal-worldproductorservice,youcantypicallycollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweighthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,money,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolvesperforminginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestionaboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark424'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 439}, page_content='CHAPTER11.PRACTICALMETHODOLOGYspeciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollectmoredata.Howcanonedetermineareasonablelevelofperformancetoexpect?Typically,intheacademicsetting,wehavesomeestimateoftheerrorratethatisattainablebasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,wehavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,cost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealisticdesirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.Anotherimportantconsiderationbesidesthetargetvalueoftheperformancemetricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetricsmaybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludesmachinelearningcomponents.Theseperformancemetricsareusuallydiﬀerentfromthecostfunctionusedtotrainthemodel. AsdescribedinSec. ,itis5.1.2commontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.However,manyapplicationsrequiremoreadvancedmetrics.Sometimesitismuchmorecostlytomakeonekindofamistakethananother.Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:incorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowingaspammessagetoappearintheinbox.Itismuchworsetoblockalegitimatemessagethantoallowaquestionablemessagetopassthrough.Ratherthanmeasuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeformoftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecostofallowingspammessages.Sometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsomerareevent.Forexample,wemightdesignamedicaltestforararedisease.Supposethatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁertoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwaytocharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemistoinsteadmeasureprecisionrecalland.Precisionisthefractionofdetectionsreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueeventsthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieveperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthediseasewouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewhohavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleinamillionhave). Whenusingprecisionandrecall,itiscommontoplota,withPRcurveprecisiononthey-axisandrecallonthex-axis.Theclassiﬁergeneratesascorethatishigheriftheeventtobedetectedoccurred. Forexample,afeedforward425'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 440}, page_content='CHAPTER11.PRACTICALMETHODOLOGYnetworkdesignedtodetectadiseaseoutputsˆy=P(y=1|x),estimatingtheprobabilitythatapersonwhosemedicalresultsaredescribedbyfeaturesxhasthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssomethreshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmanycases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumberratherthanacurve.Todoso,wecanconvertprecisionpandrecallrintoanF-scoregivenbyF=2prpr+.(11.1)AnotheroptionistoreportthetotalarealyingbeneaththePRcurve.Insomeapplications,itispossibleforthemachinelearningsystemtorefusetomakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimatehowconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncanbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreetViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskistotranscribetheaddressnumberfromaphotographinordertoassociatethelocationwherethephotowastakenwiththecorrectaddressinamap.Becausethevalueofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoaddanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystemthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,thenthebestcourseofactionistoallowahumantotranscribethephotoinstead.Ofcourse,themachinelearningsystemisonlyusefulifitisabletodramaticallyreducetheamountofphotosthatthehumanoperatorsmustprocess.Anaturalperformancemetrictouseinthissituationiscoverage.Coverageisthefractionofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.Itispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracybyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.FortheStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscriptionaccuracywhilemaintaining95%coverage.Human-levelperformanceonthistaskis98%accuracy.Manyothermetricsarepossible.Wecanforexample,measureclick-throughrates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplicationareashaveapplication-speciﬁccriteriaaswell.Whatisimportantistodeterminewhichperformancemetrictoimproveaheadoftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,itcanbediﬃculttotellwhetherchangestoamachinelearningsystemmakeprogressornot.426'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 441}, page_content='CHAPTER11.PRACTICALMETHODOLOGY11.2DefaultBaselineModelsAfterchoosingperformancemetricsandgoals, thenextstepinanypracticalapplicationistoestablishareasonableend-to-endsystemassoonaspossible.Inthissection,weproviderecommendationsforwhichalgorithmstouseastheﬁrstbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearchprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoonafterthiswriting.Dependingonthecomplexityofyourproblem,youmayevenwanttobeginwithoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedbyjustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimplestatisticalmodellikelogisticregression.Ifyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobjectrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikelytodowellbybeginningwithanappropriatedeeplearningmodel.First,choosethegeneralcategoryofmodelbasedonthestructureofyourdata.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,useafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknowntopologicalstructure(forexample,iftheinputisanimage),useaconvolutionalnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinearunit(ReLUsortheirgeneralizationslikeLeakyReLUs,PreLusandmaxout).Ifyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).AreasonablechoiceofoptimizationalgorithmisSGDwithmomentumwithadecayinglearningrate(populardecayschemesthatperformbetterorworseondiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearningrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10eachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.Batchnormalizationcanhaveadramaticeﬀectonoptimizationperformance,especiallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.Whileitisreasonabletoomitbatchnormalizationfromtheveryﬁrstbaseline,itshouldbeintroducedquicklyifoptimizationappearstobeproblematic.Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,youshouldincludesomemildformsofregularizationfromthestart.Earlystoppingshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasytoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batchnormalizationalsosometimesreducesgeneralizationerrorandallowsdropouttobeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalizeeachvariable.427'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 442}, page_content='CHAPTER11.PRACTICALMETHODOLOGYIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,youwillprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalreadyknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopyatrainedmodelfromthattask.Forexample,itiscommontousethefeaturesfromaconvolutionalnetworktrainedonImageNettosolveothercomputervisiontasks(,).Girshicketal.2015Acommonquestioniswhethertobeginbyusingunsupervisedlearning,de-scribedfurtherinPart.Thisissomewhatdomainspeciﬁc.Somedomains,suchIIIasnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-visedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inotherdomains,suchascomputervision,currentunsupervisedlearningtechniquesdonotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberoflabeledexamplesisverysmall(,;Kingmaetal.2014Rasmus2015etal.,).Ifyourapplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,thenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervisedlearninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.Youcanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitialbaselineoverﬁts.11.3DeterminingWhethertoGatherMoreDataAftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-manceofthealgorithmanddeterminehowtoimproveit.Manymachinelearningnovicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms.However,itisoftenmuchbettertogathermoredatathantoimprovethelearningalgorithm.Howdoesonedecidewhethertogathermoredata?First,determinewhethertheperformanceonthetrainingsetisacceptable.Ifperformanceonthetrainingsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalreadyavailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthesizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearningratehyperparameter.Iflargemodelsandcarefullytunedoptimizationalgorithmsdonotworkwell,thentheproblemmightbethequalityofthetrainingdata.Thedatamaybetoonoisyormaynotincludetherightinputsneededtopredictthedesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectingarichersetoffeatures.Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-428'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 443}, page_content='CHAPTER11.PRACTICALMETHODOLOGYformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,thenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethantrainingsetperformance,thengatheringmoredataisoneofthemosteﬀectivesolutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmoredata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andtheamountofdatathatisexpectedtobenecessarytoimprovetestsetperformancesigniﬁcantly. Atlargeinternetcompanieswithmillionsorbillionsofusers,itisfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderablylessthantheotheralternatives,sotheanswerisalmostalwaystogathermoretrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneofthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchasmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimplealternativetogatheringmoredataistoreducethesizeofthemodelorimproveregularization,byadjustinghyperparameterssuchasweightdecaycoeﬃcients,orbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegapbetweentrainandtestperformanceisstillunacceptableevenaftertuningtheregularizationhyperparameters,thengatheringmoredataisadvisable.Whendecidingwhethertogathermoredata,itisalsonecessarytodecidehowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetweentrainingsetsizeandgeneralizationerror,likeinFig..Byextrapolatingsuch5.4curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededtoachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotalnumberofexampleswillnothaveanoticeableimpactongeneralizationerror.Itisthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,forexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprovegeneralizationerroristoimprovethelearningalgorithmitself.Thisbecomesthedomainofresearchandnotthedomainofadviceforappliedpractitioners.11.4SelectingHyperparametersMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmanyaspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetimeandmemorycostofrunningthealgorithm.Someofthesehyperparametersaﬀectthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfercorrectresultswhendeployedonnewinputs.Therearetwobasicapproachestochoosingthesehyperparameters:choosingthemmanuallyandchoosingthemautomatically.Choosingthehyperparameters429'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 444}, page_content='CHAPTER11.PRACTICALMETHODOLOGYmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachinelearningmodelsachievegoodgeneralization.Automatichyperparameterselectionalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoftenmuchmorecomputationallycostly.11.4.1ManualHyperparameterTuningTosethyperparametersmanually,onemustunderstandtherelationshipbetweenhyperparameters,trainingerror,generalizationerrorandcomputationalresources(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-damentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfromChapter.5Thegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-izationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshowtodeterminetheruntimeandmemoryimpactofvarioushyperparametersherebecausethisishighlyplatform-dependent.Theprimarygoalofmanualhyperparametersearchistoadjusttheeﬀectivecapacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacityisconstrainedbythreefactors: therepresentationalcapacityofthemodel,theabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedtotrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedureregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhashigherrepresentationalcapacity—itiscapableofrepresentingmorecomplicatedfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,ifthetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobofminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbidsomeofthesefunctions.ThegeneralizationerrortypicallyfollowsaU-shapedcurvewhenplottedasafunctionofoneofthehyperparameters,asinFig..Atoneextreme,the5.3hyperparametervaluecorrespondstolowcapacity,andgeneralizationerrorishighbecausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,thehyperparametervaluecorrespondstohighcapacity,andthegeneralizationerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhereinthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossiblegeneralizationerror,byaddingamediumgeneralizationgaptoamediumamountoftrainingerror.Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-parameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample,430'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 445}, page_content='CHAPTER11.PRACTICALMETHODOLOGYbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyperparame-terissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzerocorrespondstothegreatesteﬀectivecapacityofthelearningalgorithm.NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.Manyhyperparametersarediscrete,suchasthenumberofunitsinalayerorthenumberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpointsalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparametersareswitchesthat specify whetherornotto usesomeoptionalcomponentofthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinputfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.Thesehyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparametershavesomeminimumormaximumvaluethatpreventsthemfromexploringsomepartofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.Thismeansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotentertheoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,somehyperparameterscanonlysubtractcapacity.Thelearningrateisperhapsthemostimportanthyperparameter.Ifyouhave timeto tuneonly onehyperparameter,tune thelearning rate.It con-trolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanotherhyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearningrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespe-ciallylargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,illustratedinFig..Whenthelearningrateistoolarge,gradientdescent11.1caninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealizedquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasitsoptimalvalue(,).Whenthelearningrateistoosmall,trainingLeCunetal.1998aisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).Tuningtheparametersotherthanthelearningraterequiresmonitoringbothtrainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,thenadjustingitscapacityappropriately.Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhavenochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouareconﬁdentthatyouroptimizationalgorithmisperformingcorrectly,thenyoumustaddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,thisincreasesthecomputationalcostsassociatedwiththemodel.Ifyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan431'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 446}, page_content='CHAPTER11.PRACTICALMETHODOLOGY\\n10−210−1100Learningrate(logarithmicscale)012345678Trainingerror\\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Noticethesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxedtrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbyafactorproportionaltothelearningratereduction. Generalizationerrorcanfollowthiscurveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeortoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorpreventoverﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralizationerror.nowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorandthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytradingoﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetrainingerrorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarilydrivenbythegapbetweentrainandtesterror. Yourgoalistoreducethisgapwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,changeregularizationhyperparameterstoreduceeﬀectivemodelcapacity,suchasbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfromalargemodelthatisregularizedwell,forexamplebyusingdropout.Mosthyperparameterscanbesetbyreasoningaboutwhethertheyincreaseordecreasemodelcapacity.SomeexamplesareincludedinTable.11.1Whilemanuallytuninghyperparameters,donotlosesightofyourendgoal:goodperformanceonthetestset.Addingregularizationisonlyonewaytoachievethisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-izationerrorbycollectingmoretrainingdata.Thebruteforcewaytopracticallyguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsizeuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputationalcostoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In432'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 447}, page_content='CHAPTER11.PRACTICALMETHODOLOGYHyperparameterIncreasescapacitywhen...ReasonCaveatsNumberofhid-denunitsincreasedIncreasingthenumberofhiddenunitsincreasestherepresentationalcapacityofthemodel.Increasingthenumberofhiddenunits increasesboththetimeandmemorycostofessentiallyeveryop-erationonthemodel.Learningratetunedop-timallyAnimproperlearningrate,whether toohigh ortoolow,resultsinamodelwithloweﬀectivecapacityduetooptimizationfailureConvolutionker-nelwidthincreasedIncreasingthekernelwidthincreasesthenumberofpa-rametersinthemodelAwiderkernelresultsinanarroweroutputdimen-sion,reducingmodelca-pacityunlessyouuseim-plicitzeropaddingtore-ducethiseﬀect.Widerkernelsrequiremoremem-oryforparameterstorageandincreaseruntime,butanarroweroutputreducesmemorycost.ImplicitzeropaddingincreasedAddingimplicitzerosbe-foreconvolutionkeepstherepresentationsizelargeIncreasedtimeandmem-orycostofmostopera-tions.Weightdecayco-eﬃcientdecreasedDecreasingtheweightde-caycoeﬃcientfreesthemodelparameterstobe-comelargerDropoutratedecreasedDroppingunitslessoftengivestheunitsmoreoppor-tunitiesto“conspire”witheachothertoﬁtthetrain-ingsetTable11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.433'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 448}, page_content='CHAPTER11.PRACTICALMETHODOLOGYprinciple,thisapproachcouldfailduetooptimizationdiﬃculties,butformanyproblemsoptimizationdoesnotseemtobeasigniﬁcantbarrier,providedthatthemodelischosenappropriately.11.4.2AutomaticHyperparameterOptimizationAlgorithmsTheideallearningalgorithmjusttakesadatasetandoutputsafunction,withoutrequiringhand-tuningofhyperparameters.ThepopularityofseverallearningalgorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilitytoperformwellwithonlyoneortwotunedhyperparameters.Neuralnetworkscansometimesperformwellwithonlyasmallnumberoftunedhyperparameters,butoftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters.Manualhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,suchasonedeterminedbyothershavingworkedonthesametypeofapplicationandarchitecture,orwhentheuserhasmonthsoryearsofexperienceinexploringhyperparametervaluesforneuralnetworksappliedtosimilartasks.However,formanyapplications,thesestartingpointsarenotavailable.Inthesecases,automatedalgorithmscanﬁndusefulvaluesofthehyperparameters.Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesforgoodvaluesofthehyperparameters,werealizethatanoptimizationistakingplace:wearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjectivefunction,suchasvalidationerror,sometimesunderconstraints(suchasabudgetfortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,todevelophyperparameteroptimizationalgorithmsthatwrapalearningalgorithmandchooseitshyperparameters,thushidingthehyperparametersofthelearningalgorithmfromtheuser.Unfortunately,hyperparameteroptimizationalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthatshouldbeexploredforeachofthelearningalgorithm’shyperparameters.However,thesesecondaryhyperparametersareusuallyeasiertochoose,inthesensethatacceptableperformancemaybeachievedonawiderangeoftasksusingthesamesecondaryhyperparametersforalltasks.11.4.3GridSearchWhentherearethreeorfewerhyperparameters,thecommonpracticeistoperformgridsearch.Foreachhyperparameter,theuserselectsasmallﬁnitesetofvaluestoexplore.ThegridsearchalgorithmthentrainsamodelforeveryjointspeciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvaluesforeachindividualhyperparameter.Theexperimentthatyieldsthebestvalidationset434'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 449}, page_content='CHAPTER11.PRACTICALMETHODOLOGY\\nGridRandomFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswedisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore.(Left)Toperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearchalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthesesets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint(Right)hyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependentfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterincludeuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofasamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjointhyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearchandrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcantinﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxishasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponentialinthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsauniquevalueofeveryinﬂuentialhyperparameteronnearlyeverytrial.\\n435'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 450}, page_content='CHAPTER11.PRACTICALMETHODOLOGYerroristhenchosenashavingfoundthebesthyperparameters.SeetheleftofFig.foranillustrationofagridofhyperparametervalues.11.2Howshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical(ordered)hyperparameters,thesmallestandlargestelementofeachlistischosenconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesurethattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agridsearchinvolvespickingvaluesapproximatelyonalogarithmicscale,e.g.,alearningratetakenwithintheset{.1,.01,10−3,10−4,10−5},oranumberofhiddenunitstakenwiththeset.{}5010020050010002000,,,,,Gridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,supposethatweranagridsearchoverahyperparameterαusingvaluesof{−1,0,1}.Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest1αliesandweshouldshiftthegridandrunanothersearchwithαin,forexample,{1,2,3}.Ifweﬁndthatthebestvalueofαis,thenwemaywishtoreﬁneour0estimatebyzoominginandrunningagridsearchover.{−}.,,.101Theobviousproblemwithgridsearchisthatitscomputationalcostgrowsexponentiallywiththenumberofhyperparameters.Iftherearemhyperparameters,eachtakingatmostnvalues,thenthenumberoftrainingandevaluationtrialsrequiredgrowsasO(nm).Thetrialsmayberuninparallelandexploitlooseparallelism(withalmostnoneedforcommunicationbetweendiﬀerentmachinescarryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,evenparallelizationmaynotprovideasatisfactorysizeofsearch.11.4.4RandomSearchFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,moreconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters:randomsearch(,).BergstraandBengio2012Arandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistributionforeachhyperparameter,e.g.,aBernoulliormultinoulliforbinaryordiscretehyperparameters,orauniformdistributiononalog-scaleforpositivereal-valuedhyperparameters.Forexample,loglearningrate__∼−−u(1,5)(11.2)learningrate_= 10loglearningrate__.(11.3)whereu(a,b)indicatesasampleoftheuniformdistributionintheinterval(a,b).Similarlythelognumberofhiddenunits____maybesampledfromu(log(50),log(2000)).436'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 451}, page_content='CHAPTER11.PRACTICALMETHODOLOGYUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevaluesofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoesnotincuradditionalcomputationalcost.Infact,asillustratedinFig.,a11.2randomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthereareseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure.Thisisstudiedatlengthin(),whofoundthatrandomBergstraandBengio2012searchreducesthevalidationseterrormuchfasterthangridsearch,intermsofthenumberoftrialsrunbyeachmethod.Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandomsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.Themainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearchisthatthetherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,whentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters)wouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameterswouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,theywouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovaluesdoesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,gridsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearchwillstillgivetwoindependentexplorationsoftheotherhyperparameters.11.4.5Model-BasedHyperparameterOptimizationThesearchforgoodhyperparameterscanbecastasanoptimizationproblem.Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthevalidationseterrorthatresultsfromtrainingusingthesehyperparameters.Insimpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiableerrormeasureonthevalidationsetwithrespecttothehyperparameters,wecansimplyfollowthisgradient(,;,;,Bengioetal.1999Bengio2000Maclaurinetal.2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,eitherduetoitshighcomputationandmemorycost,orduetohyperparametershavingintrinsicallynon-diﬀerentiableinteractionswiththevalidationseterror,asinthecaseofdiscrete-valuedhyperparameters.Tocompensateforthislackofagradient,wecanbuildamodelofthevalidationseterror,thenproposenewhyperparameterguessesbyperformingoptimizationwithinthismodel.Mostmodel-basedalgorithmsforhyperparametersearchuseaBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationseterrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-mizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters437'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 452}, page_content='CHAPTER11.PRACTICALMETHODOLOGYforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmayalsoperformpoorly)andexploitation(proposinghyperparameterswhichthemodelisconﬁdentwillperformaswellasanyhyperparametersithasseensofar—usuallyhyperparametersthatareverysimilartoonesithasseenbefore).ContemporaryapproachestohyperparameteroptimizationincludeSpearmint(,),Snoeketal.2012TPE(,)andSMAC(,).Bergstraetal.2011Hutteretal.2011Currently,wecannotunambiguouslyrecommendBayesianhyperparameteroptimizationasanestablishedtoolforachievingbetterdeeplearningresultsorforobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimizationsometimesperformscomparablytohumanexperts,sometimesbetter,butfailscatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworksonaparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeingsaid,hyperparameteroptimizationisanimportantﬁeldofresearchthat,whileoftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁtnotonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringingeneral.Onedrawbackcommontomosthyperparameteroptimizationalgorithmswithmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-perimenttoruntocompletionbeforetheyareabletoextractanyinformationfromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-mationcanbegleanedearlyinanexperiment,thanmanualsearchbyahumanpractitioner,sinceonecanusuallytellearlyonifsomesetofhyperparametersiscompletelypathological.()haveintroducedanearlyversionSwerskyetal.2014ofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustimepoints,thehyperparameteroptimizationalgorithmcanchoosetobeginanewexperiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”andresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggivenmoreinformation.11.5DebuggingStrategiesWhenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotellwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthereisabugintheimplementationofthealgorithm. Machinelearningsystemsarediﬃculttodebugforavarietyofreasons.Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthealgorithmis.Infact,theentirepointofusingmachinelearningisthatitwilldiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina438'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 453}, page_content='CHAPTER11.PRACTICALMETHODOLOGYneuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehavenewnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimalbehavior.Afurtherdiﬃcultyisthatmostmachinelearningmodelshavemultiplepartsthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstillachieveroughlyacceptableperformance.Forexample,supposethatwearetraininganeuralnetwithseverallayersparametrizedbyweightsWandbiasesb.Supposefurtherthatwehavemanuallyimplementedthegradientdescentruleforeachparameterseparately,andwemadeanerrorintheupdateforthebiases:bb←−α(11.4)whereαisthelearningrate.Thiserroneousupdatedoesnotusethegradientatall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,whichisclearlynotacorrectimplementationofanyreasonablelearningalgorithm.Thebugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.Dependingonthedistributionoftheinput,theweightsmaybeabletoadapttocompensateforthenegativebiases.Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneorbothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthecorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesonepartoftheneuralnetimplementationinisolation.Someimportantdebuggingtestsinclude:Visualizethemodelinaction:Whentrainingamodeltodetectobjectsinimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayedsuperimposedontheimage.Whentrainingagenerativemodelofspeech,listentosomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasytofallintothepracticeofonlylookingatquantitativeperformancemeasurementslikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodelperformingitstaskwillhelpyoutodeterminewhetherthequantitativeperformancenumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemostdevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemisperformingwellwhenitisnot.Visualizetheworstmistakes: Mostmodelsareabletooutputsomesortofconﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedonasoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassignedtothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasinitsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthesevaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,439'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 454}, page_content='CHAPTER11.PRACTICALMETHODOLOGYbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallylesslikelytobecorrectlylabeledreceivesmallerprobabilitiesunderthemodel.Byviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecanoftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.Forexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwheretheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomitsomeofthedigits.Thetranscriptionnetworkthenassignedverylowprobabilitytothecorrectanswerontheseimages.Sortingtheimagestoidentifythemostconﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping.Modifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetterperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneededtobeabletoprocessgreatervariationinthepositionandscaleoftheaddressnumbers.Reasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃculttodeterminewhethertheunderlyingsoftwareiscorrectlyimplemented.Somecluescanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterrorishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthemodelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibilityisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthemodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdatawasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorarehigh,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhetherthemodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenariorequiresfurthertests,describednext.Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhetheritisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmallmodelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,aclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiasesoftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectlylabelasingleexample,anautoencodertosuccessfullyreproduceasingleexamplewithhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblingasingleexample,thereisasoftwaredefectpreventingsuccessfuloptimizationonthetrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.Compareback-propagatedderivativestonumericalderivatives:Ifyouareusingasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-putations,orifyouareaddinganewoperationtoadiﬀerentiationlibraryandmustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthisgradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect440'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 455}, page_content='CHAPTER11.PRACTICALMETHODOLOGYistocomparethederivativescomputedbyyourimplementationofautomaticdiﬀerentiationtothederivativescomputedbyaﬁnitediﬀerences.Becausef\\ue030() =limx\\ue00f→0fx\\ue00ffx(+)−()\\ue00f,(11.5)wecanapproximatethederivativebyusingasmall,ﬁnite:\\ue00ff\\ue030() x≈fx\\ue00ffx(+)−()\\ue00f.(11.6)Wecanimprovetheaccuracyoftheapproximationbyusingthecentereddiﬀerence:f\\ue030() x≈fx(+12\\ue00ffx)−(−12\\ue00f)\\ue00f.(11.7)Theperturbationsize\\ue00fmustchosentobelargeenoughtoensurethatthepertur-bationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunctiong:Rm→Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasinglederivativeatatime.Wecaneitherrunﬁnitediﬀerencingmntimestoevaluateallofthepartialderivativesofg,orwecanapplythetesttoanewfunctionthatusesrandomprojectionsatboththeinputandoutputofg.Forexample,wecanapplyourtestoftheimplementationofthederivativestof(x)wheref(x) =uTg(vx),whereuandvarerandomlychosenvectors.Computingf\\ue030(x)correctlyrequiresbeingabletoback-propagatethroughgcorrectly,yetiseﬃcienttodowithﬁnitediﬀerencesbecausefhasonlyasingleinputandasingleoutput.Itisusuallyagoodideatorepeatthistestformorethanonevalueofuandvtoreducethechancethatthetestoverlooksmistakesthatareorthogonaltotherandomprojection.Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereisaveryeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbersasinputtothefunction(SquireandTrapp1998,).Themethodisbasedontheobservationthatfxi\\ue00ffxi\\ue00ff(+) = ()+\\ue030()+(xO\\ue00f2)(11.8)real((+)) = ()+(fxi\\ue00ffxO\\ue00f2)imag(,fxi\\ue00f(+)\\ue00f) = f\\ue030()+(xO\\ue00f2),(11.9)wherei=√−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀectduetotakingthediﬀerencebetweenthevalueoffatdiﬀerentpoints.Thisallowstheuseoftinyvaluesof\\ue00flike\\ue00f= 10−150,whichmaketheO(\\ue00f2)errorinsigniﬁcantforallpracticalpurposes.441'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 456}, page_content='CHAPTER11.PRACTICALMETHODOLOGYMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualizestatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamountoftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunitscantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,howoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,theaverageoftheabsolutevalueofthepre-activationstellsushowsaturatedtheunitis.Inadeepnetworkwherethepropagatedgradientsquicklygroworquicklyvanish,optimizationmaybehampered.Finally,itisusefultocomparethemagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.Assuggestedby(),wewouldlikethemagnitudeofparameterupdatesBottou2015overaminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,not50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmaybethatsomegroupsofparametersaremovingatagoodpacewhileothersarestalled.Whenthedataissparse(likeinnaturallanguage),someparametersmaybeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheirevolution.Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabouttheresultsproducedateachstep.Forexample,inPart,wewillseesomeIIIapproximateinferencealgorithmsthatworkbyusingalgebraicsolutionstoop-timizationproblems.Typicallythesecanbedebuggedbytestingeachoftheirguarantees.Someguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjectivefunctionwillneverincreaseafteronestepofthealgorithm,thatthegradientwithrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,andthatthegradientwithrespecttoallvariableswillbezeroatconvergence.Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigitalcomputer,sothedebuggingtestshouldincludesometoleranceparameter.11.6Example:Multi-DigitNumberRecognitionToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodologyinpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,fromthepointofviewofdesigningthedeeplearningcomponents.Obviously,manyothercomponentsofthecompletesystem,suchastheStreetViewcars,thedatabaseinfrastructure,andsoon,wereofparamountimportance.Fromthepointofviewofthemachinelearningtask,theprocessbeganwithdatacollection. Thecarscollectedtherawdataandhumanoperatorsprovidedlabels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset442'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 457}, page_content='CHAPTER11.PRACTICALMETHODOLOGYcuration,includingusingothermachinelearningtechniquestodetectthehousenumberspriortotranscribingthem.Thetranscriptionprojectbeganwithachoiceofperformancemetricsanddesiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthechoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyusefuliftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirementforthisproject. Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy.Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreachthislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage.Coveragethusbecamethemainperformancemetricoptimizedduringtheproject,withaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecamepossibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusestotranscribetheinput,eventuallyexceedingthegoalof95%coverage.Afterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-ogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansaconvolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbeganwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetworktooutputasequenceofpredictions.Inordertobeginwiththesimplestpossiblebaseline,theﬁrstimplementationoftheoutputlayerofthemodelconsistedofndiﬀerentsoftmaxunitstopredictasequenceofncharacters.Thesesoftmaxunitsweretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmaxunittrainedindependently.Ourrecommendedmethodologyistoiterativelyreﬁnethebaselineandtestwhethereachchangemakesanimprovement.TheﬁrstchangetotheStreetViewtranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoveragemetricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassifyaninputxwhenevertheprobabilityoftheoutputsequencep(yx|)<tforsomethresholdt.Initially,thedeﬁnitionofp(yx|)wasad-hoc,basedonsimplymultiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopmentofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipledlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunctionmuchmoreeﬀectively.Atthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoreticalproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrumentthetrainandtestsetperformanceinordertodeterminewhethertheproblemisunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearlyidentical.Indeed,themainreasonthisprojectproceededsosmoothlywastheavailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain443'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 458}, page_content='CHAPTER11.PRACTICALMETHODOLOGYandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherduetounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebuggingstrategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,thatmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethehighestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinputimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeingremovedbythecroppingoperation.Forexample,aphotoofanaddress“1849”mightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblemcouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddressnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,theteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthecropregiontobesystematicallywiderthantheaddressnumberdetectionsystempredicted.Thissinglechangeaddedtenpercentagepointstothetranscriptionsystem’scoverage.Finally,thelastfewpercentagepointsofperformancecamefromadjustinghyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-tainingsomerestrictionsonitscomputationalcost.Becausetrainandtesterrorremainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitswereduetounderﬁtting,aswellasduetoafewremainingproblemswiththedatasetitself.Overall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsofmillionsofaddressestobetranscribedbothfasterandatlowercostthanwouldhavebeenpossibleviahumaneﬀort.Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomanyothersimilarsuccesses.\\n444'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 459}, page_content='Chapter12ApplicationsInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-putervision,speechrecognition,naturallanguageprocessing,andotherapplicationareasofcommercialinterest.WebeginbydiscussingthelargescaleneuralnetworkimplementationsrequiredformostseriousAIapplications.Next,wereviewseveralspeciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve. Whileonegoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroadvarietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,visiontasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthevocabulary)perinputfeature.12.1LargeScaleDeepLearningDeeplearningisbasedonthephilosophyofconnectionism:whileanindividualbiologicalneuronoranindividualfeatureinamachinelearningmodelisnotintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercanexhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthenumberofneuronsmustbelarge.Oneofthekeyfactorsresponsiblefortheimprovementinneuralnetwork’saccuracyandtheimprovementofthecomplexityoftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreaseinthesizeofthenetworksweuse.AswesawinSec.,networksizeshavegrown1.2.3exponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksareonlyaslargeasthenervoussystemsofinsects.Becausethesizeofneuralnetworksisofparamountimportance,deeplearning445'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 460}, page_content='CHAPTER12.APPLICATIONSrequireshighperformancehardwareandsoftwareinfrastructure.12.1.1FastCPUImplementationsTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.Today,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPUcomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingtotheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscouldnotmanagethehighcomputationalworkloadrequiredbyneuralnetworks.AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyondthescopeofthisbook,butweemphasizeherethatcarefulimplementationforspeciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebestCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-pointarithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-pointimplementation,Vanhoucke2011etal.()obtaineda3×speedupoverastrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformancecharacteristics,sosometimesﬂoating-pointimplementationscanbefastertoo.Theimportantprincipleisthatcarefulspecializationofnumericalcomputationroutinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouseﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemissesandusingvectorinstructions.Manymachinelearningresearchersneglecttheseimplementationdetails,butwhentheperformanceofanimplementationrestrictsthesizeofthemodel,theaccuracyofthemodelsuﬀers.12.1.2GPUImplementationsMostmodernneuralnetworkimplementationsarebasedongraphicsprocessingunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponentsthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketforvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.Theperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobebeneﬁcialforneuralnetworksaswell.Videogamerenderingrequiresperformingmanyoperationsinparallelquickly.Modelsof characters andenvironments arespeciﬁed intermsof listsof 3-Dcoordinatesofvertices.Graphicscardsmustperformmatrixmultiplicationanddivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-Don-screencoordinates.Thegraphicscardmustthenperformmanycomputationsateachpixelinparalleltodeterminethecolorofeachpixel. Inbothcases,the446'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 461}, page_content='CHAPTER12.APPLICATIONScomputationsarefairlysimpleanddonotinvolvemuchbranchingcomparedtothecomputationalworkloadthataCPUusuallyencounters.Forexample,eachvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisnoneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiplyby.Thecomputationsarealsoentirelyindependentofeachother,andthusmaybeparallelizedeasily.Thecomputationsalsoinvolveprocessingmassivebuﬀersofmemory,containingbitmapsdescribingthetexture(colorpattern)ofeachobjecttoberendered.Together,thisresultsingraphicscardshavingbeendesignedtohaveahighdegreeofparallelismandhighmemorybandwidth,atthecostofhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditionalCPUs.Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthereal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolvelargeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,eachofwhichmustbecompletelyupdatedduringeverystepoftraining.Thesebuﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputersothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.GPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.Neuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingorsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneuralnetworkscanbedividedintomultipleindividual“neurons”thatcanbeprocessedindependentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasilybeneﬁtfromtheparallelismofGPUcomputing.GPUhardwarewasoriginallysospecializedthatitcouldonlybeusedforgraphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustomsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorstopixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybebasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingbywritingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrauetal.()implementedatwo-layerfullyconnectedneuralnetworkonaGPU2005andreporteda3XspeedupovertheirCPU-basedbaseline.Shortlythereafter,Chellapilla2006etal.()demonstratedthatthesametechniquecouldbeusedtoacceleratesupervisedconvolutionalnetworks.ThepopularityofgraphicscardsforneuralnetworktrainingexplodedaftertheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrarycode,notjustrenderingsubroutines.NVIDIA’sCUDAprogramminglanguageprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheirrelativelyconvenientprogrammingmodel,massiveparallelism,andhighmemorybandwidth,447'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 462}, page_content='CHAPTER12.APPLICATIONSGP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming.Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecameavailable(,;,).Rainaetal.2009Ciresanetal.2010WritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-cialists. ThetechniquesrequiredtoobtaingoodperformanceonGPUareverydiﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusuallydesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,mostwritablememorylocationsarenotcached,soitcanactuallybefastertocomputethesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.GPUcodeisalsoinherentlymulti-threadedandthediﬀerentthreadsmustbecoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasteriftheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscaneachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememorytransaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofreadorwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamongnthreads,threadiaccessesbytei+jofmemory,andjisamultipleofsomepowerof2. TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.AnothercommonconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthesameinstructionsimultaneously.ThismeansthatbranchingcanbediﬃcultonGPU.Threadsaredividedintosmallgroupscalled.Eachthreadinawarpwarpsexecutesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthesamewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmustbetraversedsequentiallyratherthaninparallel.DuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshouldstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotestnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibraryofhighperformanceoperationslikeconvolutionandmatrixmultiplication,thenspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,themachinelearninglibraryPylearn2(Goodfellow2013cetal.,)speciﬁesallofitsmachinelearningalgorithmsintermsofcallstoTheano(,;Bergstraetal.2010Bastien2012etal.,)andcuda-convnet(,),whichprovidetheseKrizhevsky2010high-performanceoperations.Thisfactoredapproachcanalsoeasesupportformultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunoneitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.OtherlibrarieslikeTensorFlow(,)andTorch(,Abadietal.2015Collobertetal.2011b)providesimilarfeatures.448'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 463}, page_content='CHAPTER12.APPLICATIONS12.1.3LargeScaleDistributedImplementationsInmanycases,thecomputationalresourcesavailableonasinglemachineareinsuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinferenceacrossmanymachines.Distributinginferenceissimple,becauseeachinputexamplewewanttoprocesscanberunbyaseparatemachine.Thisisknownasdataparallelism.Itisalsopossibletogetmodelparallelism, wheremultiplemachinesworktogetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthemodel.Thisisfeasibleforbothinferenceandtraining.Dataparallelismduringtrainingissomewhatharder.WecanincreasethesizeoftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinearreturnsintermsofoptimizationperformance.Itwouldbebettertoallowmultiplemachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,thestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:thegradientatstepisafunctionoftheparametersproducedbystep.tt−1Thiscanbesolvedusingasynchronousstochasticgradientdescent(Bengioetal.,;2001Recht2011etal.,).Inthisapproach,severalprocessorcoressharethememoryrepresentingtheparameters.Eachcorereadsparameterswithoutalock,thencomputesagradient,thenincrementstheparameterswithoutalock.Thisreducestheaverageamountofimprovementthateachgradientdescentstepyields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreasedrateofproductionofstepscausesthelearningprocesstobefasteroverall.Deanetal.()pioneeredthemulti-machineimplementationofthislock-freeapproach2012togradientdescent,wheretheparametersaremanagedbyaparameterserverratherthanstoredinsharedmemory.Distributedasynchronousgradientdescentremainstheprimarystrategyfortraininglargedeepnetworksandisusedbymostmajordeeplearninggroupsinindustry(,;Chilimbietal.2014Wuetal.,2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescaleofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuilddistributednetworkswithrelativelylow-costhardwareavailableintheuniversitysetting(,).Coatesetal.201312.1.4ModelCompressionInmanycommercialapplications,itismuchmoreimportantthatthetimeandmemorycostofrunninginferenceinamachinelearningmodelbelowthanthatthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire449'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 464}, page_content='CHAPTER12.APPLICATIONSpersonalization,itispossibletotrainamodelonce,thendeployittobeusedbybillionsofusers.Inmanycases,theenduserismoreresource-constrainedthanthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwithapowerfulcomputercluster,thendeployitonmobilephones.Akeystrategyforreducingthecostofinferenceismodelcompression(Buciluˇaetal.,).Thebasicideaofmodelcompressionistoreplacetheoriginal,2006expensivemodelwithasmallermodelthatrequireslessmemoryandruntimetostoreandevaluate.Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdrivenprimarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththelowestgeneralizationerrorisanensembleofseveralindependentlytrainedmodels.Evaluatingallnensemblemembersisexpensive.Sometimes,evenasinglemodelgeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).Theselargemodelslearnsomefunctionf(x),butdosousingmanymoreparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlyduetothelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunctionf(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simplybyapplyingftorandomlysampledpointsx.Wethentrainthenew,smaller,modeltomatchf(x)onthesepoints.Inordertomosteﬃcientlyusethecapacityofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistributionresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscanbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerativemodeltrainedontheoriginaltrainingset.Alternatively,onecantrainthesmallermodelonlyontheoriginaltrainingpoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposteriordistributionovertheincorrectclasses(Hinton20142015etal.,,).12.1.5DynamicStructureOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystemsthathavedynamicstructureinthegraphdescribingthecomputationneededtoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhichsubsetofmanyneuralnetworksshouldberunonagiveninput.Individualneuralnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubsetoffeatures(hiddenunits)tocomputegiveninformationfromtheinput.Thisformofdynamicstructureinsideneuralnetworksissometimescalledconditionalcomputation(,;,).SincemanycomponentsoftheBengio2013Bengioetal.2013barchitecturemayberelevantonlyforasmallamountofpossibleinputs,thesystem450'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 465}, page_content='CHAPTER12.APPLICATIONScanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.Dynamicstructureofcomputationsisabasiccomputerscienceprincipleappliedgenerallythroughoutthesoftwareengineeringdiscipline. Thesimplestversionsofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhichsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)shouldbeappliedtoaparticularinput.Avenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascadeofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthepresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,wemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun.However,becausetheobjectisrare,wecanusuallyusemuchlesscomputationtorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrainasequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,andaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesurewedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁeristrainedtohavehighprecision.Attesttime,weruninferencebyrunningtheclassiﬁersinasequence,abandoninganyexampleassoonasanyoneelementinthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswithhighconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecostoffullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascadecanachievehighcapacity.Onewayistomakethelatermembersofthecascadeindividuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhashighcapacity,becausesomeofitsindividualmembersdo. Itisalsopossibletomakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystemasawholehashighcapacityduetothecombinationofmanysmallmodels.ViolaandJones2001()usedacascadeofboosteddecisiontreestoimplementafastandrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁerlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindowsareexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascadesusestheearliermodelstoimplementasortofhardattentionmechanism:theearlymembersofthecascadelocalizeanobjectandlatermembersofthecascadeperformfurtherprocessinggiventhelocationoftheobject.Forexample,GoogletranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascadethatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthentranscribesitwithanother(Goodfellow2014detal.,).Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeachnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure451'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 466}, page_content='CHAPTER12.APPLICATIONSistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethesplittingdecision(,),thoughthishastypicallynotbeenGuoandGelfand1992donewiththeprimarygoalofacceleratinginferencecomputations.Inthesamespirit,onecanuseaneuralnetwork,calledthetoselectwhichgateroneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,giventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureofexperts(,Nowlan1990Jacobs1991;etal.,),inwhichthegateroutputsasetofprobabilitiesorweights(obtainedviaasoftmaxnonlinearity),oneperexpert,andtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputoftheexperts.Inthatcase,theuseofthegaterdoesnotoﬀerareductionincomputationalcost,butifasingleexpertischosenbythegaterforeachexample,weobtainthehardmixtureofexperts(,,),whichcanconsiderablyacceleratetrainingCollobertetal.20012002andinferencetime.Thisstrategyworkswellwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial.Butwhenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossibletousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)allthegaterconﬁgurations.Todealwiththisproblem,severalapproacheshavebeenexploredtotraincombinatorialgaters.()experimentwithseveralestimatorsofthegradientBengioetal.2013bonthegatingprobabilities,while()and()useBaconetal.2015Bengioetal.2015areinforcementlearningtechniques(policygradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandgetanactualreductionincomputationalcostwithoutimpactingnegativelyonthequalityoftheapproximation.Another kindof dynamicstructure isa switch, where ahidden unitcanreceiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicroutingapproachcanbeinterpretedasanattentionmechanism(,).Olshausenetal.1993Sofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,andthusdonotachieveallofthepossiblecomputationalbeneﬁtsofdynamicstructure.ContemporaryattentionmechanismsaredescribedinSec..12.4.5.1Onemajorobstacletousingdynamicallystructuredsystemsisthedecreaseddegreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranchesfordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribedasmatrixmultiplicationorbatchconvolutiononaminibatchofexamples.Wecanwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerentkernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumnsofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃculttoimplementeﬃciently.CPUimplementationswillbeslowduetothelackofcachecoherenceandGPUimplementationswillbeslowduetothelackofcoalesced452'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 467}, page_content='CHAPTER12.APPLICATIONSmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptakediﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningtheexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroupsofexamplessimultaneously. Thiscanbeanacceptablestrategyforminimizingthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.Inareal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioningtheworkloadcanresultinload-balancingissues.Forexample,ifweassignonemachinetoprocesstheﬁrststepinacascadeandanothermachinetoprocessthelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelastwilltendtobeunderloaded.Similarissuesariseifeachmachineisassignedtoimplementdiﬀerentnodesofaneuraldecisiontree.12.1.6SpecializedHardwareImplementationsofDeepNetworksSincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworkedonspecializedhardwareimplementationsthatcouldspeeduptrainingand/orinferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsofspecializedhardwarefordeepnetworks(,;,LindseyandLindblad1994Beiuetal.2003MisraandSaha2010;,).Diﬀerentformsofspecializedhardware(GrafandJackel1989Meadand,;Ismail2012Kim2009Pham2012Chen2014ab,;etal.,;etal.,;etal.,,)havebeendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-gratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),analog(GrafandJackel1989MeadandIsmail2012,;,)(basedonphysicalimple-mentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA(ﬁeldprogrammablegatedarray)implementations(wheretheparticularsofthecircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUsandGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpointnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,atleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley,;,;andHaggard1994SimardandGraf1994Wawrzynek1996Savich,;,;etal.,;etal.,2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearninghasgainedinpopularityinindustrialproducts,andasthegreatimpactoffasterhardwarewasdemonstratedwithGPUs.AnotherfactorthatmotivatescurrentresearchonspecializedhardwarefordeepnetworksisthattherateofprogressofasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsincomputingspeedhavecomefromparallelizationacrosscores(eitherinCPUsor453'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 468}, page_content='CHAPTER12.APPLICATIONSGPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneuralnetworkera)wherethehardwareimplementationsofneuralnetworks(whichmighttaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwiththerapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecializedhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardwaredesignsarebeingdevelopedforlow-powerdevicessuchasphones,aimingforgeneral-publicapplicationsofdeeplearning(e.g.,withspeech,computervisionornaturallanguage).Recentworkonlow-precisionimplementationsofbackprop-basedneuralnets(Vanhoucke2011Courbariaux2015Gupta2015etal.,;etal.,;etal.,)suggeststhatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeepneuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionisrequiredduringtrainingthanatinferencetime,andthatsomeformsofdynamicﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsarerequiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxedrange(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation).Dynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpointrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,powerrequirementsandcomputingtimeneededforperformingmultiplications,andmultiplicationsarethemostdemandingoftheoperationsneededtouseortrainamoderndeepnetworkwithbackprop.12.2ComputerVisionComputervisionhastraditionallybeenoneofthemostactiveresearchareasfordeeplearningapplications,becausevisionisataskthatiseﬀortlessforhumansandmanyanimalsbutchallengingforcomputers(,).ManyofBallardetal.1983themostpopularstandardbenchmarktasksfordeeplearningalgorithmsareformsofobjectrecognitionoropticalcharacterrecognition.Computervisionisaverybroadﬁeldencompassingawidevarietyofwaysofprocessingimages,andanamazingdiversityofapplications. Applicationsofcomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizingfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleofthelattercategory,onerecentcomputervisionapplicationistorecognizesoundwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davisetal.2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuchexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut454'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 469}, page_content='CHAPTER12.APPLICATIONSratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeeplearningforcomputervisionisusedforobjectrecognitionordetectionofsomeform,whetherthismeansreportingwhichobjectispresentinanimage,annotatinganimagewithboundingboxesaroundeachobject,transcribingasequenceofsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityoftheobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprincipleofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesisusingdeepmodels.Whileimagesynthesisisusuallynotconsideredaexnihilocomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulforimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesorremovingobjectsfromimages.12.2.1PreprocessingManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginalinputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturestorepresent.Computervisionusuallyrequiresrelativelylittleofthiskindofprepro-cessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthesame,reasonablerange,like[0,1]or[-1,1].Mixingimagesthatliein[0,1]withimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohavethesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Manycomputervisionarchitecturesrequireimagesofastandardsize,soimagesmustbecroppedorscaledtoﬁtthatsize.However,eventhisrescalingisnotalwaysstrictlynecessary.Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjustthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibeletal.,1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomaticallyscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinanimage(,).Hadselletal.2007Datasetaugmentationmaybeseenasawayofpreprocessingthetrainingsetonly.Datasetaugmentationisanexcellentwaytoreducethegeneralizationerrorofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshowthemodelmanydiﬀerentversionsofthesameinput(forexample,thesameimagecroppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthemodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasanensembleapproach,andhelpstoreducegeneralizationerror.Otherkindsofpreprocessingareappliedtoboththetrainandthetestsetwiththegoalofputtingeachexampleintoamorecanonicalforminordertoreducetheamountofvariationthatthemodelneedstoaccountfor.Reducingtheamountofvariationinthedatacanbothreducegeneralizationerrorandreducethesizeof455'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 470}, page_content='CHAPTER12.APPLICATIONSthemodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmallermodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessingofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinputdatathatiseasyforahumandesignertodescribeandthatthehumandesignerisconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsandlargemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojustletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.Forexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessingstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevskyetal.,).201212.2.1.1ContrastNormalizationOneofthemostobvioussourcesofvariationthatcanbesafelyremoved formanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothemagnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextofdeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinanimageorregionofanimage.SupposewehaveanimagerepresentedbyatensorX∈Rrc××3,withXi,j,1beingtheredintensityatrowiandcolumnj,Xi,j,2givingthegreenintensityandXi,j,3givingtheblueintensity.Thenthecontrastoftheentireimageisgivenby\\ue076\\ue075\\ue075\\ue07413rcr\\ue058i=1c\\ue058j=13\\ue058k=1\\ue000Xi,j,k−¯X\\ue0012(12.1)where¯Xisthemeanintensityoftheentireimage:¯X=13rcr\\ue058i=1c\\ue058j=13\\ue058k=1Xi,j,k.(12.2)Globalcontrastnormalization(GCN)aimstopreventimagesfromhavingvaryingamountsofcontrastbysubtractingthemeanfromeachimage, thenrescalingitsothatthe standarddeviation acrossits pixelsis equaltosomeconstants.Thisapproachiscomplicatedbythefactthatnoscalingfactorcanchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequalintensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformationcontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothingmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This456'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 471}, page_content='CHAPTER12.APPLICATIONSmotivatesintroducingasmall,positiveregularizationparameterλtobiastheestimateofthestandarddeviation.Alternately,onecanconstrainthedenominatortobeatleast\\ue00f.GivenaninputimageX,GCNproducesanoutputimageX\\ue030,deﬁnedsuchthatX\\ue030i,j,k= sXi,j,k−¯Xmax\\ue01a\\ue00f,\\ue071λ+13rc\\ue050ri=1\\ue050cj=1\\ue0503k=1\\ue000Xi,j,k−¯X\\ue0012\\ue01b.(12.3)Datasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikelytocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafetopracticallyignorethesmalldenominatorproblembysettingλ= 0andavoiddivisionby0inextremelyrarecasesbysetting\\ue00ftoanextremelylowvaluelike10−8. Thisistheapproachusedby()ontheCIFAR-10Goodfellowetal.2013adataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstantintensity,makingaggressiveregularizationmoreuseful.()usedCoatesetal.2011\\ue00fλ= 0and= 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.Thescaleparameterscanusuallybesetto,asdoneby(),1Coatesetal.2011orchosentomakeeachindividualpixelhavestandarddeviationacrossexamplescloseto1,asdoneby().Goodfellowetal.2013aThestandarddeviationinEq.isjustarescalingofthe12.3L2normoftheimage(assumingthemeanoftheimagehasalreadybeenremoved).ItispreferabletodeﬁneGCNintermsofstandarddeviationratherthanL2normbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCNbasedonstandarddeviationallowsthesamestobeusedregardlessofimagesize.However,theobservationthattheL2normisproportionaltothestandarddeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmappingexamplestoasphericalshell.SeeFig.foranillustration.Thiscanbeausefulproperty12.1becauseneuralnetworksareoftenbetteratrespondingtodirectionsinspaceratherthanexactlocations.Respondingtomultipledistancesinthesamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerentbiases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.Additionally,manyshallowgraphicalmodelshaveproblemswithrepresentingmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsbyreducingeachexampletoadirectionratherthanadirectionandadistance.Counterintuitively,thereisapreprocessingoperationknownasanditspheringisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedatalieonasphericalshell,butrathertorescalingtheprincipalcomponentstohaveequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhassphericalcontours.Spheringismorecommonlyknownas.whitening457'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 472}, page_content='CHAPTER12.APPLICATIONS\\n−150015...x0−15.00.15.x1Rawinput\\n−150015...x0GCN,= 10λ−2\\n−150015...x0GCN,= 0λ\\nFigure12.1:GCNmapsexamplesontoasphere. (Left)Rawinputdatamayhaveanynorm.GCNwith(Center)λ=0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuses= 1and\\ue00f= 10−8.BecauseweuseGCNbasedonnormalizingthestandarddeviationratherthantheL2norm,theresultingsphereisnottheunitsphere.(Right)RegularizedGCN,withλ>0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthevariationintheirnorm.Weleaveandthesameasbefore.s\\ue00fGlobalcontrastnormalizationwilloftenfailtohighlightimagefeatureswewouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalargedarkareaandalargebrightarea(suchasacitysquarewithhalftheimageintheshadowofabuilding)thenglobalcontrastnormalizationwillensurethereisalargediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthelightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.Thismotivateslocalcontrastnormalization.Localcontrastnormalizationensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanovertheimageasawhole.SeeFig.foracomparisonofglobalandlocalcontrast12.2normalization.Variousdeﬁnitionsoflocalcontrastnormalizationarepossible.Inallcases,onemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingbyastandarddeviationofnearbypixels.Insomecases,thisisliterallythemeanandstandarddeviationofallpixelsinarectangularwindowcenteredonthepixeltobemodiﬁed(,).Inothercases,thisisaweightedmeanPintoetal.2008andweightedstandarddeviationusingGaussianweightscenteredonthepixeltobemodiﬁed. Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolorchannelsseparatelywhileotherscombineinformationfromdiﬀerentchannelstonormalizeeachpixel(,).Sermanetetal.2012458'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 473}, page_content='CHAPTER12.APPLICATIONS\\nInputimageGCNLCNFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀectsofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesamescale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Localcontrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstantintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,suchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthenormalizationkernelbeingtoohigh.Localcontrastnormalizationcanusuallybeimplementedeﬃcientlybyusingseparableconvolution(seeSec.)tocomputefeaturemapsoflocalmeansand9.8localstandarddeviations,thenusingelement-wisesubtractionandelement-wisedivisionondiﬀerentfeaturemaps.Localcontrastnormalizationisadiﬀerentiableoperationandcanalsobeusedasanonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessingoperationappliedtotheinput.Aswithglobalcontrastnormalization,wetypicallyneedtoregularizelocalcontrastnormalizationtoavoiddivisionbyzero.Infact,becauselocalcontrastnormalizationtypicallyactsonsmallerwindows,itisevenmoreimportanttoregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearlythesameaseachother,andthusmorelikelytohavezerostandarddeviation.12.2.1.2DatasetAugmentationAsdescribedinSec.,itiseasytoimprovethegeneralizationofaclassiﬁer7.4byincreasingthesizeofthetrainingsetbyaddingextracopiesofthetrainingexamplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe459'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 474}, page_content='CHAPTER12.APPLICATIONSclass.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenabletothisform ofdataset augmentationbecause theclass isinvariant toso manytransformationsandtheinputcanbeeasilytransformedwithmanygeometricoperations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,rotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecializedcomputervisionapplications,moreadvancedtransformationsarecommonlyusedfordatasetaugmentation.Theseschemesincluderandomperturbationofthecolorsinanimage(,)andnonlineargeometricdistortionsofKrizhevskyetal.2012theinput(,).LeCunetal.1998b12.3SpeechRecognitionThetaskofspeechrecognitionistomapanacousticsignalcontainingaspokennaturallanguageutteranceintothecorrespondingsequenceofwordsintendedbythespeaker.LetX= (x(1),x(2),...,x()T)denotethesequenceofacousticinputvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Mostspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designedfeatures,butsome(,)deeplearningsystemslearnfeaturesJaitlyandHinton2011fromrawinput.Lety= (y1,y2,...,yN)denotethetargetoutputsequence(usuallyasequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)taskconsistsofcreatingafunctionf∗ASRthatcomputesthemostprobablelinguisticsequencegiventheacousticsequence:yXf∗ASR() = argmaxXyP∗(= )yX|X(12.4)whereP∗isthetrueconditionaldistributionrelatingtheinputsXtothetargetsy.Sincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognitionsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixturemodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesandphonemes(,),whileHMMsmodeledthesequenceofphonemes.Bahletal.1987TheGMM-HMM modelfamilytreats acousticwaveformsasbeinggeneratedbythefollowingprocess: ﬁrstanHMMgeneratesasequenceofphonemesanddiscretesub-phonemicstates(suchasthebeginning,middle,andendofeachphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentofaudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,speechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswereapplied,andnumerousASRsystemsfromthelate1980sandearly1990sused460'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 475}, page_content='CHAPTER12.APPLICATIONSneuralnets(BourlardandWellekens1989Waibel1989Robinsonand,;etal.,;Fallside1991Bengio19911992Konig1996,;etal.,,;etal.,).Atthetime,theperformanceofASRbasedonneuralnetsapproximatelymatchedtheperformanceofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved26%phonemeerrorrateontheTIMIT(,)corpus(with39Garofoloetal.1993phonemestodiscriminatebetween), whichwasbetterthanorcomparabletoHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphonemerecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.However,becauseofthecomplexengineeringinvolvedinsoftwaresystemsforspeechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystemsonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargumentforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,bothacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostlyfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.Later,withmuchlargeranddeepermodelsandmuchlargerdatasets,recognitionaccuracywasdramaticallyimprovedbyusingneuralnetworkstoreplaceGMMsforthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).Startingin2009,speechresearchersappliedaformofdeeplearningbasedonunsupervisedlearningtospeechrecognition.ThisapproachtodeeplearningwasbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmannmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinPart.Tosolvespeechrecognitiontasks,unsupervisedpretrainingIIIwasusedtobuilddeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow(aroundacenterframe)andpredicttheconditionalprobabilitiesofHMMstatesforthatcenterframe.TrainingsuchdeepnetworkshelpedtosigniﬁcantlyimprovetherecognitionrateonTIMIT(,Mohamedetal.20092012a,),bringingdownthephonemeerrorratefromabout26%to20.7%.See()forananalysisofreasonsforthesuccessoftheseMohamedetal.2012bmodels.Extensionstothebasicphonerecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(,)thatfurtherreducedtheMohamedetal.2011errorrate.Thiswasquicklyfollowedupbyworktoexpandthearchitecturefromphonemerecognition(whichiswhatTIMITisfocusedon)tolarge-vocabularyspeechrecognition(,),whichinvolvesnotjustrecognizingphonemesDahletal.2012butalsorecognizingsequencesofwordsfromalargevocabulary.DeepnetworksforspeechrecognitioneventuallyshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbasedontechniquessuchasrectiﬁedlinearunitsanddropout(,;,).Bythattime,severalofthemajorZeileretal.2013Dahletal.2013speechgroupsinindustryhadstartedexploringdeeplearningincollaborationwith461'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 476}, page_content='CHAPTER12.APPLICATIONSacademicresearchers.()describethebreakthroughsachievedHintonetal.2012abythesecollaborators,whicharenowdeployedinproductssuchasmobilephones.Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-ratedsomeofthemethodsforinitializing,training,andsettingupthearchitectureofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseitherunnecessaryordidnotbringanysigniﬁcantimprovement.Thesebreakthroughsinrecognitionperformanceforworderrorrateinspeechrecognitionwereunprecedented(around30%improvement)andwerefollowingalongperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwiththetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeoftrainingsets(seeFig.2.4ofDengandYu2014()).Thiscreatedarapidshiftinthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughlytwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeepneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearningalgorithmsandarchitecturesforASR,whichisstillongoingtoday.Oneoftheseinnovationswastheuseofconvolutionalnetworks(,Sainathetal.2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearliertime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenewtwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasonelongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheothertofrequencyofspectralcomponents.Anotherimportantpush, stillongoing,hasbeentowardsend-to-enddeeplearningspeechrecognitionsystemsthatcompletelyremovetheHMM.TheﬁrstmajorbreakthroughinthisdirectioncamefromGraves2013etal.()whotrainedadeepLSTMRNN(seeSec.),usingMAPinferenceovertheframe-to-phoneme10.10alignment,asin()andintheCTCframework(LeCunetal.1998bGravesetal.,2006Graves2012Graves2013;,).AdeepRNN(etal.,)hasstatevariablesfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:ordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. ThisworkbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.SeePascanu2014aChung2014etal.()andetal.()forothervariantsofdeepRNNs,appliedinothersettings.Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthesystemlearnhowto“align”theacoustic-levelinformationwiththephonetic-levelinformation(,;,).Chorowskietal.2014Luetal.2015462'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 477}, page_content='CHAPTER12.APPLICATIONS12.4NaturalLanguageProcessingNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchasEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemitspecializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimpleprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformaldescription.Naturallanguageprocessingincludesapplicationssuchasmachinetranslation,inwhichthelearnermustreadasentenceinonehumanlanguageandemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplicationsarebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequencesofwords,charactersorbytesinanaturallanguage.Aswiththeotherapplicationsdiscussedinthischapter,verygenericneuralnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.However,toachieveexcellentperformanceandtoscalewelltolargeapplications,somedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelofnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessingsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequenceofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotalnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateonanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshavebeendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputationalandinastatisticalsense.12.4.1-gramsnAlanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokensinanaturallanguage.Dependingonhowthemodelisdesigned,atokenmaybeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.Theearliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequencesoftokenscalled-grams.An-gramisasequenceoftokens.nnnModelsbasedonn-gramsdeﬁnetheconditionalprobabilityofthen-thtokengiventheprecedingn−1tokens.Themodelusesproductsoftheseconditionaldistributionstodeﬁnetheprobabilitydistributionoverlongersequences:Px(1,...,xτ) = (Px1,...,xn−1)τ\\ue059tn=Px(t|xtn−+1,...,xt−1).(12.5)Thisdecompositionisjustiﬁedbythechainruleofprobability.TheprobabilitydistributionovertheinitialsequenceP(x1,...,xn−1)maybemodeledbyadiﬀerentmodelwithasmallervalueof.n463'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 478}, page_content='CHAPTER12.APPLICATIONSTrainingn-grammodelsisstraightforwardbecausethemaximumlikelihoodestimatecanbecomputedsimplybycountinghowmanytimeseachpossiblengramoccursinthetrainingset.Modelsbasedonn-gramshavebeenthecorebuildingblockofstatisticallanguagemodelingformanydecades(JelinekandMercer1980Katz1987ChenandGoodman1999,;,;,).Forsmallvaluesofn,modelshaveparticularnames:unigramforn=1,bigramforn=2,andtrigramforn=3.ThesenamesderivefromtheLatinpreﬁxesforthecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthatiswritten.Usuallywetrainbothann-grammodelandann−1 grammodelsimultaneously.ThismakesiteasytocomputePx(t|xtn−+1,...,xt) =Pn(xtn−+1,...,xt)Pn−1(xtn−+1,...,xt−1)(12.6)simplybylookinguptwostoredprobabilities.ForthistoexactlyreproduceinferenceinPn,wemustomittheﬁnalcharacterfromeachsequencewhenwetrainPn−1.Asanexample,wedemonstratehowatrigrammodelcomputestheprobabilityofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbehandledbythedefaultformulabasedonconditionalprobabilitybecausethereisnocontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-abilityoverwordsatthestartofthesentence.WethusevaluateP3(THEDOGRAN).Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecon-ditionaldistributionP(AWAYDOGRAN|).PuttingthistogetherwithEq.,we12.6obtain:PP() = THEDOGRANAWAY3()THEDOGRANP3()DOGRANAWAY/P2()DOGRAN.(12.7)Afundamentallimitationofmaximumlikelihoodforn-grammodelsisthatPnasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,eventhoughthetuple(xtn−+1,...,xt)mayappearinthetestset.Thiscancausetwodiﬀerentkindsofcatastrophicoutcomes.WhenPn−1iszero,theratioisundeﬁned,sothemodeldoesnotevenproduceasensibleoutput.WhenPn−1isnon-zerobutPniszero,thetestlog-likelihoodis−∞. Toavoidsuchcatastrophicoutcomes,mostn-grammodelsemploysomeformofsmoothing.Smoothingtechniquesshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.See()forareviewandempiricalcomparisons.OnebasicChenandGoodman1999techniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenextsymbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform464'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 479}, page_content='CHAPTER12.APPLICATIONSorDirichletprioroverthecountparameters.Anotherverypopularideaistoformamixturemodelcontaininghigher-orderandlower-ordern-grammodels,withthehigher-ordermodelsprovidingmorecapacityandthelower-ordermodelsbeingmorelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-ordern-gramsifthefrequencyofthecontextxt−1,...,xtn−+1istoosmalltousethehigher-ordermodel.Moreformally,theyestimatethedistributionoverxtbyusingcontextsxtnk−+,...,xt−1,forincreasingk,untilasuﬃcientlyreliableestimateisfound.Classicaln-grammodelsareparticularlyvulnerabletothecurseofdimension-ality.Thereare||Vnpossiblen-gramsand||Visoftenverylarge.Evenwithamassivetrainingsetandmodestn,mostn-gramswillnotoccurinthetrainingset.Onewaytoviewaclassicaln-grammodelisthatitisperformingnearest-neighborlookup.Inotherwords,itcanbeviewedasalocalnon-parametricpredictor,similartok-nearestneighbors.ThestatisticalproblemsfacingtheseextremelylocalpredictorsaredescribedinSec..Theproblemforalanguagemodelis5.11.2evenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-tancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuchinformationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythesamecontextareusefulforlocalgeneralization. Toovercometheseproblems,alanguagemodelmustbeabletoshareknowledgebetweenonewordandothersemanticallysimilarwords.Toimprovethestatisticaleﬃciencyofn-grammodels,class-basedlanguagemodels(Brown1992NeyandKneser1993Niesler1998etal.,;,;etal.,)introducethenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthatareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthesetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswithotherwords.ThemodelcanthenusewordclassIDsratherthanindividualwordIDstorepresentthecontextontherightsideoftheconditioningbar.Compositemodelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀarealsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequencesinwhichsomewordisreplacedbyanotherofthesameclass,muchinformationislostinthisrepresentation.12.4.2NeuralLanguageModelsNeurallanguagemodelsorNLMsareaclassoflanguagemodeldesignedtoovercomethecurseofdimensionalityproblemformodelingnaturallanguagesequencesbyusingadistributedrepresentationofwords(,).Unlikeclass-Bengioetal.2001basedn-grammodels,neurallanguagemodelsareabletorecognizethattwowords465'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 480}, page_content='CHAPTER12.APPLICATIONSaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinctfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenoneword(anditscontext)andothersimilarwordsandcontexts.Thedistributedrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthemodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,iftheworddogandthewordcatmaptorepresentationsthatsharemanyattributes,thensentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadebythemodelforsentencesthatcontaintheworddog,andvice-versa.Becausetherearemanysuchattributes,therearemanywaysinwhichgeneralizationcanhappen,transferringinformationfromeachtrainingsentencetoanexponentiallylargenumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthemodeltogeneralizetoanumberofsentencesthatisexponentialinthesentencelength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoanexponentialnumberofsimilarsentences.Wesometimescallthesewordrepresentationswordembeddings.Inthisinter-pretation,weviewtherawsymbolsaspointsinaspaceofdimensionequaltothevocabularysize.Thewordrepresentationsembedthosepointsinafeaturespaceoflowerdimension.Intheoriginalspace,everywordisrepresentedbyaone-hotvector,soeverypairofwordsisatEuclideandistance√2fromeachother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts(oranypairofwordssharingsome“features”learnedbythemodel)areclosetoeachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.Fig.zooms12.3inonspeciﬁcareasofalearnedwordembeddingspacetoshowhowsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.Neuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,ahiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”UsuallyNLPpractitionersaremuchmoreinterestedinthisideaofembeddingsbecausenaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehiddenlayerhasprovidedamorequalitativelydramaticchangeinthewaythedataisrepresented.Thebasicideaofusingdistributedrepresentationstoimprovemodelsfornaturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobeusedwithgraphicalmodelsthathavedistributedrepresentationsintheformofmultiplelatentvariables(MnihandHinton2007,).466'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 481}, page_content='CHAPTER12.APPLICATIONS\\n−−−−−3432302826−14−13−12−11−10−9−8−7−6\\nCanadaEuropeOntarioNorthEnglish\\nCanadianUnionAfricanAfricaBritishFranceRussianChinaGermanyFrenchAssemblyEUJapanIraqSouthEuropean350355360365370375380.......171819202122\\n199519961997199819992000200120022003200420052006200720082009\\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneuralmachinetranslationmodel(,),zoominginonspeciﬁcareaswhereBahdanauetal.2015semanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countriesappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-Dforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigherdimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.12.4.3High-DimensionalOutputsInmanynaturallanguageapplications,weoftenwantourmodelstoproducewords(ratherthancharacters)asthefundamentalunitoftheoutput.Forlargevocabularies,itcanbeverycomputationallyexpensivetorepresentanoutputdistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmanyapplications,Vcontainshundredsofthousandsofwords.Thenaiveapproachtorepresentingsuchadistributionistoapplyanaﬃnetransformationfromahiddenrepresentationtotheoutputspace,thenapplythesoftmaxfunction.SupposewehaveavocabularyVwithsize||V.Theweightmatrixdescribingthelinearcomponentofthisaﬃnetransformationisverylarge,becauseitsoutputdimensionis||V.Thisimposesahighmemorycosttorepresentthematrix,andahighcomputationalcosttomultiplybyit.Becausethesoftmaxisnormalizedacrossall||Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattrainingtimeaswellastesttime—wecannotcalculateonlythedotproductwiththeweightvectorforthecorrectoutput.Thehighcomputationalcostsoftheoutputlayerthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)andattesttime(tocomputeprobabilitiesforallorselectedwords).Forspecializedlossfunctions,thegradientcanbecomputedeﬃciently(,),butVincentetal.2015thestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes467'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 482}, page_content='CHAPTER12.APPLICATIONSmanydiﬃculties.Supposethathisthetophiddenlayerusedtopredicttheoutputprobabilitiesˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsWandlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowingcomputations:ai= bi+\\ue058jWijhj∀∈{||}i1,...,V,(12.8)ˆyi=eai\\ue050||Vi\\ue030=1eai\\ue030.(12.9)IfhcontainsnhelementsthentheaboveoperationisO(||Vnh).Withnhinthethousandsand||Vinthehundredsofthousands,thisoperationdominatesthecomputationofmostneurallanguagemodels.12.4.3.1UseofaShortListTheﬁrstneurallanguagemodels(,,)dealtwiththehighcostBengioetal.20012003ofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabularysizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007()and()builtuponthisapproachbysplittingthevocabularyVintoashortlistLofmostfrequentwords(handledbytheneuralnet)andatailT=VL\\\\ofmorerarewords(handledbyann-grammodel). Tobeabletocombinethetwopredictions,theneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontextCbelongstothetaillist.ThismaybeachievedbyaddinganextrasigmoidoutputunittoprovideanestimateofP(iC∈|T).Theextraoutputcanthenbeusedtoachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows:VPyiC(= |) =1i∈LPyiC,iPiC(= |∈−L)(1(∈|T))(12.10)+1i∈TPyiC,iPiC(= |∈T)(∈|T)(12.11)whereP(y=iC,i|∈L)isprovidedbytheneurallanguagemodelandP(y=i|C,i∈T) isprovidedbythen-grammodel.Withslightmodiﬁcation,thisapproachcanalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmaxlayer,ratherthanaseparatesigmoidunit.Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-alizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequentwords,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulatedtheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,describedbelow.468'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 483}, page_content='CHAPTER12.APPLICATIONS12.4.3.2HierarchicalSoftmaxAclassicalapproach(,)toreducingthecomputationalburdenGoodman2001ofhigh-dimensionaloutputlayersoverlargevocabularysetsVistodecomposeprobabilitieshierarchically.Insteadofnecessitatinganumberofcomputationsproportionalto||V(andalsoproportionaltothenumberofhiddenunits,nh),the||Vfactorcanbereducedtoaslowaslog||V.()andBengio2002MorinandBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguagemodels.Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategoriesofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.Thesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,thetreehasdepthO(log||V).Theprobabilityofachoosingawordisgivenbytheproductoftheprobabilitiesofchoosingthebranchleadingtothatwordateverynodeonapathfromtherootofthetreetotheleafcontainingtheword.Fig.12.4illustratesasimpleexample. ()alsodescribehowtouseMnihandHinton2009multiplepathstoidentifyasinglewordinordertobettermodelwordsthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolvessummationoverallofthepathsthatleadtothatword.Topredicttheconditionalprobabilitiesrequiredateachnodeofthetree,wetypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethesamecontextCasinputtoallofthesemodels.Becausethecorrectoutputisencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogisticregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,correspondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.Becausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog||Vratherthan||V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnotonlythegradientwithrespecttotheoutputparametersbutalsothegradientswithrespecttothehiddenlayeractivations.Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimizetheexpectednumberofcomputations.Toolsfrominformationtheoryspecifyhowtochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.Todoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithawordisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,inpractice,thecomputationalsavingsaretypicallynotworththeeﬀortbecausethecomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputationintheneurallanguagemodel.Forexample,supposetherearelfullyconnectedhiddenlayersofwidthnh.Letnbbetheweightedaverageofthenumberofbits469'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 484}, page_content='CHAPTER12.APPLICATIONS\\n(1)(0)\\n(0,0,0)(0,0,1)(0,1,0)(0,1,1)(1,0,0)(1,0,1)(1,1,0)(1,1,1)(1,1)(1,0)(0,1)(0,0)w0w0w1w1w2w2w3w3w4w4w5w5w6w6w7w7Figure12.4:Illustrationofasimplehierarchyofwordcategories,with8wordsw0,...,w7organizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords.Internalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequenceofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)containstheclasses(0,0)(0and,1),whichrespectivelycontainthesetsofwords{w0,w1}and{w2,w3},andsimilarlysuper-classcontainstheclasses(1)(1,0)(1and,1),whichrespectivelycontainthewords(w4,w5)(andw6,w7).Ifthetreeissuﬃcientlybalanced,themaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmofthenumberofwords||V: thechoiceofoneoutof||VwordscanbeobtainedbydoingO(log||V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,computingtheprobabilityofawordycanbedonebymultiplyingthreeprobabilities,associatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfromtheroottoanodey.Letbi(y)bethei-thbinarydecisionwhentraversingthetreetowardsthevaluey.Theprobabilityofsamplinganoutputydecomposesintoaproductofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheachnodeindexedbythepreﬁxofthesebits.Forexample,node(1,0)correspondstothepreﬁx(b0(w4) = 1,b1(w4) = 0),andtheprobabilityofw4canbedecomposedasfollows:Pw(= y4) = (Pb0= 1,b1= 0,b2= 0)(12.12)= (Pb0= 1)(Pb1= 0 |b0= 1)(Pb2= 0 |b0= 1,b1= 0).(12.13)470'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 485}, page_content='CHAPTER12.APPLICATIONSrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthesewords.Inthisexample,thenumberofoperationsneededtocomputethehiddenactivationsgrowsasasO(ln2h)whiletheoutputcomputationsgrowasO(nhnb).Aslongasnb≤lnh,wecanreducecomputationmorebyshrinkingnhthanbyshrinkingnb.Indeed,nbisoftensmall.Becausethesizeofthevocabularyrarelyexceedsamillionwordsandlog2(106)≈20,itispossibletoreducenbtoabout,20butnhisoftenmuchlarger,around103ormore.Ratherthancarefullyoptimizingatreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo2andabranchingfactorof\\ue070||V.Suchatreecorrespondstosimplydeﬁningasetofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepthtwocapturesmostofthecomputationalbeneﬁtofthehierarchicalstrategy.Onequestionthatremainssomewhatopenishowtobestdeﬁnethesewordclasses,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexistinghierarchies(,)butthehierarchycanalsobelearned,ideallyMorinandBengio2005jointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexactoptimizationofthelog-likelihoodappearsintractablebecausethechoiceofawordhierarchyisadiscreteone,notamenabletogradient-basedoptimization.However,onecouldusediscreteoptimizationtoapproximatelyoptimizethepartitionofwordsintowordclasses.Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-tionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewanttocomputetheprobabilityofspeciﬁcwords.Ofcourse,computingtheprobabilityofall||Vwordswillremainexpensiveevenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthemostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnotprovideaneﬃcientandexactsolutiontothisproblem.Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworsetestresultsthansampling-basedmethodswewilldescribenext.Thismaybeduetoapoorchoiceofwordclasses.12.4.3.3ImportanceSamplingOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitlycomputingthecontributionofthegradientfromallofthewordsthatdonotappearinthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthemodel.Itcanbecomputationallycostlytoenumerateallofthesewords.Instead,itispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced471'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 486}, page_content='CHAPTER12.APPLICATIONSinEq.,thegradientcanbewrittenasfollows:12.8∂PyClog(|)∂θ=∂logsoftmaxy()a∂θ(12.14)=∂∂θlogeay\\ue050ieai(12.15)=∂∂θ(ay−log\\ue058ieai)(12.16)=∂ay∂θ−\\ue058iPyiC(= |)∂ai∂θ(12.17)whereaisthevectorofpre-softmaxactivations(orscores),withoneelementperword.Theﬁrsttermisthepositivephaseterm(pushingayup)whilethesecondtermisthenegativephaseterm(pushingaidownforalli,withweightP(iC|).Sincethenegativephasetermisanexpectation,wecanestimateitwithaMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.SamplingfromthemodelrequirescomputingP(iC|)foralliinthevocabulary,whichispreciselywhatwearetryingtoavoid.Insteadofsamplingfromthemodel,onecansamplefromanotherdistribution,calledtheproposaldistribution(denotedq),anduseappropriateweightstocorrectforthebiasintroducedbysamplingfromthewrongdistribution(BengioandSénécal2003BengioandSénécal2008,;,).Thisisanapplicationofamoregeneraltechniquecalledimportancesampling,whichwillbedescribedinmoredetailinSec..Unfortunately,evenexactimportancesamplingisnoteﬃcientbecauseit17.2requirescomputingweightspi/qi,wherepi=P(iC|),whichcanonlybecomputedifallthescoresaiarecomputed.Thesolutionadoptedforthisapplicationiscalledbiasedimportancesampling,wheretheimportanceweightsarenormalizedtosumto1.Whennegativewordniissampled,theassociatedgradientisweightedbywi=pni/qni\\ue050Nj=1pnj/qnj.(12.18)Theseweightsareusedtogivetheappropriateimportancetothemnegativesamplesfromqusedtoformtheestimatednegativephasecontributiontothegradient:||V\\ue058i=1PiC(|)∂ai∂θ≈1mm\\ue058i=1wi∂ani∂θ.(12.19)Aunigramorabigramdistributionworkswellastheproposaldistributionq.Itiseasytoestimatetheparametersofsuchadistributionfromdata.Afterestimatingtheparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently.472'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 487}, page_content='CHAPTER12.APPLICATIONSImportancesamplingisnotonlyusefulforspeedingupmodelswithlargesoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlargesparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorvwhereviindicatesthepresenceorabsenceofwordifromthevocabularyinthedocument.Alternately,vicanindicatethenumberoftimesthatwordiappears. Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrainforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychoosetomaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmightmostnaturallybedescribedintermsofcomparingeveryelementoftheoutputtoeveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisacomputationalbeneﬁttousingsparseoutputs,becausethemodelmaychoosetomakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedtobecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.Dauphin2011etal.()demonstratedthatsuchmodelscanbeacceleratedusingimportancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionforthe“positivewords”(thosethatarenon-zerointhetarget)andanequalnumberof“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristictosamplewordsthataremorelikelytobemistaken. Thebiasintroducedbythisheuristicoversamplingcanthenbecorrectedusingimportanceweights.Inallofthesecases,thecomputationalcomplexityofgradientestimationfortheoutputlayerisreducedtobeproportionaltothenumberofnegativesamplesratherthanproportionaltothesizeoftheoutputvector.12.4.3.4Noise-ContrastiveEstimationandRankingLossOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-tionalcostoftrainingneurallanguagemodelswithlargevocabularies.AnearlyexampleistherankinglossproposedbyCollobertandWeston2008a(),whichviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriestomakethescoreofthecorrectwordayberankedhighincomparisontotheotherscoresai.TherankinglossproposedthenisL=\\ue058imax(01,−ay+ai).(12.20)Thegradientiszeroforthei-thtermifthescoreoftheobservedword,ay,isgreaterthanthescoreofthenegativewordaibyamarginof1.Oneissuewiththiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities,which473'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 488}, page_content='CHAPTER12.APPLICATIONSareusefulinsomeapplications,includingspeechrecognitionandtextgeneration(includingconditionaltextgenerationtaskssuchastranslation).Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-contrastiveestimation,whichisintroducedinSec..Thisapproachhasbeen18.6successfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnihand,;Kavukcuoglu2013,).12.4.4CombiningNeuralLanguageModelswith-gramsnAmajoradvantageofn-grammodelsoverneuralnetworksisthatn-grammodelsachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)whilerequiringverylittlecomputationtoprocessanexample(bylookinguponlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortreestoaccessthecounts,thecomputationusedforn-gramsisalmostindependentofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameterstypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodelsthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingleembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasingthecomputationtimeperexample.Someothermodels,suchastiledconvolutionalnetworks,canaddparameterswhilereducingthedegreeofparametersharinginordertomaintainthesameamountofcomputation.However,typicalneuralnetworklayersbasedonmatrixmultiplicationuseanamountofcomputationproportionaltothenumberofparameters.Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensembleconsistingofaneurallanguagemodelandann-gramlanguagemodel(Bengioetal.,,).Aswithanyensemble,thistechniquecanreducetesterrorif20012003theensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearningprovidesmanywaysofcombiningtheensemblemembers’predictions,includinguniformweightingandweightschosenonavalidationset.Mikolov2011aetal.()extendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.Itisalsopossibletopairaneuralnetworkwithamaximumentropymodelandtrainbothjointly(Mikolov2011betal.,).Thisapproachcanbeviewedastraininganeuralnetworkwithanextrasetofinputsthatareconnecteddirectlytotheoutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsareindicatorsforthepresenceofparticularn-gramsintheinputcontext,sothesevariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacityishuge—thenewportionofthearchitecturecontainsupto||sVnparameters—buttheamountofaddedcomputationneededtoprocessaninputisminimalbecausetheextrainputsareverysparse.474'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 489}, page_content='CHAPTER12.APPLICATIONS12.4.5NeuralMachineTranslationMachinetranslationisthetaskofreadingasentenceinonenaturallanguageandemittingasentencewiththeequivalentmeaninginanotherlanguage. Machinetranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereisoftenonecomponentthatproposesmanycandidatetranslations.Manyofthesetranslationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.Forexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglishdirectlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggestsmanyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecondcomponentofthetranslationsystem,alanguagemodel,evaluatestheproposedtranslations,andcanscore“redapple”asbetterthan“applered.”Theearliestuseofneuralnetworksformachinetranslationwastoupgradethelanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenketal.,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshadusedann-grammodelforthiscomponent.Then-grambasedmodelsusedformachinetranslationincludenotjusttraditionalback-oﬀn-grammodels(JelinekandMercer1980Katz1987ChenandGoodman1999,;,;,)butalsomaximumentropylanguagemodels(,),inwhichanaﬃne-softmaxlayerBergeretal.1996predictsthenextwordgiventhepresenceoffrequent-gramsinthecontext.nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguagesentence.Becausemachinetranslationinvolvesproducinganoutputsentencegivenaninputsentence,itmakessensetoextendthenaturallanguagemodeltobeconditional.AsdescribedinSec.,itisstraightforwardtoextendamodel6.2.1.1thatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditionaldistributionoverthatvariablegivenacontextC,whereCmightbeasinglevariableoralistofvariables.()beatthestate-of-the-artinsomestatisticalDevlinetal.2014machinetranslationbenchmarksbyusinganMLPtoscoreaphraset1,t2,...,tkinthetargetlanguagegivenaphrases1,s2,...,sninthesourcelanguage.TheMLPestimatesP(t1,t2,...,tk|s1,s2,...,sn).TheestimateformedbythisMLPreplacestheestimateprovidedbyconditional-grammodels.nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobepreprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewouldliketouseamodelthatcanaccommodatevariablelengthinputsandvariablelengthoutputs.AnRNNprovidesthisability.Sec.describesseveralways10.2.4ofconstructinganRNNthatrepresentsaconditionaldistributionoverasequencegivensomeinput,andSec.describeshowtoaccomplishthisconditioning10.4whentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequenceandemitsadatastructurethatsummarizestheinputsequence.Wecallthis475'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 490}, page_content='CHAPTER12.APPLICATIONS\\nDecoderOutput object (English sentence)Intermediate, semantic representationSource object (French sentence or image)EncoderFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurfacerepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermappingfromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)astheinputtoadecoderforanothermodality(suchasthedecodermappingfromhiddenrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthattranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjusttomachinetranslationbutalsotocaptiongenerationfromimages.summarythe“context”C.ThecontextCmaybealistofvectors,oritmaybeavectorortensor.ThemodelthatreadstheinputtoproduceCmaybeanRNN(,;Choetal.2014aSutskever2014Jean2014etal.,;etal.,)oraconvolutionalnetwork(KalchbrennerandBlunsom2013,). Asecondmodel,usuallyanRNN,thenreadsthecontextCandgeneratesasentenceinthetargetlanguage.Thisgeneralideaofanencoder-decoderframeworkformachinetranslationisillustratedinFig..12.5Inordertogenerateanentiresentenceconditionedonthesourcesentence,themodelmusthaveawaytorepresenttheentiresourcesentence. Earliermodelswereonlyabletorepresentindividualwordsorphrases. Fromarepresentationlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentencesthathavethesamemeaninghavesimilarrepresentationsregardlessofwhethertheywerewritteninthesourcelanguageorthetargetlanguage.ThisstrategywasexploredﬁrstusingacombinationofconvolutionsandRNNs(KalchbrennerandBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposedtranslations(,)andforgeneratingtranslatedsentences(Choetal.2014aSutskeveretal.etal.,).2014Jean()scaledthesemodelstolargervocabularies.2014476'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 491}, page_content='CHAPTER12.APPLICATIONS12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\\nα(t−1)α(t−1)α()tα()tα(+1)tα(+1)th(t−1)h(t−1)h()th()th(+1)th(+1)tc c\\n× ×× ×× ×+\\nFigure12.6:Amodernattentionmechanism,asintroducedby(),isBahdanauetal.2015essentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverageoffeaturevectorsh()twithweightsα()t.Insomeapplications,thefeaturevectorsharehiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.Theweightsα()tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval[0,1]andareintendedtoconcentratearoundjustoneh()tsothattheweightedaverageapproximatesreadingthatonespeciﬁctimestepprecisely.Theweightsα()tareusuallyproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportionofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectlyindexingthedesiredh()t,butdirectindexingcannotbetrainedwithgradientdescent.Theattentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximationthatcanbetrainedwithexistingoptimizationalgorithms.Usingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofaverylongsentenceofsay60wordsisverydiﬃcult. ItcanbeachievedbytrainingasuﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyChoetal.()and2014aSutskever2014etal.().However,amoreeﬃcientapproachistoreadthewholesentenceorparagraph(togetthecontextandthegistofwhatisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtimefocusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemanticdetailsthatarerequiredtoproducethenextoutputword. Thatisexactlytheideathat()ﬁrstintroduced.TheattentionmechanismusedBahdanauetal.2015tofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedinFig..12.6Wecanthinkofanattention-basedsystemashavingthreecomponents:477'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 492}, page_content='CHAPTER12.APPLICATIONS1.Aprocessthat“reads”rawdata(suchassourcewordsinasourcesentence),andconvertsthemintodistributedrepresentations,withonefeaturevectorassociatedwitheachwordposition.2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbeunderstoodasa“” containingasequenceoffacts,whichcanbememoryretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitallofthem.3.Aprocessthat“”thecontentofthememorytosequentiallyperformexploitsatask,ateachtimestephavingtheabilityputattentiononthecontentofonememoryelement(orafew,withadiﬀerentweight).Thethirdcomponentgeneratesthetranslatedsentence.Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond-ingwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelatethecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearnakindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththewordembeddingsinanother(Kočiský2014etal.,),yieldingloweralignmenterrorratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.Thereisevenearlierworkonlearningcross-lingualwordvectors(Klementievetal.,2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcientcross-lingualalignment(,)allowstrainingonlargerdatasets.Gouwsetal.201412.4.6HistoricalPerspectiveTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhartetal.()inoneoftheﬁrstexplorationsofback-propagation,withsymbols1986acorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturingtherelationshipsbetweenfamilymembers,withtrainingexamplesformingtripletssuchas(Colin,Mother,Victoria). Theﬁrstlayeroftheneuralnetworklearnedarepresentationofeachfamilymember.Forexample, thefeaturesforColinmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewasin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkascomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthedesiredpredictions.ThemodelcanthenmakepredictionssuchasinferringwhoisthemotherofColin.TheideaofforminganembeddingforasymbolwasextendedtotheideaofanembeddingforawordbyDeerwester1990etal.().TheseembeddingswerelearnedusingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.478'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 493}, page_content='CHAPTER12.APPLICATIONSThehistoryofnaturallanguageprocessingismarkedbytransitionsinthepopularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Followingthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneuralnetworkstoNLP(,;MiikkulainenandDyer1991Schmidhuber1996,)representedtheinputasasequenceofcharacters.Bengio2001etal.()returnedthefocustomodelingwordsandintroducedneurallanguagemodels,whichproduceinterpretablewordembeddings.Theseneuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbolsinthe1980stomillionsofwords(includingpropernounsandmisspellings)inmodernapplications.ThiscomputationalscalingeﬀortledtotheinventionofthetechniquesdescribedaboveinSec..12.4.3Initially,theuseofwordsasthefundamentalunitsoflanguagemodelsyieldedimprovedlanguage modelingperformance(,).Tothisday,Bengioetal.2001newtechniquescontinuallypushbothcharacter-basedmodels(Sutskeveretal.,2011)andword-basedmodelsforward,withrecentwork(,)evenGillicketal.2015modelingindividualbytesofUnicodecharacters.Theideasbehindneurallanguagemodelshavebeenextendedintoseveralnaturallanguageprocessingapplications,suchasparsing(,,;Henderson20032004Collobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,sometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,2008aCollobert2011a;etal.,)inwhichthewordembeddingsaresharedacrosstasks.Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-alyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionalityreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-cationtovisualizationwordembeddingsbyJosephTurianin2009.12.5OtherApplicationsInthissectionwecoverafewothertypesofapplicationsofdeeplearningthatarediﬀerentfromthestandardobjectrecognition,speechrecognitionandnaturallanguageprocessingtasksdiscussedabove.PartofthisbookwillexpandIIIthatscopeevenfurthertoincludetasksrequiringtheabilitytogeneraterichhigh-dimensionalsamples(unlike“thenextword,”inlanguagemodels).479'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 494}, page_content='CHAPTER12.APPLICATIONS12.5.1RecommenderSystemsOneofthemajorfamiliesofapplicationsofmachinelearningintheinformationtechnologysectoristheabilitytomakerecommendationsofitemstopotentialusersorcustomers.Twomajortypesofapplicationscanbedistinguished:onlineadvertisinganditemrecommendations(oftentheserecommendationsarestillforthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetweenauserandanitem,eithertopredicttheprobabilityofsomeaction(theuserbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(whichmaydependonthevalueoftheproduct)ifanadisshownorarecommendationismaderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedingreatpartbyvariousformsofonlineadvertising. Therearemajorpartsoftheeconomythatrelyononlineshopping. CompaniesincludingAmazonandeBayusemachinelearning,includingdeeplearning,fortheirproductrecommendations.Sometimes,theitemsarenotproductsthatareactuallyforsale.Examplesincludeselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviestowatch,recommendingjokes,recommendingadvicefromexperts,matchingplayersforvideogames,ormatchingpeopleindatingservices.Often,thisassociationproblemishandledlikeasupervisedlearningproblem:givensomeinformationabouttheitemandabouttheuser,predicttheproxyofinterest(userclicksonad,userentersarating,userclicksona“like”button,userbuysproduct,userspendssomeamountofmoneyontheproduct,userspendstimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeitheraregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilisticclassiﬁcationproblem(predictingtheconditionalprobabilityofsomediscreteevent).Theearlyworkonrecommendersystemsreliedonminimalinformationasinputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,theonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesofthetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1anduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1anduser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrongcuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunderthenameofcollaborativeﬁltering.Bothnon-parametricapproaches(suchasnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsofpreferences)andparametricmethodsarepossible.Parametricmethodsoftenrelyonlearningadistributedrepresentation(alsocalledanembedding)foreachuserandforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isasimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponentof480'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 495}, page_content='CHAPTER12.APPLICATIONSstate-of-the-artsystems.Thepredictionisobtainedbythedotproductbetweentheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthatdependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontainingourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwithitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectivelyakindofbiasforeachuser(representinghowgrumpyorpositivethatuserisingeneral)andforeachitem(representingitsgeneralpopularity).Thebilinearpredictionisthusobtainedasfollows:ˆRu,i= bu+ci+\\ue058jAu,jBj,i.(12.21)TypicallyonewantstominimizethesquarederrorbetweenpredictedratingsˆRu,iandactualratingsRu,i.Userembeddingsanditemembeddingscanthenbeconvenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoorthree),ortheycanbeusedtocompareusersoritemsagainsteachother,justlikewordembeddings. OnewaytoobtaintheseembeddingsisbyperformingasingularvaluedecompositionofthematrixRofactualtargets(suchasratings).ThiscorrespondstofactorizingR=UDV\\ue030(oranormalizedvariant)intotheproductoftwofactors,thelowerrankmatricesA=UDandB=V\\ue030.OneproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,asiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoidpayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesumofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-basedoptimization.TheSVDandthebilinearpredictionofEq.both12.21performedverywellinthecompetitionfortheNetﬂixprize(,BennettandLanning2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsbyalargesetofanonymoususers. Manymachinelearningexpertsparticipatedinthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelofresearchinrecommendersystemsusingadvancedmachinelearningandyieldedimprovementsinrecommendersystems.Eventhoughitdidnotwinbyitself,thesimplebilinearpredictionorSVDwasacomponentoftheensemblemodelspresentedbymostofthecompetitors,includingthewinners(,;Töscheretal.2009Koren2009,).Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrstusesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirectedprobabilisticmodel(Salakhutdinov2007etal.,).RBMswereanimportantelementoftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009etal.,;Koren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrixhavealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand481'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 496}, page_content='CHAPTER12.APPLICATIONSMnih2008,).However,thereisabasiclimitationofcollaborativeﬁlteringsystems:whenanewitemoranewuserisintroduced,itslackofratinghistorymeansthatthereisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),orthedegreeofassociationbetween,say,thatnewuserandexistingitems.Thisiscalledtheproblemofcold-startrecommendations.Ageneralwayofsolvingthecold-startrecommendationproblemistointroduceextrainformationabouttheindividualusersanditems.Forexample,thisextrainformationcouldbeuserproﬁleinformationorfeaturesofeachitem.Systemsthatusesuchinformationarecalledcontent-basedrecommendersystems.Themappingfromarichsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthroughadeeplearningarchitecture(,;Huangetal.2013Elkahky2015etal.,).Specializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealsobeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusicalaudiotracks,formusicrecommendation(vandenOörd2013etal.,).Inthatwork,theconvolutionalnettakesacousticfeaturesasinputandcomputesanembeddingfortheassociatedsong.Thedotproductbetweenthissongembeddingandtheembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.12.5.1.1ExplorationVersusExploitationWhenmakingrecommendationstousers,anissuearisesthatgoesbeyondordinarysupervisedlearningandintotherealmofreinforcementlearning.Manyrecommen-dationproblemsaremostaccuratelydescribedtheoreticallyascontextualbandits(,;,). TheissueisthatwhenweusetheLangfordandZhang2008Luetal.2010recommendationsystemtocollectdata,wegetabiasedandincompleteviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitemstheywererecommendedandnottotheotheritems.Inaddition,insomecaseswemaynotgetanyinformationonusersforwhomnorecommendationhasbeenmade(forexample,withadauctions,itmaybethatthepriceproposedforanadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sotheadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhatoutcomewouldhaveresultedfromrecommendinganyoftheotheritems.Thiswouldbeliketrainingaclassiﬁerbypickingoneclassˆyforeachtrainingexamplex(typicallytheclasswiththehighestprobabilityaccordingtothemodel)andthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,eachexampleconveyslessinformationthaninthesupervisedcasewherethetruelabelyisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenotcareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisionsevenasmore482'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 497}, page_content='CHAPTER12.APPLICATIONSandmoredataiscollected,becausethecorrectdecisioninitiallyhadaverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearnaboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearningwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcementlearningcaninvolveasequenceofmanyactionsandmanyrewards.Thebanditsscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonlyasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthesensethatthelearnerknowswhichrewardisassociatedwithwhichaction.Inthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmighthavebeencausedbyarecentactionorbyanactioninthedistantpast.Thetermcontextualbanditsreferstothecasewheretheactionistakeninthecontextofsomeinputvariablethatcaninformthedecision.Forexample,weatleastknowtheuseridentity,andwewanttopickanitem.Themappingfromcontexttoactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedatadistribution(whichnowdependsontheactionsofthelearner)isacentralresearchissueinthereinforcementlearningandbanditsliterature.Reinforcementlearningrequireschoosingatradeoﬀbetweenexplorationandexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,bestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward.Explorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretrainingdata.Ifweknowthatgivencontextx,actionagivesusarewardof1,wedonotknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrentpolicyandcontinuetakingactionainordertoberelativelysureofobtainingarewardof1.However,wemayalsowanttoexplorebytryingactiona\\ue030.Wedonotknowwhatwillhappenifwetryactiona\\ue030.Wehopetogetarewardof,butwe2runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge.0Explorationcanbeimplementedinmanyways,rangingfromoccasionallytakingrandomactionsintendedtocovertheentirespaceofpossibleactions,tomodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpectedrewardandthemodel’samountofuncertaintyaboutthatreward.Manyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.Oneofthemostprominentfactorsisthetimescaleweareinterestedin. Iftheagenthasonlyashortamountoftimetoaccruereward,thenweprefermoreexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwithmoreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmoreknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetowardmoreexploitation.Supervised learninghas notradeoﬀ between explorationand exploitation483'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 498}, page_content='CHAPTER12.APPLICATIONSbecausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeachinput.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetterthanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.Anotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidestheexploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparingdiﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearnerandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardtoevaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.Thepolicyitselfdetermineswhichinputswillbeseen.()presentDudiketal.2011techniquesforevaluatingcontextualbandits.12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-sweringDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machinetranslationandnaturallanguageprocessingduetotheuseofembeddingsforsymbols(,)andwords(,;,Rumelhartetal.1986aDeerwesteretal.1990Bengioetal.2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwordsandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandforrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningforthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvancedrepresentations.12.5.2.1Knowledge,RelationsandQuestionAnsweringindexRelationsOneinterestingresearchdirectionisdetermininghowdistributedrepresentationscanbetrainedtocapturetherelationsbetweentwoentities.Theserelationsallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.Inmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairsthatareinthesetaresaidtohavetherelationwhilethosewhoarenotinthesetdonot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities{1,2,3}bydeﬁningthesetoforderedpairsS={(1,2),(1,3),(2,3)}.Oncethisrelationisdeﬁned,wecanuseitlikeaverb.Because(1,2)∈S,wesaythat1islessthan2.Because(2,1)\\ue036∈S,wecannotsaythat2islessthan1.Ofcourse,theentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnearelationcontainingtupleslike(,).is_a_type_ofdogmammalInthecontextofAI,wethinkofarelationasasentenceinasyntactically484'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 499}, page_content='CHAPTER12.APPLICATIONSsimpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,whiletwoargumentstotherelationplaytheroleofitssubjectandobject.Thesesentencestaketheformofatripletoftokens(subjectverbobject),,(12.22)withvalues(entityi,relationj,entityk).(12.23)Wecanalsodeﬁnean,aconceptanalogoustoarelation,buttakingattributeonlyoneargument:(entityi,attributej).(12.24)Forexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslikedog.Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem.Howshouldwebestdothiswithinthecontextofneuralnetworks?Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelationsbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommonstructureforthesedatabasesistherelationaldatabase,whichstoresthissamekindofinformation, albeit notformattedasthreetokensentences.When adatabaseisintendedtoconveycommonsenseknowledgeabouteverydaylifeorexpertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,wecall thedatabasea knowledgebase.Knowledgebasesrangefrom generaloneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecializedknowledgebases,likeGeneOntology.2Representationsforentitiesandrelationscanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexampleandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordesetal.,).2013aInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.Acommonapproachistoextendneurallanguagemodelstomodelentitiesandrelations.Neurallanguagemodelslearnavectorthatprovidesadistributedrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,suchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctionsofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearninganembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling1Respectivelyavailable fromthese web sites:freebase.com,cyc.com/opencyc,wordnet.princeton.eduwikiba.se,2geneontology.org485'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 500}, page_content='CHAPTER12.APPLICATIONSlanguageandmodelingknowledgeencodedasrelationsissoclosethatresearchershavetrainedrepresentationsofsuchentitiesbyusingbothandknowledgebasesnaturallanguagesentences(,,;Bordesetal.20112012Wang2014aetal.,)orcombiningdatafrommultiplerelationaldatabases(,).ManyBordesetal.2013bpossibilitiesexistfortheparticularparametrizationassociatedwithsuchamodel.Earlyworkonlearningaboutrelationsbetweenentities(,PaccanaroandHinton2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),oftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities.Forexample,()and()usedvectorsforPaccanaroandHinton2000Bordesetal.2011entitiesandmatricesforrelations,withtheideathatarelationactslikeanoperatoronentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordesetal.,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis2012putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.Apracticalshort-termapplicationofsuchmodelsislinkprediction:predictingmissingarcsintheknowledgegraph.Thisisaformofgeneralizationtonewfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthavebeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobablythemajorityoftruerelationsabsentfromtheknowledgebase.SeeWangetal.(),()and()forexamplesofsuchan2014bLinetal.2015Garcia-Duranetal.2015application.Evaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcultbecausewehaveonlyadatasetofpositiveexamples(factsthatareknowntobetrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsurewhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknownfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthemodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfactsthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamplesthatareprobablynegative(factsthatareprobablyfalse)istobeginwithatruefactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentityintherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat10%metriccountshowmanytimesthemodelranksa“correct”factamongthetop10%ofallcorruptedversionsofthatfact.Anotherapplicationofknowledgebasesanddistributedrepresentationsforthemisword-sensedisambiguation(NavigliandVelardi2005Bordes2012,;etal.,),whichisthetaskofdecidingwhichofthesensesofawordistheappropriateone,insomecontext.Eventually,knowledgeofrelationscombinedwithareasoningprocessandunderstandingofnaturallanguagecouldallowustobuildageneralquestion486'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 501}, page_content='CHAPTER12.APPLICATIONSansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocessinputinformationandrememberimportantfacts,organizedinawaythatenablesittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblemwhichcanonlybesolvedinrestricted“toy”environments.Currently,thebestapproachtorememberingandretrievingspeciﬁcdeclarativefactsistouseanexplicitmemorymechanism,asdescribedinSec..Memorynetworkswere10.12ﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumaretal.,).etal.()haveproposedanextensionthatusesGRUrecurrentnetstoread2015theinputintothememoryandtoproducetheanswergiventhecontentsofthememory.Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheonesdescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwouldbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverageofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossibleasofthiswriting.ThisconcludesPart,whichhasdescribedmodernpracticesinvolvingdeepIInetworks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,thesemethodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofamodelthatapproximatessomedesiredfunction.Withenoughtrainingdata,thisapproachisextremelypowerful.WenowturntoPart,inwhichwestepintotheIIIterritoryofresearch—methodsthataredesignedtoworkwithlesstrainingdataortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcultandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\\n487'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 502}, page_content='PartIIIDeepLearningResearch\\n488'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 503}, page_content='Thispartofthebookdescribesthemoreambitiousandadvancedapproachestodeeplearning,currentlypursuedbytheresearchcommunity.Inthepreviouspartsofthebook,wehaveshownhowtosolvesupervisedlearningproblems—howtolearntomaponevectortoanother,givenenoughexamplesofthemapping.Notallproblemswemightwanttosolvefallintothiscategory.Wemaywishtogeneratenewexamples,ordeterminehowlikelysomepointis,orhandlemissingvaluesandtakeadvantageofalargesetofunlabeledexamplesorexamplesfromrelatedtasks.Ashortcomingofthecurrentstateoftheartforindustrialapplicationsisthatourlearningalgorithmsrequirelargeamountsofsuperviseddatatoachievegoodaccuracy.Inthispartofthebook,wediscusssomeofthespeculativeapproachestoreducingtheamountoflabeleddatanecessaryforexistingmodelstoworkwellandbeapplicableacrossabroaderrangeoftasks.Accomplishingthesegoalsusuallyrequiressomeformofunsupervisedorsemi-supervisedlearning.Manydeeplearningalgorithmshavebeendesignedtotackleunsupervisedlearningproblems,butnonehavetrulysolvedtheprobleminthesamewaythatdeeplearninghaslargelysolvedthesupervisedlearningproblemforawidevarietyoftasks.Inthispartofthebook,wedescribetheexistingapproachestounsupervisedlearningandsomeofthepopularthoughtabouthowwecanmakeprogressinthisﬁeld.Acentralcauseofthediﬃcultieswithunsupervisedlearningisthehighdi-mensionalityoftherandomvariablesbeingmodeled.Thisbringstwodistinctchallenges:astatisticalchallengeandacomputationalchallenge.Thestatisticalchallengeregardsgeneralization:thenumberofconﬁgurationswemaywanttodistinguishcangrowexponentiallywiththenumberofdimensionsofinterest,andthisquicklybecomesmuchlargerthanthenumberofexamplesonecanpossiblyhave(orusewithboundedcomputationalresources).Thecomputationalchallengeassociatedwithhigh-dimensionaldistributionsarisesbecausemanyalgorithmsforlearningorusingatrainedmodel(especiallythosebasedonestimatinganexplicitprobabilityfunction)involveintractablecomputationsthatgrowexponentiallywiththenumberofdimensions.Withprobabilisticmodels,thiscomputationalchallengearisesfromtheneedtoperformintractableinferenceorsimplyfromtheneedtonormalizethedistribution.•Intractableinference:inferenceisdiscussedmostlyinChapter.It19regardsthequestionofguessingtheprobablevaluesofsomevariablesa,givenothervariablesb,withrespecttoamodelthatcapturesthejoint489'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 504}, page_content='distributionbetweena,bandc.Inordertoevencomputesuchconditionalprobabilitiesoneneedstosumoverthevaluesofthevariablesc,aswellascomputeanormalizationconstantwhichsumsoverthevaluesofaandc.•Intractablenormalizationconstants(thepartitionfunction):thepartitionfunctionisdiscussedmostlyinChapter.Normalizingconstants18ofprobabilityfunctionscomeupininference(above)aswellasinlearning.Manyprobabilisticmodelsinvolvesuchanormalizingconstant.Unfortu-nately,learningsuchamodeloftenrequirescomputingthegradientofthelogarithmofthepartitionfunctionwithrespecttothemodelparameters.Thatcomputationisgenerallyasintractableascomputingthepartitionfunctionitself.MonteCarloMarkovchain(MCMC)methods(Chapter)17areoftenusedtodealwiththepartitionfunction(computingitoritsgradi-ent).Unfortunately,MCMCmethodssuﬀerwhenthemodesofthemodeldistributionarenumerousandwell-separated,especiallyinhigh-dimensionalspaces(Sec.).17.5Onewaytoconfronttheseintractablecomputationsistoapproximatethem,andmanyapproacheshavebeenproposedasdiscussedinthisthirdpartofthebook.Anotherinterestingway, alsodiscussedhere, wouldbetoavoidtheseintractablecomputationsaltogetherbydesign,andmethodsthatdonotrequiresuchcomputationsarethusveryappealing.Severalgenerativemodelshavebeenproposedinrecentyears,withthatmotivation.AwidevarietyofcontemporaryapproachestogenerativemodelingarediscussedinChapter.20Partisthemostimportantforaresearcher—someonewhowantstoun-IIIderstandthebreadthofperspectivesthathavebeenbroughttotheﬁeldofdeeplearning,andpushtheﬁeldforwardtowardstrueartiﬁcialintelligence.\\n490'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 505}, page_content='Chapter13LinearFactorModelsManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodeloftheinput,pmodel(x).Suchamodelcan,inprinciple,useprobabilisticinferencetopredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Manyofthesemodelsalsohavelatentvariablesh,withpmodel() = xEhpmodel()xh|.Theselatentvariablesprovideanothermeansofrepresentingthedata.Distributedrepresentationsbased onlatent variablescanobtain alloftheadvantagesofrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrentnetworks.Inthischapter,wedescribesomeofthesimplestprobabilisticmodelswithlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuildingblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996etal.,;,;Roweis2002Tang2012etal.,)orlarger,deepprobabilisticmodels(etal.,).Theyalsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthatthemoreadvanceddeepmodelswillextendfurther.Alinearfactormodelisdeﬁnedbytheuseofastochastic,lineardecoderfunctionthatgeneratesbyaddingnoisetoalineartransformationof.xhThesemodelsareinterestingbecausetheyallowustodiscoverexplanatoryfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecodermadethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied.Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,wesampletheexplanatoryfactorsfromadistributionhh∼p,()h(13.1)wherep(h)isafactorialdistribution,withp(h) =\\ue051ip(hi),sothatitiseasyto491'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 506}, page_content='CHAPTER13.LINEARFACTORMODELSsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:xWhb= ++noise(13.2)wherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).ThisisillustratedinFig..13.1h1h1h2h2h3h3x1x1x2x2x3x3xhn  o i s  exhn  o i s  e=W++b=W++bFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,inwhichweassumethatanobserveddatavectorxisobtainedbyalinearcombinationofindependentlatentfactorsh,plussomenoise.Diﬀerentmodels,suchasprobabilisticPCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandoftheprior.p()h13.1ProbabilisticPCAandFactorAnalysisProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinearfactormodelsarespecialcasesoftheaboveequations(and)andonly13.113.2diﬀerinthechoicesmadeforthenoisedistributionandthemodel’sprioroverlatentvariablesbeforeobserving.hxIn(,;,),thelatentvariablefactoranalysisBartholomew1987Basilevsky1994priorisjusttheunitvarianceGaussianh0∼N(;h,I)(13.3)whiletheobservedvariablesxiareassumedtobeconditionallyindependent,givenh.Speciﬁcally,thenoiseisassumedtobedrawnfromadiagonalcovarianceGaussiandistribution,withcovariancematrixψ=diag(σ2),withσ2= [σ21,σ22,...,σ2n]\\ue03eavectorofper-variablevariances.Theroleofthelatentvariablesisthustocapturethedependenciesbetweenthediﬀerentobservedvariablesxi.Indeed,itcaneasilybeshownthatxisjustamultivariatenormalrandomvariable,withx∼N(;xbWW,\\ue03e+)ψ.(13.4)492'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 507}, page_content='CHAPTER13.LINEARFACTORMODELSInordertocastPCAinaprobabilisticframework, wecanmakeaslightmodiﬁcationtothefactoranalysismodel,makingtheconditionalvariancesσ2iequaltoeachother.InthatcasethecovarianceofxisjustWW\\ue03e+σ2I,whereσ2isnowascalar.Thisyieldstheconditionaldistributionx∼N(;xbWW,\\ue03e+σ2I)(13.5)orequivalentlyxhz= W++bσ(13.6)wherez∼N(z;0,I)isGaussiannoise.()thenshowanTippingandBishop1999iterativeEMalgorithmforestimatingtheparametersandWσ2.ThisprobabilisticPCAmodeltakesadvantageoftheobservationthatmostvariationsinthedatacanbecapturedbythelatentvariablesh,uptosomesmallresidualreconstructionerrorσ2.Asshownby(),TippingandBishop1999probabilisticPCAbecomesPCAasσ→0.Inthatcase,theconditionalexpectedvalueofhgivenxbecomesanorthogonalprojectionofxb−ontothespacespannedbythecolumnsof,likeinPCA.dWAsσ→0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharparoundtheseddimensionsspannedbythecolumnsofW.Thiscanmakethemodelassignverylowlikelihoodtothedataifthedatadoesnotactuallyclusternearahyperplane.13.2IndependentComponentAnalysis(ICA)Independentcomponentanalysis(ICA)isamongtheoldestrepresentationlearningalgorithms(,;,; ,;Herault andAns1984Jutten andHerault1991Comon1994Hyvärinen1999Hyvärinen2001aHinton2001Teh2003,;etal.,;etal.,;etal.,).Itisanapproachtomodelinglinearfactorsthatseekstoseparateanobservedsignalintomanyunderlyingsignalsthatarescaledandaddedtogethertoformtheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthanmerelydecorrelatedfromeachother.1ManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariantthatismostsimilartotheothergenerativemodelswehavedescribedhereisavariant(,)thattrainsafullyparametricgenerativemodel.ThePhametal.1992priordistributionovertheunderlyingfactors,p(h),mustbeﬁxedaheadoftimebytheuser.Themodelthendeterministicallygeneratesx=Wh.Wecanperform1SeeSec.foradiscussionofthediﬀerencebetweenuncorrelatedvariablesandindependent3.8variables.493'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 508}, page_content='CHAPTER13.LINEARFACTORMODELSanonlinearchangeofvariables(usingEq.)todetermine3.47p(x).Learningthemodelthenproceedsasusual,usingmaximumlikelihood.Themotivationforthisapproachisthatbychoosingp(h)tobeindependent,wecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.Thisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,buttorecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,eachtrainingexampleisonemomentintime,eachxiisonesensor’sobservationofthemixedsignals,andeachhiisoneestimateofoneoftheoriginalsignals.Forexample,wemighthavenpeoplespeakingsimultaneously.Ifwehavendiﬀerentmicrophonesplacedindiﬀerentlocations,ICAcandetectthechangesinthevolumebetweeneachspeakerasheardbyeachmicrophone,andseparatethesignalssothateachhicontainsonlyonepersonspeakingclearly.Thisiscommonlyusedinneuroscienceforelectroencephalography,atechnologyforrecordingelectricalsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject’sheadareusedtomeasuremanyelectricalsignalscomingfromthebody.Theexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfromthesubject’sheartandeyesarestrongenoughtoconfoundmeasurementstakenatthesubject’sscalp.Thesignalsarriveattheelectrodesmixedtogether,soICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignalsoriginatinginthebrain,andtoseparatesignalsindiﬀerentbrainregionsfromeachother.Asmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoiseinthegenerationofxratherthanusingadeterministicdecoder.Mostdonotusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsofh=W−1xindependentfromeachother.Manycriteriathataccomplishthisgoalarepossible.Eq.requirestakingthedeterminantof3.47W,whichcanbeanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthisproblematicoperationbyconstrainingtobeorthonormal.WAllvariantsofICArequirethatp(h)benon-Gaussian.Thisisbecauseifp(h)isanindependentpriorwithGaussiancomponents,thenWisnotidentiﬁable.Wecanobtainthesamedistributionoverp(x)formanyvaluesofW.ThisisverydiﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,thatoftenrequirep(h)tobeGaussianinordertomakemanyoperationsonthemodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwheretheuserexplicitlyspeciﬁesthedistribution,atypicalchoiceistousep(hi) =ddhiσ(hi).Typicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0thandoestheGaussiandistribution,sowecanalsoseemostimplementationsofICAaslearningsparsefeatures.494'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 509}, page_content='CHAPTER13.LINEARFACTORMODELSManyvariantsofICAarenotgenerativemodelsinthesensethatweusethephrase.Inthisbook,agenerativemodeleitherrepresentsp(x) orcandrawsamplesfromit.ManyvariantsofICAonlyknowhowtotransformbetweenxandh,butdonothaveanywayofrepresentingp(h),andthusdonotimposeadistributionoverp(x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisofh=W−1x,becausehighkurtosisindicatesthatp(h)isnon-Gaussian,butthisisaccomplishedwithoutexplicitlyrepresentingp(h).ThisisbecauseICAismoreoftenusedasananalysistoolforseparatingsignals,ratherthanforgeneratingdataorestimatingitsdensity.JustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedinChapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich14weuseanonlinearfunctionftogeneratetheobserveddata.SeeHyvärinenandPajunen 1999()fortheinitial work onnonlinearICAand itssuccessfulusewithensemblelearningbyRobertsand Everson 2001Lappalainen()and etal.().AnothernonlinearextensionofICAistheapproachof2000nonlinearindependentcomponentsestimation,orNICE(,),whichstacksaDinhetal.2014seriesofinvertibletransformations(encoderstages)thathavethepropertythatthedeterminantoftheJacobianofeachtransformationcanbecomputedeﬃciently.Thismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attemptstotransformthedataintoaspacewhereithasafactorizedmarginaldistribution,butismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoderisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardtogeneratesamplesfromthemodel(byﬁrstsamplingfromp(h)andthenapplyingthedecoder).AnothergeneralizationofICAistolearngroupsoffeatures,withstatisticaldependenceallowedwithinagroupbutdiscouragedbetweengroups(HyvärinenandHoyer1999Hyvärinen2001b,;etal.,).Whenthegroupsofrelatedunitsarechosentobenon-overlapping,thisiscalledindependentsubspaceanalysis.Itisalsopossibletoassignspatialcoordinatestoeachhiddenunitandformoverlappinggroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilarfeatures.Whenappliedtonaturalimages,thistopographicICAapproachlearnsGaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationorfrequency.ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithineachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.13.3SlowFeatureAnalysisSlowfeatureanalysis(SFA)isalinearfactormodelthatusesinformationfrom495'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 510}, page_content='CHAPTER13.LINEARFACTORMODELStimesignalstolearninvariantfeatures(,).WiskottandSejnowski2002Slowfeatureanalysisismotivatedbyageneralprinciplecalledtheslownessprinciple.Theideaisthattheimportantcharacteristicsofsceneschangeveryslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofascene.Forexample,incomputervision,individualpixelvaluescanchangeveryrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixelwillrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespassoverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisintheimagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwillchangeslowly. Wethereforemaywishtoregularizeourmodeltolearnfeaturesthatchangeslowlyovertime.Theslownessprinciplepredatesslowfeatureanalysisandhasbeenappliedtoawidevarietyofmodels(,;,;,;Hinton1989Földiák1989Mobahietal.2009BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoanydiﬀerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybeintroducedbyaddingatermtothecostfunctionoftheformλ\\ue058tLf((x(+1)t)(,fx()t))(13.7)whereλisahyperparameterdeterminingthestrengthoftheslownessregularizationterm,tistheindexintoatimesequenceofexamples,fisthefeatureextractortoberegularized,andLisalossfunctionmeasuringthedistancebetweenf(x()t)andf(x(+1)t).Acommonchoiceforisthemeansquareddiﬀerence.LSlowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslownessprinciple.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractor,andcanthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquiteagenerativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninputspaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthusdoesnotimposeadistributiononinputspace.p()xTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁningf(x;θ)tobealineartransformation,andsolvingtheoptimizationproblemminθEt((fx(+1)t)i−f(x()t)i)2(13.8)subjecttotheconstraintsEtf(x()t)i= 0(13.9)andEt[(fx()t)2i] = 1.(13.10)496'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 511}, page_content='CHAPTER13.LINEARFACTORMODELSTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomaketheproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeaturevaluesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective.Theconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthepathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures0areordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,wemustalsoaddtheconstraint∀i<j,Et[(fx()t)if(x()t)j] = 0.(13.11)Thisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelatedfromeachother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturetheoneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizingreconstructionerror, to forcethe featurestodiversify, but thisdecorrelationmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFAproblemmaybesolvedinclosedformbyalinearalgebrapackage.SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasisexpansiontoxbeforerunningSFA.Forexample,itiscommontoreplacexbythequadraticbasisexpansion,avectorcontainingelementsxixjforalliandj.LinearSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractorsbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasisexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractorontopofthatexpansion.Whentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwithquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswiththoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrainedonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deepSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresentedbyneuronsinratbrainsthatareusedfornavigation(Franzius2007etal.,).SFAthusseemstobeareasonablybiologicallyplausiblemodel.AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhichfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoreticalpredictions,onemustknowaboutthedynamicsoftheenvironmentintermsofconﬁgurationspace (e.g., inthe caseofrandom motioninthe 3-Drenderedenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobabilitydistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhowtheunderlyingfactorsactuallychange,itispossibletoanalyticallysolvefortheoptimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFAappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.497'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 512}, page_content='CHAPTER13.LINEARFACTORMODELSThisisincomparisontootherlearningalgorithmswherethecostfunctiondependshighlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhatfeaturesthemodelwilllearn.DeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandposeestimation(Franzius2008etal.,).Sofar,theslownessprinciplehasnotbecomethebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimiteditsperformance.Wespeculatethatperhapstheslownessprioristoostrong,andthat,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,itwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfromonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessofwhethertheobject’svelocityishighorlow,buttheslownessprincipleencouragesthemodeltoignorethepositionofobjectsthathavehighvelocity.13.4SparseCodingSparse coding (, )is a linearfactor model that hasOlshausenand Field1996beenheavilystudiedasanunsupervisedfeaturelearningandfeatureextractionmechanism. Strictlyspeaking,theterm“sparsecoding”referstotheprocessofinferringthevalueofhinthismodel,while“sparsemodeling”referstotheprocessofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedtorefertoboth.Likemostotherlinearfactormodels,itusesalineardecoderplusnoisetoobtainreconstructionsofx,asspeciﬁedinEq..Morespeciﬁcally,sparse13.2codingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewithisotropicprecision:βp,() = (;+xh|NxWhb1βI).(13.12)Thedistributionp(h)ischosentobeonewithsharppeaksnear0(OlshausenandField1996,).CommonchoicesincludefactorizedLaplace,CauchyorfactorizedStudent-tdistributions.Forexample,theLaplacepriorparametrizedintermsofthesparsitypenaltycoeﬃcientisgivenbyλph(i) = Laplace(hi;0,2λ) =λ4e−12λh|i|(13.13)andtheStudent-priorbytph(i) ∝1(1+h2iν)ν+12.(13.14)498'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 513}, page_content='CHAPTER13.LINEARFACTORMODELSTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,thetrainingalternatesbetweenencodingthedataandtrainingthedecodertobetterreconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtherasaprincipledapproximationtomaximumlikelihoodlater,inSec..19.3FormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunctionthatpredictshandconsistsonlyofmultiplicationbyaweightmatrix.Theencoderthatweusewithsparsecodingisnotaparametricencoder.Instead,theencoderisanoptimizationalgorithm,thatsolvesanoptimizationprobleminwhichweseekthesinglemostlikelycodevalue:h∗= () = argmaxfxhp.()hx|(13.15)WhencombinedwithEq.andEq.,thisyieldsthefollowingoptimization13.1313.12problem:argmaxhp()hx|(13.16)=argmaxhlog()phx|(13.17)=argminhλ||||h1+β||−||xWh22,(13.18)wherewehavedroppedtermsnotdependingonhanddividedbypositivescalingfactorstosimplifytheequation.DuetotheimpositionofanL1normonh,thatthisprocedurewillyieldasparseh∗(SeeSec.).7.1.2Totrainthemodelratherthanjustperforminference,wealternatebetweenminimizationwithrespecttohandminimizationwithrespecttoW.Inthispresentation,wetreatβasahyperparameter.Typicallyitissetto1becauseitsroleinthisoptimizationproblemissharedwithλandthereisnoneedforbothhyperparameters.Inprinciple,wecouldalsotreatβasaparameterofthemodelandlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdependonhbutdodependonβ.Tolearnβ,thesetermsmustbeincluded,orβwillcollapseto.0Notallapproachestosparsecodingexplicitlybuildap(h)andap(xh|).Oftenwearejustinterestedinlearningadictionaryoffeatureswithactivationvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.IfwesamplehfromaLaplaceprior,itisinfactazeroprobabilityeventforanelementofhtoactuallybezero.Thegenerativemodelitselfisnotespeciallysparse,onlythefeatureextractoris.()describeapproximateGoodfellowetal.2013d499'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 514}, page_content='CHAPTER13.LINEARFACTORMODELSinferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,forwhichsamplesfromthepriorusuallycontaintruezeros.Thesparsecodingapproachcombinedwiththeuseofthenon-parametricencodercaninprincipleminimizethecombinationofreconstructionerrorandlog-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthatthereisnogeneralizationerrortotheencoder.Aparametricencodermustlearnhowtomapxtohinawaythatgeneralizes.Forunusualxthatdonotresemblethetrainingdata,alearned,parametricencodermayfailtoﬁndanhthatresultsinaccuratereconstructionorasparsecode.Forthevastmajorityofformulationsofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimizationprocedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchasreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncostscanstillriseonunfamiliarpoints,butthisisduetogeneralizationerrorinthedecoderweights,ratherthangeneralizationerrorintheencoder.Thelackofgeneralizationerrorinsparsecoding’soptimization-basedencodingprocessmayresultinbettergeneralizationwhensparsecodingisusedasafeatureextractorforaclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.CoatesandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterforobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametricencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellowetal.()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature2013dextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewerlabelsperclass).Theprimarydisadvantageofthenon-parametricencoderisthatitrequiresgreatertimetocomputehgivenxbecausethenon-parametricapproachrequiresrunninganiterativealgorithm.Theparametricautoencoderapproach,developedinChapter , usesonlyaﬁxed number oflayers, oftenonlyone.Another14disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthenon-parametricencoder,whichmakesitdiﬃculttopretrainasparsecodingmodelwithanunsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion.Modiﬁedversionsofsparsecodingthatpermitapproximatederivativesdoexistbutarenotwidelyused(,).BagnellandBradley2009Sparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,asshowninFig..Thishappensevenwhenthemodelisabletoreconstruct13.2thedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateachindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencoderesultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgeneratedsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-500'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 515}, page_content='CHAPTER13.LINEARFACTORMODELS\\nFigure13.2: ExamplesamplesandweightsfromaspikeandslabsparsecodingmodeltrainedontheMNISTdataset.(Left)Thesamplesfromthemodeldonotresemblethetrainingexamples. Atﬁrstglance,onemightassumethemodelispoorlyﬁt.The(Right)weightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescompletedigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprioroverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.FewsuchsubsetsareappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentofgenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.FigurereproducedwithpermissionfromGoodfellow2013detal.().factorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmoresophisticatedshallowmodels.13.5ManifoldInterpretationofPCALinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedaslearningamanifold(,).WecanviewprobabilisticPCAasHintonetal.1997deﬁningathinpancake-shapedregionofhighprobability—aGaussiandistributionthatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsverticalaxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontalaxes.ThisisillustratedinFig..PCAcanbeinterpretedasaligningthis13.3pancakewithalinearmanifoldinahigher-dimensionalspace.ThisinterpretationappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearnsmatricesWandVwiththegoalofmakingthereconstructionofxlieasclosetoxaspossible,LettheencoderbehxW= (f) = \\ue03e()xµ−.(13.19)501'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 516}, page_content='CHAPTER13.LINEARFACTORMODELSTheencodercomputesalow-dimensionalrepresentationofh.Withtheautoencoderview,wehaveadecodercomputingthereconstructionˆxhbVh= (g) = +.(13.20)\\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensionalmanifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane”whichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldisverysmall(arrowpointingoutofplane)andcanbeconsideredlike“noise,”whiletheothervariancesarelarge(arrowsintheplane)andcorrespondto“signal,”andacoordinatesystemforthereduced-dimensiondata.ThechoicesoflinearencoderanddecoderthatminimizereconstructionerrorE[||−xˆx||2](13.21)correspondtoV=W,µ=b=E[x]andthecolumnsofWformanorthonormalbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariancematrixCxµxµ= [(E−)(−)\\ue03e].(13.22)InthecaseofPCA,thecolumnsofWaretheseeigenvectors,orderedbythemagnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).OnecanalsoshowthateigenvalueλiofCcorrespondstothevarianceofxinthedirectionofeigenvectorv()i.Ifx∈RDandh∈Rdwithd<D,thenthe502'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 517}, page_content='CHAPTER13.LINEARFACTORMODELSoptimalreconstructionerror(choosing,,andasabove)isµbVWmin[E||−xˆx||2] =D\\ue058id=+1λi.(13.23)Hence,ifthecovariancehasrankd,theeigenvaluesλd+1toλDare0andrecon-structionerroris0.Furthermore,onecanalsoshowthattheabovesolutioncanbeobtainedbymaximizingthevariancesoftheelementsof,underorthonormal,insteadofhWminimizingreconstructionerror.Linearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthesimplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersandlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinearfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilisticmodelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexiblemodelfamily.\\n503'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 518}, page_content='Chapter14AutoencodersAnautoencoderisaneuralnetworkthatistrainedtoattempttocopyitsinputtoitsoutput.Internally,ithasahiddenlayerhthatdescribesacodeusedtorepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:anencoderfunctionh=f(x)andadecoderthatproducesareconstructionr=g(h).ThisarchitectureispresentedinFig..Ifanautoencodersucceedsinsimply14.1learningtosetg(f(x)) =xeverywhere,thenitisnotespeciallyuseful.Instead,autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyarerestrictedinwaysthatallowthemtocopyonlyapproximately,andtocopyonlyinputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritizewhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthedata.Modern autoencoders havegeneralized the ideaof anencoder and ade-coderbeyonddeterministicfunctionstostochasticmappingspencoder(hx|)andpdecoder()xh|.Theideaofautoencodershasbeenpartofthehistoricallandscapeofneuralnetworksfordecades(,;,;,LeCun1987BourlardandKamp1988HintonandZemel1994).Traditionally, autoencoders wereused fordimensionalityreductionorfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersandlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerativemodeling,aswewillseeinChapter.Autoencodersmaybethoughtofasbeing20aspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesametechniques,typicallyminibatchgradientdescentfollowinggradientscomputedbyback-propagation.Unlikegeneralfeedforwardnetworks,autoencodersmayalsobetrainedusingrecirculation(,),alearningHintonandMcClelland1988algorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput504'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 519}, page_content='CHAPTER14.AUTOENCODERStotheactivationsonthereconstructedinput.Recirculationisregardedasmorebiologicallyplausiblethanback-propagation,butisrarelyusedformachinelearningapplications.\\nx xr rh hfgFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutputx(calledreconstruction)rthroughaninternalrepresentationorcodeh.Theautoencoderhastwocomponents:theencoderf(mappingxtoh)andthedecoderg(mappinghtor).14.1UndercompleteAutoencodersCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynotinterestedintheoutputofthe decoder.Instead, wehope thattrainingtheautoencodertoperformtheinputcopyingtaskwillresultinhtakingonusefulproperties.Onewaytoobtainusefulfeaturesfromtheautoencoderistoconstrainhtohavesmallerdimensionthanx.Anautoencoderwhosecodedimensionislessthantheinputdimensioniscalledundercomplete.Learninganundercompleterepresentationforcestheautoencodertocapturethemostsalientfeaturesofthetrainingdata.ThelearningprocessisdescribedsimplyasminimizingalossfunctionL,gf(x(()))x(14.1)whereLisalossfunctionpenalizingg(f(x))forbeingdissimilarfromx,suchasthemeansquarederror.WhenthedecoderislinearandListhemeansquarederror,anundercompleteautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencodertrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthetrainingdataasaside-eﬀect.Autoencoderswithnonlinearencoderfunctionsfandnonlineardecoderfunc-tionsgcanthuslearnamorepowerfulnonlineargeneralizationofPCA.Unfortu-505'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 520}, page_content='CHAPTER14.AUTOENCODERSnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencodercanlearntoperformthecopyingtaskwithoutextractingusefulinformationaboutthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoderwithaone-dimensionalcodebutaverypowerfulnonlinearencodercouldlearntorepresenteachtrainingexamplex()iwiththecodei.Thedecodercouldlearntomaptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples. Thisspeciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-codertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulaboutthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.14.2RegularizedAutoencodersUndercompleteautoencoders,withcodedimensionlessthantheinputdimension,canlearnthemostsalientfeaturesofthedatadistribution. Wehaveseenthattheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderaregiventoomuchcapacity.Asimilarproblemoccursifthehiddencodeisallowedtohavedimensionequaltotheinput,andintheovercompletecaseinwhichthehiddencodehasdimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlineardecodercanlearntocopytheinputtotheoutputwithoutlearninganythingusefulaboutthedatadistribution.Ideally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosingthecodedimensionandthecapacityoftheencoderanddecoderbasedonthecomplexityofdistributiontobemodeled.Regularizedautoencodersprovidetheabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoderanddecodershallowandthecodesizesmall,regularizedautoencodersusealossfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheabilitytocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityoftherepresentation,smallnessofthederivativeoftherepresentation,androbustnesstonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearandovercompletebutstilllearnsomethingusefulaboutthedatadistributionevenifthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.Inadditiontothemethodsdescribedherewhicharemostnaturallyinterpretedasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariablesandequippedwithaninferenceprocedure(forcomputinglatentrepresentationsgiveninput)maybeviewedasaparticularformofautoencoder.TwogenerativemodelingapproachesthatemphasizethisconnectionwithautoencodersarethedescendantsoftheHelmholtzmachine(,),suchasthevariationalHintonetal.1995b506'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 521}, page_content='CHAPTER14.AUTOENCODERSautoencoder(Sec.)andthegenerativestochasticnetworks(Sec.).20.10.320.12Thesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinputanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodingsarenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximizetheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.14.2.1SparseAutoencodersAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesasparsitypenaltyΩ(h)onthecodelayerh,inadditiontothereconstructionerror:L,gf(x(()))+Ω()xh(14.2)whereg(h)isthedecoderoutputandtypicallywehaveh=f(x),theencoderoutput.Sparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuchasclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemustrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,ratherthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthecopyingtaskwithasparsitypenaltycanyieldamodelthathaslearnedusefulfeaturesasabyproduct.Wecan thinkofthe penaltyΩ(h)simply asaregularizer termaddedtoafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask(with asupervised learning objective)thatdepends onthese sparse features.Unlikeotherregularizerssuchasweightdecay, thereisnotastraightforwardBayesianinterpretationtothisregularizer.AsdescribedinSec.,training5.6.1withweight decayand otherregularizationpenalties canbeinterpretedasaMAPapproximationtoBayesianinference,withtheaddedregularizingpenaltycorrespondingtoapriorprobabilitydistributionoverthemodelparameters.Inthisview,regularizedmaximumlikelihoodcorrespondstomaximizingp(θx|),whichisequivalenttomaximizinglogp(xθ|)+logp(θ). Thelogp(xθ|)termistheusualdatalog-likelihoodtermandthelogp(θ)term,thelog-prioroverparameters,incorporatesthepreferenceoverparticularvaluesofθ.ThisviewwasdescribedinSec..Regularizedautoencodersdefysuchaninterpretation5.6becausetheregularizerdependsonthedataandisthereforebydeﬁnitionnotapriorintheformalsenseoftheword.Wecanstillthinkoftheseregularizationtermsasimplicitlyexpressingapreferenceoverfunctions.Ratherthanthinkingofthesparsitypenaltyasaregularizerforthecopyingtask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating507'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 522}, page_content='CHAPTER14.AUTOENCODERSmaximumlikelihood trainingofagenerativemodel thathaslatent variables.Supposewehaveamodelwithvisiblevariablesxandlatentvariablesh,withanexplicitjointdistributionpmodel(xh,)=pmodel(h)pmodel(xh|).Werefertopmodel(h)asthemodel’spriordistributionoverthelatentvariables,representingthemodel’sbeliefspriortoseeingx.Thisisdiﬀerentfromthewaywehavepreviouslyusedtheword“prior,”torefertothedistributionp(θ)encodingourbeliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.Thelog-likelihoodcanbedecomposedaslogpmodel() = logx\\ue058hpmodel()hx,.(14.3)Wecanthinkoftheautoencoderasapproximatingthissumwithapointestimateforjustonehighlylikelyvalueforh.Thisissimilartothesparsecodinggenerativemodel(Sec.),butwith13.4hbeingtheoutputoftheparametricencoderratherthantheresultofanoptimizationthatinfersthemostlikelyh.Fromthispointofview,withthischosen,wearemaximizinghlogpmodel() = loghx,pmodel()+loghpmodel()xh|.(14.4)Thelogpmodel()htermcanbesparsity-inducing.Forexample,theLaplaceprior,pmodel(hi) =λ2e−|λhi|,(14.5)correspondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasanabsolutevaluepenalty,weobtainΩ() = hλ\\ue058i|hi|(14.6)−logpmodel() =h\\ue058i\\ue012λh|i|−logλ2\\ue013= Ω()+consth(14.7)wheretheconstanttermdependsonlyonλandnoth.Wetypicallytreatλasahyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameterlearning.OtherpriorssuchastheStudent-tpriorcanalsoinducesparsity.Fromthispointofviewofsparsityasresultingfromtheeﬀectofpmodel(h)onapproximatemaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermatall. Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables.Thisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisawayofapproximatelytrainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor508'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 523}, page_content='CHAPTER14.AUTOENCODERSwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatentvariablesthatexplaintheinput.Earlyworkonsparseautoencoders(,,)exploredRanzatoetal.2007a2008variousformsofsparsityandproposedaconnectionbetweenthesparsitypenaltyandthelogZtermthatariseswhenapplyingmaximumlikelihoodtoanundirectedprobabilisticmodelp(x) =1Z˜p(x).TheideaisthatminimizinglogZpreventsaprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsityon anautoencoder preventsthe autoencoder fromhaving low reconstructionerroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitiveunderstandingofageneralmechanismratherthanamathematicalcorrespondence.Theinterpretationofthesparsitypenaltyascorrespondingtologpmodel(h)inadirectedmodelpmodel()hpmodel()xh|ismoremathematicallystraightforward.Onewaytoachieveactualzerosinhforsparse(anddenoising)autoencoderswasintroducedin().TheideaistouserectiﬁedlinearunitstoGlorotetal.2011bproducethecodelayer.Withapriorthatactuallypushestherepresentationstozero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaveragenumberofzerosintherepresentation.14.2.2DenoisingAutoencodersRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoderΩ thatlearnssomethingusefulbychangingthereconstructionerrortermofthecostfunction.Traditionally,autoencodersminimizesomefunctionL,gf(x(()))x(14.8)whereLisalossfunctionpenalizingg(f(x))forbeingdissimilarfromx,suchastheL2normoftheirdiﬀerence. Thisencouragesgf◦tolearntobemerelyanidentityfunctioniftheyhavethecapacitytodoso.AdenoisingautoencoderDAEorinsteadminimizesL,gf(x((˜x))),(14.9)where˜xisacopyofxthathasbeencorruptedbysomeformofnoise.Denoisingautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheirinput.Denoisingtrainingforcesfandgtoimplicitlylearnthestructureofpdata(x),asshown by ()and().DenoisingAlainand Bengio2013Bengioet al.2013c509'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 524}, page_content='CHAPTER14.AUTOENCODERSautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemergeasabyproductofminimizingreconstructionerror.Theyarealsoanexampleofhowovercomplete,high-capacitymodelsmaybeusedasautoencoderssolongascareistakentopreventthemfromlearningtheidentityfunction.DenoisingautoencodersarepresentedinmoredetailinSec..14.514.2.3RegularizingbyPenalizingDerivativesAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparseΩautoencoders,L,gf,,(x(()))+Ω(xhx)(14.10)butwithadiﬀerentformof:ΩΩ() = hx,λ\\ue058i||∇xhi||2.(14.11)Thisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhenxchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforcestheautoencodertolearnfeaturesthatcaptureinformationaboutthetrainingdistribution.Anautoencoderregularizedinthiswayiscalledacontractiveautoencoderor.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,CAEmanifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetailinSec..14.714.3RepresentationalPower,LayerSizeandDepthAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayerdecoder.However,thisisnotarequirement.Infact,usingdeepencodersanddecodersoﬀersmanyadvantages.RecallfromSec.thattherearemanyadvantagestodepthinafeedforward6.4.1network.Becauseautoencodersarefeedforwardnetworks,theseadvantagesalsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetworkasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividuallybeneﬁtfromdepth.Onemajoradvantageofnon-trivialdepthisthattheuniversalapproximatortheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehiddenlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan510'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 525}, page_content='CHAPTER14.AUTOENCODERSarbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeansthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentityfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrominputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitraryconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withatleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximateanymappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.Depthcanexponentiallyreducethecomputationalcostofrepresentingsomefunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdataneededtolearnsomefunctions.SeeSec.forareviewoftheadvantagesof6.4.1depthinfeedforwardnetworks.Experimentally,deepautoencodersyieldmuchbettercompressionthancorre-spondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).Acommonstrategyfortrainingadeepautoencoderistogreedilypretrainthedeeparchitecturebytrainingastackofshallowautoencoders,soweoftenencountershallowautoencoders,evenwhentheultimategoalistotrainadeepautoencoder.14.4StochasticEncodersandDecodersAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutputunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedforautoencoders.AsdescribedinSec.,ageneralstrategyfordesigningtheoutputunits6.2.2.4andthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistributionp(yx|)andminimizethenegativelog-likelihood−logp(yx|).Inthatsetting,ywasavectoroftargets,suchasclasslabels.Inthecaseofanautoencoder,xisnowthetargetaswellastheinput.However,wecanstillapplythesamemachineryasbefore.Givenahiddencodeh,wemaythinkofthedecoderasprovidingaconditionaldistributionpdecoder(xh|). Wemaythentraintheautoencoderbyminimizing−logpdecoder()xh|.Theexactformofthislossfunctionwillchangedependingontheformofpdecoder.Aswithtraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrizethemeanofaGaussiandistributionifxisreal-valued.Inthatcase,thenegativelog-likelihoodyieldsameansquarederrorcriterion.Similarly,binaryxvaluescorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoidoutputunit,discretexvaluescorrespondtoasoftmaxdistribution,andsoon.511'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 526}, page_content='CHAPTER14.AUTOENCODERSTypically,theoutputvariablesaretreatedasbeingconditionallyindependentgivenhsothatthisprobabilitydistributionisinexpensivetoevaluate,butsometechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputswithcorrelations.\\nx xr rh hpencoder()hx|pdecoder()xh|Figure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthedecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthattheiroutputcanbeseenassampledfromadistribution,pencoder(hx|)fortheencoderandpdecoder()xh|forthedecoder.Tomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseenpreviously,wecanalsogeneralizethenotionofanencodingfunctionf(x)toanencodingdistributionpencoder()hx|,asillustratedinFig..14.2Anylatentvariablemodelpmodel()hx,deﬁnesastochasticencoderpencoder() = hx|pmodel()hx|(14.12)andastochasticdecoderpdecoder() = xh|pmodel()xh|.(14.13)Ingeneral,theencoderanddecoderdistributionsarenotnecessarilyconditionaldistributionscompatiblewithauniquejointdistributionpmodel(xh,).Alainetal.()showedthattrainingtheencoderanddecoderasadenoisingautoencoder2015willtendtomakethemcompatibleasymptotically(withenoughcapacityandexamples).14.5DenoisingAutoencodersThedenoisingautoencoder(DAE)isanautoencoderthatreceivesacorrupteddatapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapointasitsoutput.TheDAEtraining procedureisillustrated inFig..We introducea14.3corruptionprocessC(˜xx|)whichrepresentsaconditionaldistribution over512'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 527}, page_content='CHAPTER14.AUTOENCODERS\\n˜x˜xLLh hfg\\nx xC(˜xx|)Figure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,whichistrainedtoreconstructthecleandatapointxfromitscorruptedversion˜x.ThisisaccomplishedbyminimizingthelossL=−logpdecoder(xh|=f(˜x)),where˜xisacorruptedversionofthedataexamplex,obtainedthroughagivencorruptionprocessC(˜xx|).Typicallythedistributionpdecoderisafactorialdistributionwhosemeanparametersareemittedbyafeedforwardnetwork.gcorruptedsamples˜x,givenadatasamplex.Theautoencoderthenlearnsareconstructiondistributionpreconstruct(x|˜x)(estimatedfromtrainingpairsx,˜x),asfollows:1. Sampleatrainingexamplefromthetrainingdata.x2. Sampleacorruptedversion˜xfromC(˜xx|= )x.3.Use(x,˜x)asatrainingexampleforestimatingtheautoencoderreconstructiondistributionpreconstruct(x|˜x) =pdecoder(xh|)withhtheoutputofencoderf(˜x)andpdecodertypicallydeﬁnedbyadecoder.g()hTypicallywecansimplyperformgradient-basedapproximateminimization(suchasminibatchgradientdescent)onthenegativelog-likelihood−logpdecoder(xh|).Solongastheencoderisdeterministic,thedenoisingautoencoderisafeedforwardnetwork andmay be trainedwithexactly thesame techniques asany otherfeedforwardnetwork.WecanthereforeviewtheDAEasperformingstochasticgradientdescentonthefollowingexpectation:−Ex∼ˆpdata()xE˜x∼C(˜x|x)logpdecoder(= (xh|f˜x))(14.14)whereˆpdata()xisthetrainingdistribution.513'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 528}, page_content='CHAPTER14.AUTOENCODERS\\nx˜xgf◦\\nx˜xC(˜xx|)Figure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜xbacktotheoriginaldatapointx.Weillustratetrainingexamplesxasredcrosseslyingnearalow-dimensionalmanifoldillustratedwiththeboldblackline.WeillustratethecorruptionprocessC(˜xx|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrateshowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors||g(f(˜x))−||x2,thereconstructiong(f(˜x)) estimatesEx,˜x∼pdata()(xC˜xx|)[x|˜x].Thevectorg(f(˜x))−˜xpointsapproximatelytowardsthenearestpointonthemanifold,sinceg(f(˜x))estimatesthecenterofmassofthecleanpointsxwhichcouldhavegivenriseto˜x.Theautoencoderthuslearnsavectorﬁeldg(f(x))−xindicatedbythegreenarrows.Thisvectorﬁeldestimatesthescore∇xlogpdata(x)uptoamultiplicativefactorthatistheaveragerootmeansquarereconstructionerror.\\n514'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 529}, page_content='CHAPTER14.AUTOENCODERS14.5.1EstimatingtheScoreScorematching(,)isanalternativetomaximumlikelihood.ItHyvärinen2005providesaconsistentestimatorofprobabilitydistributionsbasedonencouragingthemodeltohavethesamescoreasthedatadistributionateverytrainingpointx.Inthiscontext,thescoreisaparticulargradientﬁeld:∇xlog()px.(14.15)Score matching isdiscussedfurther inSec..For the present discussion18.4regardingautoencoders,itissuﬃcienttounderstandthatlearningthegradientﬁeldoflogpdataisonewaytolearnthestructureofpdataitself.AveryimportantpropertyofDAEsisthattheirtrainingcriterion (withconditionallyGaussianp(xh|))makes theautoencoder learnavector ﬁeld(g(f(x))−x)thatestimatesthescoreofthedatadistribution.ThisisillustratedinFig..14.4Denoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,linearreconstructionunits)usingGaussiannoiseandmeansquarederrorasthereconstructioncostisequivalent(,)totrainingaspeciﬁckindofVincent2011undirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.ThiskindofmodelwillbedescribedindetailinSec.;forthepresentdiscussion20.5.1itsuﬃcestoknowthatitisamodelthatprovidesanexplicitpmodel(x;θ).WhentheRBMistrainedusingdenoisingscorematching(,),KingmaandLeCun2010itslearningalgorithmisequivalenttodenoisingtraininginthecorrespondingautoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistentestimator;itinsteadrecoversablurredversionofthedistribution.However,ifthenoiselevelischosentoapproach0whenthenumberofexamplesapproachesinﬁnity,thenconsistencyisrecovered. DenoisingscorematchingisdiscussedinmoredetailinSec..18.5OtherconnectionsbetweenautoencodersandRBMsexist.ScorematchingappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerrorcombinedwitharegularizationtermsimilartothecontractivepenaltyoftheCAE(Swersky2011BengioandDelalleau2009etal.,).()showedthatanautoen-codergradientprovidesanapproximationtocontrastivedivergencetrainingofRBMs.Forcontinuous-valuedx,thedenoisingcriterionwithGaussiancorruptionandreconstructiondistributionyieldsanestimatorofthescorethatisapplicabletogeneralencoderanddecoderparametrizations(,).ThisAlainandBengio2013meansagenericencoder-decoderarchitecturemaybemadetoestimatethescore515'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 530}, page_content='CHAPTER14.AUTOENCODERSbytrainingwiththesquarederrorcriterion||gf((˜xx))−||2(14.16)andcorruptionC(˜x=˜xx|) = (N˜xx;= µ,σΣ = 2I)(14.17)withnoisevarianceσ2.SeeFig.foranillustrationofhowthisworks.14.5\\nFigure14.5:Vectorﬁeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifoldnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothereconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobabilityaccordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeldhaszerosatbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminimaofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldoflocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleofthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelengthofthearrows)islarge,itmeansthatprobabilitycanbesigniﬁcantlyincreasedbymovinginthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.Theautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.Whereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmoreaccurate.Ingeneral,thereisnoguaranteethatthereconstructiong(f(x))minustheinputxcorrespondstothegradientofanyfunction,letalonetothescore.Thatis516'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 531}, page_content='CHAPTER14.AUTOENCODERSwhytheearlyresults(,)arespecializedtoparticularparametrizationsVincent2011whereg(f(x))−xmaybeobtainedbytakingthederivativeofanotherfunction.KamyshanskaandMemisevic2015Vincent2011()generalizedtheresultsof()byidentifyingafamilyofshallowautoencoderssuchthatg(f(x))−xcorrespondstoascoreforallmembersofthefamily.Sofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresentaprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderasagenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribedlater,inSec..20.1114.5.1.1HistoricalPerspectiveTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987and().Gallinarietal.1987Behnke2001()alsousedrecurrentnetworkstodenoiseimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.However,thename“denoisingautoencoder”referstoamodelthatisintendednotmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentationas asideeﬀect oflearningto denoise.This ideacame muchlater (Vincentetal.,,).Thelearnedrepresentationmaythenbeusedtopretraina20082010deeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,sparsecoding,contractiveautoencodersandotherregularizedautoencoders,themotivationforDAEswastoallowthelearningofaveryhigh-capacityencoderwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.PriortotheintroductionofthemodernDAE,InayoshiandKurita2005()exploredsomeofthesamegoalswithsomeofthesamemethods.TheirapproachminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjectingnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprovegeneralizationby introducing thereconstruction errorand theinjected noise.However,theirmethodwasbasedonalinearencoderandcouldnotlearnfunctionfamiliesaspowerfulascanthemodernDAE.14.6LearningManifoldswithAutoencodersLike many othermachine learning algorithms, autoencoders exploitthe ideathatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuchmanifolds,asdescribedinSec..Somemachinelearningalgorithmsexploit5.11.3thisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthemanifoldbutmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold.517'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 532}, page_content='CHAPTER14.AUTOENCODERSAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.Tounderstandhowautoencodersdothis,wemustpresentsomeimportantcharacteristicsofmanifolds.Animportantcharacterizationofamanifoldisthesetofits.Attangentplanesapointxonad-dimensionalmanifold,thetangentplaneisgivenbydbasisvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.AsillustratedinFig.,theselocaldirectionsspecifyhowonecanchange14.6xinﬁnitesimallywhilestayingonthemanifold.Allautoencodertrainingproceduresinvolveacompromisebetweentwoforces:1.Learningarepresentationhofatrainingexamplexsuchthatxcanbeapproximatelyrecoveredfromhthroughadecoder.Thefactthatxisdrawnfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneednotsuccessfullyreconstructinputsthatarenotprobableunderthedatageneratingdistribution.2. Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbearegularizationtermaddedtothereconstructioncost.Thesetechniquesgenerallyprefersolutionsthatarelesssensitivetotheinput.Clearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutputisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogetherareusefulbecausetheyforcethehiddenrepresentationtocaptureinformationaboutthestructureofthedatageneratingdistribution.Theimportantprincipleisthattheautoencodercanaﬀordtorepresentonlythevariationsthatareneededtoreconstructtrainingexamples.Ifthedatageneratingdistributionconcentratesnearalow-dimensionalmanifold,thisyieldsrepresentationsthatimplicitlycapturealocalcoordinatesystemforthismanifold:onlythevariationstangenttothemanifoldaroundxneedtocorrespondtochangesinh=f(x).Hencetheencoderlearnsamappingfromtheinputspacextoarepresentationspace,amappingthatisonlysensitivetochangesalongthemanifolddirections,butthatisinsensitivetochangesorthogonaltothemanifold.Aone-dimensionalexampleisillustratedinFig.,showingthatbymaking14.7thereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthedatapointswerecoverthemanifoldstructure.Tounderstandwhyautoencodersareusefulformanifoldlearning,itisinstruc-tivetocomparethemtootherapproaches. Whatismostcommonlylearnedtocharacterizeamanifoldisarepresentationofthedatapointson(ornear)the518'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 533}, page_content='CHAPTER14.AUTOENCODERS\\nFigure14.6: Anillustrationoftheconceptofatangenthyperplane.Herewecreateaone-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784pixelsandtransformitbytranslatingitvertically. Theamountofverticaltranslationdeﬁnesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpaththroughimagespace.Thisplotshowsafewpointsalongthismanifold.Forvisualization,wehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.Ann-dimensionalmanifoldhasann-dimensionaltangentplaneateverypoint.Thistangentplanetouchesthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.Itdeﬁnesthespaceofdirectionsinwhichitispossibletomovewhileremainingonthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicateanexampletangentlineatonepoint,withanimageshowinghowthistangentdirectionappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealongthetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixelsthatdarken.519'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 534}, page_content='CHAPTER14.AUTOENCODERS\\nx0x1x2x00.02.04.06.08.10.rx()IdentityOptimalreconstruction\\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmallperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Herethemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal0lineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstructionfunctioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontalarrowsatthebottomoftheplotindicatether(x)−xreconstructiondirectionvectoratthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest“manifold”(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomakethederivativeofthereconstructionfunctionr(x)smallaroundthedatapoints.Thecontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeofr(x)isaskedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.Thespacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,wherethereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpointsbackontothemanifold.manifold.Sucharepresentationforaparticularexampleisalsocalleditsem-bedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensionsthanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Somealgorithms(non-parametricmanifoldlearningalgorithms,discussedbelow)directlylearnanembeddingforeachtrainingexample,whileotherslearnamoregeneralmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsanypointintheambientspace(theinputspace)toitsembedding.Manifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthatattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearchonlearningnonlinearmanifoldshasfocusedonnon-parametricmethodsbasedonthenearest-neighborgraph.Thisgraphhasonenodepertrainingexampleandedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopfetal.,1998RoweisandSaul2000Tenenbaum2000Brand2003Belkinand;,;etal.,;,;520'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 535}, page_content='CHAPTER14.AUTOENCODERS\\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraphwhosenodesaretrainingexamplesandarcsconnectnearestneighbors.Variousprocedurescanthusobtainthetangentplaneassociatedwithaneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtrainingexamplewithareal-valuedvectorposition,orembedding.Itispossibletogeneralizesucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumberofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,theseapproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset(,).Gongetal.2000Niyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hintonand,;,;,;Roweis2003vanderMaatenandHinton2008,;,)associateeachofnodeswithatangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerencevectorsbetweentheexampleanditsneighbors,asillustratedinFig..14.8Aglobalcoordinatesystemcanthenbeobtainedthroughanoptimizationorsolvingalinearsystem.Fig.illustrateshowamanifoldcanbetiledbya14.9largenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausetheGaussiansareﬂatinthetangentdirections).However,thereisafundamentaldiﬃcultywithsuchlocalnon-parametricapproachestomanifoldlearning,raisedin():iftheBengioandMonperrus2005manifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),onemayneedaverylargenumberoftrainingexamplestocovereachoneofthesevariations,withnochancetogeneralizetounseenvariations.Indeed,thesemethods521'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 536}, page_content='CHAPTER14.AUTOENCODERS\\nFigure14.9:Ifthetangentplanes(seeFig.)ateachlocationareknown,thenthey14.6canbetiledtoformaglobalcoordinatesystemoradensityfunction.EachlocalpatchcanbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or“pancake”,withaverysmallvarianceinthedirectionsorthogonaltothepancakeandaverylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.AmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifoldParzenwindowalgorithm(,)oritsnon-localneural-netbasedVincentandBengio2003variant(,).Bengioetal.2006ccanonlygeneralizetheshapeofthemanifoldbyinterpolatingbetweenneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscanhaveverycomplicatedstructurethatcanbediﬃculttocapturefromonlylocalinterpolation.ConsiderforexamplethemanifoldresultingfromtranslationshowninFig..If14.6wewatchjustonecoordinatewithintheinputvector,xi,astheimageistranslated,wewillobservethatonecoordinateencountersapeakoratroughinitsvalueonceforeverypeakortroughinbrightnessintheimage.Inotherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimagetemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperformingsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentationsanddeeplearningforcapturingmanifoldstructure.522'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 537}, page_content='CHAPTER14.AUTOENCODERS14.7ContractiveAutoencodersThecontractiveautoencoder(,,)introducesanexplicitregularizerRifaietal.2011abonthecodeh=f(x),encouragingthederivativesofftobeassmallaspossible:Ω() = hλ\\ue00d\\ue00d\\ue00d\\ue00d∂f()x∂x\\ue00d\\ue00d\\ue00d\\ue00d2F.(14.18)ThepenaltyΩ(h)isthesquaredFrobeniusnorm(sumofsquaredelements)oftheJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.Thereisaconnectionbetweenthedenoisingautoencoderandthecontractiveautoencoder:()showedthatinthelimitofsmallGaussianAlainandBengio2013input noise, the denoising reconstructionerror is equivalentto acontractivepenaltyonthereconstructionfunctionthatmapsxtor=g(f(x)).Inotherwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbutﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethefeatureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.WhenusingtheJacobian-basedcontractivepenaltytopretrainfeaturesf(x)forusewithaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthecontractivepenaltytof(x)ratherthantog(f(x)).Acontractivepenaltyonf(x)alsohascloseconnectionstoscorematching,asdiscussedinSec..14.5.1ThenamecontractivearisesfromthewaythattheCAEwarpsspace.Speciﬁ-cally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouragedtomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.Wecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutputneighborhood.Toclarify,theCAEiscontractiveonlylocally—allperturbationsofatrainingpointxaremappedneartof(x).Globally,twodiﬀerentpointsxandx\\ue030maybemappedtof(x)andf(x\\ue030)pointsthatarefartherapartthantheoriginalpoints.Itisplausiblethatfbeexpandingin-betweenorfarfromthedatamanifolds(seeforexamplewhathappensinthe1-DtoyexampleofFig.).Whenthe14.7Ω(h)penaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianistomakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode01inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasabinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughoutmostofthehypercubethatitssigmoidalhiddenunitscanspan.WecanthinkoftheJacobianmatrixJatapointxasapproximatingthenonlinearencoderf(x)asbeingalinearoperator.Thisallowsustousetheword“contractive”moreformally. Inthetheoryoflinearoperators,alinearoperator523'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 538}, page_content='CHAPTER14.AUTOENCODERSissaidtobecontractiveifthenormofJxremainslessthanorequaltofor1allunit-normx.Inotherwords,Jiscontractiveifitshrinkstheunitsphere.WecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinearapproximationoff(x)ateverytrainingpointxinordertoencourageeachoftheselocallinearoperatortobecomeacontraction.AsdescribedinSec.,regularizedautoencoderslearnmanifoldsbybalancing14.6twoopposingforces.InthecaseoftheCAE,thesetwoforcesarereconstructionerrorandthecontractivepenaltyΩ(h).ReconstructionerroralonewouldencouragetheCAEtolearnanidentityfunction.ThecontractivepenaltyalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespecttox.Thecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives∂f()x∂xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoasmallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives.ThegoaloftheCAEistolearnthemanifoldstructureofthedata.DirectionsxwithlargeJxrapidlychangeh,sothesearelikelytobedirectionswhichapproximatethetangentplanesofthemanifold.Experimentsby()Rifaietal.2011aand()showthattrainingtheCAEresultsinmostsingularvaluesRifaietal.2011bofJdroppingbelowinmagnitudeandthereforebecomingcontractive.However,1somesingularvaluesremainabove,becausethereconstructionerrorpenalty1encouragestheCAEtoencodethedirectionswiththemostlocalvariance.Thedirectionscorrespondingtothelargestsingularvaluesareinterpretedasthetangentdirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangentdirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAEappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesasobjectsintheimagegraduallychangepose,asshowninFig..Visualizationsof14.6theexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningfultransformationsoftheinputimage,asshowninFig..14.10OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughitischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomesmuchmoreexpensiveinthecaseofdeeperautoencoders.ThestrategyfollowedbyRifai2011aetal.()istoseparatelytrainaseriesofsingle-layerautoencoders,eachtrainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecompositionoftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwasseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractiveaswell.TheresultisnotthesameaswhatwouldbeobtainedbyjointlytrainingtheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butitcapturesmanyofthedesirablequalitativecharacteristics.Anotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults524'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 539}, page_content='CHAPTER14.AUTOENCODERSInputpointTangentvectorsLocalPCA(nosharingacrossregions)ContractiveautoencoderFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCAandbyacontractiveautoencoder.ThelocationonthemanifoldisdeﬁnedbytheinputimageofadogdrawnfromtheCIFAR-10dataset. ThetangentvectorsareestimatedbytheleadingsingularvectorsoftheJacobianmatrix∂h∂xoftheinput-to-codemapping.AlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisabletoformmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparametersharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits. TheCAEtangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchastheheadorlegs).ifwedonotimposesomesortofscaleonthedecoder.Forexample,theencodercouldconsistofmultiplyingtheinputbyasmallconstant\\ue00fandthedecodercouldconsistofdividingthecodeby\\ue00f.As\\ue00fapproaches,theencoderdrivesthe0contractivepenaltyΩ(h)toapproachwithouthavinglearnedanythingaboutthe0distribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifaietal.(),thisispreventedbytyingtheweightsof2011afandg.Bothfandgarestandardneuralnetworklayersconsistingofanaﬃnetransformationfollowedbyanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixofgtobethetransposeoftheweightmatrixof.f14.8PredictiveSparseDecompositionPredictive sparse decomposition (PSD) isa model that is ahybrid of sparsecodingandparametricautoencoders(Kavukcuoglu2008etal.,).Aparametricencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeenappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo(Kavukcuoglu20092010Jarrett2009Farabet2011etal.,,;etal.,;etal.,),aswellasforaudio(,).ThemodelconsistsofanencoderHenaﬀetal.2011f(x)andadecoderg(h)thatarebothparametric.Duringtraining,hiscontrolledbythe525'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 540}, page_content='CHAPTER14.AUTOENCODERSoptimizationalgorithm.Trainingproceedsbyminimizing||−||xg()h2+λ||h1+()γf||−hx||2.(14.19)Likeinsparsecoding,thetrainingalgorithmalternatesbetweenminimizationwithrespecttohandminimizationwithrespecttothemodelparameters.Minimizationwithrespecttohisfastbecausef(x)providesagoodinitialvalueofhandthecostfunctionconstrainshtoremainnearf(x)anyway.Simplegradientdescentcanobtainreasonablevaluesofinasfewastensteps.hThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparsecodingmodelandthentrainingf(x)topredictthevaluesofthesparsecodingfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparametersforwhichcaninfergoodcodevalues.f()xPredictivesparsecodingisanexampleoflearnedapproximateinference.InSec.19.5,thistopicisdevelopedfurther.ThetoolspresentedinChaptermakeit19clearthatPSDcanbeinterpretedastrainingadirectedsparsecodingprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthemodel.InpracticalapplicationsofPSD,theiterativeoptimizationisonlyusedduringtraining.Theparametricencoderfisusedtocomputethelearnedfeatureswhenthemodelisdeployed.Evaluatingfiscomputationallyinexpensivecomparedtoinferringhviagradientdescent.Becausefisadiﬀerentiableparametricfunction,PSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrainedwithanothercriterion.14.9ApplicationsofAutoencodersAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-mationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplicationsofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivationsforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trainedastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoderwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.TheresultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsandthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetotheunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.Lower-dimensionalrepresentationscanimproveperformanceonmanytasks,suchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemoryandruntime.Manyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear526'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 541}, page_content='CHAPTER14.AUTOENCODERSeachother,asobservedbySalakhutdinovandHinton2007bTorralba()andetal.().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid2008generalization.Onetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionisinformationretrieval,thetaskofﬁndingentriesinadatabasethatresembleaqueryentry. Thistaskderivestheusualbeneﬁtsfromdimensionalityreductionthatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecomeextremelyeﬃcientincertainkindsoflowdimensionalspaces.Speciﬁcally, ifwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-dimensionalandbinary,thenwecanstorealldatabaseentriesinahashtablemappingbinarycodevectorstoentries.Thishashtableallowsustoperforminformationretrievalbyreturningalldatabaseentriesthathavethesamebinarycodeasthe query.Wecanalsosearchover slightlylesssimilarentriesveryeﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery. Thisapproachtoinformationretrievalviadimensionalityreductionandbinarizationiscalled(semantichashingSalakhutdinovandHinton2007b2009b,,),andhasbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)andimages(Torralba2008Weiss2008KrizhevskyandHinton2011etal.,;etal.,;,).Toproducebinarycodesforsemantichashing,onetypicallyusesanencodingfunctionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobesaturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplishthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduringtraining.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthatnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethemagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.Theideaoflearningahashingfunctionhasbeenfurtherexploredinseveraldirections,includingtheideaoftrainingtherepresentationssoastooptimizealossmoredirectlylinkedtothetaskofﬁndingnearbyexamplesinthehashtable(,).NorouziandFleet2011\\n527'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 542}, page_content='Chapter15RepresentationLearningInthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhowthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscusshowlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,includingusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Sharedrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransferlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutataskrepresentationexists.Finally,westepbackandargueaboutthereasonsforthesuccessofrepresentationlearning,startingwiththetheoreticaladvantagesofdistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsandendingwiththemoregeneralideaofunderlyingassumptionsaboutthedatageneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.Manyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdependingonhowtheinformationisrepresented.Thisisageneralprincipleapplicabletodailylife,computerscienceingeneral,andtomachinelearning.Forexample,itisstraightforwardforapersontodivide210by6usinglongdivision. ThetaskbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRomannumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCXbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,permittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.Moreconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusingappropriateorinappropriaterepresentations.Forexample,insertinganumberintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthelistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasared-blacktree.Inthecontextofmachinelearning,whatmakesonerepresentationbetterthan528'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 543}, page_content='CHAPTER15.REPRESENTATIONLEARNINGanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequentlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoiceofthesubsequentlearningtask.Wecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-formingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetworkistypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestofthenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwithasupervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(butmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcationtaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinputfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,thelastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershouldlearndiﬀerentpropertiesdependingonthetypeofthelastlayer.Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposinganyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentationlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationinsomeparticularway.Forexample,supposewewanttolearnarepresentationthatmakesdensityestimationeasier.Distributionswithmoreindependencesareeasiertomodel,sowecoulddesignanobjectivefunctionthatencouragestheelementsoftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalsolearnarepresentationasasideeﬀect.Regardlessofhowarepresentationwasobtained,itcancanbeusedforanothertask.Alternatively,multipletasks(somesupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternalrepresentation.Mostrepresentationlearningproblemsfaceatradeoﬀbetweenpreservingasmuchinformationabouttheinputaspossibleandattainingniceproperties(suchasindependence).Representationlearningisparticularlyinterestingbecauseitprovidesonewaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhaveverylargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtrainingdata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoftenresultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolvethisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,wecanlearngoodrepresentationsfortheunlabeleddata,andthenusetheserepresentationstosolvethesupervisedlearningtask.Humansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo529'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 544}, page_content='CHAPTER15.REPRESENTATIONLEARNINGnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhumanperformance—forexample,thebrainmayuseverylargeensemblesofclassiﬁersorBayesianinferencetechniques.Onepopularhypothesisisthatthebrainisabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanywaystoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthattheunlabeleddatacanbeusedtolearnagoodrepresentation.15.1GreedyLayer-WiseUnsupervisedPretrainingUnsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneuralnetworks,allowingfortheﬁrsttimetotrainadeepsupervisednetworkwithoutrequiringarchitecturalspecializationslikeconvolutionorrecurrence.Wecallthisprocedureunsupervisedpretraining,ormoreprecisely,greedylayer-wiseunsuper-visedpretraining.Thisprocedureisacanonicalexampleofhowarepresentationlearnedforonetask(unsupervisedlearning,tryingtocapturetheshapeoftheinputdistribution)cansometimesbeusefulforanothertask(supervisedlearningwiththesameinputdomain).Greedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-tationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparsecodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayerispretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayerandproducingasoutputanewrepresentationofthedata,whosedistribution(oritsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.SeeAlgorithmforaformaldescription.15.1Greedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelongbeenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnetforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscoverythatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitializationforajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeusedtosuccessfullytrainevenfullyconnectedarchitectures(Hinton2006Hintonetal.,;andSalakhutdinov2006Hinton2006Bengio2007Ranzato2007a,;,;etal.,;etal.,).Priortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepthresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknowthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeeparchitectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodtosucceed.Greedylayer-wisepretrainingiscalledgreedygreedyalgorithmbecauseitisa,530'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 545}, page_content='CHAPTER15.REPRESENTATIONLEARNINGmeaningthatitoptimizeseachpieceofthesolutionindependently,onepieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalledbecausetheselayer-wiseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedylayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhilekeepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrainedﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalledunsupervisedbecauseeachlayeristrainedwithanunsupervisedrepresentationlearningalgorithm.Howeveritisalsocalledpretraining,becauseitissupposedtobeonlyaﬁrststepbeforeajointtrainingalgorithmisappliedtoallthelayerstogether.Intheﬁne-tunecontextofasupervisedlearningtask,itcanbeviewedasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithoutdecreasingtrainingerror)andaformofparameterinitialization.Itiscommontousetheword“pretraining”torefernotonlytothepretrainingstageitselfbuttotheentiretwophaseprotocolthatcombinesthepretrainingphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolvetrainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,oritmayinvolvesupervisedﬁne-tuningoftheentirenetworklearnedinthepretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmorwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltrainingschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithmwillobviouslyimpactthedetails,mostapplicationsofunsupervisedpretrainingfollowthisbasicprotocol.Greedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitializationforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(HintonandSalakhutdinov2006,)andprobabilisticmodelswithmanylayersoflatentvariables.Suchmodelsincludedeepbeliefnetworks(,)anddeepHintonetal.2006Boltzmannmachines(SalakhutdinovandHinton2009a,).ThesedeepgenerativemodelswillbedescribedinChapter.20AsdiscussedinSec.,itisalsopossibletohavegreedylayer-wise8.7.4super-visedpretraining. Thisbuildsonthepremisethattrainingashallownetworkiseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveralcontexts(,).Erhanetal.201015.1.1WhenandWhyDoesUnsupervisedPretrainingWork?Onmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantialimprovementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsiblefortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,531'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 546}, page_content='CHAPTER15.REPRESENTATIONLEARNINGAlgorithm15.1Greedylayer-wiseunsupervisedpretrainingprotocol.Giventhefollowing: UnsupervisedfeaturelearningalgorithmL,whichtakesatrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.TherawinputdataisX,withonerowperexampleandf(1)(X)istheoutputoftheﬁrststageencoderonXandthedatasetusedbythesecondlevelunsupervisedfeaturelearner.Inthecasewhereﬁne-tuningisperformed,weusealearnerTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervisedﬁne-tuningcase,associatedtargets),andreturnsatunedfunction.Thenumberofstagesis.Ymf←Identityfunction˜XX= fordok,...,m= 1f()k= (L˜X)ff←()k◦f˜X←f()k(˜X)endforifﬁne-tuningthenff,,←T(XY)endifReturnf2006Bengio2007Ranzato2007a;etal.,;etal.,).Onmanyothertasks,however,unsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeableharm.()studiedtheeﬀectofpretrainingonmachinelearningMaetal.2015modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwasslightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervisedpretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstandwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticulartask.Attheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestrictedtogreedyunsupervisedpretraininginparticular.Thereareother,completelydiﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,suchasvirtualadversarialtrainingdescribedinSec. .Itisalsopossibleto7.13trainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.Examplesofthissingle-stageapproachincludethediscriminativeRBM(LarochelleandBengio2008,)andtheladdernetwork(,),inwhichthetotalRasmusetal.2015objectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonlyusingtheinput).Unsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof532'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 547}, page_content='CHAPTER15.REPRESENTATIONLEARNINGtheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhaveasigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcanimproveoptimization).Second,itmakesuseofthemoregeneralideathatlearningabouttheinputdistributioncanhelptolearnaboutthemappingfrominputstooutputs.Bothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveralpartsofthemachinelearningalgorithmthatarenotentirelyunderstood.Theﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetworkcanhaveastrongregularizingeﬀectonitsperformance,istheleastwellunderstood.Atthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthemodelinalocationthatwouldcauseittoapproachonelocalminimumratherthananother. Today,localminimaarenolongerconsideredtobeaseriousproblemforneuralnetworkoptimization.Wenowknowthatourstandardneuralnetworktrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremainspossiblethatpretraininginitializesthemodelinalocationthatwouldotherwisebeinaccessible—forexample,aregionthatissurroundedbyareaswherethecostfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonlyaverynoisyestimateofthegradient,oraregionsurroundedbyareaswheretheHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuseverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthepretrainedparametersareretainedduringthesupervisedtrainingstageislimited.Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervisedlearningandsupervisedlearningratherthantwosequentialstages.Onemayalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimizationinthesupervisedlearningstagepreservesinformationfromtheunsupervisedlearningstagebysimplyfreezingthe parametersfor thefeature extractorsand usingsupervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures.Theotheridea,thatalearningalgorithmcanuseinformationlearnedintheunsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetterunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervisedtaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrainagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowaboutwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,therepresentationofthewheelswilltakeonaformthatiseasyforthesupervisedlearnertoaccess.Thisisnotyetunderstoodatamathematical,theoreticallevel,soitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervisedlearninginthisway.Manyaspectsofthisapproacharehighlydependentonthespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron533'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 548}, page_content='CHAPTER15.REPRESENTATIONLEARNINGtopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearlyseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.Thisisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbepreferable—theconstraintsimposedbytheoutputlayerarenaturallyincludedfromthestart.Fromthepointofviewofunsupervisedpretrainingaslearningarepresentation,wecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitialrepresentationispoor. Onekeyexampleofthisistheuseofwordembeddings.Wordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwodistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distanceof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir2distancefromeachother.Becauseofthis,unsupervisedpretrainingisespeciallyusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhapsbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealowqualitysimilaritymetric.Fromthepointofviewofunsupervisedpretrainingasaregularizer,wecanexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeledexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervisedpretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretrainingtoperformbest whenthe number ofunlabeled examplesis very large.Theadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmanyunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin2011withunsupervisedpretrainingwinningtwointernationaltransferlearningcompetitions(,;,),insettingswheretheMesniletal.2011Goodfellowetal.2011numberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozensofexamplesperclass).TheseeﬀectswerealsodocumentedincarefullycontrolledexperimentsbyPaine2014etal.().Otherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretrainingislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.Unsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnotbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscoveringfeaturefunctionsthatareusefulfortheunsupervisedlearningtask. Ifthetrueunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinputdistribution,unsupervisedlearningcanbeamoreappropriateregularizer.Thesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervisedpretrainingisknowntocauseanimprovement,andexplainwhatisknownaboutwhythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenusedtoimproveclassiﬁers,andisusuallymostinterestingfromthepointofviewof534'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 549}, page_content='CHAPTER15.REPRESENTATIONLEARNING\\n\\U000f0913\\ue034\\ue030\\ue030\\ue030\\U000f0913\\ue033\\ue030\\ue030\\ue030\\U000f0913\\ue032\\ue030\\ue030\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue030\\ue030\\ue031\\ue030\\ue030\\ue030\\ue032\\ue030\\ue030\\ue030\\ue033\\ue030\\ue030\\ue030\\ue034\\ue030\\ue030\\ue030\\U000f0913\\ue031\\ue035\\ue030\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue030\\U000f0913\\ue035\\ue030\\ue030\\ue030\\ue035\\ue030\\ue030\\ue031\\ue030\\ue030\\ue030\\ue031\\ue035\\ue030\\ue030\\ue057\\ue069\\ue074\\ue068\\ue020\\ue070\\ue072\\ue065\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue057\\ue069\\ue074\\ue068\\ue06f\\ue075\\ue074\\ue020\\ue070\\ue072\\ue065\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerentneuralnetworksinfunctionspace(notparameterspace,toavoidtheissueofmany-to-onemappingsfromparametervectorstofunctions),withdiﬀerentrandominitializationsandwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerentneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisﬁgureisadaptedwithpermissionfrom().Acoordinateinfunctionspaceisaninﬁnite-Erhanetal.2010dimensionalvectorassociatingeveryinputxwithanoutputy.()madeErhanetal.2010alinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciﬁcxpoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaumetal.,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot2000(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributionsovertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,topointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhenusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.Isomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregioncorrespondingtopretrainedmodelsmayindicatethatthepretraining-basedestimatorhasreducedvariance.\\n535'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 550}, page_content='CHAPTER15.REPRESENTATIONLEARNINGreducingtestseterror.However,unsupervisedpretrainingcanhelptasksotherthanclassiﬁcation,andcanacttoimproveoptimizationratherthanbeingmerelyaregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerrorfordeepautoencoders(HintonandSalakhutdinov2006,).Erhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesofunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovementstotesterrormaybeexplainedintermsofunsupervisedpretrainingtakingtheparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetworktrainingisnon-deterministic,andconvergestoadiﬀerentfunctioneverytimeitisrun. Trainingmayhaltatapointwherethegradientbecomessmall,apointwhereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethegradientislargebutitisdiﬃculttoﬁndadownhillstepduetoproblemssuchasstochasticityorpoorconditioningoftheHessian. Neuralnetworksthatreceiveunsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,whileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.SeeFig.foravisualizationofthisphenomenon.Theregionwherepretrained15.1networksarriveissmaller,suggestingthatpretrainingreducesthevarianceoftheestimationprocess,whichcaninturnreducetheriskofsevereover-ﬁtting.Inotherwords,unsupervisedpretraininginitializesneuralnetworkparametersintoaregionthattheydonotescape,andtheresultsfollowingthisinitializationaremoreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.Erhan2010etal.()alsoprovidesomeanswersastowhenpretrainingworksbest—themeanandvarianceofthetesterrorweremostreducedbypretrainingfordeepernetworks.Keepinmindthattheseexperimentswereperformedbeforetheinventionandpopularizationofmoderntechniquesfortrainingverydeepnetworks(rectiﬁedlinearunits,dropoutandbatchnormalization)solessisknownabouttheeﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.Onehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscoverfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.Thisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervisedpretraining,andisdescribedfurtherinSec..15.3Comparedtootherwaysofincorporatingthisbeliefbyusingunsupervisedlearning,unsupervisedpretraininghasthedisadvantagethatitoperateswithtwoseparatetrainingphases.Onereasonthatthesetwotrainingphasesaredisadvantageousisthatthereisnotasinglehyperparameterthatpredictablyreducesorincreasesthestrengthoftheregularizationarisingfromtheunsupervisedpretraining.Instead,thereareverymanyhyperparameters,whoseeﬀectmaybe536'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 551}, page_content='CHAPTER15.REPRESENTATIONLEARNINGmeasuredafterthefactbutisoftendiﬃculttopredictaheadoftime.Whenweperformunsupervisedandsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,thereisasinglehyperparameter,usuallyacoeﬃcientattachedtotheunsupervisedcost,thatdetermineshowstronglytheunsupervisedobjectivewillregularizethesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationbydecreasingthiscoeﬃcient.Inthecaseofunsupervisedpretraining,thereisnotawayofﬂexiblyadaptingthestrengthoftheregularization—eitherthesupervisedmodelisinitializedtopretrainedparameters,oritisnot.Anotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphasehasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannotbepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposinghyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedbackfromthesecondphase.Themostprincipledapproachistousevalidationseterrorinthesupervisedphaseinordertoselectthehyperparametersofthepretrainingphase,asdiscussedin().Inpractice,somehyperparameters,Larochelleetal.2009likethenumberofpretrainingiterations,aremoreconvenientlysetduringthepretrainingphase,usingearlystoppingontheunsupervisedobjective,whichisnotidealbutcomputationallymuchcheaperthanusingthesupervisedobjective.Today,unsupervisedpretraininghasbeenlargelyabandoned,exceptintheﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsasone-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeledsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrainonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsofwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),andthenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthetrainingsetcontainssubstantiallyfewerexamples.ThisapproachwaspioneeredbybyCollobertandWeston2008bTurian2010Collobert(),etal.(),andetal.()andremainsincommonusetoday.2011aDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropoutorbatchnormalization,areabletoachievehuman-levelperformanceonverymanytasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesoutperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10andMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmalldatasets,suchasthealternativesplicingdataset,Bayesianmethodsoutper-formmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,thepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervisedpretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearchandcontinuestoinﬂuencecontemporaryapproaches.Theideaof537'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 552}, page_content='CHAPTER15.REPRESENTATIONLEARNINGpretraininghasbeengeneralizedtosupervisedpretrainingdiscussedinSec.,8.7.4asaverycommonapproachfortransferlearning.Supervisedpretrainingfortransferlearningispopular(,;Oquabetal.2014Yosinski2014etal.,)forusewithconvolutionalnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparametersofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsarepublishedfornaturallanguagetasks(,;Collobertetal.2011aMikolovetal.,).2013a15.2TransferLearningandDomainAdaptationTransferlearninganddomainadaptationrefertothesituationwherewhathasbeenlearnedinonesetting(i.e.,distributionP1)isexploitedtoimprovegeneralizationinanothersetting(saydistributionP2).Thisgeneralizestheideapresentedintheprevioussection,wherewetransferredrepresentationsbetweenanunsupervisedlearningtaskandasupervisedlearningtask.Intransferlearning,thelearnermustperformtwoormorediﬀerenttasks,butweassumethatmanyofthefactorsthatexplainthevariationsinP1arerelevanttothevariationsthatneedtobecapturedforlearningP2.Thisistypicallyunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthetargetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetofvisualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnaboutadiﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.Ifthereissigniﬁcantlymoredataintheﬁrstsetting(sampledfromP1),thenthatmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonlyveryfewexamplesdrawnfromP2.Manyvisualcategoriessharelow-levelnotionsofedgesandvisualshapes,theeﬀectsofgeometricchanges,changesinlighting,etc.Ingeneral,transferlearning,multi-tasklearning(Sec.),anddomainadaptation7.7canbeachievedviarepresentationlearningwhenthereexistfeaturesthatareusefulforthediﬀerentsettingsortasks,correspondingtounderlyingfactorsthatappearinmorethanonesetting.ThisisillustratedinFig.,withsharedlower7.2layersandtask-dependentupperlayers.However, sometimes, whatisshared amongthe diﬀerent tasksisnotthesemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeechrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,buttheearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversionsofthesamephonemesorsub-phonemicvocalizationsdependingonwhichpersonisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers(neartheoutput)oftheneuralnetwork,andhaveatask-speciﬁcpreprocessing,as538'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 553}, page_content='CHAPTER15.REPRESENTATIONLEARNINGillustratedinFig..15.2\\nSelection switchh(1)h(1)h(2)h(2)h(3)h(3)y yh(shared)h(shared)\\nx(1)x(1)x(2)x(2)x(3)x(3)Figure15.2: Examplearchitectureformulti-taskortransferlearningwhentheoutputvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerentyx meaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,eachuser),calledx(1),x(2)andx(3)forthreetasks.Thelowerlevels(uptotheselectionswitch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearntotranslatetheirtask-speciﬁcinputintoagenericsetoffeatures.Intherelatedcaseof,thetask(andtheoptimalinput-to-domainadaptationoutputmapping)remainsthesamebetweeneachsetting,buttheinputdistributionisslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,whichconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.Commentspostedonthewebcomefrommanycategories.Adomainadaptationscenariocanarisewhenasentimentpredictortrainedoncustomerreviewsofmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecommentsaboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimaginethatthereisanunderlyingfunctionthattellswhetheranystatementispositive,neutralornegative,butofcoursethevocabularyandstylemayvaryfromonedomaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simpleunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobeverysuccessfulforsentimentanalysiswithdomainadaptation(,).Glorotetal.2011bArelatedproblemisthatofconceptdrift,whichwecanviewasaformoftransferlearningduetogradualchangesinthedatadistributionovertime.Bothconceptdriftandtransferlearningcanbeviewedasparticularformsofmulti-tasklearning.539'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 554}, page_content='CHAPTER15.REPRESENTATIONLEARNINGWhilethephrase“multi-tasklearning”typicallyreferstosupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicabletounsupervisedlearningandreinforcementlearningaswell.Inallofthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrstsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhendirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentationlearningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthesamerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthetrainingdatathatisavailableforbothtasks.Asmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfoundsuccessinsomemachinelearningcompetitions(,;Mesniletal.2011Goodfellowetal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe2011following.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(fromdistributionP1),illustratingexamplesofsomesetofcategories.Theparticipantsmustusethistolearnagoodfeaturespace(mappingtherawinputtosomerepresentation),suchthatwhenweapplythislearnedtransformationtoinputsfromthetransfersetting(distributionP2),alinearclassiﬁercanbetrainedandgeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresultsfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperanddeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollectedintheﬁrstsetting,P1),thelearningcurveonthenewcategoriesofthesecond(transfer)settingP2becomesmuchbetter.Fordeeprepresentations,fewerlabeledexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptoticgeneralizationperformance.Twoextremeformsoftransferlearningareone-shotlearningzero-shotandlearningzero-datalearning,sometimesalsocalled.Onlyonelabeledexampleofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesaregivenatallforthezero-shotlearningtask.One-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentationlearnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthetransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmanypossibletestexamplesthatallclusteraroundthesamepointinrepresentationspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingtotheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearnedrepresentationspace,andwehavesomehowlearnedwhichfactorsdoanddonotmatterwhendiscriminatingobjectsofcertaincategories.Asanexampleofazero-shotlearningsetting,considertheproblemofhavingalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.540'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 555}, page_content='CHAPTER15.REPRESENTATIONLEARNINGItmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseenanimageofthatobject,ifthetextdescribestheobjectwellenough. Forexample,havingreadthatacathasfourlegsandpointyears,thelearnermightbeabletoguessthatanimageisacat,withouthavingseenacatbefore.Zero-datalearning(Larochelle2008Palatuccietal.,)andzero-shotlearning(etal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformationhasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenarioasincludingthreerandomvariables:thetraditionalinputsx,thetraditionaloutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.Themodelistrainedtoestimatetheconditionaldistributionp(yx|,T)whereTisadescriptionofthetaskwewishthemodeltoperform. Inourexampleofrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariableywithy= 1indicating“yes”andy= 0indicating“no.”ThetaskvariableTthenrepresentsquestionstobeansweredsuchas“Isthereacatinthisimage?”IfwehaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthesamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.Inourexampleofrecognizingcatswithouthavingseenanimageofthecat,itisimportantthatwehavehadunlabeledtextdatacontainingsentencessuchas“catshavefourlegs”or“catshavepointyears.”Zero-shotlearningrequiresTtoberepresentedinawaythatallowssomesortofgeneralization.Forexample,Tcannotbejustaone-hotcodeindicatinganobjectcategory.()provideinsteadadistributedrepresentationSocheretal.2013bofobjectcategoriesbyusingalearnedwordembeddingforthewordassociatedwitheachcategory.Asimilarphenomenonhappensinmachinetranslation(Klementiev2012etal.,;Mikolov2013bGouws2014etal.,;etal.,):wehavewordsinonelanguage,andtherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;ontheotherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewithwordsintheother.EventhoughwemaynothavelabeledexamplestranslatingwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessatranslationforwordAbecausewehavelearnedadistributedrepresentationforwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,andcreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamplesconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbemostsuccessfulifallthreeingredients(thetworepresentationsandtherelationsbetweenthem)arelearnedjointly.Zero-shotlearningisaparticularformoftransferlearning.Thesameprincipleexplainshowonecanperformmulti-modallearning,capturingarepresentationin541'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 556}, page_content='CHAPTER15.REPRESENTATIONLEARNINGhx=fx()x\\nxtestytesthy=fy()y\\ny−space\\nRelationship between embedded points within one of the domainsMaps between representation spaces fxfyx−space\\n()pairsinthetrainingsetxy,fx:encoderfunctionforxfy:encoderfunctionforyFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionfxandsimilarlywithexamplesofytolearnfy.Eachapplicationofthefxandfyfunctionsappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionisapplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpointsinxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distanceinhyspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Bothofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeledexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-wayortwo-waymap(solidbidirectionalarrow)betweentherepresentationsfx(x)andtherepresentationsfy(y)andanchortheserepresentationstoeachother.Zero-datalearningisthenenabledasfollows.Onecanassociateanimagextesttoawordytest,evenifnoimageofthatwordwaseverpresented,simplybecauseword-representationsfy(ytest)andimage-representationsfx(xtest)canberelatedtoeachotherviathemapsbetweenrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwereneverpaired,theirrespectivefeaturevectorsfx(xtest)andfy(ytest)havebeenrelatedtoeachother.FigureinspiredfromsuggestionbyHrantKhachatrian.542'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 557}, page_content='CHAPTER15.REPRESENTATIONLEARNINGonemodality,arepresentationintheother,andtherelationship(ingeneralajointdistribution)betweenpairs(xy,)consistingofoneobservationxinonemodalityandanotherobservationyintheothermodality(SrivastavaandSalakhutdinov,2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,fromytoitsrepresentation,andtherelationshipbetweenthetworepresentations),conceptsinonerepresentationareanchoredintheother,andvice-versa,allowingonetomeaningfully generalizeto newpairs.Theprocedureis illustratedinFig..15.315.3Semi-SupervisedDisentanglingofCausalFactorsAnimportantquestionaboutrepresentationlearningis“whatmakesonerepre-sentationbetterthananother?”Onehypothesisisthatanidealrepresentationisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-lyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeaturespacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthecausesfromoneanother.Thishypothesismotivatesapproachesinwhichweﬁrstseekagoodrepresentationforp(x). Sucharepresentationmayalsobeagoodrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesofx. Thisideahasguidedalargeamountofdeeplearningresearchsinceatleastthe1990s(BeckerandHinton1992HintonandSejnowski1999,;,),inmoredetail.Forotherargumentsaboutwhensemi-supervisedlearningcanoutperformpuresupervisedlearning,wereferthereadertoSec.1.2of().Chapelleetal.2006Inotherapproachestorepresentationlearning,wehaveoftenbeenconcernedwitharepresentationthatiseasytomodel—forexample,onewhoseentriesaresparse,orindependentfromeachother.Arepresentationthatcleanlyseparatestheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.However,afurtherpartofthehypothesismotivatingsemi-supervisedlearningviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwopropertiescoincide: onceweareabletoobtaintheunderlyingexplanationsforwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfromtheothers.Speciﬁcally,ifarepresentationhrepresentsmanyoftheunderlyingcausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,thenitiseasytopredictfrom.yhFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervisedlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecasewherep(x)isuniformlydistributedandwewanttolearnf(x) =E[y|x].Clearly,observingatrainingsetofvaluesalonegivesusnoinformationabout.xp()y x|543'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 558}, page_content='CHAPTER15.REPRESENTATIONLEARNING\\nxpx()y=1y=2y=3Mixturemodel\\nFigure15.4:Exampleofadensityoverxthatisamixtureoverthree components.Thecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixturecomponents(e.g., naturalobjectclassesinimagedata)arestatisticallysalient,justmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactory.Next,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.Considerthesituationwherexarisesfromamixture,withonemixturecomponentpervalueofy,asillustratedinFig..Ifthemixturecomponentsarewell-15.4separated,thenmodelingp(x)revealspreciselywhereeachcomponentis,andasinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).Butmoregenerally,whatcouldmakeandbetiedtogether?p()y x|p()xIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)andp(yx|)will bestronglytied, andunsupervisedrepresentationlearningthattriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasasemi-supervisedlearningstrategy.Considertheassumptionthatyisoneofthecausalfactorsofx,andlethrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedasstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof:h xp,pp.(hx) = ()xh|()h(15.1)Asaconsequence,thedatahasmarginalprobabilityp() = xEhp.()xh|(15.2)Fromthisstraightforwardobservation,weconcludethatthebestpossiblemodelofx(fromageneralizationpointofview)istheonethatuncoverstheabove“true”544'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 559}, page_content='CHAPTER15.REPRESENTATIONLEARNINGstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.The“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatentfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbeveryeasytolearntopredictyfromsucharepresentation.WealsoseethattheconditionaldistributionofygivenxistiedbyBayesruletothecomponentsintheaboveequation:p() =yx|pp()xy|()yp()x.(15.3)Thusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledgeofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,insituationsrespectingtheseassumptions,semi-supervisedlearningshouldimproveperformance.Animportantresearchproblemregardsthefactthatmostobservationsareformedbyanextremelylargenumberofunderlyingcauses.Supposey=hi,buttheunsupervisedlearnerdoesnotknowwhichhi.Thebruteforcesolutionisforanunsupervisedlearnertolearnarepresentationthatcapturesallthereasonablysalientgenerativefactorshjanddisentanglesthemfromeachother,thusmakingiteasytopredictfrom,regardlessofwhichhyhiisassociatedwith.yInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossibletocaptureallormostofthefactorsofvariationthatinﬂuenceanobservation.Forexample,inavisualscene,shouldtherepresentationalwaysencodeallofthesmallestobjectsinthebackground?Itisawell-documentedpsychologicalphenomenonthathumanbeingsfailtoperceivechangesintheirenvironmentthatarenotimmediatelyrelevanttothetasktheyareperforming—see,e.g.,SimonsandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningisdeterminingwhattoencodeineachsituation.Currently,twoofthemainstrategiesfordealingwithalargenumberofunderlyingcausesaretouseasupervisedlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthemodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuchlargerrepresentationsifusingpurelyunsupervisedlearning.Anemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionofwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerativemodelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomeansquarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient.Forexample,meansquarederrorappliedtothepixelsofanimageimplicitlyspeciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthebrightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewishtosolveinvolvesinteractingwithsmallobjects.SeeFig.foranexample15.5545'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 560}, page_content='CHAPTER15.REPRESENTATIONLEARNINGInputReconstruction\\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhasfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofitsspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageandarerelevanttotheroboticstask. Unfortunately,theautoencoderhaslimitedcapacity,andthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeingsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmallpingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlargerobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.Otherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixelsfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextremebrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.Onewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydevelopedapproachcalledgenerativeadversarialnetworks(,).InGoodfellowetal.2014cthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer.Thefeedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerativemodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthisframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeishighlysalient.ThegenerativeadversarialnetworkwillbedescribedinmoredetailinSec..20.10.4Forthepurposesofthepresentdiscussion,itissuﬃcienttounderstandthattheylearnhowtodeterminewhatissalient.()showedthatmodelsLotteretal.2015trainedtogenerateimagesofhumanheadswilloftenneglecttogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfullygeneratetheearswhentrainedwiththeadversarialframework.Becausetheearsarenotextremelybrightordarkcomparedtothesurroundingskin,theyarenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighlyrecognizableshapeandconsistent546'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 561}, page_content='CHAPTER15.REPRESENTATIONLEARNINGGroundTruthMSEAdversarial\\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceoflearningwhichfeaturesaresalient. Inthisexample,thepredictivegenerativenetworkhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁcviewingangle.(Left)Groundtruth.Thisisthecorrectimage,thatthenetworkshouldemit. Imageproducedbyapredictivegenerativenetworktrainedwithmean(Center)squarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightnesscomparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearntorepresentthem.Imageproducedbyamodeltrainedwithacombinationof(Right)meansquarederrorandadversarialloss. Usingthislearnedcostfunction,theearsaresalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesareimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figuresgraciouslyprovidedby().Lotteretal.2015positionmeansthatafeedforwardnetworkcaneasilylearntodetectthem,makingthemhighlysalientunderthegenerativeadversarialframework.SeeFig.15.6forexampleimages.Generativeadversarialnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.Weexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhichfactorstorepresent,anddevelopmechanismsforrepresentingdiﬀerentfactorsdependingonthetask.Abeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopfetal.(),isthatifthetruegenerativeprocesshas2012xasaneﬀectandyasacause,thenmodelingp(x y|)isrobusttochangesinp(y). Ifthecause-eﬀectrelationshipwasreversed,thiswouldnotbetrue,sincebyBayesrule,p(xy|)wouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesindistributionduetodiﬀerentdomains,temporalnon-stationarity,orchangesinthenatureofthetask,thecausalmechanismsremaininvariant(“thelawsoftheuniverseareconstant”)whilethemarginaldistributionovertheunderlyingcausescanchange.Hence,bettergeneralizationandrobustnesstoallkindsofchangescanbeexpectedvialearningagenerativemodelthatattemptstorecover547'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 562}, page_content='CHAPTER15.REPRESENTATIONLEARNINGthecausalfactorsand.hp()xh|15.4DistributedRepresentationDistributedrepresentationsofconcepts—representationscomposedofmanyele-mentsthatcanbesetseparatelyfromeachother—areoneofthemostimportanttoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecausetheycanusenfeatureswithkvaluestodescribekndiﬀerentconcepts.Aswehaveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunitsandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyofdistributedrepresentation. Wenowintroduceanadditionalobservation. Manydeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunitscanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,asdiscussedinSec..Distributedrepresentationsarenaturalforthisapproach,15.3becauseeachdirectioninrepresentationspacecancorrespondtothevalueofadiﬀerentunderlyingconﬁgurationvariable.Anexampleofadistributedrepresentationisavectorofnbinaryfeatures,whichcantake2nconﬁgurations,eachpotentiallycorrespondingtoadiﬀerentregionininputspace,asillustratedinFig..Thiscanbecomparedwith15.7asymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolorcategory.Iftherearensymbolsinthedictionary,onecanimaginenfeaturedetectors,eachcorrespondingtothedetectionofthepresenceoftheassociatedcategory.Inthatcaseonlyndiﬀerentconﬁgurationsoftherepresentationspacearepossible,carvingndiﬀerentregionsininputspace,asillustratedinFig..15.8Suchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcanbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyoneofthemcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthebroaderclassofnon-distributedrepresentations,whicharerepresentationsthatmaycontainmanyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolovereachentry.Examplesoflearningalgorithms basedonnon-distributedrepresentationsinclude:•Clusteringmethods,includingthek-meansalgorithm:eachinputpointisassignedtoexactlyonecluster.•k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamplesareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple548'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 563}, page_content='CHAPTER15.REPRESENTATIONLEARNING\\nh1h2h3\\nh= [1,,11]\\ue021h= [0,,11]\\ue021h= [1,,01]\\ue021h= [1,,10]\\ue021h= [0,,10]\\ue021h= [0,,01]\\ue021h= [1,,00]\\ue021\\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentationbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeaturesh1,h2,andh3. Eachfeatureisdeﬁnedbythresholdingtheoutputofalearned,lineartransformation.EachfeaturedividesR2intotwohalf-planes.Leth+ibethesetofinputpointsforwhichhi=1andh−ibethesetofinputpointsforwhichhi=0.Inthisillustration,eachlinerepresentsthedecisionboundaryforonehi,withthecorrespondingarrowpointingtotheh+isideoftheboundary.Therepresentationasawholetakesonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,therepresentationvalue[1,1,1]\\ue03ecorrespondstotheregionh+1∩h+2∩h+3.Comparethistothenon-distributedrepresentationsinFig..Inthegeneralcaseof15.8dinputdimensions,adistributedrepresentationdividesRdbyintersectinghalf-spacesratherthanhalf-planes.ThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diﬀerentregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonlynregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymanymoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible(thereisnoh=0inthisexample)andthatalinearclassiﬁerontopofthedistributedrepresentationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;evenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog)wherewisthenumberofweights(,).ThecombinationofapowerfulrepresentationSontag1998layerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearntheconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclasstoaninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas“manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfewhiandencouragestolearntorepresenttheclassesinalinearlyseparableway.h549'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 564}, page_content='CHAPTER15.REPRESENTATIONLEARNINGvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfromeachother,sothisdoesnotqualifyasatruedistributedrepresentation.•Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)isactivatedwhenaninputisgiven.•Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)orexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearestneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,butthosevaluescannotreadilybecontrolledseparatelyfromeachother.•KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):althoughthedegreeofactivationofeach“supportvector”ortemplateexampleisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.•Languageortranslationmodelsbasedonn-grams.Thesetofcontexts(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes.Aleafmaycorrespondtothelasttwowordsbeingw1andw2,forexample.Separateparametersareestimatedforeachleafofthetree(withsomesharingbeingpossible).Forsomeofthesenon-distributedalgorithms,theoutputisnotconstantbypartsbutinsteadinterpolatesbetweenneighboringregions.Therelationshipbetweenthenumberofparameters(orexamples)andthenumberofregionstheycandeﬁneremainslinear.Animportantrelatedconceptthatdistinguishesadistributedrepresentationfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetweendiﬀerentconcepts.Aspuresymbols,“cat”and“dog”areasfarfromeachotherasanyothertwosymbols.However,ifoneassociatesthemwithameaningfuldistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcatscangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentationmaycontainentriessuchas“has_fur”or“number_of_legs”thathavethesamevaluefortheembeddingofboth“cat”and“dog.”Neurallanguagemodelsthatoperateondistributedrepresentationsofwordsgeneralizemuchbetterthanothermodelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedinSec..Distributedrepresentationsinducearich12.4similarityspace,inwhichsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatisabsentfrompurelysymbolicrepresentations.Whenandwhycantherebeastatisticaladvantagefromusingadistributedrepresentationaspartofalearningalgorithm? Distributedrepresentationscan550'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 565}, page_content='CHAPTER15.REPRESENTATIONLEARNING\\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspaceintodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearningalgorithmbasedonanon-distributedrepresentation.Diﬀerentnon-distributedalgorithmsmayhavediﬀerentgeometry,buttheytypicallybreaktheinputspaceintoregions,withaseparatesetofparametersforeachregion.Theadvantageofanon-distributedapproachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvingadiﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutputindependentlyforeachregion.Thedisadvantageisthatsuchnon-distributedmodelsgeneralizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicatedfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrastthiswithadistributedrepresentation,Fig..15.7\\n551'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 566}, page_content='CHAPTER15.REPRESENTATIONLEARNINGhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbecompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-distributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,whichstatesthatifuv≈,thenthetargetfunctionftobelearnedhasthepropertythatf(u)≈f(v),ingeneral.Therearemanywaysofformalizingsuchanassumption,buttheendresultisthatifwehaveanexample(x,y)forwhichweknowthatf(x)≈y,thenwechooseanestimatorˆfthatapproximatelysatisﬁestheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearbyinputx+\\ue00f.Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseofdimensionality: inordertolearnatargetfunctionthatincreasesanddecreasesmanytimesinmanydiﬀerentregions,1wemayneedanumberofexamplesthatisatleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachoftheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomforeachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymboltovalue. However,thisdoesnotallowustogeneralizetonewsymbolsfornewregions.Ifwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeingsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizeanobjectregardlessofitslocationintheimage,eventhoughspatialtranslationoftheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.Letusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.EachbinaryfeatureinthisrepresentationdividesRdintoapairofhalf-spaces, asillustratedinFig..Theexponentiallylargenumberofintersectionsof15.7nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributedrepresentationlearnercandistinguish.HowmanyregionsaregeneratedbyanarrangementofnhyperplanesinRd?Byapplyingageneralresultconcerningtheintersectionofhyperplanes(,),onecanshow(Zaslavsky1975Pascanu2014betal.,)thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishisd\\ue058j=0\\ue012nj\\ue013= (Ond).(15.4)Therefore,weseeagrowththatisexponentialintheinputsizeandpolynomialinthenumberofhiddenunits.1Potentially,wemaywanttolearnafunctionwhosebehaviorisdistinctinexponentiallymanyregions:inad-dimensionalspacewithatleast2diﬀerentvaluestodistinguishperdimension,wemightwanttodiﬀerinf2ddiﬀerentregions,requiringO(2d)trainingexamples.552'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 567}, page_content='CHAPTER15.REPRESENTATIONLEARNINGThisprovidesageometricargumenttoexplainthegeneralizationpowerofdistributedrepresentation:withO(nd)parameters(fornlinear-thresholdfeaturesinRd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemadenoassumptionatallaboutthedata,andusedarepresentationwithoneuniquesymbolforeachregion,andseparateparametersforeachsymboltorecognizeitscorrespondingportionofRd,thenspecifyingO(nd)regionswouldrequireO(nd)examples.Moregenerally,theargumentinfavorofthedistributedrepresentationcouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitsweusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesinthedistributedrepresentation.Theargumentinthiscaseisthatifaparametrictransformationwithkparameterscanlearnaboutrregionsininputspace,withkr\\ue01c,andifobtainingsucharepresentationwasusefultothetaskofinterest,thenwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributedsettingwherewewouldneedO(r)examplestoobtainthesamefeaturesandassociatedpartitioningoftheinputspaceintorregions.Usingfewerparameterstorepresentthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequirefarfewertrainingexamplestogeneralizewell.Afurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-tationsgeneralizewellisthattheircapacityremainslimiteddespitebeingabletodistinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofaneuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumberofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignverymanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecodespace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspacehtotheoutputyusingalinearclassiﬁer.Theuseofadistributedrepresentationcombinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassestoberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactorscapturedbyh. Wewilltypicallywanttolearncategoriessuchasthesetofallimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthatrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartitionthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofallgreencarsandredtrucksasanotherclass.Theideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentallyvalidated.() ﬁndthathiddenunitsinadeep convolutionalZhouetal.2015networktrainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatareveryofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.Inpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomethingthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthetoplevelsofthebestcomputervisiondeepnetworks.Whatsuch553'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 568}, page_content='CHAPTER15.REPRESENTATIONLEARNING-+=Figure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentanglestheconceptofgenderfromtheconceptofwearingglasses. Ifwebeginwiththerepre-sentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingtheconceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconceptofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawomanwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorstoimagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwithpermissionfrom().Radfordetal.2015featureshaveincommonisthatonecouldimaginelearningabouteachofthemwithouthavingtoseealltheconﬁgurationsofalltheothers.Radfordetal.()demonstratedthatagenerativemodelcanlearnarepresentationof2015imagesoffaces,withseparatedirectionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation.Fig.demonstratesthatonedirectionin15.9representationspacecorrespondstowhetherthepersonismaleorfemale,whileanothercorrespondstowhetherthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically,notﬁxedapriori. Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:gradientdescentonanobjectivefunctionofinterestnaturallylearnssemanticallyinterestingfeatures,solongasthetaskrequiressuchfeatures. Wecanlearnaboutthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceofglasses,withouthavingtocharacterizealloftheconﬁgurationsofthen−1otherfeaturesbyexamplescoveringallofthesecombinationsofvalues.Thisformofstatisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurationsofaperson’sfeaturesthathaveneverbeenseenduringtraining.554'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 569}, page_content='CHAPTER15.REPRESENTATIONLEARNING15.5ExponentialGainsfromDepthWehaveseeninSec.thatmultilayerperceptronsareuniversalapproximators,6.4.1andthatsomefunctionscanberepresentedbyexponentiallysmallerdeepnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadstoimprovedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapplymoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.InSec.,wesawanexampleofagenerativemodelthatlearnedaboutthe15.4explanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgenderandwhethertheyarewearingglasses.Thegenerativemodelthataccomplishedthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpectashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationshipbetweentheseabstractexplanatoryfactorsandthepixelsintheimage.InthisandotherAItasks,thefactorsthatcanbechosenalmostindependentlyinordertogeneratedataaremorelikelytobeveryhigh-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthisdemandsdeepdistributedrepresentations,wherethehigherlevelfeatures(seenasfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthroughthecompositionofmanynonlinearities.Ithasbeenproveninmanydiﬀerentsettingsthatorganizingcomputationthroughthecompositionofmanynonlinearitiesandahierarchyofreusedfeaturescangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponentialboostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,withsaturatingnonlinearities,Booleangates,sum/products,orRBFunits)withasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodelfamilythatisauniversalapproximatorcanapproximatealargeclassoffunctions(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenoughhiddenunits. However,therequirednumberofhiddenunitsmaybeverylarge.Theoreticalresultsconcerningtheexpressivepowerofdeeparchitecturesstatethattherearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitectureofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespecttotheinputsize)withinsuﬃcientdepth(depth2ordepth).k−1InSec.,wesawthatdeterministicfeedforwardnetworksareuniversal6.4.1approximatorsoffunctions.Manystructuredprobabilisticmodelswithasinglehiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeepbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRouxandBengio20082010MontúfarandAy2011Montúfar2014Krause,,;,;,;etal.,2013).InSec.,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhavean6.4.1555'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 570}, page_content='CHAPTER15.REPRESENTATIONLEARNINGexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalsobeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilisticmodelisthesum-productnetworkSPNor(PoonandDomingos2011,).Thesemodelsusepolynomialcircuitstocomputetheprobabilitydistributionoverasetofrandomvariables.()showedthatthereexistDelalleauandBengio2011probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoidneedinganexponentiallylargemodel.Later,()MartensandMedabalimi2014showedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsofSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimittheirrepresentationalpower.Anotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressivepoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightinganexponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowedtoonlyapproximatethefunctioncomputedbythedeepcircuit(,Cohenetal.2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythecasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.15.6ProvidingCluestoDiscoverUnderlyingCausesToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesonerepresentationbetterthananother?Oneanswer,ﬁrstintroducedinSec.,15.3isthatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsofvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoourapplications.Moststrategiesforrepresentationlearningarebasedonintroducingcluesthathelpthelearningtoﬁndtheseunderlyingfactorsofvariations.Thecluescanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervisedlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusuallyspeciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,tomakeuseofabundantunlabeleddata,representationlearningmakesuseofother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformofimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposeinordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthatregularizationstrategiesarenecessarytoobtaingoodgeneralization.Whileitisimpossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeeplearningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicabletoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareabletosolve.Weprovideherealistofthesegenericregularizationstrategies.Thelistis556'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 571}, page_content='CHAPTER15.REPRESENTATIONLEARNINGclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearningalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlyingfactors.ThislistwasintroducedinSec.3.1of()andhasbeenBengioetal.2013dpartiallyexpandedhere.•Smoothness:Thisistheassumptionthatf(x+\\ue00fd)≈f(x)forunitdandsmall.Thisassumptionallowsthelearnertogeneralizefromtraining\\ue00fexamplestonearbypointsininputspace.Manymachinelearningalgorithmsleveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality.•Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensomevariablesarelinear.Thisallowsthealgorithmtomakepredictionsevenveryfarfromtheobserveddata,butcansometimesleadtooverlyextremepredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethesmoothnessassumptioninsteadmakethelinearityassumption.Theseareinfactdiﬀerentassumptions—linearfunctionswithlargeweightsappliedtohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.()forafurtherdiscussionofthelimitationsofthelinearityassumption.2014b•Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsaremotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlyingexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestateofeachofthesefactors. Sec. describeshowthisview15.3motivatessemi-supervisedlearningviarepresentationlearning.Learningthestructureofp(x)requireslearningsomeofthesamefeaturesthatareusefulformodelingp(yx|)becausebothrefertothesameunderlyingexplanatoryfactors.Sec.describeshowthisviewmotivatestheuseof15.4distributedrepresentations,withseparatedirectionsinrepresentationspacecorrespondingtoseparatefactorsofvariation.•Causalfactors:themodelisconstructedinsuchawaythatittreatsthefactorsofvariationdescribedbythelearnedrepresentationhasthecausesoftheobserveddatax,andnotvice-versa.AsdiscussedinSec.,this15.3isadvantageousforsemi-supervisedlearningandmakesthelearnedmodelmorerobustwhenthedistributionovertheunderlyingcauseschangesorwhenweusethemodelforanewtask.•Depth,orahierarchicalorganizationofexplanatoryfactors: High-level,abstractconceptscanbedeﬁnedintermsofsimpleconcepts,formingahierarchy.Fromanotherpointofview,theuseofadeeparchitectureexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-stepprogram,557'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 572}, page_content='CHAPTER15.REPRESENTATIONLEARNINGwitheachstepreferringbacktotheoutputoftheprocessingaccomplishedviaprevioussteps.•Sharedfactorsacrosstasks:Inthecontextwherewehavemanytasks,correspondingtodiﬀerentyivariablessharingthesameinputxorwhereeachtaskisassociatedwithasubsetorafunctionf()i(x)ofaglobalinputx,theassumptionisthateachyiisassociatedwithadiﬀerentsubsetfromacommonpoolofrelevantfactorsh.Becausethesesubsetsoverlap,learningalltheP(yi|x)viaasharedintermediaterepresentationP(h x|)allowssharingofstatisticalstrengthbetweenthetasks.•Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-centratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuouscase,theseregionscanbeapproximatedbylow-dimensionalmanifoldswithamuchsmallerdimensionalitythantheoriginalspacewherethedatalives.Manymachinelearningalgorithmsbehavesensiblyonlyonthismanifold(,).Somemachinelearningalgorithms,especiallyGoodfellowetal.2014bautoencoders,attempttoexplicitlylearnthestructureofthemanifold.•Naturalclustering:Manymachinelearningalgorithmsassumethateachconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.Thedatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstantwithineachoneofthese. Thisassumptionmotivatesavarietyoflearningalgorithms,includingtangentpropagation,doublebackprop,themanifoldtangentclassiﬁerandadversarialtraining.•Temporal and spatial coherence:Slowfeature analysisand relatedalgorithmsmaketheassumptionthatthemostimportantexplanatoryfactorschangeslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlyingexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.SeeSec.forfurtherdescriptionofthisapproach.13.3•Sparsity: Mostfeaturesshouldpresumablynotberelevanttodescribingmostinputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhenrepresentinganimageofacat.Itisthereforereasonabletoimposeapriorthatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbeabsentmostofthetime.•SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,thefactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplestpossibleismarginalindependence,P(h)=\\ue051iP(hi),butlinear558'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 573}, page_content='CHAPTER15.REPRESENTATIONLEARNINGdependenciesorthosecapturedbyashallowautoencoderarealsoreasonableassumptions.Thiscanbeseeninmanylawsofphysics,andisassumedwhenpluggingalinearpredictororafactorizedpriorontopofalearnedrepresentation.Theconceptofrepresentationlearningtiestogetherallofthemanyformsofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeepprobabilisticmodelsalllearnandexploitrepresentations.Learning thebestpossiblerepresentationremainsanexcitingavenueofresearch.\\n559'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 574}, page_content='Chapter16StructuredProbabilisticModelsforDeepLearningDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanusetoguidetheirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalismsistheideaofstructuredprobabilisticmodels.WehavealreadydiscussedstructuredprobabilisticmodelsbrieﬂyinSec..Thatbriefpresentationwassuﬃcientto3.14understandhowtousestructuredprobabilisticmodelsasalanguagetodescribesomeofthealgorithmsinPart.Now,inPart,structuredprobabilisticmodelsIIIIIareakeyingredientofmanyofthemostimportantresearchtopicsindeeplearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribesstructuredprobabilisticmodelsinmuchgreaterdetail. Thischapterisintendedtobeself-contained; thereaderdoesnotneedtoreviewtheearlierintroductionbeforecontinuingwiththischapter.Astructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,usingagraphtodescribewhichrandomvariablesintheprobabilitydistributioninteractwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—asetofverticesconnectedtooneanotherbyasetofedges.Becausethestructureofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoasgraphicalmodels.Thegraphicalmodelsresearchcommunityislargeandhasdevelopedmanydiﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,weprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearningresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,youmaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert560'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 575}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGmaybeneﬁtfromreadingtheﬁnalsectionofthischapter,Sec.,inwhichwe16.7highlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearningalgorithms.Deeplearningpractitionerstendtouseverydiﬀerentmodelstructures,learningalgorithmsandinferenceproceduresthanarecommonlyusedbytherestofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythesediﬀerencesinpreferencesandexplainthereasonsforthem.Inthischapterweﬁrstdescribethechallengesofbuildinglarge-scaleproba-bilisticmodels. Next,wedescribehowtouseagraphtodescribethestructureofaprobabilitydistribution.Whilethisapproachallowsustoovercomemanychallenges,itisnotwithoutitsowncomplications.Oneofthemajordiﬃcultiesingraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeractdirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem. Weoutlinetwoapproachestoresolvingthisdiﬃcultybylearningaboutthedependen-ciesinSec..Finally,weclosewithadiscussionoftheuniqueemphasisthat16.5deeplearningpractitionersplaceonspeciﬁcapproachestographicalmodelinginSec..16.716.1TheChallengeofUnstructuredModelingThegoalofdeeplearningistoscalemachinelearningtothekindsofchallengesneededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-dimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmstobeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,anddocumentscontainingmultiplewordsandpunctuationcharacters.Classiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensionaldistributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,whatwordisspokeninarecording,whattopicadocumentisabout.Theprocessofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesasingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).Theclassiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,whenrecognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundofthephoto.Itispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksareoftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultipleoutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof1Anaturalimageisanimagethatmightbecapturedbyacamerainareasonablyordinaryenvironment,asopposedtoasyntheticallyrenderedimage,ascreenshotofawebpage,etc.561'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 576}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGtheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:•Densityestimation:givenaninputx,themachinelearningsystemreturnsanestimateofthetruedensityp(x) underthedatageneratingdistribution.Thisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstandingoftheentireinput.Ifevenoneelementofthevectorisunusual,thesystemmustassignitalowprobability.•Denoising:givenadamagedorincorrectlyobservedinput˜x,themachinelearningsystemreturnsanestimateoftheoriginalorcorrectx.Forexample,themachinelearningsystemmightbeaskedtoremovedustorscratchesfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementoftheestimatedcleanexamplex)andanunderstandingoftheentireinput(sinceevenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged).•Missingvalueimputation:giventheobservationsofsomeelementsofx,themodelisaskedtoreturnestimatesoforaprobabilitydistributionoversomeoralloftheunobservedelementsofx.Thisrequiresmultipleoutputs.Becausethemodelcouldbeaskedtorestoreanyoftheelementsofx,itmustunderstandtheentireinput.•Sampling:themodelgeneratesnewsamplesfromthedistributionp(x).Applicationsincludespeechsynthesis,i.e.producingnewwaveformsthatsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesandagoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawnfromthewrongdistribution,thenthesamplingprocessiswrong.Foranexampleofasamplingtaskusingsmallnaturalimages,seeFig..16.1Modelingarichdistributionoverthousandsormillionsofrandomvariablesisachallengingtask,bothcomputationallyandstatistically.Supposeweonlywantedtomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyitseemsoverwhelming.Forasmall,32×322pixelcolor(RGB)image,thereare3072possiblebinaryimagesofthisform.Thisnumberisover10800timeslargerthantheestimatednumberofatomsintheuniverse.Ingeneral,ifwewishtomodeladistributionoverarandomvectorxcontainingndiscretevariablescapableoftakingonkvalueseach,thenthenaiveapproachofrepresentingP(x)bystoringalookuptablewithoneprobabilityvalueperpossibleoutcomerequiresknparameters!Thisisnotfeasibleforseveralreasons:562'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 577}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.1:Probabilisticmodelingofnaturalimages.(Top)Example32×32pixelcolorimagesfromtheCIFAR-10dataset(,).SamplesKrizhevskyandHinton2009(Bottom)drawnfromastructuredprobabilisticmodeltrainedonthisdataset.EachsampleappearsatthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclideanspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeenadjustedfordisplay.Figurereproducedwithpermissionfrom().Courvilleetal.2011563'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 578}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING•Memory:thecostofstoringtherepresentation:Forallbutverysmallvaluesofnandk,representingthedistributionasatablewillrequiretoomanyvaluestostore.•Statisticaleﬃciency:Asthenumberofparametersinamodelincreases,sodoestheamountoftrainingdataneededtochoosethevaluesofthoseparametersusingastatisticalestimator.Becausethetable-basedmodelhasanastronomicalnumberofparameters,itwillrequireanastronomicallylargetrainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetrainingsetverybadlyunlessadditionalassumptionsaremadelinkingthediﬀerententriesinthetable(forexample,likeinback-oﬀorsmoothedn-grammodels,Sec.).12.4.1•Runtime:thecostofinference:SupposewewanttoperformaninferencetaskwhereweuseourmodelofthejointdistributionP(x)tocomputesomeotherdistribution,suchasthemarginaldistributionP(x1)ortheconditionaldistributionP(x2|x1).Computingthesedistributionswillrequiresummingacrosstheentiretable,sotheruntimeoftheseoperationsisashighastheintractablememorycostofstoringthemodel.•Runtime:thecostofsampling: Likewise,supposewewanttodrawasamplefromthemodel.Thenaivewaytodothisistosamplesomevalueu∼U(0,1),theniteratethroughthetableaddinguptheprobabilityvaluesuntiltheyexceeduandreturntheoutcomewhoseprobabilityvaluewasaddedlast.Thisrequiresreadingthroughthewholetableintheworstcase,soithasthesameexponentialcostastheotheroperations.Theproblemwiththetable-basedapproachisthatweareexplicitlymodelingeverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.Theprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.Usually,mostvariablesinﬂuenceeachotheronlyindirectly.Forexample,considermodelingtheﬁnishingtimesofateaminarelayrace.Supposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartoftherace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompletingherlaparoundthetrack,shehandsthebatontoBob.BobthenrunshisownlapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachoftheirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoesnotdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedependsonAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlicehascompletedhers. IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing564'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 579}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGequal.Finally,Carol’sﬁnishingtimedependsonbothherteammates.IfAliceisslow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitealatestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,Carol’sﬁnishingtimedependsonlyindirectlyonAlice’sﬁnishingtimeviaBob’s.IfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’sﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeanswecanmodeltherelayraceusingonlytwointeractions:Alice’seﬀectonBobandBob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAliceandCarolfromourmodel.Structuredprobabilisticmodelsprovideaformalframeworkformodelingonlydirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohavesigniﬁcantlyfewerparameterswhichcaninturnbeestimatedreliablyfromlessdata. Thesesmallermodelsalsohavedramaticallyreducedcomputationalcostintermsofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfromthemodel.16.2UsingGraphstoDescribeModelStructureStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or“vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables.Eachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.Thesedirectinteractionsimplyother,indirectinteractions,butonlythedirectinteractionsneedtobeexplicitlymodeled.Thereis morethanonewayto describe theinteractions inaprobabilitydistributionusingagraph.Inthefollowingsectionswedescribesomeofthemostpopularandusefulapproaches.Graphicalmodelscanbelargelydividedintotwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedonundirectedgraphs.16.2.1DirectedModelsOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,otherwiseknownasthebeliefnetworkBayesiannetworkor2(Pearl1985,).Directedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,2JudeaPearlsuggestedusingtheterm“Bayesiannetwork”whenonewishesto“emphasizethejudgmental”natureofthevaluescomputedbythenetwork,i.e.tohighlightthattheyusuallyrepresentdegreesofbeliefratherthanfrequenciesofevents.565'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 580}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGt0t0t1t1t2t2AliceBobCarolFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishingtimet0inﬂuencesBob’sﬁnishingtimet1,becauseBobdoesnotgettostartrunninguntilAliceﬁnishes.Likewise,CarolonlygetstostartrunningafterBobﬁnishes,soBob’sﬁnishingtimet1directlyinﬂuencesCarol’sﬁnishingtimet2.thatis,theypointfromonevertextoanother.Thisdirectionisrepresentedinthedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’sprobabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfromatobmeansthatwedeﬁnetheprobabilitydistributionoverbviaaconditionaldistribution,withaasoneofthevariablesontherightsideoftheconditioningbar.Inotherwords,thedistributionoverbdependsonthevalueofa.ContinuingwiththerelayraceexamplefromSec.,supposewenameAlice’s16.1ﬁnishingtimet0,Bob’sﬁnishingtimet1,andCarol’sﬁnishingtimet2.Aswesawearlier,ourestimateoft1dependsont0.Ourestimateoft2dependsdirectlyont1butonlyindirectlyont0.Wecandrawthisrelationshipinadirectedgraphicalmodel,illustratedinFig..16.2Formally,adirectedgraphicalmodeldeﬁnedonvariablesxisdeﬁnedbyadirectedacyclicgraphGwhoseverticesaretherandomvariablesinthemodel,andasetoflocalconditionalprobabilitydistributionsp(xi|PaG(xi))wherePaG(xi)givestheparentsofxiin.TheprobabilitydistributionoverisgivenbyGx p() = Πxip(xi|PaG(xi)).(16.1)Inourrelayraceexample,thismeansthat,usingthegraphdrawninFig.,16.2p(t0,t1,t2) = (pt0)(pt1|t0)(pt2|t1).(16.2)Thisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.Wecanexaminethecostofusingit,inordertoobservehowstructuredmodelinghasmanyadvantagesrelativetounstructuredmodeling.Supposewerepresentedtimebydiscretizingtimerangingfromminute0tominute10into6secondchunks.Thiswouldmaket0,t1andt2eachbediscretevariableswith100possiblevalues.Ifweattemptedtorepresentp(t0,t1,t2)withatable,itwouldneedtostore999,999values(100valuesoft0×100valuesoft1×100valuesoft2,minus1,sincetheprobabilityofoneoftheconﬁgurationsismade566'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 581}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,weonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthedistributionovert0requires99values,thetabledeﬁningt1givent0requires9900values,andsodoesthetabledeﬁningt2givent1.Thiscomestoatotalof19,899values.Thismeansthatusingthedirectedgraphicalmodelreducedournumberofparametersbyafactorofmorethan50!Ingeneral,tomodelndiscretevariableseachhavingkvalues,thecostofthesingletableapproachscaleslikeO(kn),aswehaveobservedbefore.Nowsupposewebuildadirectedgraphicalmodeloverthesevariables.Ifmisthemaximumnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingleconditionalprobabilitydistribution,thenthecostofthetablesforthedirectedmodelscaleslikeO(km).Aslongaswecandesignamodelsuchthatm<<n,wegetverydramaticsavings.Inotherwords,solongaseachvariablehasfewparentsinthegraph,thedistributioncanberepresentedwithveryfewparameters. Somerestrictionsonthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethatoperationslikecomputingmarginalorconditionaldistributionsoversubsetsofvariablesareeﬃcient.Itisimportanttorealizewhatkindsofinformationcanandcannotbeencodedinthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariablesareconditionallyindependentfromeachother.Itisalsopossibletomakeotherkindsofsimplifyingassumptions. Forexample,supposeweassumeBobalwaysrunsthesameregardlessofhowAliceperformed.(Inreality,Alice’sperformanceprobablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlicerunsespeciallyfastinagivenrace,thismightencourageBobtopushhardandmatchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy).ThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’sﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.ThisobservationallowsustodeﬁneamodelwithO(k)parametersinsteadofO(k2).However,notethatt0andt1arestilldirectlydependentwiththisassumption,becauset1representstheabsolutetimeatwhichBobﬁnishes,notthetotaltimehehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfromt0tot1.TheassumptionthatBob’spersonalrunningtimeisindependentfromallotherfactorscannotbeencodedinagraphovert0,t1,andt2.Instead,weencodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.Theconditionaldistributionisnolongerakk×−1elementtableindexedbyt0andt1butisnowaslightlymorecomplicatedformulausingonlyk−1parameters.Thedirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne567'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 582}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedtotakeinasarguments.16.2.2UndirectedModelsDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredproba-bilisticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwiseknownasMarkovrandomﬁelds(MRFs)or(,).MarkovnetworksKindermann1980Astheirnameimplies,undirectedmodelsusegraphswhoseedgesareundirected.Directedmodelsaremostnaturallyapplicabletosituationswherethereisaclearreasontodraweacharrowinoneparticulardirection.Oftenthesearesituationswhereweunderstandthecausalityandthecausalityonlyﬂowsinonedirection.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀecttheﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesofearlierrunners.Notallsituationswemightwanttomodelhavesuchacleardirectiontotheirinteractions.Whentheinteractionsseemtohavenointrinsicdirection,ortooperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.Asanexampleofsuchasituation,supposewewanttomodeladistributionoverthreebinaryvariables:whetherornotyouaresick,whetherornotyourcoworkerissick,andwhetherornotyourroommateissick.Asintherelayraceexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthattakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweachother,itisveryunlikelythatoneofthemwillgivetheotheradiseasesuchasacolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodelit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,andthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionofacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthecoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyourroommate.Inthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickasitisforyourroommatetomakeyousick,sothereisnotaclean,uni-directionalnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.Aswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyanedge,thentherandomvariablescorrespondingtothosenodesinteractwitheachotherdirectly. Unlikedirectedmodels,theedgeinanundirectedmodelhasnoarrow,andisnotassociatedwithaconditionalprobabilitydistribution.Wedenotetherandomvariablerepresentingyourhealthashy,therandom568'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 583}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGhrhrhyhyhchcFigure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthhr,yourhealthhy,andyourworkcolleague’shealthhcaﬀecteachother.Youandyourroommatemightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,butassumingthatyourroommateandyourcolleaguedonotknoweachother,theycanonlyinfecteachotherindirectlyviayou.variablerepresentingyourroommate’shealthashr,andtherandomvariablerepresentingyourcolleague’shealthashc.SeeFig.foradrawingofthegraph16.3representingthisscenario.Formally,anundirectedgraphicalmodelisastructuredprobabilisticmodeldeﬁnedonanundirectedgraphG.ForeachcliqueCinthegraph,3afactorφ(C)(alsocalledacliquepotential)measurestheaﬃnityofthevariablesinthatcliqueforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobenon-negative.Togethertheydeﬁneanunnormalizedprobabilitydistribution˜p() = ΠxC∈Gφ.()C(16.3)Theunnormalizedprobabilitydistributioniseﬃcienttoworkwithsolongasallthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityaremorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothedeﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthemtogetherwillyieldavalidprobabilitydistribution.SeeFig.foranexampleof16.4readingfactorizationinformationfromanundirectedgraph.Ourexampleofthecoldspreadingbetweenyou,yourroommate,andyourcolleaguecontainstwocliques.Onecliquecontainshyandhc.Thefactorforthiscliquecanbedeﬁnedbyatable,andmighthavevaluesresemblingthese:hy= 0hy= 1hc= 021hc= 1110Astateof1indicatesgoodhealth,whileastateof0indicatespoorhealth(havingbeen infectedwitha cold).Bothof you areusuallyhealthy, so the3Acliqueofthegraphisasubsetofnodesthatareallconnectedtoeachotherbyanedgeofthegraph.569'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 584}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGcorrespondingstatehasthehighestaﬃnity. Thestatewhereonlyoneofyouissickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothofyouaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,thoughstillnotascommonasthestatewherebotharehealthy.Tocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthecliquecontaininghyandhr.16.2.3ThePartitionFunctionWhiletheunnormalizedprobabilitydistributionisguaranteedtobenon-negativeeverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalidprobabilitydistribution,wemustusethecorrespondingnormalizedprobabilitydistribution:4p() =x1Z˜p()x(16.4)whereZisthevaluethatresultsintheprobabilitydistributionsummingorintegratingto1:Z=\\ue05a˜pd.()xx(16.5)YoucanthinkofZasaconstantwhentheφfunctionsareheldconstant.Notethatiftheφfunctionshaveparameters,thenZisafunctionofthoseparameters.ItiscommonintheliteraturetowriteZwithitsargumentsomittedtosavespace.ThenormalizingconstantZisknownasthepartitionfunction,atermborrowedfromstatisticalphysics.SinceZisanintegralorsumoverallpossiblejointassignmentsofthestatexitisoftenintractabletocompute. Inordertobeabletoobtainthenormalizedprobabilitydistributionofanundirectedmodel, themodelstructureandthedeﬁnitionsoftheφfunctionsmustbeconducivetocomputingZeﬃciently.Inthecontextofdeeplearning,Zisusuallyintractable. DuetotheintractabilityofcomputingZexactly,wemustresorttoapproximations.SuchapproximatealgorithmsarethetopicofChapter.18OneimportantconsiderationtokeepinmindwhendesigningundirectedmodelsisthatitispossibletospecifythefactorsinsuchawaythatZdoesnotexist.Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegralof˜povertheirdomaindiverges.Forexample,supposewewanttomodelasingle4AdistributiondeﬁnedbynormalizingaproductofcliquepotentialsisalsocalledaGibbsdistribution.570'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 585}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGscalarvariablexwithasinglecliquepotential∈Rφxx() = 2.Inthiscase,Z=\\ue05ax2dx.(16.6)Sincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingtothischoiceofφ(x). Sometimesthechoiceofsomeparameteroftheφfunctionsdetermineswhether theprobability distributionisdeﬁned.Forexample, forφ(x;β) =exp\\ue000−βx2\\ue001,theβparameterdetermineswhetherZexists.PositiveβresultsinaGaussiandistributionoverxbutallothervaluesofβmakeφimpossibletonormalize.Onekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthatdirectedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfromthestart,whileundirectedmodelsaredeﬁnedmorelooselybyφfunctionsthatarethenconvertedintoprobabilitydistributions.Thischangestheintuitionsonemustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmindwhileworkingwithundirectedmodelsisthatthedomainofeachofthevariableshasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetofφfunctionscorrespondsto.Forexample,considerann-dimensionalvector-valuedrandomvariablexandanundirectedmodelparametrizedbyavectorofbiasesb.Supposewehaveonecliqueforeachelementofx,φ()i(xi) =exp(bixi).Whatkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedonothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx.Ifx ∈Rn,thentheintegraldeﬁningZdivergesandnoprobabilitydistributionexists.Ifx∈{0,1}n,thenp(x)factorizesintonindependentdistributions,withp(xi= 1) =sigmoid(bi).Ifthedomainofxisthesetofelementarybasisvectors({[1,0,...,0],[0,1,...,0],...,[0,0,...,1]})thenp(x)=softmax(b),soalargevalueofbiactuallyreducesp(xj=1)forj\\ue036=i. Often,itispossibletoleveragetheeﬀectofacarefullychosendomainofavariableinordertoobtaincomplicatedbehaviorfromarelativelysimplesetofφfunctions.Wewillexploreapracticalapplicationofthisidealater,inSec..20.616.2.4Energy-BasedModelsManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-sumptionthat∀x,˜p(x)>0.Aconvenientwaytoenforcethisconditionistouseanenergy-basedmodel(EBM)where˜pE() = exp(x−())x(16.7)andE(x)isknownastheenergyfunction.Becauseexp(z)ispositiveforallz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzeroforanystatex.571'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 586}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdefFigure16.4:Thisgraphimpliesthatp(abcdef,,,,,)canbewrittenas1Zφab,(ab,)φbc,(bc,)φad,(ad,)φbe,(be,)φef,(ef,)foranappropriatechoiceoftheφfunc-tions.Beingcompletelyfreetochoosetheenergyfunctionmakeslearningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouseconstrainedoptimizationtoarbitrarilyimposesomespeciﬁcminimalprobabilityvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5Theprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozerobutneverreachit.AnydistributionoftheformgivenbyEq.isanexampleofa16.7Boltzmanndistribution.Forthisreason,manyenergy-basedmodelsarecalledBoltzmannmachines(Fahlman1983Ackley1985Hinton1984Hintonetal.,;etal.,;etal.,;andSejnowski1986,).Thereisnoacceptedguidelineforwhentocallamodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.ThetermBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusivelybinaryvariables,buttodaymanymodelssuchasthemean-covariancerestrictedBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmannmachineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutlatentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignatemodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariablesaremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels.Cliquesinanundirectedgraphcorrespondtofactorsoftheunnormalizedprobabilityfunction.Becauseexp(a)exp(b) =exp(a+b),thismeansthatdiﬀerentcliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergyfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkovnetwork:theexponentiationmakeseachtermintheenergyfunctioncorrespondtoafactorforadiﬀerentclique.SeeFig.foranexampleofhowtoreadthe16.5formoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewanenergy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproductofexperts(,).EachtermintheenergyfunctioncorrespondstoanotherHinton19995Forsomemodels,wemaystillneedtouseconstrainedoptimizationtomakesureexists.Z572'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 587}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdefFigure 16.5:Thisgraph impliesthatE(abcdef,,,,,)can be writtenasEab,(ab,)+Ebc,(bc,)+Ead,(ad,)+Ebe,(be,)+Eef,(ef,)foranappropriatechoiceoftheper-cliqueenergyfunctions.NotethatwecanobtaintheφfunctionsinFig.bysettingeach16.4φtotheexponentialofthecorrespondingnegativeenergy,e.g.,φab,(ab,) =exp(())−Eab,.factorintheprobabilitydistribution.Eachtermoftheenergyfunctioncanbethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraintissatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonlyalow-dimensionalprojectionoftherandomvariables,butwhencombinedbymultiplicationofprobabilities,theexpertstogetherenforceacomplicatedhigh-dimensionalconstraint.Onepart ofthe deﬁnitionofanenergy-basedmodel serves nofunctionalpurposefromamachinelearningpointofview:the−signinEq..This16.7−signcouldbeincorporatedintothedeﬁnitionofE,orformanyfunctionsEthelearningalgorithmcouldsimplylearnparameterswithoppositesign.The−signispresentprimarilytopreservecompatibilitybetweenthemachinelearningliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodelingwereoriginallydevelopedbystatisticalphysicists,forwhomEreferstoactual,physicalenergyanddoesnothavearbitrarysign.Terminologysuchas“energy”and“partitionfunction”remainsassociatedwiththesetechniques,eventhoughtheirmathematicalapplicabilityisbroaderthanthephysicscontextinwhichtheyweredeveloped.Somemachinelearningresearchers(e.g.,(),whoSmolensky1986referredtonegativeenergyas)havechosentoemitthenegation,butthisharmonyisnotthestandardconvention.Manyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocomputepmodel(x)butonlylog ˜pmodel(x).Forenergy-basedmodelswithlatentvariablesh,thesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,calledthefreeenergy:F−() = xlog\\ue058hexp(())−Exh,.(16.8)Inthisbook,weusuallypreferthemoregenerallog ˜pmodel()xformulation.573'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 588}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGasbasb(a)(b)Figure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsisactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heresisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbisthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.16.2.5SeparationandD-SeparationTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoftenneedtoknowwhichvariablesindirectlyinteract.Someoftheseindirectinteractionscanbeenabledordisabledbyobservingothervariables.Moreformally,wewouldliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeachother,giventhevaluesofothersubsetsofvariables.Identifyingtheconditionalindependencesinagraphisverysimpleinthecaseofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraphiscalledseparation.WesaythatasetofvariablesAisseparatedfromanothersetofvariablesBgivenathirdsetofvariablesSifthegraphstructureimpliesthatAisindependentfromBgivenS.Iftwovariablesaandbareconnectedbyapathinvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifnopathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyareseparated.Werefertopathsinvolvingonlyunobservedvariablesas“active”andpathsincludinganobservedvariableas“inactive.”Whenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.SeeFig.foradepictionofhowactiveandinactivepathsinanundirected16.6modellookwhendrawninthisway.SeeFig.foranexampleofreading16.7separationfromanundirectedgraph.Similar conceptsapply todirected models, except that inthe context ofdirectedmodels,theseconceptsarereferredtoasd-separation.The“d”standsfor“dependence.”D-separationfordirectedgraphsisdeﬁnedthesameasseparationforundirectedgraphs:WesaythatasetofvariablesAisd-separatedfromanothersetofvariablesBgivenathirdsetofvariablesSifthegraphstructureimpliesthatisindependentfromgiven.ABSAswithundirectedmodels,wecanexaminetheindependencesimpliedbythegraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariablesaredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch574'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 589}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Herebisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfromatoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofbalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.Therefore,aanddarenotseparatedgivenb.pathexists.Indirectednets,determiningwhetherapathisactiveissomewhatmorecomplicated.SeeFig.foraguidetoidentifyingactivepathsinadirected16.8model.SeeFig.foranexampleofreadingsomepropertiesfromagraph.16.9Itisimportanttorememberthatseparationandd-separationtellusonlyaboutthoseconditionalindependencesthatareimpliedbythegraph.Thereisnorequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,itisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)torepresentanydistribution.Infact,somedistributionscontainindependencesthatarenotpossibletorepresentwithexistinggraphicalnotation.Context-speciﬁcindependencesareindependencesthatarepresentdependentonthevalueofsomevariablesinthenetwork.Forexample,consideramodelofthreebinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,butwhenais1,bisdeterministicallyequaltoc.Encodingthebehaviorwhena= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatbandcareindependentwhena.= 0Ingeneral,agraphwillneverimplythatanindependenceexistswhenitdoesnot.However,agraphmayfailtoencodeanindependence.16.2.6ConvertingbetweenUndirectedandDirectedGraphsWeoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected.Forexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.Thischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodelisinherentlydirectedorundirected.Instead,somemodelsaremosteasilydescribedusingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.575'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 590}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGasbasbasbasbasb\\nc(a)(b)\\n(c)(d)Figure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandomvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom(a)atoborviceversa.Thiskindofpathbecomesblockedifsisobserved. Wehavealreadyseenthiskindofpathintherelayraceexample.(b)aandbareconnectedbyacommoncauses.Forexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaandbmeasurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifweobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.Thiskindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,weexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpectedwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowingthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,thepathisactive.(c)aandbarebothparentsofs.ThisiscalledaV-structuretheorcollidercase.TheV-structurecausesaandbtoberelatedbytheexplainingawayeﬀect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,supposesisavariableindicatingthatyourcolleagueisnotatwork.Thevariablearepresentsherbeingsick,whilebrepresentsherbeingonvacation.Ifyouobservethatsheisnotatwork,youcanpresumesheisprobablysickoronvacation,butitisnotespeciallylikelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,thisfactissuﬃcienttoherabsence.Youcaninferthatsheisprobablynotalsosick.explain(d)Theexplainingawayeﬀecthappensevenifanydescendantofsisobserved!Forexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareportfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreasesyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesitmorelikelythatsheiseithersickoronvacation.TheonlywaytoblockapaththroughaV-structureistoobservenoneofthedescendantsofthesharedchild.576'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 591}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nabcdeFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examplesinclude:•aandbared-separatedgiventheemptyset.•aandeared-separatedgivenc.•dandeared-separatedgivenc.Wecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesomevariables:•aandbarenotd-separatedgivenc.•aandbarenotd-separatedgivend.\\n577'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 592}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.Hereweshowexampleswithfourrandomvariables.(Left)Thecompleteundirectedgraph.Intheundirectedcase,thecompletegraphisunique.(Right)Acompletedirectedgraph.Inthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthevariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritintheordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandomvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.Directedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-vantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,weshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartiallydependonwhichprobabilitydistributionwewishtodescribe.Wemaychoosetouseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcancapturethemostindependencesintheprobabilitydistributionorwhichapproachusesthefewestedgestodescribethedistribution.Thereareotherfactorsthatcanaﬀectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingleprobabilitydistribution,wemaysometimesswitchbetweendiﬀerentmodelinglanguages.Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserveacertainsubsetofvariables,orifwewishtoperformadiﬀerentcomputationaltask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforwardapproachtoeﬃcientlydrawsamplesfromthemodel(describedinSec.)while16.3theundirectedmodelformulationisoftenusefulforderivingapproximateinferenceprocedures(aswewillseeinChapter,wheretheroleofundirectedmodelsis19highlightedinEq.).19.56Everyprobabilitydistributioncanberepresentedbyeitheradirectedmodelorbyanundirectedmodel.Intheworstcase, onecanalwaysrepresentanydistributionbyusinga“completegraph.”Inthecaseofadirectedmodel,thecompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingontherandomvariables,andeachvariablehasallothervariablesthatprecedeitintheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecompletegraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.SeeFig.foranexample.16.10578'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 593}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsomevariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseitdoesnotimplyanyindependences.Whenwerepresentaprobabilitydistributionwithagraph,wewanttochooseagraphthatimpliesasmanyindependencesaspossible,withoutimplyinganyindependencesthatdonotactuallyexist.Fromthispointofview,somedistributionscanberepresentedmoreeﬃcientlyusingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃcientlyusing undirectedmodels.In other words,directed models can encode someindependencesthatundirectedmodelscannotencode,andviceversa.Directedmodelsareabletouseonespeciﬁckindofsubstructurethatundirectedmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.Thestructureoccurswhentworandomvariablesaandbarebothparentsofathirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineitherdirection.(Thename“immorality”mayseemstrange;itwascoinedinthegraphicalmodelsliteratureasajokeaboutunmarriedparents.)ToconvertadirectedmodelwithgraphDintoanundirectedmodel,weneedtocreateanewgraphU. Foreverypairofvariablesxandy,weaddanundirectededgeconnectingxandytoUifthereisadirectededge(ineitherdirection)connectingxandyinDorifxandyarebothparentsinDofathirdvariablez. TheresultingUisknownasamoralizedgraph.SeeFig.forexamplesofconvertingdirectedmodelsto16.11undirectedmodelsviamoralization.Likewise,undirectedmodelscanincludesubstructuresthatnodirectedmodelcanrepresentperfectly.Speciﬁcally,adirectedgraphcannotcapturealloftheDconditionalindependencesimpliedbyanundirectedgraphUifUcontainsaloopoflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopisasequenceofvariablesconnectedbyundirectededges,withthelastvariableinthesequenceconnectedbacktotheﬁrstvariableinthesequence.Achordisaconnectionbetweenanytwonon-consecutivevariablesinthesequencedeﬁningaloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsfortheseloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.AddingthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.ThegraphformedbyaddingchordstoUisknownasachordaltriangulatedorgraph,becausealltheloopscannowbedescribedintermsofsmaller,triangularloops.TobuildadirectedgraphDfromthechordalgraph,weneedtoalsoassigndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcycleinD,ortheresultdoesnotdeﬁneavaliddirectedprobabilisticmodel.OnewaytoassigndirectionstotheedgesinDistoimposeanorderingontherandom579'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 594}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nh1h1h2h2h3h3\\nv1v1v2v2v3v3ab\\nca\\ncb\\nh1h1h2h2h3h3\\nv1v1v2v2v3v3ab\\nca\\ncbFigure16.11: Examplesofconvertingdirectedmodels(toprow)toundirectedmodels(bottomrow)byconstructingmoralizedgraphs.(Left)Thissimplechaincanbeconvertedtoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.Theresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditionalindependences.This graphisthesimplestdirectedmodelthatcannot be(Center)convertedtoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirelyofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactivepathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustincludeacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab⊥.Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany(Right)impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizingedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnewdirectdependences.580'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 595}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabdcabdcabdcFigure16.12:Convertinganundirectedmodeltoadirectedmodel.(Left)Thisundirectedmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfourwithnochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthatnodirectedmodelcancapturesimultaneously:acbd⊥|{,}andbdac⊥|{,}. (Center)Toconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,byensuringthatallloopsofgreaterthanlengththreehaveachord. Todoso,wecaneitheraddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthisexample,wechoosetoaddtheedgeconnectingaandc.(Right)Toﬁnishtheconversionprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateanydirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,andalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenodethatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimposealphabeticalorder.variables,thenpointeachedgefromthenodethatcomesearlierintheorderingtothenodethatcomeslaterintheordering.SeeFig.forademonstration.16.1216.2.7FactorGraphsFactor graphsareanotherwayofdrawing undirectedmodels thatresolveanambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.Inanundirectedmodel,thescopeofeveryφfunctionmustbeasubsetofsomecliqueinthegraph.However,itisnotnecessarythatthereexistanyφwhosescopecontainstheentiretyofeveryclique.Factorgraphsexplicitlyrepresentthescopeofeachφfunction.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirectedmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawnascircles.Thesenodescorrespondtorandomvariablesasinastandardundirectedmodel. Therestofthenodesaredrawnassquares. Thesenodescorrespondtothefactorsφoftheunnormalizedprobabilitydistribution.Variablesandfactorsmaybeconnectedwithundirectededges.Avariableandafactorareconnectedinthegraphifandonlyifthevariableisoneoftheargumentstothefactorintheunnormalizedprobabilitydistribution.Nofactormaybeconnectedtoanotherfactorinthegraph,norcanavariablebeconnectedtoa581'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 596}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGvariable.SeeFig.foranexampleofhowfactorgraphscanresolveambiguity16.13intheinterpretationofundirectednetworks.abcabcf1f1abcf1f1f2f2f3f3Figure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretationofundirectednetworks.(Left)Anundirectednetworkwithacliqueinvolvingthreevariables:a,bandc.Afactorgraphcorrespondingtothesameundirected(Center)model.Thisfactorgraphhasonefactoroverallthreevariables.(Right)Anothervalidfactorgraphforthesameundirectedmodel. Thisfactorgraphhasthreefactors,eachoveronlytwovariables. Representation,inference,andlearningareallasymptoticallycheaperinthisfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethesameundirectedgraphtorepresent.16.3SamplingfromGraphicalModelsGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.Oneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientprocedurecalledancestralsamplingcanproduceasamplefromthejointdistributionrepresentedbythemodel.Thebasicideaistosortthevariablesxiinthegraphintoatopologicalordering,sothatforalliandj,jisgreaterthaniifxiisaparentofxj.Thevariablescanthenbesampledinthisorder.Inotherwords,weﬁrstsamplex1∼P(x1),thensampleP(x2|PaG(x2)),andsoon,untilﬁnallywesampleP(xn|PaG(xn)).Solongaseachconditionaldistributionp(xi|PaG(xi))iseasytosamplefrom,thenthewholemodeliseasytosamplefrom.ThetopologicalsortingoperationguaranteesthatwecanreadtheconditionaldistributionsinEq.andsample16.1fromtheminorder.Withoutthetopologicalsorting,wemightattempttosampleavariablebeforeitsparentsareavailable.Forsomegraphs,morethanonetopologicalorderingispossible.Ancestralsamplingmaybeusedwithanyofthesetopologicalorderings.Ancestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-582'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 597}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGtionaliseasy)andconvenient.Onedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphicalmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsamplingoperation.Whenwewishtosamplefromasubsetofthevariablesinadirectedgraphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-ingvariablescomeearlierthanthevariablestobesampledintheorderedgraph.Inthiscase,wecansamplefromthelocalconditionalprobabilitydistributionsspeciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionsweneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.Theseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrizedinthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswherethisisthecase,ancestralsamplingisnolongereﬃcient.Unfortunately,ancestralsamplingisapplicableonlytodirectedmodels.Wecansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthisoftenrequiressolvingintractableinferenceproblems(todeterminethemarginaldistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducingsomanyedgesthattheresultingdirectedmodelbecomesintractable.Samplingfromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemstorequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryothervariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,drawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-passprocess.Theconceptuallysimplestapproachis.SupposeweGibbssamplinghaveagraphicalmodeloverann-dimensionalvectorofrandomvariablesx.Weiterativelyvisiteachvariablexianddrawasampleconditionedonalloftheothervariables,fromp(xi|x−i).Duetotheseparationpropertiesofthegraphicalmodel,wecanequivalentlyconditionononlytheneighborsofxi.Unfortunately,afterwehavemadeonepassthroughthegraphicalmodelandsampledallnvariables,westilldonothaveafairsamplefromp(x).Instead,wemustrepeattheprocessandresampleallnvariablesusingtheupdatedvaluesoftheirneighbors.Asymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfromthecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshavereachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Samplingtechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailinChapter.17\\n583'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 598}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING16.4AdvantagesofStructuredModelingTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallowustodramaticallyreducethecostofrepresentingprobabilitydistributionsaswellaslearningandinference.Samplingisalsoacceleratedinthecaseofdirectedmodels,whilethesituationcanbecomplicatedwithundirectedmodels.Theprimarymechanismthatallowsalloftheseoperationstouselessruntimeandmemoryischoosingtonotmodelcertaininteractions.Graphicalmodelsconveyinformationbyleavingedgesout.Anywherethereisnotanedge,themodelspeciﬁestheassumptionthatwedonotneedtomodeladirectinteraction.Alessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthattheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningofknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasiertodevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsandinferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinourdata.WecanthencombinethesediﬀerentalgorithmsandstructuresandobtainaCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃculttodesignend-to-endalgorithmsforeverypossiblesituation.16.5LearningaboutDependenciesAgoodgenerativemodelneedstoaccuratelycapturethedistributionovertheobservedor“visible” variablesv.Oftenthediﬀerentelementsofvarehighlydependentoneachother.Inthecontextofdeeplearning,theapproachmostcommonlyusedtomodelthesedependenciesistointroduceseverallatentor“hidden”variables,h.Themodelcanthencapturedependenciesbetweenanypairofvariablesviandvjindirectly,viadirectdependenciesbetweenviandh,anddirectdependenciesbetweenandvhj.AgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedtohaveverylargenumbersofparentspernodeinaBayesiannetworkorverylargecliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsiscostly—bothinacomputationalsense,becausethenumberofparametersthatmustbestoredinmemoryscalesexponentiallywiththenumberofmembersinaclique,butalsoinastatisticalsense,becausethisexponentialnumberofparametersrequiresawealthofdatatoestimateaccurately.Whenthemodelisintendedtocapturedependenciesbetweenvisiblevariableswithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothegraph584'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 599}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGmustbedesignedtoconnectthosevariablesthataretightlycoupledandomitedgesbetweenothervariables.AnentireﬁeldofmachinelearningcalledstructurelearningisdevotedtothisproblemForagoodreferenceonstructurelearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesareaformofgreedysearch.Astructureisproposed,amodelwiththatstructureistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyandpenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedgesaddedorremovedarethenproposedasthenextstepofthesearch.Thesearchproceedstoanewstructurethatisexpectedtoincreasethescore.Usinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperformdiscretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisibleandhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunitstoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameterlearningtechniqueswecanlearnamodelwithaﬁxedstructurethatimputestherightstructureonthemarginal.p()vLatentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturingp(v).Thenewvariableshalsoprovideanalternativerepresentationforv.Forexample,asdiscussedinSec.,themixtureofGaussiansmodellearnsalatentvariable3.9.6thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.ThismeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodoclassiﬁcation.InChapterwesawhowsimpleprobabilisticmodelslikesparse14codinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiﬁer,orascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,butdeepermodelsandmodelswithdiﬀerentkindsofinteractionscancreateevenricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearningbylearninglatentvariables.Often,givensomemodelofvandh,experimentalobservationsshowthatE[hv|]orargmaxhp(hv,)isagoodfeaturemappingforv.16.6InferenceandApproximateInferenceOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabouthowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanaskwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwanttoextractfeaturesE[hv|]describingtheobservedvariablesv.Sometimesweneedtosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodelsusingtheprincipleofmaximumlikelihood.Becauselog() = pvEhh∼p(|v)[log()log()]phv,−phv|,(16.9)585'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 600}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGweoftenwanttocomputep(h|v)inordertoimplementalearningrule.Alloftheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueofsomevariablesgivenothervariables,orpredicttheprobabilitydistributionoversomevariablesgiventhevalueofothervariables.Unfortunately,formostinterestingdeepmodels,theseinferenceproblemsareintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.Thegraphstructureallowsustorepresentcomplicated,high-dimensionaldistributionswithareasonablenumberofparameters,butthegraphsusedfordeeplearningareusuallynotrestrictiveenoughtoalsoalloweﬃcientinference.Itisstraightforwardtoseethatcomputingthemarginalprobabilityofageneralgraphicalmodelis#Phard.Thecomplexityclass#PisageneralizationofthecomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblemhasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecountingthenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethatwedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem.Wecanimposeauniformdistributionoverthesevariables.Wecanthenaddonebinarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed.Wecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesaresatisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareductiontreeoflatentvariables,witheachnodeinthetreereportingwhethertwoothervariablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause.Therootofthetreereportswhethertheentireproblemissatisﬁed. Duetotheuniformdistributionovertheliterals,themarginaldistributionovertherootofthereductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.Whilethisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpracticalreal-worldscenarios.Thismotivates theuse ofapproximateinference.Inthe context ofdeeplearning,thisusuallyreferstovariationalinference,inwhichweapproximatethetruedistributionp(h|v)byseekinganapproximatedistributionq(hv|)thatisasclosetothetrueoneaspossible.ThisandothertechniquesaredescribedindepthinChapter.1916.7TheDeepLearningApproachtoStructuredProb-abilisticModelsDeeplearningpractitionersgenerallyusethesamebasiccomputationaltoolsasothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.586'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 601}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGHowever,inthecontextofdeeplearning,weusuallymakediﬀerentdesigndecisionsabouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthathaveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels.Deeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthecontextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthegraphicalmodelgraphratherthanthecomputationalgraph.Wecanthinkofalatentvariablehiasbeingatdepthjiftheshortestpathfromhitoanobservedvariableisjsteps.Weusuallydescribethedepthofthemodelasbeingthegreatestdepthofanysuchhi.Thiskindofdepthisdiﬀerentfromthedepthinducedbythecomputationalgraph.Manygenerativemodelsusedfordeeplearninghavenolatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputationalgraphstodeﬁnetheconditionaldistributionswithinamodel.Deeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-tations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretrainingshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalwayshaveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhavemorelatentvariablesthanobservedvariables.Complicatednonlinearinteractionsbetweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthroughmultiplelatentvariables.Bycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthatareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingatrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-ordertermsandstructurelearningtocapturecomplicatednonlinearinteractionsbetweenvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.Thewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.Thedeeplearningpractitionertypicallydoesnotintendforthelatentvariablestotakeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreetoinventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesareusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualizationtechniquesmayallowsomeroughcharacterizationofwhattheyrepresent.Whenlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyareoftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,theintelligenceofastudent,thediseasecausingapatient’ssymptoms,etc.Thesemodelsareoftenmuchmoreinterpretablebyhumanpractitionersandoftenhavemoretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandarenotreusableinasmanydiﬀerentcontextsasdeepmodels.Anotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthedeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits587'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 602}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetweentwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodelshaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybeindividuallydesigned.Thedesignofthemodelstructureistightlylinkedwiththechoiceofinferencealgorithm.Traditionalapproachestographicalmodelstypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraintistoolimiting,apopularapproximateinferencealgorithmisanalgorithmcalledloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithverysparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendtoconnecteachvisibleunitvitoverymanyhiddenunitshj,sothathcanprovideadistributedrepresentationofvi(andprobablyseveralotherobservedvariablestoo).Distributedrepresentationshavemanyadvantages,butfromthepointofviewofgraphicalmodelsandcomputationalcomplexity,distributedrepresentationshavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughforthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtoberelevant.Asaconsequence,oneofthemoststrikingdiﬀerencesbetweenthelargergraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthatloopybeliefpropagationisalmostneverusedfordeeplearning.MostdeepmodelsareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithmseﬃcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylargenumberoflatentvariables,makingeﬃcientnumericalcodeessential.Thisprovidesanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,forgroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetweentwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplementedwitheﬃcientmatrixproductoperations,orsparselyconnectedgeneralizations,likeblockdiagonalmatrixproductsorconvolutions.Finally,thedeeplearningapproachtographicalmodelingischaracterizedbyamarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntilallquantitieswemightwantcanbecomputedexactly,weincreasethepowerofthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodelswhosemarginaldistributionscannotbecomputed,andaresatisﬁedsimplytodrawapproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractableobjectivefunctionthatwecannotevenapproximateinareasonableamountoftime,butwearestillabletoapproximatelytrainthemodelifwecaneﬃcientlyobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproachisoftentoﬁgureoutwhattheminimumamountofinformationweabsolutelyneedis,andthentoﬁgureouthowtogetareasonableapproximationofthatinformationasquicklyaspossible.588'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 603}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGh1h1h2h2h3h3v1v1v2v2v3v3h4h4Figure16.14:AnRBMdrawnasaMarkovnetwork.16.7.1Example:TheRestrictedBoltzmannMachineTherestrictedBoltzmannmachineharmonium(RBM)(,)orSmolensky1986isthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.TheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariablesthatmaybeusedtolearnarepresentationfortheinput.InChapter,wewill20seehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowtheRBMexempliﬁesmanyofthepracticesusedinawidevarietyofdeepgraphicalmodels: itsunitsareorganizedintolargegroupscalledlayers,theconnectivitybetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,themodelisdesignedtoalloweﬃcientGibbssampling,andtheemphasisofthemodeldesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemanticswerenotspeciﬁedbythedesigner.Later,inSec.,wewillrevisittheRBMin20.2moredetail.ThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhiddenunits.ItsenergyfunctionisE,(vhb) = −\\ue03evc−\\ue03ehv−\\ue03eWh,(16.10)whereb,c,andWareunconstrained,real-valued,learnableparameters.Wecanseethatthemodelisdividedintotwogroupsofunits:vandh,andtheinteractionbetweenthemisdescribedbyamatrixW.ThemodelisdepictedgraphicallyinFig..Asthisﬁguremakesclear,animportantaspectofthismodelis16.14thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenanytwohiddenunits(hencethe“restricted,”ageneralBoltzmannmachinemayhavearbitraryconnections).TherestrictionsontheRBMstructureyieldthenicepropertiesp() = Πhv|ip(hi|v)(16.11)andp() = Πvh|ip(vi|h).(16.12)589'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 604}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwithpermissionfrom(). LISA2008(Left)SamplesfromamodeltrainedonMNIST,drawnusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrowrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesarehighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare(Right)thistothesamplesandweightsofalinearfactormodel,showninFig. .Thesamples13.2herearemuchbetterbecausetheRBMpriorp(h)isnotconstrainedtobefactorial.TheRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,theRBMposteriorisfactorial,whilethesparsecodingposteriorisnot,p()hv|p()hv|sothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareabletohavebothanon-factorialandanon-factorial.p()hp()hv|Theindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBMweobtain:P(hi= 1 ) = |vσ\\ue010v\\ue03eW:,i+bi\\ue011,(16.13)P(hi= 0 ) = 1|v−σ\\ue010v\\ue03eW:,i+bi\\ue011.(16.14)TogetherthesepropertiesallowforeﬃcientblockGibbssampling,whichalternatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultaneously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshowninFig.16.15.Sincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itiseasytotakederivativesoftheenergyfunction.Forexample,∂∂Wi,jE,(vh) = −vihj.(16.15)Thesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—maketrainingconvenient. InChapter,wewillseethatundirectedmodelsmaybe18trainedbycomputingsuchderivativesappliedtosamplesfromthemodel.Trainingthemodelinducesarepresentationhofthedatav.WecanoftenuseEhh∼p(|v)[]hasasetoffeaturestodescribe.v590'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 605}, page_content='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-icalmodels: representationlearningaccomplishedvialayersoflatentvariables,combinedwitheﬃcientinteractionsbetweenlayersparametrizedbymatrices.Thelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguagefordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,amongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\\n591'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 606}, page_content='Chapter17MonteCarloMethodsRandomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsandMonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrectanswer(orreportthattheyfailed).Thesealgorithmsconsumearandomamountofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturnanswerswitharandomamountoferror.Theamountoferrorcantypicallybereducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Foranyﬁxedcomputationalbudget,aMonteCarloalgorithmcanprovideanapproximateanswer.Manyproblemsinmachinelearningaresodiﬃcultthatwecanneverexpecttoobtainpreciseanswerstothem.ThisexcludesprecisedeterministicalgorithmsandLasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithmsorMonteCarloapproximations.Bothapproachesareubiquitousinmachinelearning.Inthischapter,wefocusonMonteCarlomethods.17.1SamplingandMonteCarloMethodsManyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebasedondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplestoformaMonteCarloestimateofsomedesiredquantity.17.1.1WhySampling?Therearemanyreasonsthatwemaywishtodrawsamplesfromaprobabilitydistribution.Samplingprovidesaﬂexiblewaytoapproximatemanysumsand592'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 607}, page_content='CHAPTER17.MONTECARLOMETHODSintegralsatreducedcost.Sometimesweusethistoprovideasigniﬁcantspeeduptoacostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcostwithminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximateanintractablesumorintegral,suchasthegradientofthelogpartitionfunctionofanundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthesensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution.17.1.2BasicsofMonteCarloSamplingWhenasumoranintegralcannotbecomputedexactly(forexamplethesumhasanexponentialnumberoftermsandnoexactsimpliﬁcationisknown)itisoftenpossibletoapproximateitusingMonteCarlosampling.Theideaistoviewthesumorintegralasifitwasanexpectationundersomedistributionandtoapproximatetheexpectationbyacorrespondingaverage.Lets=\\ue058xpfE()x() = xp[()]fx(17.1)ors=\\ue05apfdE()x()xx= p[()]fx(17.2)bethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraintthatpisaprobabilitydistribution(forthesum)oraprobabilitydensity(fortheintegral)overrandomvariable.xWecanapproximatesbydrawingnsamplesx(1),...,x()nfrompandthenformingtheempiricalaverageˆsn=1nn\\ue058i=1f(x()i).(17.3)Thisapproximationisjustiﬁedbyafewdiﬀerentproperties.Theﬁrsttrivialobservationisthattheestimatorˆsisunbiased,sinceE[ˆsn] =1nn\\ue058i=1E[(fx()i)] =1nn\\ue058i=1ss.= (17.4)Butinaddition,thelawoflargenumbersstatesthatifthesamplesx()iarei.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:limn→∞ˆsn= s,(17.5)593'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 608}, page_content='CHAPTER17.MONTECARLOMETHODSprovidedthatthevarianceoftheindividualterms,Var[f(x()i)],isbounded.Toseethismoreclearly,considerthevarianceofˆsnasnincreases.ThevarianceVar[ˆsn]decreasesandconvergesto0,solongasVar[(fx()i)] <∞:Var[ˆsn] =1n2n\\ue058i=1Var[()]fx(17.6)=Var[()]fxn.(17.7)ThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonteCarloaverageorequivalentlytheamountofexpectederroroftheMonteCarloapproximation.Wecomputeboththeempiricalaverageofthef(x()i)andtheirempiricalvariance,1andthendividetheestimatedvariancebythenumberofsamplesntoobtainanestimatorofVar[ˆsn].Thecentrallimittheoremtellsusthatthedistributionoftheaverage,ˆsn,convergestoanormaldistributionwithmeansandvarianceVar[()]fxn.Thisallowsustoestimateconﬁdenceintervalsaroundtheestimateˆsn,usingthecumulativedistributionofthenormaldensity.However,allthisreliesonourabilitytoeasilysamplefromthebasedistributionp(x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefromp,analternativeistouseimportancesampling,presentedinSec..Amore17.2generalapproachistoformasequenceofestimatorsthatconvergetowardsthedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains(Sec.).17.317.2ImportanceSamplingAnimportantstepinthedecompositionoftheintegrand(orsummand)usedbytheMonteCarlomethodinEq.isdecidingwhichpartoftheintegrandshould17.2playtheroletheprobabilityp(x)andwhichpartoftheintegrandshouldplaytheroleofthequantityf(x) whoseexpectedvalue(underthatprobabilitydistribution)istobeestimated.Thereisnouniquedecompositionbecausep(x)f(x)canalwaysberewrittenaspfq()x() = x()xpf()x()xq()x,(17.8)wherewenowsamplefromqandaveragepfq.Inmanycases,wewishtocomputeanexpectationforagivenpandanf,andthefactthattheproblemisspeciﬁed1Theunbiasedestimatorofthevarianceisoftenpreferred,inwhichthesumofsquareddiﬀerencesisdividedbyinsteadof.n−1n594'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 609}, page_content='CHAPTER17.MONTECARLOMETHODSfromthestartasanexpectationsuggeststhatthispandfwouldbeanaturalchoiceofdecomposition.However,theoriginalspeciﬁcationoftheproblemmaynotbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtainagivenlevelofaccuracy. Fortunately,theformoftheoptimalchoiceq∗canbederivedeasily.Theoptimalq∗correspondstowhatiscalledoptimalimportancesampling.BecauseoftheidentityshowninEq.,anyMonteCarloestimator17.8ˆsp=1nn\\ue058i,=1x()i∼pf(x()i)(17.9)canbetransformedintoanimportancesamplingestimatorˆsq=1nn\\ue058i,=1x()i∼qp(x()i)(fx()i)q(x()i).(17.10)Weseereadilythattheexpectedvalueoftheestimatordoesnotdependon:qEq[ˆsq] = Eq[ˆsp] = s.(17.11)However,thevarianceofanimportancesamplingestimatorcanbegreatlysensitivetothechoiceof.ThevarianceisgivenbyqVar[ˆsq] = Var[pf()x()xq()x]/n.(17.12)Theminimumvarianceoccurswhenisqq∗() =xpf()x|()x|Z,(17.13)whereZisthenormalizationconstant,chosensothatq∗(x)sumsorintegratesto1asappropriate.Betterimportancesamplingdistributionsputmoreweightwheretheintegrandislarger.Infact,whenf(x)doesnotchangesign,Var[ˆsq∗]=0,meaningthatasinglesampleissuﬃcientwhentheoptimaldistributionisused.Ofcourse,thisisonlybecausethecomputationofq∗hasessentiallysolvedtheoriginalproblem,soitisusuallynotpracticaltousethisapproachofdrawingasinglesamplefromtheoptimaldistribution.Anychoiceofsamplingdistributionqisvalid(inthesenseofyieldingthecorrectexpectedvalue)andq∗istheoptimalone(inthesenseofyieldingminimumvariance).Samplingfromq∗isusuallyinfeasible,butotherchoicesofqcanbefeasiblewhilestillreducingthevariancesomewhat.595'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 610}, page_content='CHAPTER17.MONTECARLOMETHODSAnotherapproachistousebiasedimportancesampling,whichhastheadvantageofnotrequiringnormalizedporq.Inthecaseofdiscretevariables,thebiasedimportancesamplingestimatorisgivenbyˆsBIS=\\ue050ni=1p(x()i)q(x()i)f(x()i)\\ue050ni=1p(x()i)q(x()i)(17.14)=\\ue050ni=1p(x()i)˜q(x()i)f(x()i)\\ue050ni=1p(x()i)˜q(x()i)(17.15)=\\ue050ni=1˜p(x()i)˜q(x()i)f(x()i)\\ue050ni=1˜p(x()i)˜q(x()i),(17.16)where˜pand˜qaretheunnormalizedformsofpandqandthex()iarethesamplesfromq.ThisestimatorisbiasedbecauseE[ˆsBIS]\\ue036=s,exceptasymptoticallywhenn→∞andthedenominatorofEq.convergesto1.Hencethisestimatoris17.14calledasymptoticallyunbiased.AlthoughagoodchoiceofqcangreatlyimprovetheeﬃciencyofMonteCarloestimation,apoorchoiceofqcanmaketheeﬃciencymuchworse. GoingbacktoEq.,weseethatiftherearesamplesof17.12qforwhichpf()x|()x|q()xislarge,thenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhenq(x)istinywhileneitherp(x)norf(x)aresmallenoughtocancelit.Theqdistributionisusuallychosentobeaverysimpledistributionsothatitiseasytosamplefrom.Whenxishigh-dimensional,thissimplicityinqcausesittomatchporpf||poorly.Whenq(x()i)\\ue01dp(x()i)|f(x()i)|,importancesamplingcollectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,whenq(x()i)\\ue01cp(x()i)|f(x()i)|,whichwillhappenmorerarely,theratiocanbehuge.Becausetheselattereventsarerare,theymaynotshowupinatypicalsample,yieldingtypicalunderestimationofs,compensatedrarelybygrossoverestimation.Suchverylargeorverysmallnumbersaretypicalwhenxishighdimensional,becauseinhighdimensionthedynamicrangeofjointprobabilitiescanbeverylarge.Inspiteofthisdanger,importancesamplinganditsvariantshavebeenfoundveryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms.Forexample,seetheuseofimportancesamplingtoacceleratetraininginneurallanguagemodelswith alargevocabulary(Sec.)orotherneuralnets12.4.3.3withalargenumberofoutputs.Seealsohowimportancesamplinghasbeenusedtoestimateapartitionfunction(thenormalizationconstantofaprobability596'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 611}, page_content='CHAPTER17.MONTECARLOMETHODSdistribution)inSec.,andtoestimatethelog-likelihoodindeepdirectedmodels18.7suchasthevariationalautoencoder,inSec..Importancesamplingmay20.10.3alsobeusedtoimprovetheestimateofthegradientofthecostfunctionusedtotrainmodelparameterswithstochasticgradientdescent,particularlyformodelssuchasclassiﬁerswheremostofthetotalvalueofthecostfunctioncomesfromasmallnumberofmisclassiﬁedexamples.Samplingmorediﬃcultexamplesmorefrequentlycanreducethevarianceofthegradientinsuchcases(,).Hinton200617.3MarkovChainMonteCarloMethodsInmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractablemethodfordrawingexactsamplesfromthedistributionpmodel(x)orfromagood(lowvariance)importancesamplingdistributionq(x).Inthecontextofdeeplearning,thismostoftenhappenswhenpmodel(x)isrepresentedbyanundirectedmodel.Inthesecases,weintroduceamathematicaltoolcalledatoMarkovchainapproximatelysamplefrompmodel(x).ThefamilyofalgorithmsthatuseMarkovchainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlomethods(MCMC).MarkovchainMonteCarlomethodsformachinelearningaredescribedatgreaterlengthinKollerandFriedman2009(). Themoststandard,genericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodeldoesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenientto present these techniques assampling froman energy-basedmodel (EBM)p(x)∝−exp(E())xasdescribedinSec..IntheEBMformulation,every16.2.4stateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfactmorebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthatcontainzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthebehaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordiﬀerentfamiliesofsuchdistributions.Inthecontextofdeeplearning,itismostcommontorelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoallenergy-basedmodels.Tounderstandwhydrawingsamplesfromanenergy-basedmodelisdiﬃcult,consideranEBMoverjusttwovariables,deﬁningadistributionab.Inorderp(,)tosamplea,wemustdrawafromp(ab|),andinordertosampleb,wemustdrawitfromp(ba|).Itseemstobeanintractablechicken-and-eggproblem.Directedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperformancestralsamplingonesimplysampleseachofthevariablesintopologicalorder,conditioningoneachvariable’sparents,whichareguaranteedtohavealreadybeensampled(Sec.).Ancestralsamplingdeﬁnesaneﬃcient,single-passmethodof16.3597'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 612}, page_content='CHAPTER17.MONTECARLOMETHODSobtainingasample.InanEBM,wecanavoidthischickenandeggproblembysamplingusingaMarkovchain.ThecoreideaofaMarkovchainistohaveastatexthatbeginsasanarbitraryvalue.Overtime,werandomlyupdatexrepeatedly.Eventuallyxbecomes(verynearly)afairsamplefromp(x).Formally,aMarkovchainisdeﬁnedbyarandomstatexandatransitiondistributionT(x\\ue030|x)specifyingtheprobabilitythatarandomupdatewillgotostatex\\ue030ifitstartsinstatex.RunningtheMarkovchainmeansrepeatedlyupdatingthestatextoavaluex\\ue030sampledfromT(x\\ue030|x).TogainsometheoreticalunderstandingofhowMCMCmethodswork,itisusefultoreparametrizetheproblem.First,werestrictourattentiontothecasewheretherandomvariablexhascountablymanystates.Inthiscase,wecanrepresentthestateasjustapositiveintegerx.Diﬀerentintegervaluesofxmapbacktodiﬀerentstatesintheoriginalproblem.xConsiderwhathappenswhenweruninﬁnitelymanyMarkovchainsinparallel.AllofthestatesofthediﬀerentMarkovchainsaredrawnfromsomedistributionq()t(x),wheretindicatesthenumberoftimestepsthathaveelapsed.Atthebeginning,q(0)issomedistributionthatweusedtoarbitrarilyinitializexforeachMarkovchain.Later,q()tisinﬂuencedbyalloftheMarkovchainstepsthathaverunsofar.Ourgoalisforq()t()xtoconvergeto.px()Becausewehavereparametrizedtheproblemintermsofpositiveintegerx,wecandescribetheprobabilitydistributionusingavector,withqvqiv(= x) = i.(17.17)ConsiderwhathappenswhenweupdateasingleMarkovchain’sstatextoanewstatex\\ue030.Theprobabilityofasinglestatelandinginstatex\\ue030isgivenbyq(+1)t(x\\ue030) =\\ue058xq()t()(xTx\\ue030|x.)(17.18)Usingourintegerparametrization,wecanrepresenttheeﬀectofthetransitionoperatorusingamatrix.WedeﬁnesothatTAAAi,j= (Tx\\ue030= = )i|xj.(17.19)Usingthisdeﬁnition,wecannowrewriteEq..Ratherthanwritingitin17.18termsofqandTtounderstandhowasinglestateisupdated,wemaynowusevandAtodescribehowtheentiredistributionoverallthediﬀerentMarkovchainsruninparallelshiftsasweapplyanupdate:v()t= Av(1)t−.(17.20)598'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 613}, page_content='CHAPTER17.MONTECARLOMETHODSApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythematrixArepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiatingthematrix:Av()t= Atv(0).(17.21)ThematrixAhasspecialstructurebecauseeachofitscolumnsrepresentsaprobabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthereisanon-zeroprobabilityoftransitioningfromanystatextoanyotherstatex\\ue030forsomepowert,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,)guaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan1seethatalloftheeigenvaluesareexponentiated:v()t=\\ue000VλVdiag()−1\\ue001tv(0)= ()VdiagλtV−1v(0).(17.22)Thisprocesscausesalloftheeigenvaluesthatarenotequaltotodecayto1zero.Under someadditional mild conditions,Ais guaranteed to haveonlyoneeigenvectorwitheigenvalue.Theprocessthusconvergestoa1stationarydistributionequilibriumdistribution,sometimesalsocalledthe.Atconvergence,v\\ue030= = Avv,(17.23)andthissameconditionholdsforeveryadditionalstep.Thisisaneigenvectorequation.Tobeastationarypoint,vmustbeaneigenvectorwithcorrespondingeigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary1distribution,repeatedapplicationsofthetransitionsamplingproceduredonotchangetheoverthestatesofallthevariousMarkovchains(althoughdistributiontransitionoperatordoeschangeeachindividualstate,ofcourse).IfwehavechosenTcorrectly,thenthestationarydistributionqwillbeequaltothedistributionpwewishtosamplefrom.WewilldescribehowtochooseTshortly,inSec..17.4MostpropertiesofMarkovChainswithcountablestatescanbegeneralizedtocontinuousvariables.Inthissituation,someauthorscalltheMarkovChainabutweusethetermMarkovChaintodescribebothconditions.HarrischainIngeneral,aMarkovchainwithtransitionoperatorTwillconverge,undermildconditions,toaﬁxedpointdescribedbytheequationq\\ue030(x\\ue030) = Ex∼qT(x\\ue030|x),(17.24)whichinthediscretecaseisjustrewritingEq..When17.23xisdiscrete,theexpectationcorrespondstoasum,andwhenxiscontinuous,theexpectationcorrespondstoanintegral.599'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 614}, page_content='CHAPTER17.MONTECARLOMETHODSRegardlessofwhetherthestateiscontinuousordiscrete,allMarkovchainmethodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythestatebeginstoyieldsamplesfromtheequilibriumdistribution.RunningtheMarkovchainuntilitreachesitsequilibriumdistributioniscalled“”theMarkovburninginchain.Afterthechainhasreachedequilibrium,asequenceofinﬁnitelymanysamplesmaybedrawnfromfromtheequilibriumdistribution.Theyareidenticallydistributedbutanytwosuccessivesampleswillbehighlycorrelatedwitheachother.Aﬁnitesequenceofsamplesmaythusnotbeveryrepresentativeoftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturnonlyeverynsuccessivesamples,sothatourestimateofthestatisticsoftheequilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMCsampleandthenextseveralsamples.Markovchainsarethusexpensivetousebecauseofthetimerequiredtoburnintotheequilibriumdistributionandthetimerequiredtotransitionfromonesampletoanotherreasonablydecorrelatedsampleafterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrunmultipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputationtoeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerateallsamplesandthestrategyofusingoneMarkovchainforeachdesiredsamplearetwoextremes;deeplearningpractitionersusuallyuseanumberofchainsthatissimilartothenumberofexamplesinaminibatchandthendrawasmanysamplesasareneededfromthisﬁxedsetofMarkovchains.AcommonlyusednumberofMarkovchainsis100.AnotherdiﬃcultyisthatwedonotknowinadvancehowmanystepstheMarkovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthoftimeiscalledthe.ItisalsoverydiﬃculttotestwhetheraMarkovmixingtimechainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguidingusinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnotmuchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrixAactingonavectorofprobabilitiesv,thenweknowthatthechainmixeswhenAthaseﬀectivelylostalloftheeigenvaluesfromAbesidestheuniqueeigenvalueof.1Thismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethemixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchainintermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisitisexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresentv,A,ortheeigenvaluesofA. Duetotheseandotherobstacles,weusuallydonotknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkovchainforanamountoftimethatweroughlyestimatetobesuﬃcient,anduseheuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristicmethodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetweensuccessivesamples.600'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 615}, page_content='CHAPTER17.MONTECARLOMETHODS17.4GibbsSamplingSofarwehavedescribedhowtodrawsamplesfromadistributionq(x)byrepeatedlyupdatingxx←\\ue030∼T(x\\ue030|x).However,wehavenotdescribedhowtoensurethatq(x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook.TheﬁrstoneistoderiveTfromagivenlearnedpmodel,describedbelowwiththecaseofsamplingfromEBMs.ThesecondoneistodirectlyparametrizeTandlearnit,sothatitsstationarydistributionimplicitlydeﬁnesthepmodelofinterest.ExamplesofthissecondapproacharediscussedinSec.andSec..20.1220.13Inthecontextofdeeplearning,wecommonlyuseMarkovchainstodrawsamplesfromanenergy-basedmodeldeﬁningadistributionpmodel(x).Inthiscase,wewanttheq(x)fortheMarkovchaintobepmodel(x).Toobtainthedesiredq()x,wemustchooseanappropriateT(x\\ue030|x).AconceptuallysimpleandeﬀectiveapproachtobuildingaMarkovchainthatsamplesfrompmodel(x)istouse,inwhichsamplingfromGibbssamplingT(x\\ue030|x)isaccomplishedbyselectingonevariablexiandsamplingitfrompmodelconditionedonitsneighborsintheundirectedgraphGdeﬁningthestructureoftheenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesametimesolongastheyareconditionallyindependentgivenalloftheirneighbors.AsshownintheRBMexampleinSec.,allofthehiddenunitsofanRBMmay16.7.1besampledsimultaneouslybecausetheyareconditionallyindependentfromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmaybesampledsimultaneouslybecausetheyareconditionallyindependentfromeachothergivenallofthehiddenunits.GibbssamplingapproachesthatupdatemanyvariablessimultaneouslyinthiswayarecalledblockGibbssampling.AlternateapproachestodesigningMarkovchainstosamplefrompmodelarepossible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinotherdisciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,itisraretouseanyapproachotherthanGibbssampling.Improvedsamplingtechniquesareonepossibleresearchfrontier.17.5TheChallengeofMixingbetweenSeparatedModesTheprimarydiﬃcultyinvolvedwithMCMCmethodsisthattheyhaveatendencytopoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosamplemixfromp(x)wouldbecompletelyindependentfromeachotherandwouldvisitmanydiﬀerentregionsinxspaceproportionaltotheirprobability.Instead,especiallyinhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer601'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 616}, page_content='CHAPTER17.MONTECARLOMETHODStosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswithslowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisygradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingontheprobability,withrespecttothestateofthechain(therandomvariablesbeingsampled). Thechaintendstotakesmallsteps(inthespaceofthestateoftheMarkovchain),fromaconﬁgurationx(1)t−toaconﬁgurationx()t,withtheenergyE(x()t)generallylowerorapproximatelyequaltotheenergyE(x(1)t−),withapreferenceformovesthatyieldlowerenergyconﬁgurations.Whenstartingfromaratherimprobableconﬁguration(higherenergythanthetypicalonesfromp(x)),thechaintendstograduallyreducetheenergyofthestateandonlyoccasionallymovetoanothermode.Oncethechainhasfoundaregionoflowenergy(forexample,ifthevariablesarepixelsinanimage,aregionoflowenergymightbeaconnectedmanifoldofimagesofthesameobject),whichwecallamode,thechainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Onceinawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitﬁndsanescaperoute)movetowardsanothermode.Theproblemisthatsuccessfulescaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwillcontinuetosamplethesamemodelongerthanitshould.ThisisveryclearwhenweconsidertheGibbssamplingalgorithm(Sec.).17.4Inthiscontext,considertheprobabilityofgoingfromonemodetoanearbymodewithinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshapeofthe“energybarrier”betweenthesemodes.Transitionsbetweentwomodesthatareseparatedbyahighenergybarrier(aregionoflowprobability)areexponentiallylesslikely(intermsoftheheightoftheenergybarrier).ThisisillustratedinFig..Theproblemariseswhentherearemultiplemodeswith17.1highprobabilitythatareseparatedbyregionsoflowprobability,especiallywheneachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhosevaluesarelargelydeterminedbytheothervariables.Asasimpleexample,consideranenergy-basedmodelovertwovariablesaandb,whicharebothbinarywithasign,takingonvalues−1 1and.IfE(ab,) =−wabforsomelargepositivenumberw,thenthemodelexpressesastrongbeliefthataandbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwitha= 1. TheconditionaldistributionoverbisgivenbyP(b= 1|a= 1)=σ(w).Ifwislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe1iscloseto1.Likewise,ifa=−1,theprobabilityofassigningbtobe−1iscloseto1.AccordingtoPmodel(ab,),bothsignsofbothvariablesareequallylikely.AccordingtoPmodel(ab|),bothvariablesshouldhavethesamesign.ThismeansthatGibbssamplingwillonlyveryrarelyﬂipthesignsofthesevariables.602'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 617}, page_content='CHAPTER17.MONTECARLOMETHODS\\nFigure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkovchaininitializedatthemodeinbothcases.(Left)Amultivariatenormaldistributionwithtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesareindependent.Amultivariatenormaldistributionwithhighlycorrelatedvariables.(Center)ThecorrelationbetweenvariablesmakesitdiﬃcultfortheMarkovchaintomix.Becauseeachvariablemustbeupdatedconditionedontheother,thecorrelationreducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint.(Right)AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned.Gibbssamplingmixesveryslowlybecauseitisdiﬃculttochangemodeswhilealteringonlyonevariableatatime.Inmorepracticalscenarios,thechallengeisevengreaterbecausewecarenotonlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetweenallthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitionsarediﬃcultbecauseofthediﬃcultyofmixingbetweenmodes,thenitbecomesveryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,andconvergenceofthechaintoitsstationarydistributionisveryslow.Sometimesthisproblemcanberesolvedbyﬁndinggroupsofhighlydependentunitsandupdatingallofthemsimultaneouslyinablock. Unfortunately,whenthedependenciesarecomplicated,itcanbecomputationallyintractabletodrawasamplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginallyintroducedtosolveisthisproblemofsamplingfromalargegroupofvariables.Inthecontextofmodelswithlatentvariables,whichdeﬁneajointdistributionpmodel(xh,),weoftendrawsamplesofxbyalternatingbetweensamplingfrompmodel(xh|)andsamplingfrompmodel(hx|).Fromthepointofviewofmixingrapidly,wewouldlikepmodel(hx|)tohaveveryhighentropy.However,fromthepointofviewoflearningausefulrepresentationofh,wewouldlikehtoencode603'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 618}, page_content='CHAPTER17.MONTECARLOMETHODS\\nFigure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels.Eachpanelshouldbereadlefttoright,toptobottom.(Left)ConsecutivesamplesfromGibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset.Consecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformedinadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisualfeatures,butitisstilldiﬃcultfortheGibbschaintotransitionfromonemodeofthedistributiontoanother,forexamplebychangingthedigitidentity.Consecutive(Right)ancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsamplinggenerateseachsampleindependentlyfromtheothers,thereisnomixingproblem.enoughinformationaboutxtoreconstructitwell,whichimpliesthathandxshouldhaveveryhighmutualinformation.Thesetwogoalsareatoddswitheachother.Weoftenlearngenerativemodelsthatverypreciselyencodexintohbutarenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmannmachines—thesharperthedistributionaBoltzmannmachinelearns,theharderitisforaMarkovchainsamplingfromthemodeldistributiontomixwell.ThisproblemisillustratedinFig..17.2AllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinteresthasamanifoldstructurewithaseparatemanifoldforeachclass:thedistributionisconcentratedaroundmanymodesandthesemodesareseparatedbyvastregionsofhighenergy.ThistypeofdistributioniswhatweexpectinmanyclassiﬁcationproblemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoormixingbetweenmodes.17.5.1TemperingtoMixbetweenModesWhenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsoflowprobability,itisdiﬃculttomixbetweenthediﬀerentmodesofthedistribution.604'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 619}, page_content='CHAPTER17.MONTECARLOMETHODSSeveraltechniquesforfastermixingarebasedonconstructingalternativeversionsofthetargetdistributioninwhichthepeaksarenotashighandthesurroundingvalleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewaytodoso.Sofar,wehavedescribedanenergy-basedmodelasdeﬁningaprobabilitydistributionpE.() exp(x∝−())x(17.25)Energy-basedmodelsmaybeaugmentedwithanextraparameterβcontrollinghowsharplypeakedthedistributionis:pβ() exp(())x∝−βEx.(17.26)Theβparameterisoftendescribedasbeingthereciprocalofthetemperature,reﬂectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthetemperaturefallstozeroandrisestoinﬁnity,theenergy-basedmodelbecomesβdeterministic.Whenthetemperaturerisestoinﬁnityandβfallstozero,thedistribution(fordiscrete)becomesuniform.xTypically,amodelistrainedtobeevaluatedatβ= 1.However,wecanmakeuseofothertemperatures,particularlythosewhereβ<1.Temperingisageneralstrategyofmixingbetweenmodesofp1rapidlybydrawingsampleswith.β<1Markovchainsbasedontemperedtransitions(,)temporarilysampleNeal1994fromhigher-temperaturedistributionsinordertomixtodiﬀerentmodes,thenresumesamplingfromtheunittemperaturedistribution.ThesetechniqueshavebeenappliedtomodelssuchasRBMs(Salakhutdinov2010,).Anotherapproachistouseparalleltempering(,),inwhichtheMarkovchainsimulatesmanyIba2001diﬀerentstatesinparallel,atdiﬀerenttemperatures.Thehighesttemperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature,provide1accuratesamplesfromthemodel.Thetransitionoperatorincludesstochasticallyswappingstatesbetweentwodiﬀerenttemperaturelevels,sothatasuﬃcientlyhigh-probabilitysamplefromahigh-temperatureslotcanjumpintoalowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(,;Desjardinsetal.2010Choetal.,).Althoughtemperingisapromisingapproach,atthispointithasnot2010allowedresearcherstomakeastrongadvanceinsolvingthechallengeofsamplingfromcomplexEBMs.Onepossiblereasonisthattherearecriticaltemperaturesaroundwhichthetemperaturetransitionmustbeveryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobeeﬀective.17.5.2DepthMayHelpMixingWhendrawingsamplesfromalatentvariablemodelp(hx,),wehaveseenthatifp(hx|)encodesxtoowell,thensamplingfromp(xh|)willnotchangexvery605'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 620}, page_content='CHAPTER17.MONTECARLOMETHODSmuchandmixingwillbepoor.Onewaytoresolvethisproblemistomakehbeadeeprepresentation,thatencodesintoinsuchawaythataMarkovchaininxhthespaceofhcanmixmoreeasily.Manyrepresentationlearningalgorithms,suchasautoencodersandRBMs,tendtoyieldamarginaldistributionoverhthatismoreuniformandmoreunimodalthantheoriginaldatadistributionoverx.Itcanbearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusingalloftheavailablerepresentationspace,becauseminimizingreconstructionerroroverthetrainingexampleswillbebetterachievedwhendiﬀerenttrainingexamplesareeasilydistinguishablefromeachotherinh-space,andthuswellseparated.Bengio2013aetal.()observedthatdeeperstacksofregularizedautoencodersorRBMsyieldmarginaldistributionsinthetop-levelh-spacethatappearedmorespreadoutandmoreuniform,withlessofagapbetweentheregionscorrespondingtodiﬀerentmodes(categories,intheexperiments).TraininganRBMinthathigher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremainshoweverunclearhowtoexploitthisobservationtohelpbettertrainandsamplefromdeepgenerativemodels.Despitethediﬃcultyofmixing,MonteCarlotechniquesareusefulandareoftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfronttheintractablepartitionfunctionofundirectedmodels,discussednext.\\n606'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 621}, page_content='Chapter18ConfrontingthePartitionFunctionInSec.wesawthatmanyprobabilisticmodels(commonlyknownasundi-16.2.2rectedgraphicalmodels)aredeﬁnedbyanunnormalizedprobabilitydistribution˜p(x;θ).Wemustnormalize˜pbydividingbyapartitionfunctionZ(θ)inordertoobtainavalidprobabilitydistribution:p(;) =xθ1Z()θ˜p.(;)xθ(18.1)Thepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscretevariables)overtheunnormalizedprobabilityofallstates:\\ue05a˜pd()xx(18.2)or\\ue058x˜p.()x(18.3)Thisoperationisintractableformanyinterestingmodels.AswewillseeinChapter,severaldeeplearningmodelsaredesignedto20haveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdonotinvolvecomputingp(x)atall. However,othermodelsdirectlyconfrontthechallengeofintractablepartitionfunctions.Inthischapter,wedescribetechniquesusedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions.607'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 622}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION18.1TheLog-LikelihoodGradientWhat makes learning undirectedmodels by maximum likelihood particularlydiﬃcultisthatthepartitionfunctiondependsontheparameters.Thegradientofthelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothegradientofthepartitionfunction:∇θlog(;) = pxθ∇θlog ˜p(;)xθ−∇θlog()Zθ.(18.4)Thisisawell-knowndecompositionintothepositivephasenegativephaseandoflearning.Formostundirectedmodelsofinterest,thenegativephaseisdiﬃcult.Modelswithnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypicallyhaveatractablepositivephase.ThequintessentialexampleofamodelwithastraightforwardpositivephaseanddiﬃcultnegativephaseistheRBM,whichhashiddenunitsthatareconditionallyindependentfromeachothergiventhevisibleunits.Thecasewherethepositivephaseisdiﬃcult,withcomplicatedinteractionsbetweenlatentvariables,isprimarilycoveredinChapter.Thischapterfocuses19onthediﬃcultiesofthenegativephase.Letuslookmorecloselyatthegradientof:logZ∇θlogZ(18.5)=∇θZZ(18.6)=∇θ\\ue050x˜p()xZ(18.7)=\\ue050x∇θ˜p()xZ.(18.8)Formodelsthatguaranteep(x)>0forallx,wecansubstituteexp(log ˜p())xfor˜p()x:\\ue050x∇θexp(log ˜p())xZ(18.9)=\\ue050xexp(log ˜p())x∇θlog ˜p()xZ(18.10)=\\ue050x˜p()x∇θlog ˜p()xZ(18.11)=\\ue058xp()x∇θlog ˜p()x(18.12)608'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 623}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION= Exx∼p()∇θlog ˜p.()x(18.13)Thisderivationmadeuseofsummationoverdiscretex,butasimilarresultappliesusingintegrationovercontinuousx.Inthecontinuousversionofthederivation,weuseLeibniz’srulefordiﬀerentiationundertheintegralsigntoobtaintheidentity∇θ\\ue05a˜pd()xx=\\ue05a∇θ˜pd.()xx(18.14)Thisidentityisapplicableonlyundercertainregularityconditionson˜pand∇θ˜p(x).Inmeasuretheoreticterms,theconditionsare:(i)Theunnormalizeddistribution˜pmustbeaLebesgue-integrablefunctionofxforeveryvalueofθ;(ii)Thegradient∇θ˜p(x)mustexistforallθandalmostallx;(iii)TheremustexistanintegrablefunctionR(x)thatbounds∇θ˜p(x)inthesensethatmaxi|∂∂θi˜p(x)|≤R(x)forallθandalmostallx.Fortunately,mostmachinelearningmodelsofinteresthavetheseproperties.Thisidentity∇θlog= ZExx∼p()∇θlog ˜p()x(18.15)isthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizingthelikelihoodofmodelswithintractablepartitionfunctions.TheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitiveframeworkinwhichwecanthinkofboththepositivephaseandthenegativephase.Inthepositivephase,weincreaselog ˜p(x)forxdrawnfromthedata.Inthenegativephase,wedecreasethepartitionfunctionbydecreasinglog ˜p(x) drawnfromthemodeldistribution.Inthedeeplearningliterature,itiscommontoparametrizelog ˜pintermsofanenergyfunction(Eq. ).Inthiscase,wecaninterpretthepositivephase16.7aspushingdownontheenergyoftrainingexamplesandthenegativephaseaspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedinFig.18.1.18.2StochasticMaximumLikelihoodandContrastiveDivergenceThenaivewayofimplementingEq.istocomputeitbyburninginaset18.15ofMarkovchainsfromarandominitializationeverytimethegradientisneeded.Whenlearningisperformedusingstochasticgradientdescent,thismeansthechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe609'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 624}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONtrainingprocedurepresentedinAlgorithm.Thehighcostofburninginthe18.1Markovchainsintheinnerloopmakesthisprocedurecomputationallyinfeasible,butthisprocedureisthestartingpointthatothermorepracticalalgorithmsaimtoapproximate.Algorithm18.1AnaiveMCMCalgorithmformaximizingthelog-likelihoodwithanintractablepartitionfunctionusinggradientascent.Set,thestepsize,toasmallpositivenumber.\\ue00fSetk,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100totrainanRBMonasmallimagepatch.whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1m\\ue050mi=1∇θlog ˜p(x()i;)θ.Initializeasetofmsamples{˜x(1),...,˜x()m}torandomvalues(e.g.,fromauniformornormaldistribution,orpossiblyadistributionwithmarginalsmatchedtothemodel’smarginals).fordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1m\\ue050mi=1∇θlog ˜p(˜x()i;)θ.θθ←+\\ue00f.gendwhileWecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachievebalancebetweentwoforces,onepushinguponthemodeldistributionwherethedataoccurs,andanotherpushingdownonthemodeldistributionwherethemodelsamplesoccur.Fig.illustratesthisprocess.Thetwoforcescorrespondto18.1maximizinglog ˜pandminimizinglogZ.Severalapproximationstothenegativephasearepossible.Eachoftheseapproximationscanbeunderstoodasmakingthenegativephasecomputationallycheaperbutalsomakingitpushdowninthewronglocations.Becausethenegativephaseinvolvesdrawingsamplesfromthemodel’sdistri-bution,wecanthinkofitasﬁndingpointsthatthemodelbelievesinstrongly.Becausethenegativephaseactstoreducetheprobabilityofthosepoints,theyaregenerallyconsideredtorepresentthemodel’sincorrectbeliefsabouttheworld.Theyarefrequentlyreferredtointheliteratureas“hallucinations” or“fantasyparticles.”Infact,thenegativephasehasbeenproposedasapossibleexplanation610'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 625}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nxp(x)Thepositivephasepmodel()xpdata()x\\nxp(x)Thenegativephasepmodel()xpdata()x\\nFigure18.1:TheviewofAlgorithmashavinga“positivephase”and“negativephase.”18.1(Left)Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupontheirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpusheduponmore. (Right)Inthenegativephase,wesamplepointsfromthemodeldistribution,andpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase’stendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.Whenthedatadistributionandthemodeldistributionareequal,thepositivephasehasthesamechancetopushupatapointasthenegativephasehastopushdown.Whenthisoccurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate.fordreaminginhumansandotheranimals(,),theideaCrickandMitchison1983beingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollowsthegradientoflog ˜pwhileexperiencingrealeventswhileawakeandfollowsthenegativegradientoflog ˜ptominimizelogZwhilesleepingandexperiencingeventssampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedtodescribealgorithmswithapositiveandnegativephase,butithasnotbeenproventobecorrectwithneuroscientiﬁcexperiments.Inmachinelearningmodels,itisusuallynecessarytousethepositiveandnegativephasesimultaneously,ratherthaninseparatetimeperiodsofwakefulnessandREMsleep.AswewillseeinSec.,19.5othermachinelearningalgorithmsdrawsamplesfromthemodeldistributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccountforthefunctionofdreamsleep.Giventhisunderstandingoftheroleofthepositiveandnegativephaseoflearning,wecanattempttodesignalessexpensivealternativetoAlgorithm.18.1ThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkovchainsfromarandominitializationateachstep.AnaturalsolutionistoinitializetheMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,611'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 626}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONsothattheburninoperationdoesnottakeasmanysteps.Thecontrastivedivergence(CD,orCD-ktoindicateCDwithkGibbssteps)algorithminitializestheMarkovchainateachstepwithsamplesfromthedatadistribution(Hinton20002010,,).ThisapproachispresentedasAlgorithm.18.2Obtainingsamplesfromthedatadistributionisfree,becausetheyarealreadyavailableinthedataset.Initially,thedatadistributionisnotclosetothemodeldistribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositivephasecanstillaccuratelyincreasethemodel’sprobabilityofthedata.Afterthepositivephasehashadsometimetoact,themodeldistributionisclosertothedatadistribution,andthenegativephasestartstobecomeaccurate.Algorithm18.2Thecontrastivedivergencealgorithm,usinggradientascentastheoptimizationprocedure.Set,thestepsize,toasmallpositivenumber.\\ue00fSetk,thenumberofGibbssteps,highenoughtoallowaMarkovchainsamplingfromp(x;θ)tomixwheninitializedfrompdata.Perhaps1-20totrainanRBMonasmallimagepatch.whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1m\\ue050mi=1∇θlog ˜p(x()i;)θ.fordoim= 1to˜x()i←x()i.endforfordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1m\\ue050mi=1∇θlog ˜p(˜x()i;)θ.θθ←+\\ue00f.gendwhileOfcourse,CDisstillanapproximationtothecorrectnegativephase.ThemainwaythatCDqualitativelyfailstoimplementthecorrectnegativephaseisthatitfailstosuppressregionsofhighprobabilitythatarefarfromactualtrainingexamples.Theseregionsthathavehighprobabilityunderthemodelbutlowprobabilityunderthedatageneratingdistributionarecalledspuriousmodes.Fig.illustrateswhythishappens.Essentially,itisbecausemodesinthe18.2modeldistributionthatarefarfromthedatadistributionwillnotbevisitedby612'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 627}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nxp(x)Aspuriousmodepmodel()xpdata()x\\nFigure18.2:Anillustrationofhowthenegativephaseofcontrastivedivergence(Algorithm18.2)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatispresentinthemodeldistributionbutabsentinthedatadistribution.BecausecontrastivedivergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainforonlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedatapoints.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamplesthatdonotresemblethedata. Italsomeansthatduetowastingsomeofitsprobabilitymassonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrectmodes.Forthepurposeofvisualization,thisﬁgureusesasomewhatsimpliﬁedconceptofdistance—thespuriousmodeisfarfromthecorrectmodealongthenumberlineinR.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasinglexvariableinR.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbssamplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallofthevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheeditdistancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceinahighdimensionalspaceisdiﬃculttodepictina2-Dplot.Markovchainsinitializedattrainingpoints,unlessisverylarge.kCarreira-PerpiñanandHinton2005()showedexperimentallythattheCDestimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatitconvergestodiﬀerentpointsthanthemaximumlikelihoodestimator.Theyarguethatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitializeamodelthatcouldlaterbeﬁne-tunedviamoreexpensiveMCMCmethods.BengioandDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallesttermsofthecorrectMCMCupdategradient,whichexplainsthebias.CDisusefulfortrainingshallowmodelslikeRBMs.ThesecaninturnbestackedtoinitializedeepermodelslikeDBNsorDBMs. However,CDdoesnotprovidemuchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdiﬃcult613'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 628}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONtoobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethehiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannotsolvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstillneedtoburninaMarkovchainsamplingfromthedistributionoverthehiddenunitsconditionedonthosevisiblesamples.TheCDalgorithmcanbethoughtofaspenalizingthemodelforhavingaMarkovchainthatchangestheinputrapidlywhentheinputcomesfromthedata.ThismeanstrainingwithCDsomewhatresemblesautoencodertraining.EventhoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbeusefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecausetheearliestmodelsinthestackareencouragedtocopymoreinformationuptotheirlatentvariables,therebymakingitavailabletothelatermodels.Thisshouldbethoughtofmoreofasanoften-exploitablesideeﬀectofCDtrainingratherthanaprincipleddesignadvantage.SutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthegradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,butinpracticethisisnotaseriousproblem.AdiﬀerentstrategythatresolvesmanyoftheproblemswithCDistoinitializetheMarkovchainsateachgradientstepwiththeirstatesfromthepreviousgradientstep.Thisapproachwasﬁrstdiscoveredunderthenamestochasticmaximumlikelihood(SML)intheappliedmathematicsandstatisticscommunity(Younes,1998)andlaterindependentlyrediscoveredunderthenamepersistentcontrastivedivergence(PCD,orPCD-ktoindicatetheuseofkGibbsstepsperupdate)inthedeeplearningcommunity(,). SeeAlgorithm. ThebasicTieleman200818.3ideaofthisapproachisthat,solongasthestepstakenbythestochasticgradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilartothemodelfromthecurrentstep.Itfollowsthatthesamplesfromthepreviousmodel’sdistributionwillbeveryclosetobeingfairsamplesfromthecurrentmodel’sdistribution,soaMarkovchaininitializedwiththesesampleswillnotrequiremuchtimetomix.BecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearningprocess,ratherthanrestartedateachgradientstep,thechainsarefreetowanderfarenoughtoﬁndallofthemodel’smodes.SMListhusconsiderablymoreresistanttoformingmodelswithspuriousmodesthanCDis.Moreover,becauseitispossibletostorethestateofallofthesampledvariables,whethervisibleorlatent,SMLprovidesaninitializationpointforboththehiddenandvisibleunits.CDisonlyabletoprovideaninitializationforthevisibleunits,andthereforerequiresburn-infordeepmodels.SMLisabletotraindeepmodelseﬃciently.614'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 629}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONMarlin2010etal.()comparedSMLtomanyoftheothercriteriapresentedinthischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodforanRBM,andthatiftheRBM’shiddenunitsareusedasfeaturesforanSVMclassiﬁer,SMLresultsinthebestclassiﬁcationaccuracy.SMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithmcanmovethemodelfasterthantheMarkovchaincanmixbetweensteps.Thiscanhappenifkistoosmallor\\ue00fistoolarge.Thepermissiblerangeofvaluesisunfortunatelyhighlyproblem-dependent.Thereisnoknownwaytotestformallywhetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearningrateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeabletoobservethatthereismuchmorevarianceinthenegativephasesamplesacrossgradientstepsratherthanacrossdiﬀerentMarkovchains.Forexample,amodeltrainedonMNISTmightsampleexclusively7sononestep.Thelearningprocesswillthenpushdownstronglyonthemodecorrespondingto7s,andthemodelmightsampleexclusively9sonthenextstep.Algorithm18.3Thestochasticmaximumlikelihood/persistentcontrastivedivergencealgorithmusinggradientascentastheoptimizationprocedure.Set,thestepsize,toasmallpositivenumber.\\ue00fSetk,thenumberofGibbssteps,highenoughtoallowaMarkovchainsamplingfromp(x;θ+\\ue00fg)toburnin,startingfromsamplesfromp(x;θ).Perhaps1forRBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM.Initializeasetofmsamples{˜x(1),...,˜x()m}torandomvalues(e.g.,fromauniformornormaldistribution,orpossiblyadistributionwithmarginalsmatchedtothemodel’smarginals).whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1m\\ue050mi=1∇θlog ˜p(x()i;)θ.fordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1m\\ue050mi=1∇θlog ˜p(˜x()i;)θ.θθ←+\\ue00f.gendwhileCaremustbetakenwhenevaluatingthesamplesfromamodeltrainedwithSML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain615'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 630}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONinitializedfromarandomstartingpointafterthemodelisdonetraining.Thesamplespresentinthepersistentnegativechainsusedfortraininghavebeeninﬂuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodelappeartohavegreatercapacitythanitactuallydoes.BerglundandRaiko2013()performedexperimentstoexaminethebiasandvarianceintheestimateofthegradientprovidedbyCDandSML.CDprovestohavelowervariancethantheestimatorbasedonexactsampling.SMLhashighervariance.ThecauseofCD’slowvarianceisitsuseofthesametrainingpointsinboththepositiveandnegativephase.Ifthenegativephaseisinitializedfromdiﬀerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedonexactsampling.AllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodelcaninprinciplebeusedwithalmostanyvariantofMCMC.ThismeansthattechniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMCtechniquesdescribedinChapter,suchasparalleltempering(,17Desjardinsetal.2010Cho2010;etal.,).OneapproachtoacceleratingmixingduringlearningreliesnotonchangingtheMonteCarlosamplingtechnologybutratheronchangingtheparametrizationofthemodelandthecostfunction.FastPCDorFPCD(TielemanandHinton2009,)involvesreplacingtheparametersofatraditionalmodelwithanexpressionθθθ= ()slow+θ()fast.(18.16)Therearenowtwiceasmanyparametersasbefore,andtheyareaddedtogetherelement-wisetoprovidetheparametersusedbytheoriginalmodeldeﬁnition.Thefastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowingittoadaptrapidlyinresponsetothenegativephaseoflearningandpushtheMarkovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,thoughthiseﬀectonlyoccursduringlearningwhilethefastweightsarefreetochange.Typicallyonealsoappliessigniﬁcantweightdecaytothefastweights,encouragingthemtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslongenoughtoencouragetheMarkovchaintochangemodes.OnekeybeneﬁttotheMCMC-basedmethodsdescribedinthissectionisthattheyprovideanestimateofthegradientoflogZ,andthuswecanessentiallydecomposetheproblemintothelog ˜pcontributionandthelogZcontribution.Wecanthenuseanyothermethodtotacklelog ˜p(x),andjustaddournegativephasegradientontotheothermethod’sgradient.Inparticular,thismeansthatourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon˜p.MostoftheothermethodsofdealingwithlogZpresentedinthischapterare616'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 631}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONincompatiblewithbound-basedpositivephasemethods.18.3PseudolikelihoodMonteCarloapproximationstothepartitionfunctionanditsgradientdirectlyconfrontthepartitionfunction.Otherapproachessidesteptheissue,bytrainingthemodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesarebasedontheobservationthatitiseasytocomputeratiosofprobabilitiesinanundirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsinboththenumeratorandthedenominatoroftheratioandcancelsout:p()xp()y=1Z˜p()x1Z˜p()y=˜p()x˜p()y.(18.17)Thepseudolikelihoodisbasedontheobservationthatconditionalprobabilitiestakethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthepartitionfunction.Supposethatwepartitionxintoa,bandc,whereacontainsthevariableswewanttoﬁndtheconditionaldistributionover,bcontainsthevariableswewanttoconditionon,andccontainsthevariablesthatarenotpartofourquery.p() =ab|p,(ab)p()b=p,(ab)\\ue050ac,p,,(abc)=˜p,(ab)\\ue050ac,˜p,,(abc).(18.18)Thisquantityrequiresmarginalizingouta,whichcanbeaveryeﬃcientoperationprovidedthataandcdonotcontainverymanyvariables.Intheextremecase,acanbeasinglevariableandccanbeempty,makingthisoperationrequireonlyasmanyevaluationsof˜pastherearevaluesofasinglerandomvariable.Unfortunately,inordertocomputethelog-likelihood,weneedtomarginalizeoutlargesetsofvariables.Iftherearenvariablestotal,wemustmarginalizeasetofsize.Bythechainruleofprobability,n−1log() = log(pxpx1)+log(px2|x1)++(···pxn|x1:1n−).(18.19)Inthiscase,wehavemadeamaximallysmall,butccanbeaslargeasx2:n.Whatifwesimplymovecintobtoreducethecomputationalcost?Thisyieldsthepseudolikelihood(,)objectivefunction,basedonpredictingthevalueofBesag1975featurexigivenalloftheotherfeaturesx−i:n\\ue058i=1log(pxi|x−i).(18.20)617'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 632}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONIfeachrandomvariablehaskdiﬀerentvalues,thisrequiresonlykn×evaluationsof˜ptocompute,asopposedtotheknevaluationsneededtocomputethepartitionfunction.Thismaylooklikeanunprincipledhack,butitcanbeproventhatestimationbymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995Ofcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,pseudolikelihoodmaydisplaydiﬀerentbehaviorfromthemaximumlikelihoodestimator.Itispossibletotradecomputationalcomplexityfordeviationfrommaximumlikelihoodbehaviorbyusingthegeneralizedpseudolikelihoodestimator(HuangandOgata2002,).ThegeneralizedpseudolikelihoodestimatorusesmdiﬀerentsetsS()i,i= 1,...,mofindicesofvariablesthatappeartogetherontheleftsideoftheconditioningbar.Intheextremecaseofm= 1andS(1)=1,...,nthegeneralizedpseudolikelihoodrecoversthelog-likelihood.Intheextremecaseofm=nandS()i={}i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.Thegeneralizedpseudolikelihoodobjectivefunctionisgivenbym\\ue058i=1log(pxS()i|x−S()i).(18.21)Theperformanceofpseudolikelihood-basedapproachesdependslargelyonhowthemodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthatrequireagoodmodelofthefulljointp(x),suchasdensityestimationandsampling.However,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonlytheconditionaldistributionsusedduringtraining,suchasﬁllinginsmallamountsofmissingvalues.GeneralizedpseudolikelihoodtechniquesareespeciallypowerfulifthedatahasregularstructurethatallowstheSindexsetstobedesignedtocapturethemostimportantcorrelationswhileleavingoutgroupsofvariablesthatonlyhavenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidelyseparatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihoodcanbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow.SOneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwithotherapproximationsthatprovideonlyalowerboundon˜p(x),suchasvariationalinference,whichwillbecoveredinChapter.Thisisbecause19˜pappearsinthedenominator.Alowerboundonthedenominatorprovidesonlyanupperboundontheexpressionasawhole,andthereisnobeneﬁttomaximizinganupperbound.ThismakesitdiﬃculttoapplypseudolikelihoodapproachestodeepmodelssuchasdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominantapproachestoapproximatelymarginalizingoutthemanylayersofhiddenvariables618'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 633}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONthatinteractwitheachother. However,pseudolikelihoodisstillusefulfordeeplearning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusingapproximateinferencemethodsthatarenotbasedonlowerbounds.PseudolikelihoodhasamuchgreatercostpergradientstepthanSML,duetoitsexplicitcomputationofalloftheconditionals.However,generalizedpseudo-likelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselectedconditionaliscomputedperexample(,),therebybringingGoodfellowetal.2013bthecomputationalcostdowntomatchthatofSML.ThoughthepseudolikelihoodestimatordoesnotexplicitlyminimizelogZ,itcanstillbethoughtofashavingsomethingresemblinganegativephase.Thedenominatorsofeachconditionaldistributionresultinthelearningalgorithmsuppressingtheprobabilityofallstatesthathaveonlyonevariablediﬀeringfromatrainingexample.SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptoticeﬃciencyofpseudolikelihood.18.4ScoreMatchingandRatioMatchingScorematching(,)providesanotherconsistentmeansoftrainingaHyvärinen2005modelwithoutestimatingZoritsderivatives.Thenamescorematchingcomesfromterminologyinwhichthederivativesofalogdensitywithrespecttoitsargument,∇xlogp(x),arecalleditsscore.Thestrategyusedbyscorematchingistominimizetheexpectedsquareddiﬀerencebetweenthederivativesofthemodel’slogdensitywithrespecttotheinputandthederivativesofthedata’slogdensitywithrespecttotheinput:L,(xθ) =12||∇xlogpmodel(;)xθ−∇xlogpdata()x||22(18.22)J() =θ12Epdata()xL,(xθ)(18.23)θ∗= minθJ()θ(18.24)ThisobjectivefunctionavoidsthediﬃcultiesassociatedwithdiﬀerentiatingthepartitionfunctionZbecauseZisnotafunctionofxandtherefore∇xZ= 0.Initially,scorematchingappearstohaveanewdiﬃculty: computingthescoreofthedatadistributionrequiresknowledgeofthetruedistributiongeneratingthetrainingdata,pdata.Fortunately,minimizingtheexpectedvalueofisL,(xθ)619'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 634}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONequivalenttominimizingtheexpectedvalueof˜L,(xθ) =n\\ue058j=1\\ue020∂2∂x2jlogpmodel(;)+xθ12\\ue012∂∂xjlogpmodel(;)xθ\\ue0132\\ue021(18.25)whereisthedimensionalityof.nxBecausescorematchingrequirestakingderivativeswithrespecttox,itisnotapplicabletomodelsofdiscretedata.However,thelatentvariablesinthemodelmaybediscrete.Likethepseudolikelihood,scorematchingonlyworkswhenweareabletoevaluatelog ˜p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethodsthatonlyprovidealowerboundonlog ˜p(x),becausescorematchingrequiresthederivativesandsecondderivativesoflog ˜p(x)andalowerboundconveysnoinformationaboutitsderivatives.Thismeansthatscorematchingcannotbeappliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehiddenunits,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescorematchingcanbeusedtopretraintheﬁrsthiddenlayerofalargermodel,ithasnotbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel.Thisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsomediscretevariables.Whilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbeviewedasaversionofcontrastivedivergenceusingaspeciﬁckindofMarkovchain(,).TheMarkovchaininthiscaseisnotGibbssampling,butHyvärinen2007aratheradiﬀerentapproachthatmakeslocalmovesguidedbythegradient.ScorematchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthelocalmovesapproacheszero.Lyu2009()generalizedscorematchingtothediscretecase(butmadeanerrorintheirderivationthatwascorrectedby()).()Marlinetal.2010Marlinetal.2010foundthatgeneralizedscorematching(GSM)doesnotworkinhighdimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0.Amoresuccessfulapproachtoextendingthebasicideasofscorematchingtodiscretedataisratiomatching(,).RatiomatchingappliesHyvärinen2007bspeciﬁcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageoverexamplesofthefollowingobjectivefunction:L()RM() =xθ,n\\ue058j=1\\uf8eb\\uf8ed11+pmodel(;)xθpmodel(());)fx,jθ\\uf8f6\\uf8f82,(18.26)620'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 635}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONwherereturnswiththebitatpositionﬂipped.Ratiomatchingavoidsf,j(x)x jthepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:inaratiooftwoprobabilities,thepartitionfunctioncancelsout.()Marlinetal.2010foundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMintermsoftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages.Likethepseudolikelihoodestimator,ratiomatchingrequiresnevaluationsof˜pperdatapoint,makingitscomputationalcostperupdateroughlyntimeshigherthanthatofSML.Aswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofaspushingdownonallfantasystatesthathaveonlyonevariablediﬀerentfromatrainingexample.Sinceratiomatchingappliesspeciﬁcallytobinarydata,thismeansthatitactsonallfantasystateswithinHammingdistance1ofthedata.Ratiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensionalsparsedata,suchaswordcountvectors.ThiskindofdataposesachallengeforMCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentindenseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodelhaslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio()overcamethisissuebydesigninganunbiasedstochasticapproximationto2013ratiomatching.Theapproximationevaluatesonlyarandomlyselectedsubsetofthetermsoftheobjective,anddoesnotrequirethemodeltogeneratecompletefantasysamples.SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptoticeﬃciencyofratiomatching.18.5DenoisingScoreMatchingInsomecaseswemaywishtoregularizescorematching,byﬁttingadistributionpsmoothed() =x\\ue05apdata()()yqxy|dy(18.27)ratherthanthetruepdata.Thedistributionq(xy|) isacorruptionprocess,usuallyonethatformsbyaddingasmallamountofnoiseto.xyDenoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydonothaveaccesstothetruepdatabutratheronlyanempiricaldistributiondeﬁnedbysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,makepmodelintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothingbyqhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty621'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 636}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONdescribedinSec..()introducedaprocedurefor5.4.5KingmaandLeCun2010performingregularizedscorematchingwiththesmoothingdistributionqbeingnormallydistributednoise.Recall fromSec.thatseveralautoencoder training algorithms are14.5.1 equivalenttoscorematchingordenoisingscorematching.Theseautoencodertrainingalgorithmsaretherefore awayof overcomingthepartition functionproblem.18.6Noise-ContrastiveEstimationMosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonotprovideanestimateofthepartitionfunction.SMLandCDestimateonlythegradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself.Scorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothepartitionfunctionaltogether.Noise-contrastiveestimation(NCE)(GutmannandHyvarinen2010,)takesadiﬀerentstrategy.Inthisapproach,theprobabilitydistributionestimatedbythemodelisrepresentedexplicitlyaslogpmodel() = log ˜xpmodel(;)+xθc,(18.28)wherecisexplicitlyintroducedasanapproximationof−logZ(θ).Ratherthanestimatingonlyθ,thenoisecontrastiveestimationproceduretreatscasjustanotherparameterandestimatesθandcsimultaneously,usingthesamealgorithmforboth.Theresultinglogpmodel(x)thusmaynotcorrespondexactlytoavalidprobabilitydistribution,butwillbecomecloserandclosertobeingvalidastheestimateofimproves.c1Suchanapproachwouldnotbepossibleusingmaximumlikelihoodasthecriterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetosetccarbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution.NCEworksbyreducingtheunsupervisedlearningproblemofestimatingp(x)tothatoflearningaprobabilisticbinaryclassiﬁerinwhichoneofthecategoriescorrespondstothedatageneratedbythemodel.Thissupervisedlearningproblemisconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisnoneedtointroducetheextraparameterc.However,ithasgeneratedthemostinterestasameansofestimatingmodelswithdiﬃcultpartitionfunctions.622'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 637}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONlearningproblemdeﬁnesanasymptoticallyconsistentestimatoroftheoriginalproblem.Speciﬁcally,weintroduceaseconddistribution,thenoisedistributionpnoise(x).Thenoisedistributionshouldbetractabletoevaluateandtosamplefrom. Wecannowconstructamodeloverbothxandanew,binaryclassvariabley.Inthenewjointmodel,wespecifythatpjoint(= 1) =y12,(18.29)pjoint(= 1) = x|ypmodel()x,(18.30)andpjoint(= 0) = x|ypnoise()x.(18.31)Inotherwords,yisaswitchvariablethatdetermineswhetherwewillgeneratexfromthemodelorfromthenoisedistribution.Wecanconstructasimilarjointmodeloftrainingdata.Inthiscase,theswitchvariabledetermineswhetherwedrawxfromtheorfromthenoisedatadistribution.Formally,ptrain(y=1)=12,ptrain(x|y=1)=pdata(x), andptrain(= 0) = x|ypnoise()x.Wecannowjustusestandardmaximumlikelihoodlearningonthesupervisedlearningproblemofﬁttingpjointtoptrain:θ,c= argmaxθ,cEx,py∼trainlogpjoint()y|x.(18.32)Thedistributionpjointisessentiallyalogisticregressionmodelappliedtothediﬀerenceinlogprobabilitiesofthemodelandthenoisedistribution:pjoint(= 1 ) =y|xpmodel()xpmodel()+xpnoise()x(18.33)=11+pnoise()xpmodel()x(18.34)=11+exp\\ue010logpnoise()xpmodel()x\\ue011(18.35)= σ\\ue012−logpnoise()xpmodel()x\\ue013(18.36)= (logσpmodel()logx−pnoise())x.(18.37)623'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 638}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONNCEisthussimpletoapplysolongaslog ˜pmodeliseasytoback-propagatethrough,and,asspeciﬁedabove,pnoiseiseasytoevaluate(inordertoevaluatepjoint)andsamplefrom(inordertogeneratethetrainingdata).NCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,butcanworkwellevenifthoserandomvariablescantakeonahighnumberofvalues.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditionaldistributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyoneword.WhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomeslesseﬃcient.Thelogisticregressionclassiﬁercanrejectanoisesamplebyidentifyinganyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdowngreatlyafterpmodelhaslearnedthebasicmarginalstatistics.Imaginelearningamodelofimagesoffaces,usingunstructuredGaussiannoiseaspnoise.Ifpmodellearnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithouthavinglearnedanythingaboutotherfacialfeatures,suchasmouths.Theconstraintthatpnoisemustbeeasytoevaluateandeasytosamplefromcanbeoverlyrestrictive.Whenpnoiseissimple,mostsamplesarelikelytobetooobviouslydistinctfromthedatatoforcepmodeltoimprovenoticeably.Likescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalowerboundon˜pisavailable.Suchalowerboundcouldbeusedtoconstructalowerboundonpjoint(y= 1|x),butitcanonlybeusedtoconstructanupperboundonpjoint(y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise,alowerboundonpnoiseisnotuseful,becauseitprovidesonlyanupperboundonpjoint(= 1 )y|x.Whenthemodeldistributioniscopiedtodeﬁneanewnoisedistributionbeforeeachgradientstep,NCEdeﬁnesaprocedurecalledself-contrastiveestimation,whose expected gradientisequivalent tothe expected gradientofmaximumlikelihood(Goodfellow2014,).ThespecialcaseofNCEwherethenoisesamplesare thosegeneratedbythemodel suggests thatmaximumlikelihood can beinterpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguishrealityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachievessomereducedcomputationalcostbyonlyforcingthemodeltodistinguishrealityfromaﬁxedbaseline(thenoisemodel).Usingthesupervisedtaskofclassifyingbetweentrainingsamplesandgeneratedsamples(withthemodelenergyfunctionusedindeﬁningtheclassiﬁer)toprovideagradientonthemodelwasintroducedearlierinvariousforms(Wellingetal.,2003bBengio2009;,).624'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 639}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONNoisecontrastiveestimationisbasedontheideathatagoodgenerativemodelshouldbeabletodistinguishdatafromnoise.Acloselyrelatedideaisthatagoodgenerativemodelshouldbeabletogeneratesamplesthatnoclassiﬁercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks(Sec.20.10.4).18.7EstimatingthePartitionFunctionWhilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneedingtocomputetheintractablepartitionfunctionZ(θ)associatedwithanundirectedgraphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimatingthepartitionfunction.Estimatingthepartitionfunctioncanbeimportantbecausewerequireitifwewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantinevaluatingthemodel,monitoringtrainingperformance,andcomparingmodelstoeachother.Forexample,imaginewehavetwomodels:modelMAdeﬁningaprobabil-itydistributionpA(x;θA)=1ZA˜pA(x;θA)andmodelMBdeﬁningaprobabilitydistributionpB(x;θB)=1ZB˜pB(x;θB).Acommonwaytocomparethemodelsistoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d.testdataset.Supposethetestsetconsistsofmexamples{x(1),...,x()m}.If\\ue051ipA(x()i;θA) >\\ue051ipB(x()i;θB)orequivalentlyif\\ue058ilogpA(x()i;θA)−\\ue058ilogpB(x()i;θB) 0>,(18.38)thenwesaythatMAisabettermodelthanMB(or,atleast,itisabettermodelofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,testingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction.Unfortunately,Eq.seemstorequireevaluatingthelogprobabilitythat18.38themodelassignstoeachpoint,whichinturnrequiresevaluatingthepartitionfunction.Wecansimplifythesituationslightlybyre-arrangingEq.intoa18.38formwhereweneedtoknowonlytheratioofthetwomodel’spartitionfunctions:\\ue058ilogpA(x()i;θA)−\\ue058ilogpB(x()i;θB) =\\ue058i\\ue020log˜pA(x()i;θA)˜pB(x()i;θB)\\ue021−mlogZ(θA)Z(θB).(18.39)WecanthusdeterminewhetherMAisabettermodelthanMBwithoutknowingthepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,625'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 640}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONwecanestimatethisratiousingimportancesampling,providedthatthetwomodelsaresimilar.If,however,wewantedtocomputetheactualprobabilityofthetestdataundereitherMAorMB,wewouldneedtocomputetheactualvalueofthepartitionfunctions.Thatsaid,ifweknewtheratiooftwopartitionfunctions,r=Z(θB)Z(θA),andweknewtheactualvalueofjustoneofthetwo,sayZ(θA),wecouldcomputethevalueoftheother:Z(θB) = (rZθA) =Z(θB)Z(θA)Z(θA).(18.40)Asimple waytoestimatethepartition functionisto usea Monte Carlomethodsuchassimpleimportancesampling.Wepresenttheapproachintermsofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscretevariablesbyreplacingtheintegralswithsummation.Weuseaproposaldistributionp0(x)=1Z0˜p0(x)whichsupportstractablesamplingandtractableevaluationofboththepartitionfunctionZ0andtheunnormalizeddistribution˜p0()x.Z1=\\ue05a˜p1()xdx(18.41)=\\ue05ap0()xp0()x˜p1()xdx(18.42)= Z0\\ue05ap0()x˜p1()x˜p0()xdx(18.43)ˆZ1=Z0KK\\ue058k=1˜p1(x()k)˜p0(x()k)st:..x()k∼p0(18.44)Inthelastline,wemakeaMonteCarloestimator,ˆZ1,oftheintegralusingsamplesdrawnfromp0(x)andthenweighteachsamplewiththeratiooftheunnormalized˜p1andtheproposalp0.Weseealsothatthisapproachallowsustoestimatetheratiobetweenthepartitionfunctionsas1KK\\ue058k=1˜p1(x()k)˜p0(x()k)st:..x()k∼p0.(18.45)ThisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedinEq.18.39.Ifthedistributionp0isclosetop1,Eq.canbeaneﬀectivewayof18.44estimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime626'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 641}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONp1isbothcomplicated(usuallymultimodal)anddeﬁnedoverahighdimensionalspace.Itisdiﬃculttoﬁndatractablep0thatissimpleenoughtoevaluatewhilestillbeingcloseenoughtop1toresultinahighqualityapproximation.Ifp0andp1arenotclose,mostsamplesfromp0willhavelowprobabilityunderp1andthereforemake(relatively)negligiblecontributiontothesuminEq..18.44Havingfew sampleswith signiﬁcant weightsinthissumwillresultin anestimatorthatisofpoorqualityduetohighvariance.ThiscanbeunderstoodquantitativelythroughanestimateofthevarianceofourestimateˆZ1:ˆVar\\ue010ˆZ1\\ue011=Z0K2K\\ue058k=1\\ue020˜p1(x()k)˜p0(x()k)−ˆZ1\\ue0212.(18.46)Thisquantityislargestwhenthereissigniﬁcantdeviationinthevaluesoftheimportanceweights˜p1(x()k)˜p0(x()k).Wenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-ingtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-dimensionalspaces:annealedimportancesamplingandbridgesampling.Bothstartwiththesimpleimportancesamplingstrategyintroducedaboveandbothattempttoovercometheproblemoftheproposalp0beingtoofarfromp1byintroducingintermediatedistributionsthatattempttobetweenbridgethegapp0andp1.18.7.1AnnealedImportanceSamplingInsituationswhereDKL(p0\\ue06bp1)islarge(i.e.,wherethereislittleoverlapbetweenp0andp1),astrategycalledannealedimportancesampling(AIS)attemptstobridgethegapbyintroducingintermediatedistributions(,;,Jarzynski1997Neal2001).Considerasequenceofdistributionspη0,...,pηn,with0 =η0<η1<<···ηn−1<ηn= 1sothattheﬁrstandlastdistributionsinthesequencearep0andp1respectively.Thisapproachallowsustoestimatethepartitionfunctionofamultimodaldistributiondeﬁnedoverahigh-dimensionalspace(suchasthedistributiondeﬁnedbyatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction(suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwomodel’spartitionfunctions. Theestimateofthisratioisbasedontheestimateoftheratiosofasequenceofmanysimilardistributions,suchasthesequenceofRBMswithweightsinterpolatingbetweenzeroandthelearnedweights.627'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 642}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONWecannowwritetheratioZ1Z0asZ1Z0=Z1Z0Zη1Zη1···Zηn−1Zηn−1(18.47)=Zη1Z0Zη2Zη1···Zηn−1Zηn−2Z1Zηn−1(18.48)=n−1\\ue059j=0Zηj+1Zηj(18.49)Providedthedistributionspηjandpηj+1,forall0≤≤−jn1,aresuﬃcientlyclose,wecanreliablyestimateeachofthefactorsZηj+1ZηjusingsimpleimportancesamplingandthenusethesetoobtainanestimateofZ1Z0.Wheredotheseintermediatedistributionscomefrom?Justastheoriginalproposaldistributionp0isadesignchoice,soisthesequenceofdistributionspη1...pηn−1.Thatis,itcanbespeciﬁcallyconstructedtosuittheproblemdomain.Onegeneral-purposeandpopularchoicefortheintermediatedistributionsistousetheweightedgeometricaverageofthetargetdistributionp1andthestartingproposaldistribution(forwhichthepartitionfunctionisknown)p0:pηj∝pηj1p1−ηj0(18.50)Inordertosamplefromtheseintermediatedistributions,wedeﬁneaseriesofMarkovchaintransitionfunctionsTηj(x\\ue030|x) thatdeﬁnetheconditionalprobabilitydistributionoftransitioningtox\\ue030givenwearecurrentlyatx.ThetransitionoperatorTηj(x\\ue030|x)isdeﬁnedtoleavepηj()xinvariant:pηj() =x\\ue05apηj(x\\ue030)Tηj(xx|\\ue030)dx\\ue030(18.51)ThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod(e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepassesthroughalloftherandomvariablesorotherkindsofiterations.TheAISsamplingstrategyisthentogeneratesamplesfromp0andthenusethetransitionoperatorstosequentiallygeneratesamplesfromtheintermediatedistributionsuntilwearriveatsamplesfromthetargetdistributionp1:•fork...K= 1–Samplex()kη1∼p0()x628'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 643}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION–Samplex()kη2∼Tη1(x()kη2|x()kη1)–...–Samplex()kηn−1∼Tηn−2(x()kηn−1|x()kηn−2)–Samplex()kηn∼Tηn−1(x()kηn|x()kηn−1)•endForsamplek,wecanderivetheimportanceweightbychainingtogethertheimportanceweightsforthejumpsbetweentheintermediatedistributionsgiveninEq.:18.49w()k=˜pη1(x()kη1)˜p0(x()kη1)˜pη2(x()kη2)˜pη1(x()kη2)...˜p1(x()k1)˜pηn−1(x()kηn).(18.52)Toavoidnumericalissuessuchasoverﬂow,itisprobablybesttocomputelogw()kbyaddingandsubtractinglogprobabilities,ratherthancomputingw()kbymultiplyinganddividingprobabilities.WiththesamplingprocedurethusdeﬁnedandtheimportanceweightsgiveninEq.,theestimateoftheratioofpartitionfunctionsisgivenby:18.52Z1Z0≈1KK\\ue058k=1w()k(18.53)Inordertoverifythatthisproceduredeﬁnesavalidimportancesamplingscheme,wecanshow(,)thattheAISprocedurecorrespondstosimpleNeal2001importancesamplingonanextendedstatespacewithpointssampledovertheproductspace[xη1,...,xηn−1,x1].Todothis,wedeﬁnethedistributionovertheextendedspaceas:˜p(xη1,...,xηn−1,x1)(18.54)=˜p1(x1)˜Tηn−1(xηn−1|x1)˜Tηn−2(xηn−2|xηn−1)...˜Tη1(xη1|xη2),(18.55)where˜TaisthereverseofthetransitionoperatordeﬁnedbyTa(viaanapplicationofBayes’rule):˜Ta(x\\ue030|x) =pa(x\\ue030)pa()xTa(xx|\\ue030) =˜pa(x\\ue030)˜pa()xTa(xx|\\ue030).(18.56)PluggingtheaboveintotheexpressionforthejointdistributionontheextendedstatespacegiveninEq.,weget:18.55˜p(xη1,...,xηn−1,x1)(18.57)629'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 644}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION=˜p1(x1)˜pηn−1(xηn−1)˜pηn−1(x1)Tηn−1(x1|xηn−1)n−2\\ue059i=1˜pηi(xηi)˜pηi(xηi+1)Tηi(xηi+1|xηi)(18.58)=˜p1(x1)˜pηn−1(x1)Tηn−1(x1|xηn−1)˜pη1(xη1)n−2\\ue059i=1˜pηi+1(xηi+1)˜pηi(xηi+1)Tηi(xηi+1|xηi).(18.59)Wenowhavemeansofgeneratingsamplesfromthejointproposaldistributionqovertheextendedsampleviaasamplingschemegivenabove,withthejointdistributiongivenby:q(xη1,...,xηn−1,x1) = p0(xη1)Tη1(xη2|xη1)...Tηn−1(x1|xηn−1).(18.60)WehaveajointdistributionontheextendedspacegivenbyEq. .Taking18.59q(xη1,...,xηn−1,x1)astheproposaldistributionontheextendedstatespacefromwhichwewilldrawsamples,itremainstodeterminetheimportanceweights:w()k=˜p(xη1,...,xηn−1,x1)q(xη1,...,xηn−1,x1)=˜p1(x()k1)˜pηn−1(x()kηn−1)...˜pη2(x()kη2)˜p1(x()kη1)˜pη1(x()kη1)˜p0(x()k0).(18.61)TheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISassimpleimportancesamplingappliedtoanextendedstateanditsvalidityfollowsimmediatelyfromthevalidityofimportancesampling.Annealedimportancesampling(AIS)wasﬁrstdiscoveredby()Jarzynski1997andthenagain,independently,by().ItiscurrentlythemostcommonNeal2001wayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.Thereasonsforthismayhavemoretodowiththepublicationofaninﬂuentialpaper(SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthepartitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthanwithanyinherentadvantagethemethodhasovertheothermethoddescribedbelow.AdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceandeﬃciency)canbefoundin().Neal200118.7.2BridgeSamplingBridgesampling()isanothermethodthat,likeAIS,addressestheBennett1976shortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof630'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 645}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONintermediatedistributions,bridgesamplingreliesonasingledistributionp∗,knownasthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,p0,andadistributionp1forwhichwearetryingtoestimatethepartitionfunctionZ1.BridgesamplingestimatestheratioZ1/Z0astheratiooftheexpectedimpor-tanceweightsbetween˜p0and˜p∗andbetween˜p1and˜p∗:Z1Z0≈K\\ue058k=1˜p∗(x()k0)˜p0(x()k0)\\ue02cK\\ue058k=1˜p∗(x()k1)˜p1(x()k1)(18.62)Ifthebridgedistributionp∗ischosencarefullytohavealargeoverlapofsupportwithbothp0andp1,thenbridgesamplingcanallowthedistancebetweentwodistributions(ormoreformally,DKL(p0\\ue06bp1))tobemuchlargerthanwithstandardimportancesampling.Itcanbeshownthattheoptimalbridgingdistributionisgivenbyp()opt∗(x)∝˜p0()˜xp1()xr˜p0()+˜xp1()xwherer=Z1/Z0.Atﬁrst,thisappearstobeanunworkablesolutionasitwouldseemtorequiretheveryquantitywearetryingtoestimate,Z1/Z0.However,itispossibletostartwithacoarseestimateofrandusetheresultingbridgedistributiontoreﬁneourestimateiteratively(,).Thatis,weNeal2005iterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof.rLinkedimportancesamplingBothAISandbridgesamplinghavetheirad-vantages.IfDKL(p0\\ue06bp1)isnottoolarge(becausep0andp1aresuﬃcientlyclose)bridgesamplingcanbeamoreeﬀectivemeansofestimatingtheratioofpartitionfunctionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingledistributionp∗tobridgethegapthenonecanatleastuseAISwithpotentiallymanyintermediatedistributionstospanthedistancebetweenp0andp1.Neal()showedhowhislinkedimportancesamplingmethodleveragedthepowerof2005thebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIStosigniﬁcantlyimprovetheoverallpartitionfunctionestimates.EstimatingthepartitionfunctionwhiletrainingWhileAIShasbecomeacceptedasthestandardmethodforestimatingthepartitionfunctionformanyundirectedmodels,itissuﬃcientlycomputationallyintensivethatitremainsinfeasibletouseduringtraining.However,alternativestrategiesthathavebeenexploredtomaintainanestimateofthepartitionfunctionthroughouttrainingUsingacombinationofbridgesampling,short-chainAISandparalleltempering,Desjardins2011etal.()devisedaschemetotrackthepartitionfunctionofan631'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 646}, page_content='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONRBMthroughoutthetrainingprocess.ThestrategyisbasedonthemaintenanceofindependentestimatesofthepartitionfunctionsoftheRBMateverytemperatureoperatingintheparalleltemperingscheme.Theauthorscombinedbridgesamplingestimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.fromparalleltempering)withAISestimatesacrosstimetocomeupwithalowvarianceestimateofthepartitionfunctionsateveryiterationoflearning.Thetoolsdescribedinthischapterprovidemanydiﬀerentwaysofovercomingtheproblemofintractablepartitionfunctions,buttherecanbeseveralotherdiﬃcultiesinvolvedintrainingandusinggenerativemodels.Foremostamongtheseistheproblemofintractableinference,whichweconfrontnext.\\n632'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 647}, page_content='Chapter19ApproximateInferenceManyprobabilisticmodelsarediﬃculttotrainbecauseitisdiﬃculttoperforminferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisiblevariablesvandasetoflatentvariablesh.Thechallengeofinferenceusuallyreferstothediﬃcultproblemofcomputingp(hv|)ortakingexpectationswithrespecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihoodlearning.Manysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestrictedBoltzmannmachinesandprobabilisticPCA,aredeﬁnedinawaythatmakesinferenceoperationslikecomputingp(hv|),ortakingexpectationswithrespecttoit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhiddenvariableshaveintractableposteriordistributions.Exactinferencerequiresanexponentialamountoftimeinthesemodels.Evensomemodelswithonlyasinglelayer,suchassparsecoding,havethisproblem.Inthischapter,weintroduceseveralofthetechniquesforconfrontingtheseintractableinferenceproblems.Later,inChapter,wewilldescribehowtouse20thesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,suchasdeepbeliefnetworksanddeepBoltzmannmachines.Intractableinferenceproblemsindeeplearningusuallyarisefrominteractionsbetweenlatentvariablesinastructuredgraphicalmodel.SeeFig.forsome19.1examples.Theseinteractionsmaybeduetodirectinteractionsinundirectedmodelsor“explainingaway”interactionsbetweenmutualancestorsofthesamevisibleunitindirectedmodels.633'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 648}, page_content='CHAPTER19.APPROXIMATEINFERENCE\\nFigure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultofinteractionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbeduetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpathsthatareactivatedwhenthechildofaV-structureisobserved.(Left)semi-restrictedBoltzmannAmachine(,)withconnectionsbetweenhiddenunits.TheseOsinderoandHinton2008directconnectionsbetweenlatentvariablesmaketheposteriordistributionintractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine,organized(Center)intolayersofvariableswithoutintra-layerconnections,stillhasanintractableposteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodelhas(Right)interactionsbetweenlatentvariableswhenthevisiblevariablesareobserved,becauseeverytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovidetractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructuresdepictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosentointroduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample,probabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinferenceduetospecialpropertiesofthespeciﬁcconditionaldistributionsituses(linear-Gaussianconditionalswithmutuallyorthogonalbasisvectors).\\n634'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 649}, page_content='CHAPTER19.APPROXIMATEINFERENCE19.1InferenceasOptimizationManyapproachestoconfrontingtheproblemofdiﬃcultinferencemakeuseoftheobservationthatexactinferencecanbedescribedasanoptimizationproblem.Approximateinferencealgorithmsmaythenbederivedbyapproximatingtheunderlyingoptimizationproblem.Toconstructtheoptimizationproblem,assumewehaveaprobabilisticmodelconsistingofobservedvariablesvandlatentvariablesh.Wewouldliketocomputethelogprobabilityoftheobserveddata,logp(v;θ).Sometimesitistoodiﬃculttocomputelogp(v;θ)ifitiscostlytomarginalizeouth.Instead,wecancomputealowerboundL(vθ,,q)onlogp(v;θ). Thisboundiscalledtheevidencelowerbound(ELBO).Anothercommonlyusednameforthislowerboundisthenegativevariationalfreeenergy.Speciﬁcally,theevidencelowerboundisdeﬁnedtobeL−() = log(;)vθ,,qpvθDKL(()(;))qhv|\\ue06bphv|θ(19.1)whereisanarbitraryprobabilitydistributionover.qhBecausethediﬀerencebetweenlogp(v)andL(vθ,,q)isgivenbytheKLdivergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethatLalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoareequalifandonlyifisthesamedistributionas.qp()hv|Surprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributionsq.SimplealgebrashowsthatwecanrearrangeLintoamuchmoreconvenientform:L−() =log(;)vθ,,qpvθDKL(()(;))qhv|\\ue06bphv|θ(19.2)=log(;)pvθ−Eh∼qlogq()hv|p()hv|(19.3)=log(;)pvθ−Eh∼qlogq()hv|p,(hvθ;)p(;)vθ(19.4)=log(;)pvθ−Eh∼q[log()log(;)+log(;)]qhv|−phv,θpvθ(19.5)=−Eh∼q[log()log(;)]qhv|−phv,θ.(19.6)Thisyieldsthemorecanonicaldeﬁnitionoftheevidencelowerbound,L() = vθ,,qEh∼q[log()]+()phv,Hq.(19.7)Foranappropriatechoiceofq,Listractabletocompute.Foranychoiceofq,Lprovidesalowerboundonthelikelihood.Forq(hv|)thatarebetter635'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 650}, page_content='CHAPTER19.APPROXIMATEINFERENCEapproximationsofp(hv|),thelowerboundLwillbetighter,inotherwords,closertologp(v). Whenq(hv|)=p(hv|),theapproximationisperfect,andL() = log(;)vθ,,qpvθ.WecanthusthinkofinferenceastheprocedureforﬁndingtheqthatmaximizesL.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctionsqthatincludesp(hv|).Throughoutthischapter,wewillshowhowtoderivediﬀerentformsofapproximateinferencebyusingapproximateoptimizationtoﬁndq.Wecanmaketheoptimizationprocedurelessexpensivebutapproximatebyrestrictingthefamilyofdistributionsqtheoptimizationisallowedtosearchoverorbyusinganimperfectoptimizationprocedurethatmaynotcompletelymaximizebutmerelyincreaseitbyasigniﬁcantamount.LNomatterwhatchoiceofqweuse,Lisalowerbound.Wecangettighterorlooserboundsthatarecheaperormoreexpensivetocomputedependingonhowwechoosetoapproachthisoptimizationproblem. Wecanobtainapoorlymatchedqbutreducethecomputationalcostbyusinganimperfectoptimizationprocedure,orbyusingaperfectoptimizationprocedureoverarestrictedfamilyofqdistributions.19.2ExpectationMaximizationTheﬁrstalgorithmweintroducebasedonmaximizingalowerboundListheexpectationmaximization(EM)algorithm,apopulartrainingalgorithmformodelswithlatentvariables.WedescribehereaviewontheEMalgorithmdevelopedbyNealandHinton1999().Unlikemostoftheotheralgorithmswedescribeinthischapter,EMisnotanapproachtoapproximateinference,butratheranapproachtolearningwithanapproximateposterior.TheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:•The(Expectationstep):LetE-stepθ(0)denotethevalueoftheparametersatthebeginningofthestep.Setq(h()i|v) =p(h()i|v()i;θ(0)) forallindicesiofthetrainingexamplesv()iwewanttotrainon(bothbatchandminibatchvariantsarevalid). Bythiswemeanqisdeﬁnedintermsofthecurrentparametervalueofθ(0);ifwevaryθthenp(hv|;θ)willchangebutq(hv|)willremainequaltop(;hv|θ(0)).•TheM-step(Maximizationstep):Completelyorpartiallymaximize\\ue058iL(v()i,,qθ)(19.8)636'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 651}, page_content='CHAPTER19.APPROXIMATEINFERENCEwithrespecttousingyouroptimizationalgorithmofchoice.θThiscanbeviewedasacoordinateascentalgorithmtomaximizeL.Ononestep,wemaximizeLwithrespecttoq,andontheother,wemaximizeLwithrespectto.θStochasticgradientascentonlatentvariablemodelscanbeseenasaspecialcaseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradientstep.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsomemodelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthewaytotheoptimalsolutionforgiventhecurrent.θqEventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEMalgorithmasusingapproximateinferenceinsomesense.Speciﬁcally,theM-stepassumesthatthesamevalueofqcanbeusedforallvaluesofθ.ThiswillintroduceagapbetweenLandthetruelogp(v)astheM-stepmovesfurtherandfurtherawayfromthevalueθ(0)usedintheE-step.Fortunately,theE-stepreducesthegaptozeroagainasweentertheloopforthenexttime.TheEMalgorithmcontainsafewdiﬀerentinsights.First,thereisthebasicstructureofthelearningprocess,inwhichweupdatethemodelparameterstoimprovethelikelihoodofacompleteddataset,whereallmissingvariableshavetheirvaluesprovidedbyanestimateoftheposteriordistribution.ThisparticularinsightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescenttomaximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradientcomputationsrequiretakingexpectationswithrespecttotheposteriordistributionoverthehiddenunits. AnotherkeyinsightintheEMalgorithmisthatwecancontinuetouseonevalueofqevenafterwehavemovedtoadiﬀerentvalueofθ.ThisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelargeM-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplextoadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecondinsightwhichismoreuniquetotheEMalgorithmisrarelyused.19.3MAPInferenceandSparseCodingWeusuallyusetheterminferencetorefertocomputingtheprobabilitydistributionoveronesetofvariablesgivenanother.Whentrainingprobabilisticmodelswithlatentvariables,weareusuallyinterestedincomputingp(hv|).Analternativeformofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,ratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext637'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 652}, page_content='CHAPTER19.APPROXIMATEINFERENCEoflatentvariablemodels,thismeanscomputingh∗= argmaxhp.()hv|(19.9)Thisisknownasmaximumaposterioriinference,abbreviatedMAPinference.MAPinferenceisusuallynotthoughtofasapproximateinference—itdoescomputetheexactmostlikelyvalueofh∗.However,ifwewishtodevelopalearningprocessbasedonmaximizingL(vh,,q),thenitishelpfultothinkofMAPinferenceasaprocedurethatprovidesavalueofq.Inthissense,wecanthinkofMAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimalq.RecallfromSec.thatexactinferenceconsistsofmaximizing19.1L() = vθ,,qEh∼q[log()]+()phv,Hq(19.10)withrespecttoqoveranunrestrictedfamilyofprobabilitydistributions,usinganexactoptimizationalgorithm.WecanderiveMAPinferenceasaformofapproximateinferencebyrestrictingthefamilyofdistributionsqmaybedrawnfrom.Speciﬁcally,werequiretotakeonaDiracdistribution:qqδ.() = hv|()hµ−(19.11)Thismeansthatwecannowcontrolqentirelyviaµ.DroppingtermsofLthatdonotvarywith,weareleftwiththeoptimizationproblemµµ∗= argmaxµlog(= )phµv,,(19.12)whichisequivalenttotheMAPinferenceproblemh∗= argmaxhp.()hv|(19.13)WecanthusjustifyalearningproceduresimilartoEM,inwhichwealternatebetweenperformingMAPinferencetoinferh∗andthenupdateθtoincreaselogp(h∗,v).AswithEM,thisisaformofcoordinateascentonL,wherewealternatebetweenusing inferenceto optimizeLwithrespect toqandusingparameterupdatestooptimizeLwithrespecttoθ.TheprocedureasawholecanbejustiﬁedbythefactthatLisalowerboundonlogp(v).InthecaseofMAPinference,thisjustiﬁcationisrathervacuous,becausetheboundisinﬁnitelyloose,duetotheDiracdistribution’sdiﬀerentialentropyofnegativeinﬁnity.However,addingnoisetowouldmaketheboundmeaningfulagain.µ638'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 653}, page_content='CHAPTER19.APPROXIMATEINFERENCEMAPinferenceiscommonlyusedindeeplearningasbothafeatureextractorandalearningmechanism.Itisprimarilyusedforsparsecodingmodels.RecallfromSec.thatsparsecodingisalinearfactormodelthatimposesa13.4sparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplaceprior,withph(i) =λ2e−|λhi|.(19.14)Thevisibleunitsarethengeneratedbyperformingalineartransformationandaddingnoise:p,β() = (;+xh|NvWhb−1I).(19.15)Computingorevenrepresentingp(hv|)isdiﬃcult.Everypairofvariableshiandhjarebothparentsofv.Thismeansthatwhenvisobserved,thegraphicalmodelcontainsanactivepathconnectinghiandhj.Allofthehiddenunitsthusparticipateinonemassivecliqueinp(hv|).IfthemodelwereGaussianthentheseinteractionscouldbemodeledeﬃcientlyviathecovariancematrix,butthesparsepriormakestheseinteractionsnon-Gaussian.Becausep(hv|)isintractable,soisthecomputationofthelog-likelihoodanditsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,weuseMAPinferenceandlearntheparametersbymaximizingtheELBOdeﬁnedbytheDiracdistributionaroundtheMAPestimateof.hIfweconcatenateallofthehvectorsinthetrainingsetintoamatrixH,andconcatenateallofthevectorsintoamatrix,thenthesparsecodinglearningvVprocessconsistsofminimizingJ,(HW) =\\ue058i,j|Hi,j|+\\ue058i,j\\ue010VHW−\\ue03e\\ue0112i,j.(19.16)MostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inordertopreventthepathologicalsolutionwithextremelysmallandlarge.HWWecanminimizeJbyalternatingbetweenminimizationwithrespecttoHandminimizationwithrespecttoW.Bothsub-problemsareconvex.Infact,theminimizationwithrespecttoWisjustalinearregressionproblem.However,minimizationofJwithrespecttobothargumentsisusuallynotaconvexproblem.MinimizationwithrespecttoHrequiresspecializedalgorithmssuchasthefeature-signsearchalgorithm(,).Leeetal.2007639'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 654}, page_content='CHAPTER19.APPROXIMATEINFERENCE19.4VariationalInferenceandLearningWe have seen how the evidence lower boundL(vθ,,q)is a lower bound onlogp(v;θ),howinferencecanbeviewedasmaximizingLwithrespecttoq,andhowlearningcanbeviewedasmaximizingLwithrespecttoθ.WehaveseenthattheEMalgorithmallowsustomakelargelearningstepswithaﬁxedqandthatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapointestimateofp(hv|)ratherthaninferringtheentiredistribution.Nowwedevelopthemoregeneralapproachtovariationallearning.ThecoreideabehindvariationallearningisthatwecanmaximizeLoverarestrictedfamilyofdistributionsq.ThisfamilyshouldbechosensothatitiseasytocomputeEqlogp(hv,). Atypicalwaytodothisistointroduceassumptionsabouthowfactorizes.qAcommonapproachtovariationallearningistoimposetherestrictionthatqisafactorialdistribution:q() =hv|\\ue059iqh(i|v).(19.17)Thisiscalledthemeanﬁeldapproach.Moregenerally,wecanimposeanygraphicalmodelstructurewechooseonq,toﬂexiblydeterminehowmanyinteractionswewantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproachiscalledstructuredvariationalinference(,).SaulandJordan1996Thebeautyofthevariationalapproachisthatwedonotneedtospecifyaspeciﬁcparametricformforq.Wespecifyhowitshouldfactorize,butthentheoptimizationproblemdeterminestheoptimalprobabilitydistributionwithinthosefactorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatweusetraditionaloptimizationtechniquestooptimizeaﬁnitenumberofvariablesdescribingtheqdistribution.Forcontinuouslatentvariables,thismeansthatweuseabranchofmathematicscalledcalculusofvariationstoperformoptimizationoveraspaceoffunctions,andactuallydeterminewhichfunctionshouldbeusedtorepresentq.Calculusof variations istheorigin ofthenames “variationallearning”and“variationalinference,”thoughthesenamesapplyevenwhenthelatentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecaseofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethatremovesmuchoftheresponsibilityfromthehumandesignerofthemodel,whonowmustspecifyonlyhowqfactorizes,ratherthanneedingtoguesshowtodesignaspeciﬁcthatcanaccuratelyapproximatetheposterior.qBecauseL(vθ,,q)isdeﬁnedtobelogp(v;θ)−DKL(q(hv|)\\ue06bp(hv|;θ)),wecanthinkofmaximizingLwithrespecttoqasminimizingDKL(q(hv|)\\ue06bp(hv|)).640'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 655}, page_content='CHAPTER19.APPROXIMATEINFERENCEInthissense,weareﬁttingqtop. However,wearedoingsowiththeoppositedirectionoftheKLdivergencethanweareusedtousingforﬁttinganapproximation.Whenweusemaximumlikelihoodlearningtoﬁtamodeltodata,weminimizeDKL(pdata\\ue06bpmodel).AsillustratedinFig.,thismeansthatmaximumlikelihood3.6encouragesthemodeltohavehighprobabilityeverywherethatthedatahashighprobability, whileouroptimization-basedinferenceprocedureencouragesqtohavelowprobabilityeverywherethetrueposteriorhaslowprobability.BothdirectionsoftheKLdivergencecanhavedesirableandundesirableproperties.Thechoiceofwhichtousedependsonwhichpropertiesarethehighestpriorityforeachapplication.Inthecaseoftheinferenceoptimizationproblem,wechoosetouseDKL(q(hv|)\\ue06bp(hv|))forcomputationalreasons.Speciﬁcally,computingDKL(q(hv|)\\ue06bp(hv|))involvesevaluatingexpectationswithrespecttoq,sobydesigningqtobesimple,wecansimplifytherequiredexpectations.TheoppositedirectionoftheKLdivergencewouldrequirecomputingexpectationswithrespecttothetrueposterior.Becausetheformofthetrueposteriorisdeterminedbythechoiceofmodel,wecannotdesignareduced-costapproachtocomputingDKL(()())phv|\\ue06bqhv|exactly.19.4.1DiscreteLatentVariablesVariationalinferencewithdiscretelatentvariablesisrelativelystraightforward.Wedeﬁneadistributionq,typicallyonewhereeachfactorofqisjustdeﬁnedbyalookuptableoverdiscretestates. Inthesimplestcase,hisbinaryandwemakethemeanﬁeldassumptionthatfactorizesovereachindividualqhi.Inthiscasewecanparametrizeqwithavectorˆhwhoseentriesareprobabilities.Thenqh(i= 1 ) =|vˆhi.Afterdetermininghowtorepresentq,wesimplyoptimizeitsparameters.Inthecaseofdiscretelatentvariables,thisisjustastandardoptimizationproblem.Inprincipletheselectionofqcouldbedonewithanyoptimizationalgorithm,suchasgradientdescent.Becausethisoptimizationmustoccurintheinnerloopofalearningalgorithm,itmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimizationalgorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsinveryfewiterations.Apopularchoiceistoiterateﬁxedpointequations,inotherwords,tosolve∂∂ˆhiL= 0(19.18)forˆhi.Werepeatedlyupdatediﬀerentelementsofˆhuntilwesatisfyaconvergence641'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 656}, page_content='CHAPTER19.APPROXIMATEINFERENCEcriterion.Tomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothebinarysparsecodingmodel(wepresentherethemodeldevelopedbyHennigesetal.()butdemonstratetraditional,genericmeanﬁeldappliedtothemodel,while2010theyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderablemathematicaldetailandisintendedforthereaderwhowishestofullyresolveanyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceandlearningwehavepresentedsofar.Readerswhodonotplantoderiveorimplementvariationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissinganynewhigh-levelconcepts.ReaderswhoproceedwiththebinarysparsecodingexampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthatcommonlyariseinprobabilisticmodelsinSec..Weusetheseproperties3.10liberallythroughoutthefollowingderivationswithouthighlightingexactlywhereweuseeachone.Inthebinarysparsecodingmodel,theinputv∈RnisgeneratedfromthemodelbyaddingGaussiannoisetothesumofmdiﬀerentcomponentswhichcaneachbepresentorabsent.Eachcomponentisswitchedonoroﬀbythecorrespondinghiddenunitinh∈{}01,m:ph(i= 1) = (σbi)(19.19)p,() = (;vh|NvWhβ−1)(19.20)wherebisalearnablesetofbiases,Wisalearnableweightmatrix,andβisalearnable,diagonalprecisionmatrix.Trainingthismodelwithmaximumlikelihoodrequirestakingthederivativewithrespecttotheparameters.Considerthederivativewithrespecttooneofthebiases:∂∂bilog()pv(19.21)=∂∂bip()vp()v(19.22)=∂∂bi\\ue050hp,(hv)p()v(19.23)=∂∂bi\\ue050hpp()h()vh|p()v(19.24)642'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 657}, page_content='CHAPTER19.APPROXIMATEINFERENCEh1h1h2h2h3h3v1v1v2v2v3v3h4h4h1h1h2h2h3h3h4h4Figure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits.(Left)Thegraphstructureofp(hv,).Notethattheedgesaredirected,andthateverytwohiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof(Right)p(hv|).Inordertoaccountfortheactivepathsbetweenco-parents,theposteriordistributionneedsanedgebetweenallofthehiddenunits.=\\ue050hp()vh|∂∂bip()hp()v(19.25)=\\ue058hp()hv|∂∂bip()hp()h(19.26)=Eh∼|p(hv)∂∂bilog()ph.(19.27)Thisrequirescomputingexpectationswithrespecttop(hv|).Unfortunately,p(hv|)isacomplicateddistribution.SeeFig.forthegraphstructureof19.2p(hv,)andp(hv|).Theposteriordistributioncorrespondstothecompletegraphoverthehiddenunits,sovariableeliminationalgorithmsdonothelpustocomputetherequiredexpectationsanyfasterthanbruteforce.Wecanresolvethisdiﬃcultybyusingvariationalinferenceandvariationallearninginstead.Wecanmakeameanﬁeldapproximation:q() =hv|\\ue059iqh(i|v).(19.28)Thelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresentafactorialqwesimplyneedtomodelmBernoullidistributionsq(hi|v).AnaturalwaytorepresentthemeansoftheBernoullidistributionsiswithavectorˆhofprobabilities,withq(hi=1|v)=ˆhi.Weimposearestrictionthatˆhiisneverequalto0orto1,inordertoavoiderrorswhencomputing,forexample,logˆhi.Wewillseethatthevariationalinferenceequationsneverassignorto01ˆhi643'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 658}, page_content='CHAPTER19.APPROXIMATEINFERENCEanalytically.However,inasoftwareimplementation,machineroundingerrorcouldresultinorvalues.Insoftware,wemaywishtoimplementbinarysparse01codingusinganunrestrictedvectorofvariationalparameterszandobtainˆhviatherelationˆh=σ(z).Wecanthussafelycomputelogˆhionacomputerbyusingtheidentitylog(σzi) = (−ζ−zi)relatingthesigmoidandthesoftplus.Tobeginourderivationofvariationallearninginthebinarysparsecodingmodel,weshowthattheuseofthismeanﬁeldapproximationmakeslearningtractable.TheevidencelowerboundisgivenbyL()vθ,,q(19.29)=Eh∼q[log()]+()phv,Hq(19.30)=Eh∼q[log()+log()log()]phpvh|−qhv|(19.31)=Eh∼q\\ue022m\\ue058i=1log(phi)+n\\ue058i=1log(pvi|−h)m\\ue058i=1log(qhi|v)\\ue023(19.32)=m\\ue058i=1\\ue068ˆhi(log(σbi)log−ˆhi)+(1−ˆhi)(log(σ−bi)log(1−−ˆhi))\\ue069(19.33)+Eh∼q\\ue022n\\ue058i=1log\\ue072βi2πexp\\ue012−βi2(vi−Wi,:h)2\\ue013\\ue023(19.34)=m\\ue058i=1\\ue068ˆhi(log(σbi)log−ˆhi)+(1−ˆhi)(log(σ−bi)log(1−−ˆhi))\\ue069(19.35)+12n\\ue058i=1\\uf8ee\\uf8f0logβi2π−βi\\uf8eb\\uf8edv2i−2viWi,:ˆh+\\ue058j\\uf8ee\\uf8f0W2i,jˆhj+\\ue058kj\\ue036=Wi,jWi,kˆhjˆhk\\uf8f9\\uf8fb\\uf8f6\\uf8f8\\uf8f9\\uf8fb.(19.36)Whiletheseequationsaresomewhatunappealingaesthetically,theyshowthatLcanbeexpressedinasmallnumberofsimplearithmeticoperations.TheevidencelowerboundListhereforetractable.WecanuseLasareplacementfortheintractablelog-likelihood.Inprinciple,wecouldsimplyrungradientascentonbothvandhandthiswouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm.Usually,however,wedonotdothis,fortworeasons.First,thiswouldrequirestoringˆhforeachv.Wetypicallypreferalgorithmsthatdonotrequireper-examplememory.Itisdiﬃculttoscalelearningalgorithmstobillionsofexamplesifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample.644'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 659}, page_content='CHAPTER19.APPROXIMATEINFERENCESecond,wewouldliketobeabletoextractthefeaturesˆhveryquickly,inordertorecognizethecontentofv. Inarealisticdeployedsetting,wewouldneedtobeabletocomputeˆhinrealtime.Forboththesereasons,wetypicallydonotusegradientdescenttocomputethemeanﬁeldparametersˆh.Instead,werapidlyestimatethemwithﬁxedpointequations.Theideabehindﬁxedpointequationsisthatweareseekingalocalmaximumwithrespecttoˆh, where∇hL(vθ,,ˆh)=0.Wecannoteﬃcientlysolvethisequationwithrespecttoallofˆhsimultaneously.However,wecansolveforasinglevariable:∂∂ˆhiL(vθ,,ˆh) = 0.(19.37)Wecantheniterativelyapplythesolutiontotheequationfori=1,...,m,andrepeatthecycleuntilwesatisfyaconvergecriterion.CommonconvergencecriteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymorethansometoleranceamount,orwhenthecycledoesnotchangeˆhbymorethansomeamount.Iteratingmeanﬁeldﬁxedpointequationsisageneraltechniquethatcanprovidefastvariationalinferenceinabroadvarietyofmodels.Tomakethismoreconcrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelinparticular.First,wemustwriteanexpressionforthederivativeswithrespecttoˆhi.Todoso,wesubstituteEq.intotheleftsideofEq.:19.3619.37∂∂ˆhiL(vθ,,ˆh)(19.38)=∂∂ˆhi\\uf8ee\\uf8f0m\\ue058j=1\\ue068ˆhj(log(σbj)log−ˆhj)+(1−ˆhj)(log(σ−bj)log(1−−ˆhj))\\ue069(19.39)+12n\\ue058j=1\\uf8ee\\uf8f0logβj2π−βj\\uf8eb\\uf8edv2j−2vjWj,:ˆh+\\ue058k\\uf8ee\\uf8f0W2j,kˆhk+\\ue058lk\\ue036=Wj,kWj,lˆhkˆhl\\uf8f9\\uf8fb\\uf8f6\\uf8f8\\uf8f9\\uf8fb\\uf8f9\\uf8fb(19.40)=log(σbi)log−ˆhi−−1+log(1ˆhi)+1log(−σ−bi)(19.41)+n\\ue058j=1\\uf8ee\\uf8f0βj\\uf8eb\\uf8edvjWj,i−12W2j,i−\\ue058ki\\ue036=Wj,kWj,iˆhk\\uf8f6\\uf8f8\\uf8f9\\uf8fb(19.42)645'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 660}, page_content='CHAPTER19.APPROXIMATEINFERENCE=bi−logˆhi+log(1−ˆhi)+v\\ue03eβW:,i−12W\\ue03e:,iβW:,i−\\ue058ji\\ue036=W\\ue03e:,jβW:,iˆhj.(19.43)Toapplytheﬁxedpointupdateinferencerule,wesolvefortheˆhithatsetsEq.19.43to0:ˆhi= σ\\uf8eb\\uf8edbi+v\\ue03eβW:,i−12W\\ue03e:,iβW:,i−\\ue058ji\\ue036=W\\ue03e:,jβW:,iˆhj\\uf8f6\\uf8f8.(19.44)Atthispoint,wecanseethatthereisacloseconnectionbetweenrecurrentneuralnetworksandinferenceingraphicalmodels.Speciﬁcally,themeanﬁeldﬁxedpointequationsdeﬁnedarecurrentneuralnetwork.Thetaskofthisnetworkistoperforminference.Wehavedescribedhowtoderivethisnetworkfromamodeldescription,butitisalsopossibletotraintheinferencenetworkdirectly.SeveralideasbasedonthisthemearedescribedinChapter.20Inthecaseofbinarysparsecoding,wecanseethattherecurrentnetworkconnectionspeciﬁedbyEq.consistsofrepeatedlyupdatingthehidden19.44unitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinputalwayssendsaﬁxedmessageofv\\ue03eβWtothehiddenunits,butthehiddenunitsconstantlyupdatethemessagetheysendtoeachother.Speciﬁcally,twounitsˆhiandˆhjinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformofcompetition—betweentwohiddenunitsthatbothexplaintheinput,onlytheonethatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionisthemeanﬁeldapproximation’sattempttocapturetheexplainingawayinteractionsinthebinarysparsecodingposterior.Theexplainingawayeﬀectactuallyshouldcauseamulti-modalposterior,sothatifwedrawsamplesfromtheposterior,somesampleswillhaveoneunitactive,othersampleswillhavetheotherunitactive,butveryfewsampleshavebothactive.Unfortunately,explainingawayinteractionscannotbemodeledbythefactorialqusedformeanﬁeld,sothemeanﬁeldapproximationisforcedtochooseonemodetomodel.ThisisaninstanceofthebehaviorillustratedinFig..3.6WecanrewriteEq. intoanequivalentformthatrevealssomefurther19.44insights:ˆhi= σ\\uf8eb\\uf8ec\\uf8edbi+\\uf8eb\\uf8edv−\\ue058ji\\ue036=W:,jˆhj\\uf8f6\\uf8f8\\ue03eβW:,i−12W\\ue03e:,iβW:,i\\uf8f6\\uf8f7\\uf8f8.(19.45)Inthisreformulation,weseetheinputateachstepasconsistingofv−\\ue050ji\\ue036=W:,jˆhjratherthanv.Wecanthusthinkofunitiasattemptingtoencodetheresidual646'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 661}, page_content='CHAPTER19.APPROXIMATEINFERENCEerrorinvgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingasaniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attemptingtoﬁxmistakesinthereconstructionaftereachiteration.Inthisexample,wehavederivedanupdaterulethatupdatesasingleunitatatime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously.Somegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsuchawaythatwecansolveformanyentriesofˆhsimultaneously.Unfortunately,binarysparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristictechniquecalledtoperformblockupdates.Inthedampingapproach,wedampingsolvefortheindividuallyoptimalvaluesofeveryelementofˆh,thenmoveallofthevaluesinasmallstepinthatdirection.ThisapproachisnolongerguaranteedtoincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKollerandFriedman2009()formoreinformationaboutchoosingthedegreeofsynchronyanddampingstrategiesinmessagepassingalgorithms.19.4.2CalculusofVariationsBeforecontinuingwithourpresentationofvariationallearning,wemustbrieﬂyintroduceanimportantsetofmathematicaltoolsusedinvariationallearning:calculusofvariations.ManymachinelearningtechniquesarebasedonminimizingafunctionJ(θ)byﬁndingtheinputvectorθ∈Rnforwhichittakesonitsminimalvalue.Thiscanbeaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthecriticalpointswhere∇θJ(θ) =0.Insomecases,weactuallywanttosolveforafunctionf(x),suchaswhenwewanttoﬁndtheprobabilitydensityfunctionoversomerandomvariable.Thisiswhatcalculusofvariationsenablesustodo.AfunctionofafunctionfisknownasafunctionalJ[f].Muchaswecantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-valuedargument,wecantake,alsoknownas,functionalderivativesvariationalderivativesofafunctionalJ[f]withrespecttoindividualvaluesofthefunctionf(x)atanyspeciﬁcvalueofx.ThefunctionalderivativeofthefunctionalJwithrespecttothevalueofthefunctionatpointisdenotedfxδδfx()J.Acompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeofthisbook.Forourpurposes,itissuﬃcienttostatethatfordiﬀerentiablefunctionsfgy,()xanddiﬀerentiablefunctions(x)withcontinuousderivatives,thatδδf()x\\ue05agf,d(()xx)x=∂∂ygf,.(()xx)(19.46)647'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 662}, page_content='CHAPTER19.APPROXIMATEINFERENCETogainsomeintuitionforthisidentity,onecanthinkoff(x)asbeingavectorwithuncountablymanyelements,indexedbyarealvectorx.Inthis(somewhatincompleteview),theidentityprovidingthefunctionalderivativesisthesameaswewouldobtainforavectorθ∈Rnindexedbypositiveintegers:∂∂θi\\ue058jgθ(j,j) =∂∂θigθ(i,i.)(19.47)ManyresultsinothermachinelearningpublicationsarepresentedusingthemoregeneralEuler-Lagrangeequationwhichallowsgtodependonthederivativesoffaswellasthevalueoff,butwedonotneedthisfullygeneralformfortheresultspresentedinthisbook.Tooptimizeafunctionwithrespecttoavector,wetakethegradientofthefunctionwithrespecttothevectorandsolveforthepointwhereeveryelementofthegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingforthefunctionwherethefunctionalderivativeateverypointisequaltozero.Asanexampleofhowthisprocessworks,considertheproblemofﬁndingtheprobabilitydistributionfunctionoverx∈Rthathasmaximaldiﬀerentialentropy.Recallthattheentropyofaprobabilitydistributionisdeﬁnedaspx()Hp[] = −Exlog()px.(19.48)Forcontinuousvalues,theexpectationisanintegral:Hp[] = −\\ue05apxpxdx.()log()(19.49)WecannotsimplymaximizeH[p] withrespecttothefunctionp(x),becausetheresultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrangemultipliers toadd aconstraint thatp(x)integratesto 1.Also, the entropyincreaseswithoutboundasthevarianceincreases.Thismakesthequestionofwhichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhichdistributionhasmaximalentropyforﬁxedvarianceσ2.Finally,theproblemisunderdeterminedbecausethedistributioncanbeshiftedarbitrarilywithoutchangingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthemeanofthedistributionbeµ. TheLagrangianfunctionalforthisoptimizationproblemisL[] = pλ1\\ue012\\ue05apxdx()−1\\ue013+λ2([])+Ex−µλ3\\ue000E[()xµ−2]−σ2\\ue001+[]Hp(19.50)648'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 663}, page_content='CHAPTER19.APPROXIMATEINFERENCE=\\ue05a\\ue000λ1pxλ()+2pxxλ()+3pxxµ()(−)2−pxpx()log()\\ue001dxλ−1−µλ2−σ2λ3.(19.51)TominimizetheLagrangianwithrespecttop,wesetthefunctionalderivativesequalto0:∀x,δδpx()L= λ1+λ2xλ+3()xµ−2−−1log() = 0px.(19.52)Thisconditionnowtellsusthefunctionalformofp(x).Byalgebraicallyre-arrangingtheequation,weobtainpx() = exp\\ue000λ1+λ2xλ+3()xµ−2−1\\ue001.(19.53)Weneverassumeddirectlythatp(x)wouldtakethisfunctionalform;weobtainedtheexpressionitselfbyanalyticallyminimizingafunctional.Toﬁnishtheminimizationproblem,wemustchoosetheλvaluestoensurethatallofourconstraintsaresatisﬁed.Wearefreetochooseanyλvalues,becausethegradientoftheLagrangianwithrespecttotheλvariablesiszerosolongastheconstraintsaresatisﬁed.Tosatisfyalloftheconstraints,wemaysetλ1=1−logσ√2π,λ2= 0,andλ3= −12σ2toobtainpxxµ,σ() = (N;2).(19.54)Thisisonereasonforusingthenormaldistributionwhenwedonotknowthetruedistribution.Becausethenormaldistributionhasthemaximumentropy,weimposetheleastpossibleamountofstructurebymakingthisassumption.WhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,wefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyforﬁxedvariance.Whatabouttheprobabilitydistributionfunctionthatminimizestheentropy?Whydidwenotﬁndasecondcriticalpointcorrespondingtotheminimum?Thereasonisthatthereisnospeciﬁcfunctionthatachievesminimalentropy.Asfunctionsplacemoreprobabilitydensityonthetwopointsx=µ+σandx=µσ−,andplacelessprobabilitydensityonallothervaluesofx,theyloseentropywhilemaintainingthedesiredvariance.However,anyfunctionplacingexactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnotavalidprobabilitydistribution.Therethusisnosingleminimalentropyprobabilitydistributionfunction,muchasthereisnosingleminimalpositiverealnumber.Instead,wecansaythatthereisasequenceofprobabilitydistributionsconvergingtowardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe649'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 664}, page_content='CHAPTER19.APPROXIMATEINFERENCEdescribedasamixtureofDiracdistributions.BecauseDiracdistributionsarenotdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureofDiracdistributioncorrespondstoasinglespeciﬁcpointinfunctionspace.Thesedistributionsarethusinvisibletoourmethodofsolvingforaspeciﬁcpointwherethefunctionalderivativesarezero.Thisisalimitationofthemethod.DistributionssuchastheDiracmustbefoundbyothermethods,suchasguessingthesolutionandthenprovingthatitiscorrect.19.4.3ContinuousLatentVariablesWhenourgraphicalmodelcontainscontinuouslatentvariables, wemaystillperformvariationalinferenceandlearningbymaximizingL.However,wemustnowusecalculusofvariationswhenmaximizingwithrespectto.Lq()hv|Inmostcases,practitionersneednotsolveanycalculusofvariationsproblemsthemselves.Instead,thereisageneralequationforthemeanﬁeldﬁxedpointupdates.Ifwemakethemeanﬁeldapproximationq() =hv|\\ue059iqh(i|v),(19.55)andﬁxq(hj|v)forallj\\ue036=i,thentheoptimalq(hi|v)maybeobtainedbynormalizingtheunnormalizeddistribution˜qh(i|v) = exp\\ue000Eh−i∼q(h−i|v)log ˜p,(vh)\\ue001(19.56)solongaspdoesnotassignprobabilitytoanyjointconﬁgurationofvariables.0Carryingouttheexpectationinsidetheequationwillyieldthecorrectfunctionalformofq(hi|v).Itisonlynecessarytoderivefunctionalformsofqdirectlyusingcalculusofvariationsifonewishestodevelopanewformofvariationallearning;Eq.yieldsthemeanﬁeldapproximationforanyprobabilisticmodel.19.56Eq.isaﬁxedpointequation,designedtobeiterativelyappliedforeach19.56valueofirepeatedlyuntilconvergence.However,italsotellsusmorethanthat.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whetherwearrivetherebyﬁxedpointequationsornot.Thismeanswecantakethefunctionalformfromthatequationbutregardsomeofthevaluesthatappearinitasparameters,thatwecanoptimizewithanyoptimizationalgorithmwelike.Asanexample,consideraverysimpleprobabilisticmodel,withlatentvariablesh∈R2andjustonevisiblevariable,v.Supposethatp(h)=N(h;0,I)andp(v|h)=N(v;w\\ue03eh;1).Wecouldactuallysimplifythismodelbyintegratingouth;theresultisjustaGaussiandistributionoverv. Themodelitselfisnot650'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 665}, page_content='CHAPTER19.APPROXIMATEINFERENCEinteresting;wehaveconstructeditonlytoprovideasimpledemonstrationofhowcalculusofvariationsmaybeappliedtoprobabilisticmodeling.Thetrueposteriorisgiven,uptoanormalizingconstant,byp()hv|(19.57)∝p,(hv)(19.58)=(ph1)(ph2)()pvh|(19.59)∝exp\\ue012−12\\ue002h21+h22+(vh−1w1−h2w2)2\\ue003\\ue013(19.60)=exp\\ue012−12\\ue002h21+h22+v2+h21w21+h22w22−2vh1w1−2vh2w2+2h1w1h2w2\\ue003\\ue013.(19.61)Duetothepresenceofthetermsmultiplyingh1andh2together,wecanseethatthetrueposteriordoesnotfactorizeoverh1andh2.ApplyingEq.,weﬁndthat19.56˜qh(1|v)(19.62)=exp\\ue000Eh2∼q(h2|v)log ˜p,(vh)\\ue001(19.63)=exp\\ue012−12Eh2∼q(h2|v)\\ue002h21+h22+v2+h21w21+h22w22(19.64)−2vh1w1−2vh2w2+2h1w1h2w2]\\ue013.(19.65)Fromthis,wecanseethatthereareeﬀectivelyonlytwovaluesweneedtoobtainfromq(h2|v):Eh2∼|q(hv)[h2]andEh2∼|q(hv)[h22]. Writingtheseas\\ue068h2\\ue069and\\ue068h22\\ue069,weobtain˜qh(1|v) = exp\\ue012−12\\ue002h21+\\ue068h22\\ue069+v2+h21w21+\\ue068h22\\ue069w22(19.66)−2vh1w1−\\ue0682vh2\\ue069w2+2h1w1\\ue068h2\\ue069w2]\\ue013.(19.67)Fromthis,wecanseethat˜qhasthefunctionalformofaGaussian.Wecanthusconcludeq(hv|)=N(h;µβ,−1)whereµanddiagonalβarevariationalparametersthatwecanoptimizeusinganytechniquewechoose.ItisimportanttorecallthatwedidnoteverassumethatqwouldbeGaussian;itsGaussianformwasderivedautomaticallybyusingcalculusofvariationstomaximizeqwith651'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 666}, page_content='CHAPTER19.APPROXIMATEINFERENCErespecttoL.Usingthesameapproachonadiﬀerentmodelcouldyieldadiﬀerentfunctionalformof.qThiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes.Forexamplesofrealapplicationsofvariationallearningwithcontinuousvariablesinthecontextofdeeplearning,see().Goodfellowetal.2013d19.4.4InteractionsbetweenLearningandInferenceUsingapproximateinferenceaspartofalearningalgorithmaﬀectsthelearningprocess,andthisinturnaﬀectstheaccuracyoftheinferencealgorithm.Speciﬁcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakestheapproximatingassumptionsunderlyingtheapproximateinferencealgorithmbecomemoretrue.Whentrainingtheparameters,variationallearningincreasesEh∼qlog()pvh,.(19.68)Foraspeciﬁcv,thisincreasesp(hv|)forvaluesofhthathavehighprobabilityunderq(hv|)anddecreasesp(hv|)forvaluesofhthathavelowprobabilityunder.q()hv|Thisbehaviorcausesourapproximatingassumptionstobecomeself-fulﬁllingprophecies.Ifwetrainthemodelwithaunimodalapproximateposterior,wewillobtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewouldhaveobtainedbytrainingthemodelwithexactinference.Computingthetrueamountofharmimposedonamodelbyavariationalapproximationisthusverydiﬃcult.Thereexistseveralmethodsforestimatinglogp(v).Weoftenestimatelogp(v;θ)aftertrainingthemodel,andﬁndthatthegapwithL(vθ,,q)issmall.Fromthis,wecanconcludethatourvariationalapproximationisaccurateforthespeciﬁcvalueofθthatweobtainedfromthelearningprocess.Weshouldnotconcludethatourvariationalapproximationisaccurateingeneralorthatthevariationalapproximationdidlittleharmtothelearningprocess.Tomeasurethetrueamountofharminducedbythevariationalapproximation,wewouldneedtoknowθ∗=maxθlogp(v;θ). ItispossibleforL(vθ,,q)≈logp(v;θ)andlogp(v;θ)\\ue01clogp(v;θ∗)toholdsimultaneously.IfmaxqL(vθ,∗,q)\\ue01clogp(v;θ∗),becauseθ∗inducestoocomplicatedofaposteriordistributionforourqfamilytocapture, thenthelearningprocesswillneverapproachθ∗.Suchaproblemisverydiﬃculttodetect,becausewecanonlyknowforsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanﬁndθ∗forcomparison.652'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 667}, page_content='CHAPTER19.APPROXIMATEINFERENCE19.5LearnedApproximateInferenceWehaveseenthatinferencecanbethoughtofasanoptimizationprocedurethatincreasesthevalueofafunctionL.Explicitlyperformingoptimizationviaiterativeproceduressuchasﬁxedpointequationsorgradient-basedoptimizationisoftenveryexpensiveandtime-consuming.Manyapproachestoinferenceavoidthisexpensebylearningtoperformapproximateinference. Speciﬁcally,wecanthinkoftheoptimizationprocessasafunctionfthatmapsaninputvtoanapproximatedistributionq∗=argmaxqL(v,q).Oncewethinkofthemulti-stepiterativeoptimizationprocessasjustbeingafunction,wecanapproximateitwithaneuralnetworkthatimplementsanapproximationˆf(;)vθ.19.5.1Wake-SleepOneofthemaindiﬃcultieswithtrainingamodeltoinferhfromvisthatwedonothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givenav,wedonotknowtheappropriateh.Themappingfromvtohdependsonthechoiceofmodelfamily,andevolvesthroughoutthelearningprocessasθchanges.Thewake-sleepalgorithm(Hinton1995bFrey1996etal.,;etal.,)resolvesthisproblembydrawingsamplesofbothhandvfromthemodeldistribution. Forexample,inadirectedmodel,thiscanbedonecheaplybyperformingancestralsamplingbeginningathandendingatv.Theinferencenetworkcanthenbetrainedtoperformthereversemapping: predictingwhichhcausedthepresentv.Themaindrawbacktothisapproachisthatwewillonlybeabletotraintheinferencenetworkonvaluesofvthathavehighprobabilityunderthemodel.Earlyinlearning,themodeldistributionwillnotresemblethedatadistribution,sotheinferencenetworkwillnothaveanopportunitytolearnonsamplesthatresembledata.InSec.wesawthatonepossibleexplanationfortheroleofdreamsleepin18.2humanbeingsandanimalsisthatdreamscouldprovidethenegativephasesamplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegativegradientofthelogpartitionfunctionofundirectedmodels.Anotherpossibleexplanationforbiologicaldreamingisthatitisprovidingsamplesfromp(hv,)whichcanbeusedtotrainaninferencenetworktopredicthgivenv.Insomesenses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation.MonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonlythepositivephaseofthegradientforseveralstepsthenwithonlythenegativephaseofthegradientforseveralsteps.Humanbeingsandanimalsareusuallyawakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itisnotreadilyapparenthowthis653'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 668}, page_content='CHAPTER19.APPROXIMATEINFERENCEschedulecouldsupportMonteCarlotrainingofanundirectedmodel. LearningalgorithmsbasedonmaximizingLcanberunwithprolongedperiodsofimprovingqandprolongedperiodsofimprovingθ,however.Iftheroleofbiologicaldreamingistotrainnetworksforpredictingq,thenthisexplainshowanimalsareabletoremainawakeforseveralhours(thelongertheyareawake,thegreaterthegapbetweenLandlogp(v),butLwillremainalowerbound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnotmodiﬁedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,theseideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreamingaccomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearningratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromtheanimal’stransitionmodel,onwhichtotraintheanimal’spolicy.Orsleepmayservesomeotherpurposenotyetanticipatedbythemachinelearningcommunity.19.5.2OtherFormsofLearnedInferenceThisstrategyoflearnedapproximateinferencehasalsobeenappliedtoothermodels.SalakhutdinovandLarochelle2010()showedthatasinglepassinalearnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanﬁeldﬁxedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningtheinferencenetwork,thenapplyingonestepofmeanﬁeldtoimproveitsestimates,andtrainingtheinferencenetworktooutputthisreﬁnedestimateinsteadofitsoriginalestimate.WehavealreadyseeninSec.thatthepredictivesparsedecomposition14.8modeltrainsashallowencodernetworktopredictasparsecodefortheinput.Thiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itispossibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencodermaybeviewedasperforminglearnedapproximateMAPinference.Duetoitsshallowencoder,PSDisnotabletoimplementthekindofcompetitionbetweenunitsthatwehaveseeninmeanﬁeldinference.However,thatproblemcanberemediedbytrainingadeepencodertoperformlearnedapproximateinference,asintheISTAtechnique(,).GregorandLeCun2010bLearned approximate inference hasrecently become one of the dominantapproachestogenerativemodeling,intheformofthevariationalautoencoder(,;,).Inthiselegantapproach,thereisnoneedtoKingma2013Rezendeetal.2014constructexplicittargetsfortheinferencenetwork.Instead,theinferencenetworkissimplyusedtodeﬁneLelegantapproach,thereisnoneedtheinferencenetworkareadaptedtoincreaseL.Thismodelisdescribedindepthlater,inSec..20.10.3654'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 669}, page_content='CHAPTER19.APPROXIMATEINFERENCEUsingapproximateinference,itispossibletotrainanduseawidevarietyofmodels.Manyofthesemodelsaredescribedinthenextchapter.\\n655'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 670}, page_content='Chapter20DeepGenerativeModelsInthischapter,wepresentseveralofthespeciﬁckindsofgenerativemodelsthatcanbebuiltandtrainedusingthetechniquespresentedinChapters,,and16171819.Allofthesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsomeway.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly.Othersdonotallowtheevaluationoftheprobabilitydistributionfunction,butsupportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamplesfromthedistribution.Someofthesemodelsarestructuredprobabilisticmodelsdescribedintermsofgraphsandfactors,usingthelanguageofgraphicalmodelspresentedinChapter.Otherscannoteasilybedescribed16intermsoffactors,butrepresentprobabilitydistributionsnonetheless.20.1BoltzmannMachinesBoltzmannmachineswereoriginallyintroducedasageneral“connectionist”ap-proachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlmanetal.,;1983Ackley1985Hinton1984HintonandSejnowski1986etal.,;etal.,;,).VariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelongagosurpassedthepopularityoftheoriginal.InthissectionwebrieﬂyintroducethebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingtotrainandperforminferenceinthemodel.WedeﬁnetheBoltzmannmachineoverad-dimensionalbinaryrandomvectorx∈{0,1}d.TheBoltzmannmachineisanenergy-basedmodel(Sec.),16.2.4656'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 671}, page_content='CHAPTER20.DEEPGENERATIVEMODELSmeaningwedeﬁnethejointprobabilitydistributionusinganenergyfunction:P() =xexp(())−ExZ,(20.1)whereE(x)istheenergyfunctionandZisthepartitionfunctionthatensuresthat\\ue050xP() = 1x.TheenergyfunctionoftheBoltzmannmachineisgivenbyE() = x−x\\ue03eUxb−\\ue03ex,(20.2)whereUisthe“weight”matrixofmodelparametersandbisthevectorofbiasparameters.InthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftrainingexamples,eachofwhicharen-dimensional.Eq.describesthejointprobability20.1distributionovertheobservedvariables.Whilethisscenarioiscertainlyviable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablestothosedescribedbytheweightmatrix.Speciﬁcally,itmeansthattheprobabilityofoneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesoftheotherunits.TheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesareobserved.Inthiscase,thenon-observedvariables,orvariables,canlatentactsimilarlytohiddenunitsinamulti-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits.JustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresultsintheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachinewithhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetweenvariables.Instead,theBoltzmannmachinebecomesauniversalapproximatorofprobabilitymassfunctionsoverdiscretevariables(,).LeRouxandBengio2008Formally,wedecomposetheunitsxintotwosubsets:thevisibleunitsvandthelatent(orhidden)units.TheenergyfunctionbecomeshE,(vhv) = −\\ue03eRvv−\\ue03eWhh−\\ue03eShb−\\ue03evc−\\ue03eh.(20.3)BoltzmannMachineLearningLearningalgorithmsforBoltzmannmachinesareusuallybasedonmaximumlikelihood.AllBoltzmannmachineshaveanintractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-proximatedusingthetechniquesdescribedinChapter.18OneinterestingpropertyofBoltzmannmachineswhentrainedwithlearningrulesbasedonmaximumlikelihoodisthattheupdateforaparticularweightconnectingtwounitsdependsonlythestatisticsofthosetwounits, collected657'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 672}, page_content='CHAPTER20.DEEPGENERATIVEMODELSunderdiﬀerentdistributions:Pmodel(v)andˆPdata(v)Pmodel(hv|).Therestofthenetworkparticipatesinshapingthosestatistics,buttheweightcanbeupdatedwithoutknowinganythingabouttherestofthenetworkorhowthosestatisticswereproduced.Thismeansthatthelearningruleis“local,”whichmakesBoltzmannmachinelearningsomewhatbiologicallyplausible. ItisconceivablethatifeachneuronwerearandomvariableinaBoltzmannmachine,thentheaxonsanddendritesconnectingtworandomvariablescouldlearnonlybyobservingtheﬁringpatternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthepositivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnectionstrengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949summarizedwiththemnemonic“ﬁretogether,wiretogether.” Hebbianlearningrulesareamongtheoldesthypothesizedexplanationsforlearninginbiologicalsystemsandremainrelevanttoday(,).Giudiceetal.2009Otherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseemtorequireustohypothesizetheexistenceofmoremachinerythanthis.Forexample,forthebraintoimplementback-propagationinamultilayerperceptron,itseemsnecessaryforthebraintomaintainasecondarycommunicationnetworkfortransmittinggradientinformationbackwardsthroughthenetwork.Proposalsforbiologicallyplausibleimplementations(andapproximations)ofback-propagationhavebeenmade(,;,)butremaintobevalidated,andHinton2007aBengio2015Bengio2015()linksback-propagationofgradientstoinferenceinenergy-basedmodelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables).ThenegativephaseofBoltzmannmachinelearningissomewhathardertoexplainfromabiologicalpointofview.AsarguedinSec.,dreamsleepmay18.2beaformofnegativephasesampling.Thisideaismorespeculativethough.20.2RestrictedBoltzmannMachinesInventedunderthename(,),restrictedBoltzmannharmoniumSmolensky1986machinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels.WehavebrieﬂydescribedRBMspreviously,inSec. . Herewereviewthe16.7.1previousinformationandgointomoredetail.RBMsareundirectedprobabilisticgraphicalmodelscontainingalayerofobservablevariablesandasinglelayeroflatentvariables.RBMsmaybestacked(oneontopoftheother)toformdeepermodels.SeeFig.forsomeexamples.Inparticular,Fig.ashowsthegraph20.120.1structureoftheRBMitself.Itisabipartitegraph,withnoconnectionspermittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthelatentlayer.658'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 673}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nh1h1h2h2h3h3v1v1v2v2v3v3h4h4h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4abh(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4cFigure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines.(a)TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedonabipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsintheotherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamongthehiddenunits. TypicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutitispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A(b)deepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirectedconnections.LikeanRBM,ithasnointra-layerconnections.However,aDBNhasmultiplehiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparatelayers.AllofthelocalconditionalprobabilitydistributionsneededbythedeepbeliefnetworkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofitsconstituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwithacompletelyundirectedgraph,butitwouldneedintra-layerconnectionstocapturethedependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical(c)modelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintra-layerconnections. DBMsarelesscloselytiedtoRBMsthanDBNsare. WheninitializingaDBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.SomekindsofDBMsmaybetrainedwithoutﬁrsttrainingasetofRBMs.659'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 674}, page_content='CHAPTER20.DEEPGENERATIVEMODELSWebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butasweseelaterthereareextensionstoothertypesofvisibleandhiddenunits.Moreformally,lettheobservedlayerconsistofasetofnvbinaryrandomvariableswhichwerefertocollectivelywiththevectorv.Werefertothelatentorhiddenlayerofnhbinaryrandomvariablesas.hLikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisanenergy-basedmodelwiththejointprobabilitydistributionspeciﬁedbyitsenergyfunction:P,(= vvh= ) =h1Zexp(())−Evh,.(20.4)TheenergyfunctionforanRBMisgivenbyE,(vhb) = −\\ue03evc−\\ue03ehv−\\ue03eWh,(20.5)andisthenormalizingconstantknownasthepartitionfunction:ZZ=\\ue058v\\ue058hexp(){−Evh,}.(20.6)ItisapparentfromthedeﬁnitionofthepartitionfunctionZthatthenaivemethodofcomputingZ(exhaustivelysummingoverallstates)couldbecomputationallyintractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesintheprobabilitydistributiontocomputeZfaster.InthecaseofrestrictedBoltzmannmachines,()formallyprovedthatthepartitionfunctionLongandServedio2010Zisintractable.TheintractablepartitionfunctionZimpliesthatthenormalizedjointprobabilitydistributionisalsointractabletoevaluate.P()v20.2.1ConditionalDistributionsThoughP(v)isintractable,thebipartitegraphstructureoftheRBMhastheveryspecialpropertythatitsconditionaldistributionsP(hv|)andP(vh|)arefactorialandrelativelysimpletocomputeandtosamplefrom.Derivingtheconditionaldistributionsfromthejointdistributionisstraightfor-ward:P() =hv|P,(hv)P()v(20.7)=1P()v1Zexp\\ue06eb\\ue03evc+\\ue03ehv+\\ue03eWh\\ue06f(20.8)=1Z\\ue030exp\\ue06ec\\ue03ehv+\\ue03eWh\\ue06f(20.9)660'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 675}, page_content='CHAPTER20.DEEPGENERATIVEMODELS=1Z\\ue030exp\\uf8f1\\uf8f2\\uf8f3nh\\ue058j=1cjhj+nh\\ue058j=1v\\ue03eW:,jhj\\uf8fc\\uf8fd\\uf8fe(20.10)=1Z\\ue030nh\\ue059j=1exp\\ue06ecjhj+v\\ue03eW:,jhj\\ue06f(20.11)Sinceweareconditioningonthevisibleunitsv,wecantreattheseasconstantwithrespecttothedistributionP(hv|).ThefactorialnatureoftheconditionalP(hv|)followsimmediatelyfromourabilitytowritethejointprobabilityoverthevectorhastheproductof(unnormalized)distributionsovertheindividualelements,hj.Itisnowasimplematterofnormalizingthedistributionsovertheindividualbinaryhj.Ph(j= 1 ) =|v˜Ph(j= 1 )|v˜Ph(j= 0 )+|v˜Ph(j= 1 )|v(20.12)=exp\\ue008cj+v\\ue03eW:,j\\ue009exp0+exp{}{cj+v\\ue03eW:,j}(20.13)= σ\\ue010cj+v\\ue03eW:,j\\ue011.(20.14)Wecannowexpressthefullconditionaloverthehiddenlayerasthefactorialdistribution:P() =hv|nh\\ue059j=1σ\\ue010(21)(+h−\\ue00ccW\\ue03ev)\\ue011j.(20.15)Asimilarderivationwillshowthattheotherconditionofinteresttous,P(vh|),isalsoafactorialdistribution:P() =vh|nv\\ue059i=1σ((21)(+))v−\\ue00cbWhi.(20.16)20.2.2TrainingRestrictedBoltzmannMachinesBecausetheRBMadmitseﬃcientevaluationanddiﬀerentiationof˜P(v)andeﬃcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybetrainedwithanyofthetechniquesdescribedinChapterfortrainingmodels18thathaveintractablepartitionfunctions. ThisincludesCD,SML(PCD),ratiomatchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning,theRBMisrelativelystraightforwardtotrainbecausewecancomputeP(h|v)661'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 676}, page_content='CHAPTER20.DEEPGENERATIVEMODELSexactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmannmachine,combineboththediﬃcultyofanintractablepartitionfunctionandthediﬃcultyofintractableinference.20.3DeepBeliefNetworksDeepbeliefnetworks(DBNs)wereoneoftheﬁrstnon-convolutionalmodelstosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hintonetal.,;,2007b).Theintroductionofdeepbeliefnetworksin2006beganthecurrentdeeplearningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodelswereconsideredtoodiﬃculttooptimize.Kernelmachineswithconvexobjectivefunctionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstratedthatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupportvectormachinesontheMNISTdataset(,).Today,deepbeliefHintonetal.2006networkshavemostlyfallenoutoffavorandarerarelyused,evencomparedtootherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedlyrecognizedfortheirimportantroleindeeplearninghistory.Deepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables.Thelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinaryorreal.Therearenointra-layerconnections.Usually,everyunitineachlayerisconnectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstructmoresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersareundirected.Theconnectionsbetweenallotherlayersaredirected,withthearrowspointedtowardthelayerthatisclosesttothedata.SeeFig.bforanexample.20.1ADBNwithlhiddenlayerscontainslweightmatrices:W(1),...,W()l.Italsocontainsl+1biasvectors:b(0),...,b()l,withb(0)providingthebiasesforthevisiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenbyP(h()l,h(1)l−) exp∝\\ue010b()l\\ue03eh()l+b(1)l−\\ue03eh(1)l−+h(1)l−\\ue03eW()lh()l\\ue011,(20.17)Ph(()ki= 1 |h(+1)k) = σ\\ue010b()ki+W(+1)k\\ue03e:,ih(+1)k\\ue011∀∀∈−i,k1,...,l2,(20.18)Pv(i= 1 |h(1)) = σ\\ue010b(0)i+W(1)\\ue03e:,ih(1)\\ue011∀i.(20.19)Inthecaseofreal-valuedvisibleunits,substitutev∼N\\ue010vb;(0)+W(1)\\ue03eh(1),β−1\\ue011(20.20)662'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 677}, page_content='CHAPTER20.DEEPGENERATIVEMODELSwithβdiagonalfortractability.Generalizationstootherexponentialfamilyvisibleunitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayerisjustanRBM.TogenerateasamplefromaDBN,weﬁrstrunseveralstepsofGibbssamplingonthetoptwohiddenlayers.ThisstageisessentiallydrawingasamplefromtheRBMdeﬁnedbythetoptwohiddenlayers.Wecanthenuseasinglepassofancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisibleunits.Deepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirectedmodelsandundirectedmodels.Inferenceinadeepbeliefnetworkisintractableduetotheexplainingawayeﬀectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohiddenlayersthathaveundirectedconnections.Evaluatingormaximizingthestandardevidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidencelowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetworkwidth.Evaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingtheproblemofintractableinferencetomarginalizeoutthelatentvariables,butalsotheproblemofanintractablepartitionfunctionwithintheundirectedmodelofthetoptwolayers.Totrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximizeEv∼pdatalogp(v)usingcontrastivedivergenceorstochasticmaximumlikelihood.TheparametersoftheRBMthendeﬁnetheparametersoftheﬁrstlayeroftheDBN.Next,asecondRBMistrainedtoapproximatelymaximizeEv∼pdataEh(1)∼p(1)(h(1)|v)logp(2)(h(1))(20.21)wherep(1)istheprobabilitydistributionrepresentedbytheﬁrstRBMandp(2)istheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,thesecondRBMistrainedtomodelthedistributiondeﬁnedbysamplingthehiddenunitsoftheﬁrstRBM,whentheﬁrstRBMisdrivenbythedata.Thisprocedurecanberepeatedindeﬁnitely,toaddasmanylayerstotheDBNasdesired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBMdeﬁnesanotherlayeroftheDBN.Thisprocedurecanbejustiﬁedasincreasingavariationallowerboundonthelog-likelihoodofthedataundertheDBN(Hintonetal.,).2006Inmostapplications,noeﬀortismadetojointlytraintheDBNafterthegreedylayer-wiseprocedureiscomplete.However,itispossibletoperformgenerativeﬁne-tuningusingthewake-sleepalgorithm.663'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 678}, page_content='CHAPTER20.DEEPGENERATIVEMODELSThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostoftheinterestinDBNsarosefromtheirabilitytoimproveclassiﬁcationmodels.WecantaketheweightsfromtheDBNandusethemtodeﬁneanMLP:h(1)= σ\\ue010b(1)+v\\ue03eW(1)\\ue011.(20.22)h()l= σ\\ue010b()li+h(1)l−\\ue03eW()l\\ue011∀∈l2,...,m,(20.23)AfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerativetrainingoftheDBN,wemaytraintheMLPtoperformaclassiﬁcationtask.ThisadditionaltrainingoftheMLPisanexampleofdiscriminativeﬁne-tuning.ThisspeciﬁcchoiceofMLPissomewhatarbitrary,comparedtomanyoftheinferenceequationsinChapterthatarederivedfromﬁrstprinciples.ThisMLP19isaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistentlyintheliterature.Manyapproximateinferencetechniquesaremotivatedbytheirabilitytoﬁndamaximallyvariationallowerboundonthelog-likelihoodtightundersomesetofconstraints.Onecanconstructavariationallowerboundonthelog-likelihoodusingthehiddenunitexpectationsdeﬁnedbytheDBN’sMLP,butthisistrueofprobabilitydistributionoverthehiddenunits,andthereisnoanyreasontobelievethatthisMLPprovidesaparticularlytightbound. Inparticular,theMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.TheMLPpropagatesinformationupwardfromthevisibleunitstothedeepesthiddenunits,butdoesnotpropagateanyinformationdownwardorsideways.TheDBNgraphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunitswithinthesamelayeraswellastop-downinteractionsbetweenlayers.Whilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwithAIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasagenerativemodel.Theterm“deepbeliefnetwork”iscommonlyusedincorrectlytorefertoanykindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics.Theterm“deepbeliefnetwork”shouldreferspeciﬁcallytomodelswithundirectedconnectionsinthedeepestlayeranddirectedconnectionspointingdownwardbetweenallotherpairsofconsecutivelayers.Theterm“deepbeliefnetwork”mayalsocausesomeconfusionbecausetheterm“beliefnetwork”issometimesusedtorefertopurelydirectedmodels,whiledeepbeliefnetworkscontainanundirectedlayer.DeepbeliefnetworksalsosharetheacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,),whichareBayesiannetworksforrepresentingMarkovchains.664'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 679}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nh(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4Figure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer(bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers.Therearenointra-layerlayerconnections.20.4DeepBoltzmannMachinesAdeepBoltzmannmachineDBMor(SalakhutdinovandHinton2009a,)isanotherkindofdeep,generativemodel. Unlikethedeepbeliefnetwork(DBN),itisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayersoflatentvariables(RBMshavejustone).ButliketheRBM,withineachlayer,eachofthevariablesaremutuallyindependent,conditionedonthevariablesintheneighboringlayers.SeeFig.forthegraphstructure.DeepBoltzmannmachineshavebeen20.2appliedtoavarietyoftasksincludingdocumentmodeling(Srivastava2013etal.,).LikeRBMsandDBNs, DBMstypicallycontainonlybinaryunits—asweassumeforsimplicityofourpresentationofthemodel—butitisstraightforwardtoincludereal-valuedvisibleunits.ADBMisanenergy-basedmodel,meaningthatthethejointprobabilitydistributionoverthemodelvariablesisparametrizedbyanenergyfunctionE.InthecaseofadeepBoltzmannmachinewithonevisiblelayer,v,andthreehiddenlayers,h(1),h(2)andh(3),thejointprobabilityisgivenby:P\\ue010vh,(1),h(2),h(3)\\ue011=1Z()θexp\\ue010−E,(vh(1),h(2),h(3);)θ\\ue011.(20.24)Tosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergyfunctionisthendeﬁnedasfollows:E,(vh(1),h(2),h(3);) = θ−v\\ue03eW(1)h(1)−h(1)\\ue03eW(2)h(2)−h(2)\\ue03eW(3)h(3).(20.25)665'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 680}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nh(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(3)1h(3)1h(3)2h(3)2\\nv1v2h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3h(3)1h(3)1h(3)2h(3)2\\nFigure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure.IncomparisontotheRBMenergyfunction(Eq.),theDBMenergy20.5functionincludesconnectionsbetweenthehiddenunits(latentvariables)intheformoftheweightmatrices(W(2)andW(3)).Aswewillsee,theseconnectionshavesigniﬁcantconsequencesforboththemodelbehavioraswellashowwegoaboutperforminginferenceinthemodel.IncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-nectedtoeveryotherunit),theDBMoﬀerssomeadvantagesthataresimilartothoseoﬀeredbytheRBM.Speciﬁcally,asillustratedinFig.,theDBMlayers20.3canbeorganizedintoabipartitegraph,withoddlayersononesideandevenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthevariablesintheevenlayer,thevariablesintheoddlayersbecomeconditionallyindependent.Ofcourse,whenweconditiononthevariablesintheoddlayers,thevariablesintheevenlayersalsobecomeconditionallyindependent.ThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-tionswehavepreviouslyusedfortheconditionaldistributionsofanRBMtodeterminetheconditionaldistributionsinaDBM.Theunitswithinalayerareconditionallyindependentfromeachothergiventhevaluesoftheneighboringlayers,sothedistributionsoverbinaryvariablescanbefullydescribedbytheBernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inourexamplewithtwohiddenlayers,theactivationprobabilitiesaregivenby:Pv(i= 1 |h(1)) = σ\\ue010W(1)i,:h(1)\\ue011,(20.26)666'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 681}, page_content='CHAPTER20.DEEPGENERATIVEMODELSPh((1)i= 1 |vh,(2)) = σ\\ue010v\\ue03eW(1):,i+W(2)i,:h(2)\\ue011(20.27)andPh((2)k= 1 |h(1)) = σ\\ue010h(1)\\ue03eW(2):,k\\ue011.(20.28)ThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachineeﬃcient. ThenaiveapproachtoGibbssamplingistoupdateonlyonevariableatatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandallofthehiddenunitstobeupdatedinasecondblock.OnemightnaivelyassumethataDBMwithllayersrequiresl+1updates,witheachiterationupdatingablockconsistingofonelayerofunits.Instead,itispossibletoupdatealloftheunitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksofupdates,oneincludingallevenlayers(includingthevisiblelayer)andtheotherincludingalloddlayers.DuetothebipartiteDBMconnectionpattern,giventheevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbesampledsimultaneouslyandindependentlyasablock. Likewise,giventheoddlayers,theevenlayerscanbesampledsimultaneouslyandindependentlyasablock.Eﬃcientsamplingisespeciallyimportantfortrainingwiththestochasticmaximumlikelihoodalgorithm.20.4.1InterestingPropertiesDeepBoltzmannmachineshavemanyinterestingproperties.DBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-tionP(hv|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityofthisposteriordistributionallowsricherapproximationsoftheposterior.InthecaseoftheDBN,weperformclassiﬁcationusingaheuristicallymotivatedapproximateinferenceprocedure,inwhichweguessthatareasonablevalueforthemeanﬁeldexpectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthenetworkinanMLPthatusessigmoidactivationfunctionsandthesameweightsastheoriginalDBN.AnydistributionQ(h)maybeusedtoobtainavariationallowerboundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtainsuchabound.However,theboundisnotexplicitlyoptimizedinanyway,sotheboundmaybefarfromtight.Inparticular,theheuristicestimateofQignoresinteractionsbetweenhiddenunitswithinthesamelayeraswellasthetop-downfeedbackinﬂuenceofhiddenunitsindeeperlayersonhiddenunitsthatareclosertotheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBNisnotabletoaccountfortheseinteractions,theresultingQispresumablyfar667'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 682}, page_content='CHAPTER20.DEEPGENERATIVEMODELSfromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionallyindependentgiventheotherlayers.Thislackofintra-layerinteractionmakesitpossibletouseﬁxedpointequationstoactuallyoptimizethevariationallowerboundandﬁndthetrueoptimalmeanﬁeldexpectations(towithinsomenumericaltolerance).TheuseofpropermeanﬁeldallowstheapproximateinferenceprocedureforDBMstocapturetheinﬂuenceoftop-downfeedbackinteractions.ThismakesDBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrainisknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,DBMshavebeenusedascomputationalmodelsofrealneuroscientiﬁcphenomena(,;,).Seriesetal.2010Reichertetal.2011OneunfortunatepropertyofDBMsisthatsamplingfromthemisrelativelydiﬃcult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.Theotherlayersareusedonlyattheendofthesamplingprocess,inoneeﬃcientancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessarytouseMCMCacrossalllayers,witheverylayerofthemodelparticipatingineveryMarkovchaintransition.20.4.2DBMMeanFieldInferenceTheconditionaldistributionoveroneDBMlayergiventheneighboringlayersisfactorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributionsareP(vh|(1)),P(h(1)|vh,(2))andP(h(2)|h(1)).Thedistributionoverallhiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers.Intheexamplewithtwohiddenlayers,P(h(1),h(2)|v)doesnotfactorizedueduetotheinteractionweightsW(2)betweenh(1)andh(2)whichrenderthesevariablesmutuallydependent.AswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximatetheDBMposteriordistribution. However,unliketheDBN,theDBMposteriordistributionovertheirhiddenunits—whilecomplicated—iseasytoapproximatewithaapproximation(asdiscussedinSec.),speciﬁcallyameanvariational19.4ﬁeldapproximation.Themeanﬁeldapproximationisasimpleformofvariationalinference,wherewerestricttheapproximatingdistributiontofullyfactorialdistri-butions.InthecontextofDBMs,themeanﬁeldequationscapturethebidirectionalinteractionsbetweenlayers. InthissectionwederivetheiterativeapproximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton2009a().Invariationalapproximationstoinference,weapproachthetaskofapproxi-matingaparticulartargetdistribution—inourcase,theposteriordistributionover668'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 683}, page_content='CHAPTER20.DEEPGENERATIVEMODELSthehiddenunitsgiventhevisibleunits—bysomereasonablysimplefamilyofdis-tributions.Inthecaseofthemeanﬁeldapproximation,theapproximatingfamilyisthesetofdistributionswherethehiddenunitsareconditionallyindependent.Wenowdevelopthemeanﬁeldapproachfortheexamplewithtwohiddenlayers.LetQ(h(1),h(2)|v)betheapproximationofP(h(1),h(2)|v).ThemeanﬁeldassumptionimpliesthatQ(h(1),h(2)|v) =\\ue059jQh((1)j|v)\\ue059kQh((2)k|v).(20.29)ThemeanﬁeldapproximationattemptstoﬁndamemberofthisfamilyofdistributionsthatbestﬁtsthetrueposteriorP(h(1),h(2)|v). Importantly,theinferenceprocessmustberunagaintoﬁndadiﬀerentdistributionQeverytimeweuseanewvalueof.vOnecanconceiveofmanywaysofmeasuringhowwellQ(hv|)ﬁtsP(hv|).ThemeanﬁeldapproachistominimizeKL() =QP\\ue06b\\ue058hQ(h(1),h(2)|v)log\\ue020Q(h(1),h(2)|v)P(h(1),h(2)|v)\\ue021.(20.30)Ingeneral,wedonothavetoprovideaparametricformoftheapproximatingdistributionbeyondenforcingtheindependenceassumptions.Thevariationalapproximationprocedureisgenerallyabletorecoverafunctionalformoftheapproximatedistribution.However,inthecaseofameanﬁeldassumptiononbinaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgeneralityresultingfromﬁxingaparametrizationofthemodelinadvance.WeparametrizeQasaproductofBernoullidistributions,thatisweassociatetheprobabilityofeachelementofh(1)withaparameter.Speciﬁcally,foreachj,ˆh(1)j=Q(h(1)j= 1|v),whereˆh(1)j∈[0,1]andforeachk,ˆh(2)k=Q(h(2)k= 1|v),whereˆh(2)k∈[01],.Thuswehavethefollowingapproximationtotheposterior:Q(h(1),h(2)|v) =\\ue059jQh((1)j|v)\\ue059kQh((2)k|v)(20.31)=\\ue059j(ˆh(1)j)h(1)j(1−ˆh(1)j)(1−h(1)j)×\\ue059k(ˆh(2)k)h(2)k(1−ˆh(2)k)(1−h(2)k).(20.32)Ofcourse,forDBMswithmorelayerstheapproximateposteriorparametrizationcanbeextendedintheobviousway,exploitingthebipartitestructureofthegraph669'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 684}, page_content='CHAPTER20.DEEPGENERATIVEMODELStoupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheoddlayerssimultaneously,followingthesamescheduleasGibbssampling.NowthatwehavespeciﬁedourfamilyofapproximatingdistributionsQ,itremainstospecifyaprocedureforchoosingthememberofthisfamilythatbestﬁtsP.ThemoststraightforwardwaytodothisistousethemeanﬁeldequationsspeciﬁedbyEq..Theseequationswerederivedbysolvingforwherethe19.56derivativesofthevariationallowerboundarezero.Theydescribeinanabstractmannerhowtooptimizethevariationallowerboundforanymodel,simplybytakingexpectationswithrespectto.QApplyingthesegeneralequations,weobtaintheupdaterules(again,ignoringbiasterms):ˆh(1)j= σ\\ue020\\ue058iviW(1)i,j+\\ue058k\\ue030W(2)j,k\\ue030ˆh(2)k\\ue030\\ue021,j∀(20.33)ˆh(2)k= σ\\uf8eb\\uf8ed\\ue058j\\ue030W(2)j\\ue030,kˆh(1)j\\ue030\\uf8f6\\uf8f8,k.∀(20.34)Ataﬁxedpointofthissystemofequations,wehavealocalmaximumofthevariationallowerboundL(Q).Thustheseﬁxedpointupdateequationsdeﬁneaniterativealgorithmwherewealternateupdatesofˆh(1)j(usingEq.)and20.33updatesofˆh(2)k(usingEq.).OnsmallproblemssuchasMNIST,asfew20.34asteniterationscanbesuﬃcienttoﬁndanapproximatepositivephasegradientforlearning,andﬁftyusuallysuﬃcetoobtainahighqualityrepresentationofasinglespeciﬁcexampletobeusedforhigh-accuracyclassiﬁcation.ExtendingapproximatevariationalinferencetodeeperDBMsisstraightforward.20.4.3DBMParameterLearning.LearningintheDBMmustconfrontboththechallengeof anintractablepartitionfunction,usingthetechniquesfromChapter,andthechallengeofan18intractableposteriordistribution,usingthetechniquesfromChapter.19AsdescribedinSec.,variationalinferenceallowstheconstructionofa20.4.2distributionQ(hv|)thatapproximatestheintractableP(hv|).LearningthenproceedsbymaximizingL(vθ,Q,),thevariationallowerboundontheintractablelog-likelihood,.log(;)Pvθ670'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 685}, page_content='CHAPTER20.DEEPGENERATIVEMODELSForadeepBoltzmannmachinewithtwohiddenlayers,isgivenbyLL() =Q,θ\\ue058i\\ue058j\\ue030viW(1)i,j\\ue030ˆh(1)j\\ue030+\\ue058j\\ue030\\ue058k\\ue030ˆh(1)j\\ue030W(2)j\\ue030,k\\ue030ˆh(2)k\\ue030−Hlog()+Zθ()Q.(20.35)Thisexpressionstillcontainsthelogpartitionfunction,logZ(θ).BecauseadeepBoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,thehardnessresultsforcomputingthepartitionfunctionandsamplingthatapplytorestrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.ThismeansthatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequiresapproximatemethodssuchasannealedimportancesampling.Likewise,trainingthemodelrequiresapproximationstothegradientofthelogpartitionfunction.SeeChapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained18usingstochasticmaximumlikelihood.ManyoftheothertechniquesdescribedinChapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe18abilitytoevaluatetheunnormalizedprobabilities,ratherthanmerelyobtainavariationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmannmachinesbecausetheydonotalloweﬃcientsamplingofthehiddenunitsgiventhevisibleunits—instead,contrastivedivergencewouldrequireburninginaMarkovchaineverytimeanewnegativephasesampleisneeded.Thenon-variationalversionofstochasticmaximumlikelihoodalgorithmwasdiscussedearlier,inSec..Variationalstochasticmaximumlikelihoodasapplied18.2totheDBMisgiveninAlgorithm.Recallthatwedescribeasimpliﬁedvarient20.1oftheDBMthatlacksbiasparameters;includingthemistrivial.20.4.4Layer-WisePretrainingUnfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribedabove)fromarandominitializationusuallyresultsinfailure.Insomecases,themodelfailstolearntorepresentthedistributionadequately.Inothercases,theDBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancouldbeobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheﬁrstlayerrepresentsapproximatelythesamedistributionasanRBM.VarioustechniquesthatpermitjointtraininghavebeendevelopedandaredescribedinSec..However,theoriginalandmostpopularmethodfor20.4.5overcomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining.Inthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.Theﬁrstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedtomodelsamplesfromthepreviousRBM’sposteriordistribution. Afterallofthe671'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 686}, page_content='CHAPTER20.DEEPGENERATIVEMODELSAlgorithm20.1ThevariationalstochasticmaximumlikelihoodalgorithmfortrainingaDBMwithtwohiddenlayers.Set,thestepsize,toasmallpositivenumber\\ue00fSetk,thenumberofGibbssteps,highenoughtoallowaMarkovchainofp(vh,(1),h(2);θ+\\ue00f∆θ)toburnin,startingfromsamplesfromp(vh,(1),h(2);θ).Initializethreematrices,˜V,˜H(1)and˜H(2)eachwithmrowssettorandomvalues(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedtothemodel’smarginals).whilenotconverged(learningloop)doSampleaminibatchofmexamplesfromthetrainingdataandarrangethemastherowsofadesignmatrix.VInitializematricesˆH(1)andˆH(2),possiblytothemodel’smarginals.whilenotconverged(meanﬁeldinferenceloop)doˆH(1)←σ\\ue010VW(1)+ˆH(2)W(2)\\ue03e\\ue011.ˆH(2)←σ\\ue010ˆH(1)W(2)\\ue011.endwhile∆W(1)←1mV\\ue03eˆH(1)∆W(2)←1mˆH(1)\\ue03eˆH(2)fordolk= 1to(Gibbssampling)Gibbsblock1:∀i,j,˜Vi,jsampledfromP(˜Vi,j= 1) = σ\\ue012W(1)j,:\\ue010˜H(1)i,:\\ue011\\ue03e\\ue013.∀i,j,˜H(2)i,jsampledfromP(˜H(2)i,j= 1) = σ\\ue010˜H(1)i,:W(2):,j\\ue011.Gibbsblock2:∀i,j,˜H(1)i,jsampledfromP(˜H(1)i,j= 1) = σ\\ue010˜Vi,:W(1):,j+˜H(2)i,:W(2)\\ue03ej,:\\ue011.endfor∆W(1)←∆W(1)−1mV\\ue03e˜H(1)∆W(2)←∆W(2)−1m˜H(1)\\ue03e˜H(2)W(1)←W(1)+\\ue00f∆W(1)(thisisacartoonillustration,inpracticeuseamoreeﬀectivealgorithm,suchasmomentumwithadecayinglearningrate)W(2)←W(2)+∆\\ue00fW(2)endwhile672'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 687}, page_content='CHAPTER20.DEEPGENERATIVEMODELSRBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.TheDBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlyasmallchangeinthemodel’sparametersanditsperformanceasmeasuredbythelog-likelihooditassignstothedata,oritsabilitytoclassifyinputs.SeeFig.20.4foranillustrationofthetrainingprocedure.Thisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbearssomepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetoftheparametersateachstep.However,inthecaseofthegreedylayer-wisetrainingprocedure,weactuallyuseadiﬀerentobjectivefunctionateachstep.Greedylayer-wisepretrainingofaDBMdiﬀersfromgreedylayer-wisepre-trainingofaDBN.TheparametersofeachindividualRBMmaybecopiedtothecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparametersmustbemodiﬁedbeforeinclusionintheDBM.AlayerinthemiddleofthestackofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombinedtoformtheDBM,thelayerwillhavebothbottom-upandtop-downinput. Toaccountforthiseﬀect,SalakhutdinovandHinton2009a()advocatedividingtheweightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintotheDBM.Additionally,thebottomRBMmustbetrainedusingtwo“copies”ofeachvisibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeansthattheweightsareeﬀectivelydoubledduringtheupwardpass.Similarly,thetopRBMshouldbetrainedwithtwocopiesofthetopmostlayer.ObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequiresamodiﬁcationofthestandardSMLalgorithm,whichistouseasmallamountofmeanﬁeldduringthenegativephaseofthejointPCDtrainingstep(SalakhutdinovandHinton2009a,). Speciﬁcally,theexpectationoftheenergygradientshouldbecomputedwithrespecttothemeanﬁelddistributioninwhichalloftheunitsareindependentfromeachother.Theparametersofthismeanﬁelddistributionshouldbeobtainedbyrunningthemeanﬁeldﬁxedpointequationsforjustonestep.SeeGoodfellow2013betal.()foracomparisonoftheperformanceofcenteredDBMswithandwithouttheuseofpartialmeanﬁeldinthenegativephase.20.4.5JointlyTrainingDeepBoltzmannMachinesClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassiﬁcationwell,requireaseparateMLP-basedclassiﬁerontopofthehiddenfeaturestheyextract.Thishassomeundesirableproperties.ItishardtotrackperformanceduringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhiletrainingtheﬁrstRBM.Thus,itishardtotellhowwellourhyperparameters673'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 688}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nd)a)b)\\nc)\\nFigure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNISTdataset(SalakhutdinovandHinton2009aSrivastava2014,;etal.,).TrainanRBM(a)byusingCDtoapproximatelymaximizelogP(v).TrainasecondRBMthatmodels(b)h(1)andtargetclassybyusingCD-ktoapproximatelymaximizelogP(h(1),y)whereh(1)isdrawnfromtheﬁrstRBM’sposteriorconditionedonthedata.Increasekfrom1to20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately(c)maximizelogP(v,y)usingstochasticmaximumlikelihoodwithk= 5.Delete(d)yfromthemodel.Deﬁneanewsetoffeaturesh(1)andh(2)thatareobtainedbyrunningmeanﬁeldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhosestructureisthesameasanadditionalpassofmeanﬁeld,withanadditionaloutputlayerfortheestimateofy.InitializetheMLP’sweightstobethesameastheDBM’sweights.TraintheMLPtoapproximatelymaximizelogP(y|v)usingstochasticgradientdescentanddropout.Figurereprintedfrom(Goodfellow2013betal.,).674'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 689}, page_content='CHAPTER20.DEEPGENERATIVEMODELSareworkinguntilquitelateinthetrainingprocess.SoftwareimplementationsofDBMsneedtohavemanydiﬀerentcomponentsforCDtrainingofindividualRBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagationthroughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmanyoftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeingabletoperforminferencewhensomeinputvaluesaremissing.TherearetwomainwaystoresolvethejointtrainingproblemofthedeepBoltzmannmachine.TheﬁrstisthecentereddeepBoltzmannmachine(MontavonandMuller2012,),whichreparametrizesthemodelinordertomaketheHessianofthecostfunctionbetter-conditionedatthebeginningofthelearningprocess.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wisepretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihoodandproduceshighqualitysamples.Unfortunately,itremainsunabletocompetewithappropriatelyregularizedMLPsasaclassiﬁer.ThesecondwaytojointlytrainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmannmachine(Goodfellowetal.,2013b).Thismodelusesanalternativetrainingcriterionthatallowstheuseoftheback-propagationalgorithminordertoavoidtheproblemswithMCMCestimatesofthegradient.Unfortunately,thenewcriteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMCapproach,itdoesleadtosuperiorclassiﬁcationperformanceandabilitytoreasonwellaboutmissinginputs.ThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwereturntothegeneralviewofaBoltzmannmachineasconsistingofasetofunitsxwithaweightmatrixUandbiasesb.RecallfromEq.thatheenergyfunction20.2isgivenbyE() = x−x\\ue03eUxb−\\ue03ex.(20.36)Using diﬀerent sparsity patternsin theweightmatrixU, wecan implementstructuresofBoltzmannmachines,suchasRBMs,orDBMswithdiﬀerentnumbersoflayers.ThisisaccomplishedbypartitioningxintovisibleandhiddenunitsandzeroingoutelementsofUforunitsthatdonotinteract.ThecenteredBoltzmannmachineintroducesavectorthatissubtractedfromallofthestates:µE\\ue030(;) = ()xUb,−xµ−\\ue03eUxµxµ(−)(−−)\\ue03eb.(20.37)Typicallyµisahyperparameterﬁxedatthebeginningoftraining.Itisusu-allychosentomakesurethatxµ−≈0whenthemodelisinitialized.Thisreparametrizationdoesnotchangethesetofprobabilitydistributionsthatthemodelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescentappliedtothelikelihood.Speciﬁcally,inmanycases,thisreparametrizationresultsinaHessianmatrixthatisbetterconditioned.Melchior2013etal.()experimentally675'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 690}, page_content='CHAPTER20.DEEPGENERATIVEMODELSconﬁrmedthattheconditioningoftheHessianmatriximproves,andobservedthatthecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,theenhancedgradient(,).TheimprovedconditioningofChoetal.2011theHessianmatrixallowslearningtosucceed,evenindiﬃcultcasesliketrainingadeepBoltzmannmachinewithmultiplelayers.TheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-predictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemeanﬁeldequationsasdeﬁningafamilyofrecurrentnetworksforapproximatelysolvingeverypossibleinferenceproblem(,).RatherthantrainingGoodfellowetal.2013bthemodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrentnetworkobtainanaccurateanswertothecorrespondinginferenceproblem.ThetrainingprocessisillustratedinFig..Itconsistsofrandomlysamplinga20.5trainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,andthentrainingtheinferencenetworktopredictthevaluesoftheremainingunits.Thisgeneralprincipleofback-propagatingthroughthecomputationalgraphforapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011etal.,;Brakel2013etal.,).InthesemodelsandintheMP-DBM,theﬁnallossisnotthelowerboundonthelikelihood.Instead,theﬁnallossistypicallybasedontheapproximateconditionaldistributionthattheapproximateinferencenetworkimposesoverthemissingvalues.Thismeansthatthetrainingofthesemodelsissomewhatheuristicallymotivated.Ifweinspectthep(v)representedbytheBoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,inthesensethatGibbssamplingyieldspoorsamples.Back-propagationthroughtheinferencegraphhastwomainadvantages.First,ittrainsthemodelasitisreallyused—withapproximateinference.Thismeansthatapproximateinference,forexample,toﬁllinmissinginputs,ortoperformclassiﬁcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-DBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurateclassiﬁeronitsown;thebestclassiﬁcationresultswiththeoriginalDBMwerebasedontrainingaseparateclassiﬁertousefeaturesextractedbytheDBM,ratherthanbyusinginferenceintheDBMtocomputethedistributionovertheclasslabels.MeanﬁeldinferenceintheMP-DBMperformswellasaclassiﬁerwithoutspecialmodiﬁcations.Theotheradvantageofback-propagatingthroughapproximateinferenceisthatback-propagationcomputestheexactgradientoftheloss.ThisisbetterforoptimizationthantheapproximategradientsofSMLtraining,whichsuﬀerfrombothbiasandvariance.ThisprobablyexplainswhyMP-DBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining.676'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 691}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmannmachine.Eachrowindicatesadiﬀerentexamplewithinaminibatchforthesametrainingstep. Eachcolumnrepresentsatimestepwithinthemeanﬁeldinferenceprocess. Foreachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinferenceprocess.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthemeanﬁeldinferenceprocess,witharrowsindicatingwhichvariablesinﬂuencewhichothervariablesintheprocess.Inpracticalapplications,weunrollmeanﬁeldforseveralsteps.Inthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocesscouldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstotheinferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessforeachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationtotraintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.ThistrainsthemeanﬁeldprocessfortheMP-DBMtoproduceaccurateestimates.Figureadaptedfrom().Goodfellowetal.2013b677'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 692}, page_content='CHAPTER20.DEEPGENERATIVEMODELSThedisadvantageofback-propagatingthroughtheapproximateinferencegraphisthatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristicapproximationofthegeneralizedpseudolikelihood.TheMP-DBMinspiredtheNADE-k(Raiko2014etal.,)extensiontotheNADEframework,whichisdescribedinSec..20.10.10TheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-rametersamongmanydiﬀerentcomputationalgraphs,withthediﬀerencebetweeneachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalsosharesparametersacrossmanycomputationalgraphs.InthecaseoftheMP-DBM,thediﬀerencebetweenthegraphsiswhethereachinputunitisobservedornot.Whenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasinthecaseofdropout.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.OnecouldimagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunitsratherthanmakingthemlatent.20.5BoltzmannMachinesforReal-ValuedDataWhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,manyapplicationssuchasimageandaudiomodelingseemtorequiretheabilitytorepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossibletotreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofabinaryvariable.Forexample,Hinton2000()treatsgrayscaleimagesinthetrainingsetasdeﬁning[0,1]probabilityvalues.Eachpixeldeﬁnestheprobabilityofabinaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfromeachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscaleimagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach,andbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.Inthissection,wepresentBoltzmannmachinesthatdeﬁneaprobabilitydensityoverreal-valueddata.20.5.1Gaussian-BernoulliRBMsRestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamilyconditionaldistributions(Welling2005etal.,).Ofthese,themostcommonistheRBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditionaldistributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisafunctionofthehiddenunits.TherearemanywaysofparametrizingGaussian-BernoulliRBMs.First,wemay678'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 693}, page_content='CHAPTER20.DEEPGENERATIVEMODELSchoosewhethertouseacovariancematrixoraprecisionmatrixfortheGaussiandistribution.Herewepresenttheprecisionformulation.Themodiﬁcationtoobtainthecovarianceformulationisstraightforward. Wewishtohavetheconditionaldistributionp,() = (;vh|NvWhβ−1).(20.38)Wecanﬁndthetermsweneedtoaddtotheenergyfunctionbyexpandingtheunnormalizedlogconditionaldistribution:log(;NvWhβ,−1) = −12()vWh−\\ue03eβvWhβ(−)+(f).(20.39)Herefencapsulatesallthetermsthatareafunctiononlyoftheparametersandnottherandomvariablesinthemodel.Wecandiscardfbecauseitsonlyroleistonormalizethedistribution,andthepartitionfunctionofwhateverenergyfunctionwechoosewillcarryoutthatrole.Ifweincludealloftheterms(withtheirsignﬂipped)involvingvfromEq.20.39inourenergyfunctionanddonotaddanyothertermsinvolvingv,thenourenergyfunctionwillrepresentthedesiredconditional.p()vh|Wehavesomefreedomregardingtheotherconditionaldistribution,p(hv|).NotethatEq.containsaterm20.3912h\\ue03eW\\ue03eβWh.(20.40)Thistermcannotbeincludedinitsentiretybecauseitincludeshihjterms.Thesecorrespondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,wewouldhavea linearfactormodelinsteadof arestrictedBoltzmannmachine.WhendesigningourBoltzmannmachine,wesimplyomitthesehihjcrossterms.Omittingthemdoesnotchangetheconditionalp(vh|)soEq.isstill20.39respected.However,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonlyasinglehi.Ifweassumeadiagonalprecisionmatrix,weﬁndthatforeachhiddenunithiwehaveaterm12hi\\ue058jβjW2j,i.(20.41)Intheabove,weusedthefactthath2i=hibecausehi∈{0,1}.Ifweincludethisterm(withitssignﬂipped)intheenergyfunction,thenitwillnaturallybiashitobeturnedoﬀwhentheweightsforthatunitarelargeandconnectedtovisibleunitswithhighprecision.Thechoiceofwhetherornottoincludethisbiastermdoesnotaﬀectthefamilyofdistributionsthemodelcanrepresent(assumingthat679'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 694}, page_content='CHAPTER20.DEEPGENERATIVEMODELSweincludebiasparametersforthehiddenunits)butitdoesaﬀectthelearningdynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivationsremainreasonableevenwhentheweightsrapidlyincreaseinmagnitude.OnewaytodeﬁnetheenergyfunctiononaGaussian-BernoulliRBMisthusE,(vh) =12v\\ue03e()()βv\\ue00c−vβ\\ue00c\\ue03eWhb−\\ue03eh(20.42)butwemayalsoaddextratermsorparametrizetheenergyintermsofthevarianceratherthanprecisionifwechoose.Inthisderivation,wehavenotincludedabiastermonthevisibleunits,butonecouldeasilybeadded.OneﬁnalsourceofvariabilityintheparametrizationofaGaussian-BernoulliRBMisthechoiceofhowtotreattheprecisionmatrix.Itmayeitherbeﬁxedtoaconstant(perhapsestimatedbasedonthemarginalprecisionofthedata)orlearned. Itmayalsobeascalartimestheidentitymatrix,oritmaybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobenon-diagonalinthiscontext,becausesomeoperationswouldthenrequireinvertingthematrix.Inthesectionsahead,wewillseethatotherformsofBoltzmannmachinespermitmodelingthecovariancestructure,usingvarioustechniquestoavoidinvertingtheprecisionmatrix.20.5.2UndirectedModelsofConditionalCovarianceWhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valueddata,()arguethattheGaussianRBMinductivebiasisnotRanzatoetal.2010awellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,especiallynaturalimages.Theproblemisthatmuchoftheinformationcontentpresentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthanintherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsandnottheirabsolutevalueswheremostoftheusefulinformationinimagesresides.SincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhehiddenunits,itcannotcaptureconditionalcovarianceinformation.Inresponsetothesecriticisms,alternativemodelshavebeenproposedthatattempttobetteraccountforthecovarianceofreal-valueddata.ThesemodelsincludethemeanandcovarianceRBM(mcRBM1),themean-productoft-distribution(mPoT)modelandthespikeandslabRBM(ssRBM).1Theterm“mcRBM”ispronouncedbysayingthenameofthelettersM-C-R-B-M;the“mc”isnotpronouncedlikethe“Mc”in“McDonald’s.”680'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 695}, page_content='CHAPTER20.DEEPGENERATIVEMODELSMeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen-dentlyencodetheconditionalmeanandcovarianceofallobservedunits.ThemcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovarianceunits.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM.TheotherhalfisacovarianceRBM(,),alsocalledacRBM,Ranzatoetal.2010awhosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow.Speciﬁcally,withbinarymeanunitsh()mandbinarycovarianceunitsh()c,themcRBMmodelisdeﬁnedasthecombinationoftwoenergyfunctions:Emc(xh,()m,h()c) = Em(xh,()m)+Ec(xh,()c),(20.43)whereEmisthestandardGaussian-BernoulliRBMenergyfunction:2Em(xh,()m) =12x\\ue03ex−\\ue058jx\\ue03eW:,jh()mj−\\ue058jb()mjh()mj,(20.44)andEcisthe cRBMenergyfunctionthat models theconditionalcovarianceinformation:Ec(xh,()c) =12\\ue058jh()cj\\ue010x\\ue03er()j\\ue0112−\\ue058jb()cjh()cj.(20.45)Theparameterr()jcorrespondstothecovarianceweightvectorassociatedwithh()cjandb()cisavectorofcovarianceoﬀsets.Thecombinedenergyfunctiondeﬁnesajointdistribution:pmc(xh,()m,h()c) =1Zexp\\ue06e−Emc(xh,()m,h()c)\\ue06f,(20.46)andacorrespondingconditionaldistributionovertheobservationsgivenh()mandh()casamultivariateGaussiandistribution:pmc(xh|()m,h()c) = N\\uf8eb\\uf8edxC;mcxh|\\uf8eb\\uf8ed\\ue058jW:,jh()mj\\uf8f6\\uf8f8,Cmcxh|\\uf8f6\\uf8f8.(20.47)NotethatthecovariancematrixCmcxh|=\\ue010\\ue050jh()cjr()jr()j\\ue03e+I\\ue011−1isnon-diagonalandthatWistheweightmatrixassociatedwiththeGaussianRBMmodelingthe2ThisversionoftheGaussian-BernoulliRBMenergyfunctionassumestheimagedatahaszeromean,perpixel.Pixeloﬀsetscaneasilybeaddedtothemodeltoaccountfornonzeropixelmeans.681'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 696}, page_content='CHAPTER20.DEEPGENERATIVEMODELSconditionalmeans.ItisdiﬃculttotrainthemcRBMviacontrastivedivergenceorpersistentcontrastivedivergencebecauseofitsnon-diagonalconditionalcovariancestructure.CDandPCDrequiresamplingfromthejointdistributionofxh,()m,h()cwhich,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals.However,inthemcRBM,samplingfrompmc(xh|()m,h()c)requirescomputing(Cmc)−1ateveryiterationoflearning.Thiscanbeanimpracticalcomputationalburdenforlargerobservations.()avoiddirectsamplingRanzatoandHinton2010fromtheconditionalpmc(xh|()m,h()c)bysamplingdirectlyfromthemarginalp(x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfreeNeal1993energy.Mean-ProductofStudent’s-distributionstThemean-productofStudent’st-distribution(mPoT)model(,)extendsthePoTmodel(Ranzatoetal.2010bWellingetal.,)inamannersimilartohowthemcRBMextendsthecRBM.This2003aisachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussianRBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionovertheobservationisamultivariateGaussian(withnon-diagonalcovariance)distribution;however,unlikethemcRBM,thecomplementaryconditionaldistributionoverthehiddenvariablesisgivenbyconditionallyindependentGammadistributions.TheGammadistributionG(k,θ) isaprobabilitydistributionoverpositiverealnumbers,withmeankθ.ItisnotnecessarytohaveamoredetailedunderstandingoftheGammadistributiontounderstandthebasicideasunderlyingthemPoTmodel.ThemPoTenergyfunctionis:EmPoT(xh,()m,h()c)(20.48)= Em(xh,()m)+\\ue058j\\ue012h()cj\\ue0121+12\\ue010r()j\\ue03ex\\ue0112\\ue013+(1−γj)logh()cj\\ue013(20.49)wherer()jisthecovarianceweightvectorassociatedwithunith()cjandEm(xh,()m)isasdeﬁnedinEq..20.44JustaswiththemcRBM,themPoTmodelenergyfunctionspeciﬁesamul-tivariateGaussian,withaconditionaldistributionoverxthathasnon-diagonalcovariance.LearninginthemPoTmodel—again,likethemcRBM—iscompli-catedbytheinabilityto samplefromthe non-diagonalGaussianconditionalpmPoT(xh|()m,h()c),so()alsoadvocatedirectsamplingofRanzatoetal.2010bp()xviaHamiltonian(hybrid)MonteCarlo.682'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 697}, page_content='CHAPTER20.DEEPGENERATIVEMODELSSpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestrictedBoltzmannmachines(,)orssRBMsprovideanothermeansCourvilleetal.2011ofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,ssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonianMonteCarlomethods. Asamodelofnaturalimages,thessRBMisinterestinginthat,likethemcRBMandthemPoTmodel,itsbinaryhiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseofauxiliaryreal-valuedvariables.ThespikeandslabRBMhastwosetsofhiddenunits:binaryunitsspikeh,andreal-valuedunitsslabs.Themeanofthevisibleunitsconditionedonthehiddenunitsisgivenby(hs\\ue00c)W\\ue03e.Inotherwords,eachcolumnW:,ideﬁnesacomponentthatcanappearintheinputwhenhi= 1.Thecorrespondingspikevariablehidetermineswhetherthatcomponentispresentatall.Thecorrespondingslabvariablesideterminestheintensityofthatcomponent,ifitispresent.Whenaspikevariableisactive,thecorrespondingslabvariableaddsvariancetotheinputalongtheaxisdeﬁnedbyW:,i.Thisallowsustomodelthecovarianceoftheinputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergencewithGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix.Formally,thessRBMmodelisdeﬁnedviaitsenergyfunction:Ess() =xsh,,−\\ue058ix\\ue03eW:,isihi+12x\\ue03e\\ue020Λ+\\ue058iΦihi\\ue021x(20.50)+12\\ue058iαis2i−\\ue058iαiµisihi−\\ue058ibihi+\\ue058iαiµ2ihi,(20.51)wherebiistheoﬀsetofthespikehiandΛisadiagonalprecisionmatrixontheobservationsx.Theparameterαi>0isascalarprecisionparameterforthereal-valuedslabvariablesi.TheparameterΦiisanon-negativediagonalmatrixthatdeﬁnesanh-modulatedquadraticpenaltyonx.Eachµiisameanparameterfortheslabvariablesi.Withthejointdistributiondeﬁnedviatheenergyfunction,itisrelativelystraightforwardtoderivethe ssRBMconditional distributions.For example,bymarginalizingouttheslabvariabless,theconditionaldistributionovertheobservationsgiventhebinaryspikevariablesisgivenby:hpss()=xh|1P()h1Z\\ue05aexp(){−Exsh,,}ds(20.52)=N\\ue020xC;ssxh|\\ue058iW:,iµihi,Cssxh|\\ue021(20.53)683'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 698}, page_content='CHAPTER20.DEEPGENERATIVEMODELSwhereCssxh|=\\ue000Λ+\\ue050iΦihi−\\ue050iα−1ihiW:,iW\\ue03e:,i\\ue001−1.ThelastequalityholdsonlyifthecovariancematrixCssxh|ispositivedeﬁnite.Gatingbythespikevariablesmeansthatthetruemarginaldistributionoverhs\\ue00cissparse.Thisisdiﬀerentfromsparsecoding,wheresamplesfromthemodel“almostnever”(inthemeasuretheoreticsense)containzerosinthecode,andMAPinferenceisrequiredtoimposesparsity.ComparingthessRBMtothemcRBMandthemPoTmodels,thessRBMparametrizestheconditionalcovarianceoftheobservationinasigniﬁcantlydiﬀerentway.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservationas\\ue010\\ue050jh()cjr()jr()j\\ue03e+I\\ue011−1,usingtheactivationofthehiddenunitshj>0toenforceconstraintsontheconditionalcovarianceinthedirectionr()j.Incontrast,thessRBMspeciﬁestheconditionalcovarianceoftheobservationsusingthehiddenspikeactivationshi= 1topinchtheprecisionmatrixalongthedirectionspeciﬁedbythecorrespondingweightvector. ThessRBMconditionalcovarianceisverysimilartothatgivenbyadiﬀerentmodel:theproductofprobabilisticprincipalcomponentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercompletesetting,sparseactivationswiththessRBMparametrizationpermitsigniﬁcantvariance(abovethenominalvariancegivenbyΛ−1)onlyintheselecteddirectionsofthesparselyactivatedhi. InthemcRBMormPoTmodels,anovercompleterepresentationwouldmeanthattocapturevariationinaparticulardirectionintheobservationspacerequiresremovingpotentiallyallconstraintswithpositiveprojectioninthatdirection. Thiswouldsuggestthatthesemodelsarelesswellsuitedtotheovercompletesetting.TheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachineisthatsomesettingsoftheparameterscancorrespondtoacovariancematrixthatisnotpositivedeﬁnite.Suchacovariancematrixplacesmoreunnormalizedprobabilityonvaluesthatarefartherfromthemean,causingtheintegraloverallpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimpleheuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Usingconstrainedoptimizationtoexplicitlyavoidtheregionswheretheprobabilityisundeﬁnedisdiﬃculttodowithoutbeingoverlyconservativeandalsopreventingthemodelfromaccessinghigh-performingregionsofparameterspace.Qualitatively,convolutionalvariantsofthessRBMproduceexcellentsamplesofnaturalimages.SomeexamplesareshowninFig..16.1ThessRBMallowsforseveralextensions.Includinghigher-orderinteractionsandaverage-poolingoftheslabvariables(,)enablesthemodelCourvilleetal.2014tolearnexcellentfeaturesforaclassiﬁerwhenlabeleddataisscarce.Addinga684'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 699}, page_content='CHAPTER20.DEEPGENERATIVEMODELStermtotheenergyfunctionthatpreventsthepartitionfunctionfrombecomingundeﬁnedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellowetal.,),alsoknownasS3C.2013d20.6ConvolutionalBoltzmannMachinesAsseeninChapter,extremelyhighdimensionalinputssuchasimagesplace9greatstrainonthecomputation,memoryandstatisticalrequirementsofmachinelearningmodels.Replacingmatrixmultiplicationbydiscreteconvolutionwithasmallkernelisthestandardwayofsolvingtheseproblemsforinputsthathavetranslationinvariantspatialortemporalstructure.()DesjardinsandBengio2008showedthatthisapproachworkswellwhenappliedtoRBMs.Deepconvolutionalnetworksusuallyrequireapoolingoperationsothatthespatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworksoftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled.Itisunclearhowtogeneralizethistothesettingofenergy-basedmodels.Wecouldintroduceabinarypoolingunitpovernbinarydetectorunitsdandenforcep=maxidibysettingtheenergyfunctiontobe∞wheneverthatconstraintisviolated.Thisdoesnotscalewellthough,asitrequiresevaluating2ndiﬀerentenergyconﬁgurationstocomputethenormalizationconstant.Forasmall3×3poolingregionthisrequires29= 512energyfunctionevaluationsperpoolingunit!Lee2009etal.()developedasolutiontothisproblemcalledprobabilisticmaxpooling(nottobeconfusedwith“stochasticpooling,” whichisatechniqueforimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).Thestrategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitssoatmostonemaybeactiveatatime.Thismeansthereareonlyn+ 1totalstates(onestateforeachofthendetectorunitsbeingon,andanadditionalstatecorrespondingtoallofthedetectorunitsbeingoﬀ).Thepoolingunitisonifandonlyifoneofthedetectorunitsison.Thestatewithallunitsoﬀisassignedenergyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethathasn+1states,orequivalentlyasamodelthathasn+1variablesthatassignsenergytoallbutjointassignmentsofvariables.∞n+1Whileeﬃcient,probabilisticmaxpoolingdoesforcethedetectorunitstobemutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontextsoraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupportoverlappingpoolingregions.Overlappingpoolingregionsareusuallyrequiredtoobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothisconstraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann685'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 700}, page_content='CHAPTER20.DEEPGENERATIVEMODELSmachines.Lee2009etal.()demonstratedthatprobabilisticmaxpoolingcouldbeusedtobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperformoperationssuchasﬁllinginmissingportionsofitsinput.Whileintellectuallyappealing,thismodelischallengingtomakeworkinpractice,andusuallydoesnotperformaswellasaclassiﬁerastraditionalconvolutionalnetworkstrainedwithsupervisedlearning.Manyconvolutionalmodelsworkequallywellwithinputsofmanydiﬀerentspatialsizes.ForBoltzmannmachines,itisdiﬃculttochangetheinputsizeforavarietyofreasons. Thepartitionfunctionchangesasthesizeoftheinputchanges.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscalingupthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscalingBoltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneuralnetworkscanuseaﬁxednumberofpoolingunitsanddynamicallyincreasethesizeoftheirpoolingregionsinordertoobtainaﬁxed-sizerepresentationofavariable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometooexpensiveforthenaiveapproach. Theapproachof()ofmakingLeeetal.2009eachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolvesthecomputationalproblems,butstilldoesnotallowvariable-sizepoolingregions.Forexample,supposewelearnamodelwith2×2probabilisticmaxpoolingoverdetectorunitsthatlearnedgedetectors. Thisenforcestheconstraintthatonlyoneoftheseedgesmayappearineach2×2region.Ifwethenincreasethesizeoftheinputimageby50%ineachdirection,wewouldexpectthenumberofedgestoincreasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby50%ineachdirectionto3×3,thenthemutualexclusivityconstraintnowspeciﬁesthateachoftheseedgesmayonlyappearonceina3×3region.Aswegrowamodel’sinputimageinthisway,themodelgeneratesedgeswithlessdensity.Ofcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsofpoolinginordertoemitaﬁxed-sizeoutputvector.Modelsthatuseprobabilisticmaxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputofthemodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage.Pixelsattheboundaryoftheimagealsoposesomediﬃculty,whichisexac-erbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.Ifwedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthanvisibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled3Thepublicationdescribesthemodelasa“deepbeliefnetwork”butbecauseitcanbedescribedasapurelyundirectedmodelwithtractablelayer-wisemeanﬁeldﬁxedpointupdates,itbestﬁtsthedeﬁnitionofadeepBoltzmannmachine.686'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 701}, page_content='CHAPTER20.DEEPGENERATIVEMODELSwellbecausetheylieinthereceptiveﬁeldoffewerhiddenunits.However,ifwedoimplicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenbyfewerinputpixels,andmayfailtoactivatewhenneeded.20.7BoltzmannMachinesforStructuredorSequentialOutputsInthestructuredoutputscenario,wewishtotrainamodelthatcanmapfromsomeinputxtosomeoutputy,andthediﬀerententriesofyarerelatedtoeachotherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,yisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance.Anaturalwaytorepresenttherelationshipsbetweentheentriesinyistouseaprobabilitydistributionp(y|x).Boltzmannmachines,extendedtomodelconditionaldistributions,cansupplythisprobabilisticmodel.ThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeusednotjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelattercase,ratherthanmappinganinputxtoanoutputy,themodelmustestimateaprobabilitydistributionoverasequenceofvariables,p(x(1),...,x()τ).ConditionalBoltzmannmachinescanrepresentfactorsoftheformp(x()t|x(1),...,x(1)t−)inordertoaccomplishthistask.Animportantsequencemodelingtaskforthevideogameandﬁlmindustryismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters.Thesesequencesareoftencollectedusingmotioncapturesystemstorecordthemovementsofactors.Aprobabilisticmodelofacharacter’smovementallowsthegeneration ofnew, previouslyunseen, butrealistic animations.Tosolvethissequencemodelingtask,Taylor2007etal.()introducedaconditionalRBMmodelingp(x()t|x(1)t−,...,x()tm−)forsmallm.ThemodelisanRBMoverp(x()t)whosebiasparametersarealinearfunctionoftheprecedingmvaluesofx.Whenweconditionondiﬀerentvaluesofx(1)t−andearliervariables,wegetanewRBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningondiﬀerentpastvalues,wecanchangetheprobabilityofdiﬀerenthiddenunitsintheRBMbeingactive.Byactivatinganddeactivatingdiﬀerentsubsetsofhiddenunits,wecanmakelargechangestotheprobabilitydistributioninducedonx. OthervariantsofconditionalRBM(,)andothervariantsofsequenceMnihetal.2011modelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever,;etal.,;2009Boulanger-Lewandowski2012etal.,).Anothersequencemodelingtaskistomodelthedistributionoversequences687'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 702}, page_content='CHAPTER20.DEEPGENERATIVEMODELSofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012etal.()introducedthesequencemodelandappliedittothistask.TheRNN-RNN-RBMRBMisagenerativemodelofasequenceofframesx()tconsistingofanRNNthatemitstheRBMparametersforeachtimestep.Unlikethemodeldescribedabove,theRNNemitsalloftheparametersoftheRBM,includingtheweights.Totrainthemodel,weneedtobeabletoback-propagatethegradientofthelossfunctionthroughtheRNN.ThelossfunctionisnotapplieddirectlytotheRNNoutputs.Instead, itisappliedtotheRBM.ThismeansthatwemustapproximatelydiﬀerentiatethelosswithrespecttotheRBMparametersusingcontrastivedivergenceorarelatedalgorithm.Thisapproximategradientmaythenbeback-propagatedthroughtheRNNusingtheusualback-propagationthroughtimealgorithm.20.8OtherBoltzmannMachinesManyothervariantsofBoltzmannmachinesarepossible.Boltzmannmachinesmaybeextendedwithdiﬀerenttrainingcriteria.WehavefocusedonBoltzmannmachinestrainedtoapproximatelymaximizethegenerativecriterionlogp(v).ItisalsopossibletotraindiscriminativeRBMsthataimtomaximizelogp(y|v)instead(,).ThisapproachoftenLarochelleandBengio2008performsthebestwhenusingalinearcombinationofboththegenerativeandthediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerfulsupervisedlearnersasMLPs,atleastusingexistingmethodology.MostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractionsintheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmanytermsandeachindividualtermonlyincludestheproductbetweentworandomvariables.AnexampleofsuchatermisviWi,jhj.Itisalsopossibletotrainhigher-orderBoltzmannmachines(,)whoseenergyfunctiontermsSejnowski1987involvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweenahiddenunitandtwodiﬀerentimagescanmodelspatialtransformationsfromoneframeofvideotothenext(,,).MultiplicationbyaMemisevicandHinton20072010one-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunitsdependingonwhichclassispresent(,).OnerecentexampleNairandHinton2009oftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsofhiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisibleunitsvandtheclasslabely,andanothergroupofhiddenunitsthatinteractonlywiththevinputvalues(,).ThiscanbeinterpretedasencouragingLuoetal.2011somehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevantto688'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 703}, page_content='CHAPTER20.DEEPGENERATIVEMODELStheclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthatarenecessaryforthesamplesofvtoberealisticbutdonotdeterminetheclassoftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures.Sohn2013etal.()introducedaBoltzmannmachinewiththird-orderinteractionswithbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemaskingvariablesaresettozero,theyremovetheinﬂuenceofavisibleunitonthehiddenunits.Thisallowsvisibleunitsthatarenotrelevanttotheclassiﬁcationproblemtoberemovedfromtheinferencepathwaythatestimatestheclass.Moregenerally,theBoltzmannmachineframeworkisarichspaceofmodelspermittingmanymoremodelstructuresthanhavebeenexploredsofar.DevelopinganewformofBoltzmannmachinerequiressomemorecareandcreativitythandevelopinganewneuralnetworklayer,becauseitisoftendiﬃculttoﬁndanenergyfunctionthatmaintainstractabilityofallofthediﬀerentconditionaldistributionsneededtousetheBoltzmannmachine,butdespitethisrequiredeﬀorttheﬁeldremainsopentoinnovation.20.9Back-PropagationthroughRandomOperationsTraditionalneuralnetworksimplementadeterministictransformationofsomeinputvariablesx.Whendevelopinggenerativemodels,weoftenwishtoextendneuralnetworkstoimplementstochastictransformationsofx.Onestraightforwardwaytodothisistoaugmenttheneuralnetworkwithextrainputszthataresampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussiandistribution.Theneuralnetworkcanthencontinuetoperformdeterministiccomputationinternally, butthe functionf(xz,)willappearstochastictoanobserverwhodoesnothaveaccess toz.Providedthatfiscontinuousanddiﬀerentiable,wecanthencomputethegradientsnecessaryfortrainingusingback-propagationasusual.Asanexample,letusconsidertheoperationconsistingofdrawingsamplesyfromaGaussiandistributionwithmeanandvarianceµσ2:y∼N(µ,σ2).(20.54)Becauseanindividualsampleofyisnotproducedbyafunction,butratherbyasamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseemcounterintuitivetotakethederivativesofywithrespecttotheparametersofitsdistribution,µandσ2.However, wecanrewritethesamplingprocessastransforminganunderlyingrandomvaluez∼N(z;0,1)toobtainasamplefrom689'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 704}, page_content='CHAPTER20.DEEPGENERATIVEMODELSthedesireddistribution:yµσz= +(20.55)Wearenowabletoback-propagatethroughthesamplingoperation,byregard-ingitasadeterministicoperationwithanextrainputz.Crucially,theextrainputisarandomvariablewhosedistributionisnotafunctionofanyofthevariableswhosederivativeswewanttocalculate. Theresulttellsushowaninﬁnitesimalchangeinµorσwouldchangetheoutputifwecouldrepeatthesamplingoperationagainwiththesamevalueofz.Beingabletoback-propagatethroughthissamplingoperationallowsustoincorporateitintoalargergraph.Wecanbuildelementsofthegraphontopoftheoutputofthesamplingdistribution.Forexample,wecancomputethederivativesofsomelossfunctionJ(y).Wecanalsobuildelementsofthegraphwhoseoutputsaretheinputsortheparametersofthesamplingoperation.Forexample,wecouldbuildalargergraphwithµ=f(x;θ)andσ=g(x;θ).Inthisaugmentedgraph,wecanuseback-propagationthroughthesefunctionstoderive∇θJy().TheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-cable.Wecanexpressanyprobabilitydistributionoftheformp(y;θ)orp(y|x;θ)asp(y|ω),whereωisavariablecontainingbothparametersθ,andifapplicable,theinputsx.Givenavalueysampledfromdistributionp(y|ω),whereωmayinturnbeafunctionofothervariables,wecanrewriteyy ∼p(|ω)(20.56)asyzω= (f;),(20.57)wherezisasourceofrandomness.Wemaythencomputethederivativesofywithrespecttoωusingtraditionaltoolssuchastheback-propagationalgorithmappliedtof,solongasfiscontinuousanddiﬀerentiablealmosteverywhere.Crucially,ωmustnotbeafunctionofz,andzmustnotbeafunctionofω.Thistechniqueisoftencalledthereparametrizationtrickstochasticback-propagationperturbation,oranalysis.Therequirementthatfbecontinuousanddiﬀerentiableofcourserequiresytobecontinuous.Ifwewishtoback-propagatethroughasamplingprocessthatproducesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradientonω,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCEalgorithm(,),discussedinSec..Williams199220.9.1Inneuralnetworkapplications,wetypicallychooseztobedrawnfromsomesimpledistribution,suchasaunituniformorunitGaussiandistribution,and690'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 705}, page_content='CHAPTER20.DEEPGENERATIVEMODELSachievemorecomplexdistributionsbyallowingthedeterministicportionofthenetworktoreshapeitsinput.Theideaofpropagatinggradientsoroptimizingthroughstochasticoperationsdatesbacktothemid-twentiethcentury(,;,)andwasPrice1958Bonnet1964ﬁrstusedformachinelearninginthecontextofreinforcementlearning(,Williams1992). Morerecently,ithasbeenappliedtovariationalapproximations(OpperandArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengioetal.,;,;2013bKingma2013KingmaandWelling2014baRezende2014,,;etal.,;Goodfellow2014cetal.,).Manynetworks,suchasdenoisingautoencodersornetworksregularized withdropout,arealso naturallydesigned totakenoiseasaninputwithoutrequiringanyspecialreparametrizationtomakethenoiseindependentfromthemodel.20.9.1Back-PropagatingthroughDiscreteStochasticOperationsWhenamodelemitsadiscretevariabley,thereparametrizationtrickisnotapplicable.Supposethat themodel takesinputsxandparametersθ, bothencapsulatedinthevectorω,andcombinesthemwithrandomnoiseztoproducey:yzω= (f;).(20.58)Becauseyisdiscrete,fmustbeastepfunction.Thederivativesofastepfunctionarenotusefulatanypoint.Rightateachstepboundary,thederivativesareundeﬁned,butthatisasmallproblem.Thelargeproblemisthatthederivativesarezeroalmosteverywhere,ontheregionsbetweenstepboundaries.ThederivativesofanycostfunctionJ(y)thereforedonotgiveanyinformationforhowtoupdatethemodelparameters.θTheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor×OﬀsetReinforcement×CharacteristicEligibility)providesaframeworkdeﬁningafamilyofsimplebutpowerfulsolutions(,).ThecoreideaisthatWilliams1992eventhoughJ(f(z;ω))isastepfunctionwithuselessderivatives,theexpectedcostEzz∼p()Jf((;))zωisoftenasmoothfunctionamenabletogradientdescent.Althoughthatexpectationistypicallynottractablewhenyishigh-dimensional(oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbeestimatedwithoutbiasusingaMonteCarloaverage.ThestochasticestimateofthegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimizationtechniques.ThesimplestversionofREINFORCEcanbederivedbysimplydiﬀerentiating691'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 706}, page_content='CHAPTER20.DEEPGENERATIVEMODELStheexpectedcost:Ez[()] =Jy\\ue058yJp()y()y(20.59)∂JE[()]y∂ω=\\ue058yJ()y∂p()y∂ω(20.60)=\\ue058yJp()y()y∂plog()y∂ω(20.61)≈1mm\\ue058y()i∼p,i()y=1J(y()i)∂plog(y()i)∂ω.(20.62)Eq.reliesontheassumptionthat20.60Jdoesnotreferenceωdirectly.Itistrivialtoextendtheapproachtorelaxthisassumption.Eq.exploitsthederivative20.61ruleforthelogarithm,∂plog()y∂ω=1p()y∂p()y∂ω.Eq.givesanunbiasedMonte20.62Carloestimatorofthegradient.Anywherewewritep(y)inthissection,onecouldequallywritep(yx|).Thisisbecausep(y)isparametrizedbyω,andωcontainsbothθandx,ifxispresent.OneissuewiththeabovesimpleREINFORCEestimatoristhatithasaveryhighvariance,sothatmanysamplesofyneedtobedrawntoobtainagoodestimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwillconvergeveryslowlyandwillrequireasmallerlearningrate.Itispossibletoconsiderablyreducethevarianceofthatestimatorbyusingvariancereductionmethods(,;,).TheideaistomodifytheestimatorsoWilson1984L’Ecuyer1994thatitsexpectedvalueremainsunchangedbutitsvariancegetreduced. InthecontextofREINFORCE,theproposedvariancereductionmethodsinvolvethecomputationofabaselinethatisusedtooﬀsetJ(y). Notethatanyoﬀsetb(ω)thatdoesnotdependonywouldnotchangetheexpectationoftheestimatedgradientbecauseEp()y\\ue014∂plog()y∂ω\\ue015=\\ue058yp()y∂plog()y∂ω(20.63)=\\ue058y∂p()y∂ω(20.64)=∂∂ω\\ue058yp() =y∂∂ω1 = 0,(20.65)692'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 707}, page_content='CHAPTER20.DEEPGENERATIVEMODELSwhichmeansthatEp()y\\ue014(()())Jy−bω∂plog()y∂ω\\ue015= Ep()y\\ue014J()y∂plog()y∂ω\\ue015−bE()ωp()y\\ue014∂plog()y∂ω\\ue015(20.66)= Ep()y\\ue014J()y∂plog()y∂ω\\ue015.(20.67)Furthermore,wecanobtaintheoptimalb(ω) bycomputingthevarianceof(J(y)−b(ω))∂plog()y∂ωunderp(y)andminimizingwithrespecttob(ω).Whatweﬁndisthatthisoptimalbaselineb∗()ωiisdiﬀerentforeachelementωiofthevector:ωb∗()ωi=Ep()y\\ue068J()y∂plog()y∂ωi2\\ue069Ep()y\\ue068∂plog()y∂ωi2\\ue069.(20.68)Thegradientestimatorwithrespecttoωithenbecomes(()()Jy−bωi)∂plog()y∂ωi(20.69)whereb(ω)iestimatestheaboveb∗(ω)i.TheestimatebisusuallyobtainedbyaddingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimateEp()y[J(y)∂plog()y∂ωi2]andEp()y\\ue068∂plog()y∂ωi2\\ue069foreachelementofω.Theseextraoutputscanbetrainedwiththemeansquarederrorobjective,usingrespectivelyJ(y)∂plog()y∂ωi2and∂plog()y∂ωi2astargetswhenyissampledfromp(y),foragivenω.TheestimatebmaythenberecoveredbysubstitutingtheseestimatesintoEq.20.68.()preferredtouseasinglesharedoutput(acrossallMnihandGregor2014elementsiofω)trainedwiththetargetJ(y),usingasbaselineb(ω)≈Ep()y[J(y)].Variancereductionmethodshavebeenintroducedinthereinforcementlearningcontext(,;Suttonetal.2000WeaverandTao2001,),generalizingpreviousworkonthecaseofbinaryrewardbyDayan1990Bengio2013bMnih(). Seeetal.(),andGregor2014Ba2014Mnih2014Xu2015(),etal.(),etal.(),oretal.()forexamplesofmodernusesoftheREINFORCEalgorithmwithreducedvarianceinthecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaselineb(ω)(,()foundthatthescaleofMnihandGregor2014J(y)−b(ω))couldbeadjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbyamovingaverageduringtraining,asakindofadaptivelearningrate,tocountertheeﬀectofimportantvariationsthatoccurduringthecourseoftraininginthemagnitudeofthisquantity.()calledthisheuristicMnihandGregor2014variancenormalization.693'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 708}, page_content='CHAPTER20.DEEPGENERATIVEMODELSREINFORCE-basedestimatorscanbeunderstoodasestimatingthegradientbycorrelatingchoicesofywithcorrespondingvaluesofJ(y).Ifagoodvalueofyisunlikelyunderthecurrentparametrization,itmighttakealongtimetoobtainitbychance,andgettherequiredsignalthatthisconﬁgurationshouldbereinforced.20.10DirectedGenerativeNetsAsdiscussedinChapter,directedgraphicalmodelsmakeupaprominentclass16ofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopularwithinthegreatermachinelearningcommunity,withinthesmallerdeeplearningcommunitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodelssuchastheRBM.Inthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthathavetraditionallybeenassociatedwiththedeeplearningcommunity.Wehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirectedmodel.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethoughtofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearnersinthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsamplegenerationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirectedmodels.20.10.1SigmoidBeliefNetsSigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodelNeal1990withaspeciﬁckindofconditionalprobabilitydistribution.Ingeneral,wecanthinkofasigmoidbeliefnetworkashavingavectorofbinarystatess,witheachelementofthestateinﬂuencedbyitsancestors:ps(i) = σ\\uf8eb\\uf8ed\\ue058j<iWj,isj+bi\\uf8f6\\uf8f8.(20.70)Themostcommonstructureofsigmoidbeliefnetworkisonethatisdividedintomanylayers,withancestralsamplingproceedingthroughaseriesofmanyhiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureisverysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningofthesamplingprocessareindependentfromeachother,ratherthansampledfromarestrictedBoltzmannmachine. Suchastructureisinterestingforavarietyof694'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 709}, page_content='CHAPTER20.DEEPGENERATIVEMODELSreasons.Onereasonisthatthestructureisauniversalapproximatorofprobabilitydistributionsoverthevisibleunits, inthesensethatitcanapproximateanyprobabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,evenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthevisiblelayer(SutskeverandHinton2008,).Whilegeneratingasampleofthevisibleunitsisveryeﬃcientinasigmoidbeliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiventhevisibleunitsisintractable.Meanﬁeldinferenceisalsointractablebecausethevariationallowerboundinvolvestakingexpectationsofcliquesthatencompassentirelayers.Thisproblemhasremaineddiﬃcultenoughtorestrictthepopularityofdirecteddiscretenetworks.Oneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstructadiﬀerentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Sauletal.1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.AnotherapproachistouselearnedinferencemechanismsasdescribedinSec. .The19.5Helmholtzmachine(Dayan1995DayanandHinton1996etal.,;,)isasigmoidbeliefnetworkcombinedwithaninferencenetworkthatpredictstheparametersofthemeanﬁelddistributionoverthehiddenunits.Modernapproaches(,Gregoretal.2014MnihandGregor2014;,)tosigmoidbeliefnetworksstillusethisinferencenetworkapproach.Thesetechniquesremaindiﬃcultduetothediscretenatureofthelatentvariables.Onecannotsimplyback-propagatethroughtheoutputoftheinferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback-propagatingthroughdiscretesamplingprocesses,describedinSec..Recent20.9.1approachesbasedonimportancesampling,reweightedwake-sleep(BornscheinandBengio2015,)andbidirectionalHelmholtzmachines(Bornschein2015etal.,)makeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-artperformanceonbenchmarktasks.Aspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatentvariables.Learninginthiscaseiseﬃcient,becausethereisnoneedtomarginalizelatentvariablesoutofthelikelihood. Afamilyofmodelscalledauto-regressivenetworksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariablesbesidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog-linearrelationships.Auto-regressivenetworksaredescribedlater,inSec..20.10.720.10.2DiﬀerentiableGeneratorNetsManygenerativemodelsarebasedontheideaofusingadiﬀerentiablegeneratornetwork.Themodeltransformssamplesoflatentvariablesztosamplesxor695'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 710}, page_content='CHAPTER20.DEEPGENERATIVEMODELStodistributionsoversamplesxusingadiﬀerentiablefunctiong(z;θ()g)whichistypicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariationalautoencoders, whichpair thegeneratornetwithaninferencenet, generativeadversarial networks, which pairthe generator network witha discriminatornetwork,andtechniquesthattraingeneratornetworksinisolation.Generatornetworksareessentiallyjustparametrizedcomputationalproceduresforgeneratingsamples,wherethearchitectureprovidesthefamilyofpossibledistributionstosamplefromandtheparametersselectadistributionfromwithinthatfamily.Asanexample,thestandardprocedurefordrawingsamplesfromanormaldistributionwithmeanµandcovarianceΣistofeedsampleszfromanormaldistributionwithzeromeanandidentitycovarianceintoaverysimplegeneratornetwork.Thisgeneratornetworkcontainsjustoneaﬃnelayer:xzLz= (g) = +µ(20.71)whereisgivenbytheCholeskydecompositionof.LΣPseudorandomnumbergeneratorscanalsousenonlineartransformationsofsimpledistributions.Forexample,inversetransformsampling(Devroye2013,)drawsascalarzfromU(0,1)andappliesanonlineartransformationtoascalarx.Inthiscaseg(z)isgivenbytheinverseofthecumulativedistributionfunctionF(x) =\\ue052x−∞p(v)dv.Ifweareabletospecifyp(x),integrateoverx,andinverttheresultingfunction,wecansamplefromwithoutusingmachinelearning.px()Togeneratesamplesfrommorecomplicateddistributionsthatarediﬃculttospecifydirectly,diﬃculttointegrateover,orwhoseresultingintegralsarediﬃculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamilyofnonlinearfunctionsg,andusetrainingdatatoinfertheparametersselectingthedesiredfunction.Wecanthinkofgasprovidinganonlinearchangeofvariablesthattransformsthedistributionoverintothedesireddistributionover.z xRecallfromEq.that,forinvertible,diﬀerentiable,continuous,3.47gpz() = zpx(())gz\\ue00c\\ue00c\\ue00c\\ue00cdet(∂g∂z)\\ue00c\\ue00c\\ue00c\\ue00c.(20.72)Thisimplicitlyimposesaprobabilitydistributionover:xpx() =xpz(g−1())x\\ue00c\\ue00c\\ue00cdet(∂g∂z)\\ue00c\\ue00c\\ue00c.(20.73)696'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 711}, page_content='CHAPTER20.DEEPGENERATIVEMODELSOfcourse,thisformulamaybediﬃculttoevaluate,dependingonthechoiceofg,soweoftenuseindirectmeansoflearningg,ratherthantryingtomaximizelog()pxdirectly.Insomecases,ratherthanusinggtoprovideasampleofxdirectly,weusegtodeﬁneaconditionaldistributionoverx.Forexample,wecoulduseageneratornetwhoseﬁnallayerconsistsofsigmoidoutputstoprovidethemeanparametersofBernoullidistributions:p(xi= 1 ) = ()|zgzi.(20.74)Inthiscase,whenweusegtodeﬁnep(xz|),weimposeadistributionoverxbymarginalizing:zp() = xEzp.()xz|(20.75)Bothapproachesdeﬁneadistributionpg(x)andallowustotrainvariouscriteriaofpgusingthereparametrizationtrickofSec..20.9Thetwodiﬀerentapproachestoformulatinggeneratornets—emittingtheparametersofaconditionaldistributionversusdirectlyemittingsamples—havecomplementarystrengthsandweaknesses.Whenthegeneratornetdeﬁnesaconditionaldistributionoverx,itiscapableofgeneratingdiscretedataaswellascontinuousdata.When thegenerator netprovidessamples directly, it iscapableofgeneratingonlycontinuousdata(wecouldintroducediscretizationintheforwardpropagation,butthiswouldlosetheabilitytolearnthemodelusingback-propagation).Theadvantagetodirectsamplingisthatwearenolongerforcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownandalgebraicallymanipulatedbyahumandesigner.Approachesbasedondiﬀerentiablegeneratornetworksaremotivatedbythesuccessofgradientdescentapplied todiﬀerentiablefeedforwardnetworks forclassiﬁcation. Inthecontextofsupervisedlearning,deepfeedforwardnetworkstrainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgivenenoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccesstransfertogenerativemodeling?Generativemodelingseemstobemorediﬃcultthanclassiﬁcationorregressionbecausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontextofdiﬀerentiablegeneratornets,thecriteriaareintractablebecausethedatadoesnotspecifyboththeinputszandtheoutputsxofthegeneratornet.Inthecaseofsupervisedlearning,boththeinputsxandtheoutputsyweregiven,andtheoptimizationprocedureneedsonlytolearnhowtoproducethespeciﬁedmapping.Inthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehowtoarrangespaceinausefulwayandadditionallyhowtomapfromto.zzx697'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 712}, page_content='CHAPTER20.DEEPGENERATIVEMODELSDosovitskiy2015etal.()studiedasimpliﬁedproblem,wherethecorrespondencebetweenzandxisgiven.Speciﬁcally,thetrainingdataiscomputer-renderedimageryofchairs.Thelatentvariableszareparametersgiventotherenderingenginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,andotherconﬁgurationdetailsthataﬀecttherenderingoftheimage.Usingthissyntheticallygenerateddata,aconvolutionalnetworkisabletolearntomapzdescriptionsofthecontentofanimagetoxapproximationsofrenderedimages.Thissuggeststhatcontemporarydiﬀerentiablegeneratornetworkshavesuﬃcientmodelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimizationalgorithmshavetheabilitytoﬁtthem.Thediﬃcultyliesindetermininghowtotraingeneratornetworkswhenthevalueofzforeachxisnotﬁxedandknownaheadofeachtime.Thefollowingsectionsdescribeseveralapproachestotrainingdiﬀerentiablegeneratornetsgivenonlytrainingsamplesof.x20.10.3VariationalAutoencodersThevariationalautoencoderVAEor(,;,)isaKingma2013Rezendeetal.2014directedmodelthatuseslearnedapproximateinferenceandcanbetrainedpurelywithgradient-basedmethods.Togenerateasamplefromthemodel,theVAEﬁrstdrawsasamplezfromthecodedistributionpmodel(z).Thesampleisthenrunthroughadiﬀerentiablegeneratornetworkg(z).Finally,xissampledfromadistributionpmodel(x;g(z)) =pmodel(xz|). However,duringtraining,theapproximateinferencenetwork(orencoder)q(zx|)isusedtoobtainzandpmodel(xz|)isthenviewedasadecodernetwork.Thekeyinsightbehindvariationalautoencodersisthattheymaybetrainedbymaximizingthevariationallowerboundassociatedwithdatapoint:L()qxL() = qEzzx∼q(|)logpmodel()+(())zx,Hqz|x(20.76)= Ezzx∼q(|)logpmodel()xz|−DKL(()qz|x||pmodel())z(20.77)≤logpmodel()x.(20.78)InEq.,werecognizetheﬁrsttermasthejointlog-likelihoodofthevisible20.76andhiddenvariablesundertheapproximateposterioroverthelatentvariables(justlikewithEM,exceptthatweuseanapproximateratherthantheexactposterior).Werecognizealsoasecondterm,theentropyoftheapproximateposterior.WhenqischosentobeaGaussiandistribution,withnoiseaddedtoapredictedmeanvalue,maximizingthisentropytermencouragesincreasingthestandarddeviation698'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 713}, page_content='CHAPTER20.DEEPGENERATIVEMODELSofthisnoise.Moregenerally,thisentropytermencouragesthevariationalposteriortoplacehighprobabilitymassonmanyzvaluesthatcouldhavegeneratedx,ratherthancollapsingtoasinglepointestimateofthemostlikelyvalue.InEq.20.77,werecognizetheﬁrsttermasthereconstructionlog-likelihoodfoundinotherautoencoders.Thesecondtermtriestomaketheapproximateposteriordistributionandthemodelpriorq()z|xpmodel()zapproacheachother.Traditionalapproachestovariationalinferenceandlearninginferqviaanoptimizationalgorithm,typicallyiteratedﬁxedpointequations(Sec.).These19.4approachesareslowandoftenrequiretheabilitytocomputeEz∼qlogpmodel(zx,)inclosedform.Themainideabehindthevariationalautoencoderistotrainaparametricencoder(alsosometimescalledaninferencenetworkorrecognitionmodel)thatproducestheparametersofq.Solongaszisacontinuousvariable,wecanthenback-propagatethroughsamplesofzdrawnfromq(zx|) =q(z;f(x;θ))inordertoobtainagradientwithrespecttoθ.LearningthenconsistssolelyofmaximizingLwithrespecttotheparametersoftheencoderanddecoder.AlloftheexpectationsinmaybeapproximatedbyMonteCarlosampling.LThevariationalautoencoderapproachiselegant,theoreticallypleasing,andsimpletoimplement.Italsoobtainsexcellentresultsandisamongthestateoftheartapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfromvariationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecausesofthisphenomenonarenotyetknown.Onepossibilityisthattheblurrinessisanintrinsiceﬀectofmaximumlikelihood,whichminimizesDKL(pdata\\ue06bpmodel).AsillustratedinFig.,thismeansthatthemodelwillassignhighprobabilityto3.6pointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytootherpoints.Theseotherpointsmayincludeblurryimages.PartofthereasonthatthemodelwouldchoosetoputprobabilitymassonblurryimagesratherthansomeotherpartofthespaceisthatthevariationalautoencodersusedinpracticeusuallyhaveaGaussiandistributionforpmodel(x;g(z)). Maximizingalowerboundonthelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoderwithmeansquarederror,inthesensethatithasatendencytoignorefeaturesoftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthebrightnessofthepixelsthattheyoccupy.ThisissueisnotspeciﬁctoVAEsandissharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,DKL(pdata\\ue06bpmodel),asarguedby()andby().AnotherTheisetal.2015Huszar2015troublingissuewithcontemporaryVAEmodelsisthattheytendtouseonlyasmallsubsetofthedimensionsofz,asiftheencoderwasnotabletotransformenoughofthelocaldirectionsininputspacetoaspacewherethemarginaldistributionmatchesthefactorizedprior.699'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 714}, page_content='CHAPTER20.DEEPGENERATIVEMODELSTheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodelarchitectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequireextremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwithadiversefamilyofdiﬀerentiableoperators. OneparticularlysophisticatedVAEisthedeeprecurrentattentionwriterDRAWormodel(,).Gregoretal.2015DRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattentionmechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentiallyvisitingdiﬀerentsmallimagepatchesanddrawingthevaluesofthepixelsatthosepoints.VAEscanalsobeextendedtogeneratesequencesbydeﬁningvariationalRNNs(Chung2015betal.,)byusingarecurrentencoderanddecoderwithintheVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonlynon-deterministicoperationsattheoutputspace.VariationalRNNsalsohaverandomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAElatentvariables.TheVAEframeworkhasbeenextendedtomaximizenotjustthetraditionalvariationallowerbound,butinsteadtheimportanceweightedautoencoder(Burdaetal.,)objective:2015Lk() = x,qEz(1),...,z()k∼|q(zx)\\ue022log1kk\\ue058i=1pmodel(xz,()i)q(z()i|x)\\ue023.(20.79)ThisnewobjectiveisequivalenttothetraditionallowerboundLwhenk=1.However,itmayalsobeinterpretedasforminganestimateofthetruelogpmodel(x)usingimportancesamplingofzfromproposaldistributionq(zx|).Theimportanceweightedautoencoderobjectiveisalsoalowerboundonlogpmodel(x) andbecomestighterasincreases.kVariationalautoencodershavesomeinterestingconnectionstotheMP-DBMandotherapproachesthatinvolveback-propagationthroughtheapproximateinferencegraph(,;Goodfellowetal.2013bStoyanov2011Brakel2013etal.,;etal.,).Thesepreviousapproachesrequiredaninferenceproceduresuchasmeanﬁeldﬁxedpointequationstoprovidethecomputationalgraph.Thevariationalautoencoderisdeﬁnedforarbitrarycomputationalgraphs,whichmakesitapplicabletoawiderrangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoiceofmodelstothosewithtractablemeanﬁeldﬁxedpointequations.Thevariationalautoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihoodofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremoreheuristicandhavelittleprobabilisticinterpretationbeyondmakingtheresultsofapproximateinferenceaccurate.Onedisadvantageofthevariationalautoencoderisthatitlearnsaninferencenetworkforonlyoneproblem,inferringzgivenx.700'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 715}, page_content='CHAPTER20.DEEPGENERATIVEMODELSTheoldermethodsareabletoperformapproximateinferenceoveranysubsetofvariablesgivenanyothersubsetofvariables,becausethemeanﬁeldﬁxedpointequationsspecifyhowtoshareparametersbetweenthecomputationalgraphsforallofthesediﬀerentproblems.Oneverynicepropertyofthevariationalautoencoderisthatsimultaneouslytrainingaparametricencoderincombinationwiththegeneratornetworkforcesthemodeltolearnapredictablecoordinatesystemthattheencodercancapture.Thismakesitanexcellentmanifoldlearningalgorithm.SeeFig.forexamples20.6oflow-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthecasesdemonstratedintheﬁgure,thealgorithmdiscoveredtwoindependentfactorsofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression.\\nFigure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani-folds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensionsmaybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingofhowthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievetheintrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenotexamplesfromthetrainingsetbutimagesxactuallygeneratedbythemodelp(xz|),simplybychangingthe2-D“code”z(eachimagecorrespondstoadiﬀerentchoiceof“code”zona2-Duniformgrid). (Left)Thetwo-dimensionalmapoftheFreyfacesmanifold.Onedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationoftheface,whiletheother(vertical)correspondstotheemotionalexpression.The(Right)two-dimensionalmapoftheMNISTmanifold.701'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 716}, page_content='CHAPTER20.DEEPGENERATIVEMODELS20.10.4GenerativeAdversarialNetworksGenerativeadversarialnetworksorGANs(,)areanotherGoodfellowetal.2014cgenerativemodelingapproachbasedondiﬀerentiablegeneratornetworks.Generativeadversarialnetworksarebasedonagametheoreticscenarioinwhichthegeneratornetworkmustcompeteagainstanadversary.Thegeneratornetworkdirectlyproducessamplesx=g(z;θ()g).Itsadversary,thediscriminatornetwork,attemptstodistinguishbetweensamplesdrawnfromthetrainingdataandsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvaluegivenbyd(x;θ()d),indicatingtheprobabilitythatxisarealtrainingexampleratherthanafakesampledrawnfromthemodel.Thesimplestwaytoformulatelearningingenerativeadversarialnetworksisasazero-sumgame,inwhichafunctionv(θ()g,θ()d)determinesthepayoﬀofthediscriminator.Thegeneratorreceives−v(θ()g,θ()d)asitsownpayoﬀ.Duringlearning,eachplayerattemptstomaximizeitsownpayoﬀ,sothatatconvergenceg∗= argmingmaxdvg,d.()(20.80)Thedefaultchoiceforisvv(θ()g,θ()d) = Ex∼pdatalog()+dxEx∼pmodellog(1())−dx.(20.81)Thisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasrealorfake.Simultaneously,thegeneratorattemptstofooltheclassiﬁerintobelievingitssamplesarereal.Atconvergence,thegenerator’ssamplesareindistinguishablefromrealdata,andthediscriminatoroutputs12everywhere.Thediscriminatormaythenbediscarded.ThemainmotivationforthedesignofGANsisthatthelearningprocessrequiresneitherapproximateinferencenorapproximationofapartitionfunctiongradient.Inthecasewheremaxdv(g,d)isconvexinθ()g(suchasthecasewhereoptimizationisperformeddirectlyinthespaceofprobabilitydensityfunctions)thentheprocedureisguaranteedtoconvergeandisasymptoticallyconsistent.Unfortunately,learninginGANscanbediﬃcultinpracticewhenganddarerepresentedbyneuralnetworksandmaxdv(g,d)isnotconvex.Goodfellow()identiﬁednon-convergenceasanissuethatmaycauseGANstounderﬁt.2014Ingeneral,simultaneousgradientdescentontwoplayers’costsisnotguaranteedtoreachanequilibrium.Considerforexamplethevaluefunctionv(a,b)=ab,whereoneplayercontrolsaandincurscostab,whiletheotherplayercontrolsbandreceivesacost−ab.Ifwemodeleachplayerasmakinginﬁnitesimallysmall702'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 717}, page_content='CHAPTER20.DEEPGENERATIVEMODELSgradientsteps,eachplayerreducingtheirowncostattheexpenseoftheotherplayer,thenaandbgointoastable,circularorbit,ratherthanarrivingattheequilibriumpointattheorigin.Notethattheequilibriaforaminimaxgamearenotlocalminimaofv.Instead,theyarepointsthataresimultaneouslyminimaforbothplayers’costs.Thismeansthattheyaresaddlepointsofvthatarelocalminimawithrespecttotheﬁrstplayer’sparametersandlocalmaximawithrespecttothesecondplayer’sparameters.Itispossibleforthetwoplayerstotaketurnsincreasingthendecreasingvforever,ratherthanlandingexactlyonthesaddlepointwhereneitherplayeriscapableofreducingtheircost.Itisnotknowntowhatextentthisnon-convergenceproblemaﬀectsGANs.Goodfellow 2014()identiﬁed analternative formulationof thepayoﬀs,inwhich thegame isnolonger zero-sum, that hasthe sameexpectedgradientasmaximumlikelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximumlikelihoodtrainingconverges,thisreformulationoftheGANgameshouldalsoconverge,givenenoughsamples.Unfortunatley,thisalternativeformulationdoesnotseemtoperformwellinpractice,possiblyduetosuboptimalityofthediscriminator,orpossiblyduetohighvariancearoundtheexpectedgradient.Inpractice,thebest-performingformulationoftheGANgameisadiﬀerentfor-mulationthatisneitherzero-sumnorequivalenttomaximumlikelihood,introducedbyGoodfellow2014cetal.()withaheuristicmotivation.Inthisbest-performingformulation,thegeneratoraimstoincreasethelogprobabilitythatthediscrimina-tormakesamistake,ratherthanaimingtodecreasethelogprobabilitythatthediscriminatormakesthecorrectprediction.Thisreformulationismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator’scostfunctionwithrespecttothediscriminator’slogitstoremainlargeeveninthesituationwherethediscriminatorconﬁdentlyrejectsallgeneratorsamples.StabilizationofGANlearningremainsanopenproblem.Fortunately,GANlearningperformswellwhenthemodelarchitectureandhyperparametersarecare-fullyselected.()craftedadeepconvolutionalGAN(DCGAN)Radfordetal.2015thatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepresentationspacecapturesimportantfactorsofvariation,asshowninFig..15.9SeeFig.forexamplesofimagesgeneratedbyaDCGANgenerator.20.7TheGANlearningproblemcanalsobesimpliﬁedbybreakingthegenerationprocessintomanylevelsofdetail.ItispossibletotrainconditionalGANs(MirzaandOsindero2014,)thatlearntosamplefromadistributionp(xy|)ratherthansimplysamplingfromamarginaldistributionp(x).Denton2015etal.()showedthataseriesofconditionalGANscanbetrainedtoﬁrstgenerateaverylow-resolutionversionofanimage,thenincrementallyadddetailstotheimage.703'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 718}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.7: ImagesgeneratedbyGANstrainedontheLSUNdataset.(Left)ImagesofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadfordetal.(). ImagesofchurchesgeneratedbyaLAPGANmodel,reproduced2015(Right)withpermissionfrom().Dentonetal.2015ThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramidtogeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgeneratorsareabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,withexperimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkasbeingrealdata.SeeFig.forexamplesofimagesgeneratedbyaLAPGANgenerator.20.7OneunusualcapabilityoftheGANtrainingprocedureisthatitcanﬁtproba-bilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthanmaximizingthelogprobabilityofspeciﬁcpoints,thegeneratornetlearnstotraceoutamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-doxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinﬁnitytothetestset,whilestillrepresentingamanifoldthatahumanobserverjudgestocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageoradisadvantage,andonemayalsoguaranteethatthegeneratornetworkassignsnon-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegeneratornetworkaddGaussiannoisetoallofthegeneratedvalues. GeneratornetworksthataddGaussiannoiseinthismannersamplefromthesamedistributionthatoneobtainsbyusingthegeneratornetworktoparametrizethemeanofaconditionalGaussiandistribution.Dropoutseemstobeimportantinthediscriminatornetwork. Inparticular,units shouldbe stochasticallydropped whilecomputing the gradient for thegeneratornetworktofollow.Followingthegradientofthedeterministicversionofthediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseﬀective.704'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 719}, page_content='CHAPTER20.DEEPGENERATIVEMODELSLikewise,neverusingdropoutseemstoyieldpoorresults.WhiletheGANframeworkisdesignedfordiﬀerentiablegeneratornetworks,similarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-supervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogisticregressiondiscriminator(Welling2002etal.,).20.10.5GenerativeMomentMatchingNetworksGenerativemomentmatchingnetworks(,;,)Lietal.2015Dziugaiteetal.2015areanotherformofgenerativemodelbasedondiﬀerentiablegeneratornetworks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetworkwithanyothernetwork—neitheraninferencenetworkasusedwithVAEsnoradiscriminatornetworkasusedwithGANs.Thesenetworksaretrainedwithatechniquecalled.Themomentmatchingbasicideabehindmomentmatchingistotrainthegeneratorinsuchawaythatmanyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossibletothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,amomentisanexpectationofdiﬀerentpowersofarandomvariable.Forexample,theﬁrstmomentisthemean,thesecondmomentisthemeanofthesquaredvalues,andsoon.Inmultipledimensions,eachelementoftherandomvectormayberaisedtodiﬀerentpowers,sothatamomentmaybeanyquantityoftheformExΠixnii(20.82)wheren= [n1,n2,...,nd]\\ue03eisavectorofnon-negativeintegers.Uponﬁrstexamination,thisapproachseemstobecomputationallyinfeasible.Forexample,ifwewanttomatchallthemomentsoftheformxixj,thenweneedtominimizethediﬀerencebetweenanumberofvaluesthatisquadraticinthedimensionofx.Moreover,evenmatchingalloftheﬁrstandsecondmomentswouldonlybesuﬃcienttoﬁtamultivariateGaussiandistribution,whichcapturesonlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksaretocapturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments.GANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusingadynamicallyupdateddiscriminator,thatautomaticallyfocusesitsattentiononwhicheverstatisticthegeneratornetworkismatchingtheleasteﬀectively.Instead,generativemomentmatchingnetworkscanbetrainedbyminimizingacostfunctioncalledmaximummeandiscrepancy(SchölkopfandSmola2002,;Gretton2012etal.,)orMMD.Thiscostfunctionmeasurestheerrorintheﬁrstmomentsinaninﬁnite-dimensionalspace,usinganimplicitmappingtofeature705'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 720}, page_content='CHAPTER20.DEEPGENERATIVEMODELSspacedeﬁnedbyakernelfunctioninordertomakecomputationsoninﬁnite-dimensionalvectorstractable.TheMMDcostiszeroifandonlyifthetwodistributionsbeingcomparedareequal.Visually,thesamplesfromgenerativemomentmatchingnetworksaresomewhatdisappointing.Fortunately,theycanbeimprovedbycombiningthegeneratornetworkwithanautoencoder.First,anautoencoderistrainedtoreconstructthetrainingset.Next,theencoderoftheautoencoderisusedtotransformtheentiretrainingsetintocodespace.Thegeneratornetworkisthentrainedtogeneratecodesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder.UnlikeGANs,thecostfunctionisdeﬁnedonlywithrespecttoabatchofexamplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossibletomakeatrainingupdateasafunctionofonlyonetrainingexampleoronlyonesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbecomputedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoosmall,MMDcanunderestimatethetrueamountofvariationinthedistributionsbeingsampled.Noﬁnitebatchsizeissuﬃcientlylargetoeliminatethisproblementirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatchsizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemanyexamplesmustbeprocessedinordertocomputeasinglesmallgradientstep.AswithGANs,itispossibletotrainageneratornetusingMMDevenifthatgeneratornetassignszeroprobabilitytothetrainingpoints.20.10.6ConvolutionalGenerativeNetworksWhengeneratingimages,itisoftenusefultouseageneratornetworkthatincludesaconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiyetal.()oretal.()).Todoso, weusethe“transpose”oftheconvolutionoperator,2015describedinSec..Thisapproachoftenyieldsmorerealisticimagesanddoes9.5sousingfewerparametersthanusingfullyconnectedlayerswithoutparametersharing.Convolutionalnetworksforrecognitiontaskshaveinformationﬂowfromtheimagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel.Asthisimageﬂowsupwardthroughthenetwork,informationisdiscardedastherepresentationoftheimagebecomesmoreinvarianttonuisancetransformations.Inageneratornetwork, theoppositeistrue.Richdetailsmustbeaddedastherepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,culminatingintheﬁnalrepresentationoftheimage,whichisofcoursetheimageitself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesand706'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 721}, page_content='CHAPTER20.DEEPGENERATIVEMODELSlighting. Theprimarymechanismfordiscardinginformationinaconvolutionalrecognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedtoaddinformation.Wecannotputtheinverseofapoolinglayerintothegeneratornetworkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationistomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseemstoperformacceptablyistousean“un-pooling”asintroducedbyDosovitskiyetal.().Thislayercorrespondstotheinverseofthemax-poolingoperationunder2015certainsimplifyingconditions. First,thestrideofthemax-poolingoperationisconstrainedtobeequaltothewidthofthepoolingregion.Second,themaximuminputwithineachpoolingregionisassumedtobetheinputintheupper-leftcorner.Finally,allnon-maximalinputswithineachpoolingregionareassumedtobezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthemax-poolingoperatortobeinverted.Theinverseun-poolingoperationallocatesatensorofzeros,thencopieseachvaluefromspatialcoordinateioftheinputtospatialcoordinateik×oftheoutput.Theintegervaluekdeﬁnesthesizeofthepoolingregion.Eventhoughtheassumptionsmotivatingthedeﬁnitionoftheun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearntocompensateforitsunusualoutput,sothesamplesgeneratedbythemodelasawholearevisuallypleasing.20.10.7Auto-RegressiveNetworksAuto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandomvariables.Theconditionalprobabilitydistributionsinthesemodelsarerepresentedbyneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogisticregression).Thegraphstructureofthesemodelsisthecompletegraph.TheydecomposeajointprobabilityovertheobservedvariablesusingthechainruleofprobabilitytoobtainaproductofconditionalsoftheformP(xd|xd−1,...,x1).Suchmodelshavebeencalled(FVBNs)andusedfully-visibleBayesnetworkssuccessfully inmanyforms,ﬁrst withlogistic regressionfor eachconditionaldistribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(BengioandBengio2000bLarochelleand Murray2011, ;,).Insome formsofauto-regressivenetworks,suchasNADE(,),describedinLarochelleandMurray2011Sec.below,wecanintroduceaformofparametersharingthatbringsboth20.10.10astatisticaladvantage(feweruniqueparameters)andacomputationaladvantage(lesscomputation).Thisisonemoreinstanceoftherecurringdeeplearningmotifofreuseoffeatures.707'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 722}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nx1x1x2x2x3x3x4x4Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1)x1x1x2x2x3x3x4x4\\nFigure20.8:A fullyvisiblebelief networkpredictsthei-thvariable fromthei−1previousones.(Top)(Bottom)ThedirectedgraphicalmodelforanFVBN.Correspondingcomputationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadebyalinearpredictor.20.10.8LinearAuto-RegressiveNetworksThesimplestformofauto-regressivenetworkhasnohiddenunitsandnosharingofparametersorfeatures.EachP(xi|xi−1,...,x1)isparametrizedasalinearmodel(linearregressionforreal-valueddata,logisticregressionforbinarydata,softmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()andhasO(d2)parameterswhentherearedvariablestomodel.ItisillustratedinFig..20.8Ifthevariablesarecontinuous,alinearauto-regressivemodelismerelyanotherwaytoformulateamultivariateGaussiandistribution,capturinglinearpairwiseinteractionsbetweentheobservedvariables.Linearauto-regressivenetworksareessentiallythegeneralizationoflinearclassiﬁcationmethodstogenerativemodeling.Theythereforehavethesameadvantagesanddisadvantagesaslinearclassiﬁers.Likelinearclassiﬁers,theymaybetrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions(asintheGaussiancase).Likelinearclassiﬁers,themodelitselfdoesnotoﬀerawayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslikebasisexpansionsoftheinputorthekerneltrick.708'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 723}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nx1x1x2x2x3x3x4x4h1h1h2h2h3h3Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1)\\nFigure20.9:Aneuralauto-regressivenetworkpredictsthei-thvariablexifromthei−1previousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenotedhi)thatarefunctionsofx1,...,xicanbereusedinpredictingallofthesubsequentvariablesxi+1,xi+2,...,xd.20.10.9NeuralAuto-RegressiveNetworksNeuralauto-regressivenetworks(,,)havethesameBengioandBengio2000ableft-to-rightgraphicalmodelaslogisticauto-regressivenetworks(Fig.)but20.8employadiﬀerentparametrizationoftheconditionaldistributionswithinthatgraphicalmodelstructure.Thenewparametrizationismorepowerfulinthesensethatitscapacitycanbeincreasedasmuchasneeded,allowingapproximationofanyjointdistribution.Thenewparametrizationcanalsoimprovegeneralizationbyintroducingaparametersharingandfeaturesharingprinciplecommontodeeplearningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthecurseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharingthesamestructureasFig..Intabulardiscreteprobabilisticmodels,each20.8conditionaldistributionisrepresentedbyatableofprobabilities,withoneentryandoneparameterforeachpossibleconﬁgurationofthevariablesinvolved.Byusinganeuralnetworkinstead,twoadvantagesareobtained:1.TheparametrizationofeachP(xi|xi−1,...,x1)byaneuralnetworkwith(i−1)×kinputsandkoutputs(ifthevariablesarediscreteandtakekvalues,encodedone-hot)allowsonetoestimatetheconditionalprobabilitywithoutrequiringanexponentialnumberofparameters(andexamples),yetstillisabletocapturehigh-orderdependenciesbetweentherandomvariables.2.Insteadofhavingadiﬀerentneuralnetworkforthepredictionofeachxi,709'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 724}, page_content='CHAPTER20.DEEPGENERATIVEMODELSaconnectivityillustratedinFig.allowsonetomergeallleft-to-right20.9theneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayerfeaturescomputedforpredictingxicanbereusedforpredictingxik+(k>0).Thehiddenunitsarethusorganizedingroupsthathavetheparticularitythatalltheunitsinthei-thgrouponlydependontheinputvaluesx1,...,xi.Theparametersusedtocomputethesehiddenunitsarejointlyoptimizedto improvethe prediction ofall thevariablesinthe sequence.This isaninstanceofthereuseprinciplethatrecursthroughoutdeeplearninginscenariosrangingfromrecurrentandconvolutionalnetworkarchitecturestomulti-taskandtransferlearning.EachP(xi|xi−1,...,x1)canrepresentaconditionaldistributionbyhavingoutputsoftheneuralnetworkpredictparametersoftheconditionaldistributionofxi,asdiscussedinSec..Althoughtheoriginalneuralauto-regressive6.2.1.1networkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariatedata(withasigmoidoutputforaBernoullivariableorsoftmaxoutputforamultinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesorjointdistributionsinvolvingbothdiscreteandcontinuousvariables.20.10.10NADETheneuralautoregressivedensityestimator(NADE)isaverysuccessfulrecentformofneuralauto-regressivenetwork(LarochelleandMurray2011,).Theconnectivityisthe sameas forthe originalneural auto-regressivenetworkofBengioandBengio2000b()butNADEintroducesanadditionalparametersharingscheme,asillustratedinFig..Theparametersofthehiddenunitsofdiﬀerentgroups20.10jareshared.TheweightsW\\ue030j,k,ifromthei-thinputxitothek-thelementofthej-thgroupofhiddenunith()jk()aresharedamongthegroups:ji≥W\\ue030j,k,i= Wk,i.(20.83)Theremainingweights,where,arezero.j<iLarochelleandMurray2011()chosethissharingschemesothatforwardpropagationinaNADEmodellooselyresemblesthecomputationsperformedinmeanﬁeldinferencetoﬁllinmissinginputsinanRBM.ThismeanﬁeldinferencecorrespondstorunningarecurrentnetworkwithsharedweightsandtheﬁrststepofthatinferenceisthesameasinNADE.TheonlydiﬀerenceisthatwithNADE,theoutputweightsconnectingthehiddenunitstotheoutputareparametrized710'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 725}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nx1x1x2x2x3x3x4x4h1h1h2h2h3h3Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1)\\nW:1,W:1,W:1,W:2,W:2,W:3,Figure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).Thehiddenunitsareorganizedingroupsh()jsothatonlytheinputsx1,...,xiparticipateincomputingh()iandpredictingP(xj|xj−1,...,x1),forj>i.NADEisdiﬀerentiatedfromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharingpattern:W\\ue030j,k,i=Wk,iisshared(indicatedintheﬁgurebytheuseofthesamelinepatternforeveryinstanceofareplicatedweight)foralltheweightsgoingoutfromxitothek-thunitofanygroup.Recallthatthevectorji≥(W1,i,W2,i,...,Wn,i)isdenotedW:,i.independentlyfromtheweightsconnectingtheinputunitstothehiddenunits.IntheRBM,thehidden-to-outputweightsarethetransposeoftheinput-to-hiddenweights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestepofthemeanﬁeldrecurrentinferencebuttomimicksteps.ThisapproachiscalledNADE-(kRaiko2014etal.,).Asmentionedpreviously,auto-regressivenetworksmaybeextendtoprocesscontinuous-valueddata.AparticularlypowerfulandgenericwayofparametrizingacontinuousdensityisasaGaussianmixture(introducedinSec.)withmixture3.9.6weightsαi(thecoeﬃcientorpriorprobabilityforcomponenti),per-componentconditionalmeanµiandper-componentconditionalvarianceσ2i.AmodelcalledRNADE(,)usesthisparametrizationtoextendNADEtorealUriaetal.2013values.Aswithothermixturedensitynetworks,theparametersofthisdistributionareoutputsofthenetwork,withthemixtureweightprobabilitiesproducedbyasoftmaxunit,andthevariancesparametrizedsothattheyarepositive.Stochasticgradientdescentcanbenumericallyill-behavedduetotheinteractionsbetweentheconditionalmeansµiandtheconditionalvariancesσ2i.Toreducethisdiﬃculty,Uria2013etal.()useapseudo-gradientthatreplacesthegradientonthemean,intheback-propagationphase.711'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 726}, page_content='CHAPTER20.DEEPGENERATIVEMODELSAnotherveryinterestingextensionoftheneuralauto-regressivearchitecturesgetsridoftheneedtochooseanarbitraryorderfortheobservedvariables(MurrayandLarochelle2014,).Inauto-regressivenetworks,theideaistotrainthenetworktobeabletocopewithanyorderbyrandomlysamplingordersandprovidingtheinformationtohiddenunitsspecifyingwhichoftheinputsareobserved(ontherightsideoftheconditioningbar)andwhicharetobepredictedandarethusconsideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecauseitallowsonetouseatrainedauto-regressivenetworktoperformanyinferenceproblem(i.e.predictorsamplefromtheprobabilitydistributionoveranysubsetofvariablesgivenanysubset)extremelyeﬃciently.Finally,sincemanyordersofvariablesarepossible(n!fornvariables)andeachorderoofvariablesyieldsadiﬀerent,wecanformanensembleofmodelsformanyvaluesof:po(x|)opensemble() =x1kk\\ue058i=1po(x|()i).(20.84)Thisensemblemodelusuallygeneralizesbetterandassignshigherprobabilitytothetestsetthandoesanindividualmodeldeﬁnedbyasingleordering.Inthesamepaper,theauthorsproposedeepversionsofthearchitecture,butunfortunatelythatimmediatelymakescomputationasexpensiveasintheoriginalneuralauto-regressiveneuralnetwork(,).TheﬁrstlayerBengioandBengio2000bandtheoutputlayercanstillbecomputedinO(nh)multiply-addoperations,asintheregularNADE,wherehisthenumberofhiddenunits(thesizeofthegroupshi,inFig.andFig.),whereasitis20.1020.9O(n2h)inBengioandBengio().However,fortheotherhiddenlayers,thecomputationis2000bO(n2h2)ifevery“previous”groupatlayerlparticipatesinpredictingthe“next”groupatlayerl+1,assumingngroupsofhhiddenunitsateachlayer.Makingthei-thgroupatlayerl+1onlydependonthei-thgroup,asinMurrayandLarochelle2014()atlayerlreducesittoOnh(2),whichisstilltimesworsethantheregularNADE.h20.11DrawingSamplesfromAutoencodersInChapter,wesawthatmanykindsofautoencoderslearnthedatadistribution.14Therearecloseconnectionsbetweenscorematching,denoisingautoencoders,andcontractiveautoencoders.Theseconnectionsdemonstratethatsomekindsofautoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhowtodrawsamplesfromsuchmodels.Somekindsofautoencoders,suchasthevariationalautoencoder,explicitly712'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 727}, page_content='CHAPTER20.DEEPGENERATIVEMODELSrepresentaprobabilitydistributionandadmitstraightforwardancestralsampling.MostotherkindsofautoencodersrequireMCMCsampling.Contractiveautoencodersaredesignedtorecoveranestimateofthetangentplaneofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwithinjectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifaietal.etal.,;2012Mesnil,).Thismanifolddiﬀusiontechniqueisakindof2012Markovchain.ThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoisingautoencoder.20.11.1MarkovChainAssociatedwithanyDenoisingAutoen-coderTheabovediscussionleftopenthequestionofwhatnoisetoinjectandwhere,inordertoobtainaMarkovchainthatwouldgeneratefromthedistributionestimatedbytheautoencoder.()showedhowtoconstructsuchaMarkovBengioetal.2013cchainforgeneralizeddenoisingautoencoders.Generalizeddenoisingautoencodersarespeciﬁedbyadenoisingdistributionforsamplinganestimateofthecleaninputgiventhecorruptedinput.EachstepoftheMarkovchainthatgeneratesfromtheestimateddistributionconsistsofthefollowingsub-steps,illustratedinFig.:20.111.Startingfromthepreviousstatex,injectcorruptionnoise,sampling˜xfromC(˜xx|).2. Encode˜xintoh= (f˜x).3. Decodetoobtaintheparametersofhωh= (g)pgp(= x |ω()) = h(x|˜x).4. Samplethenextstatefromxpgp(= x|ω()) = h(x |˜x).Bengio2014etal.()showedthatiftheautoencoderp(x |˜x)formsaconsistentestimatorofthecorrespondingtrueconditionaldistribution,thenthestationarydistributionoftheaboveMarkovchainformsaconsistentestimator(albeitanimplicitone)ofthedatageneratingdistributionof.x20.11.2ClampingandConditionalSamplingSimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations(suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-butionp(xf|xo),simplybyclampingtheobservedunitsxfandonlyresampling713'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 728}, page_content='CHAPTER20.DEEPGENERATIVEMODELS\\nx x˜x˜xh hω ωˆxˆxC(˜xx|)p()x|ωfg\\nFigure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-coder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythedenoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruptionprocessCinstatex,yielding˜x,(b)encodingitwithfunctionf,yieldingh=f(˜x),(c)decodingtheresultwithfunctiong,yieldingparametersωforthereconstructiondistribution,and(d)givenω,samplinganewstatefromthereconstructiondistributionp(x |ω=g(f(˜x))).Inthetypicalsquaredreconstructionerrorcase,g(h)=ˆx,whichestimatesE[x|˜x],corruptionconsistsinaddingGaussiannoiseandsamplingfromp(x|ω)consistsinaddingGaussiannoise,asecondtime,tothereconstructionˆx.Thelatternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereastheinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellastheextenttowhichtheestimatorsmoothstheempiricaldistribution(Vincent2011,).Intheexampleillustratedhere,onlytheCandpconditionalsarestochasticsteps(fandgaredeterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,asingenerativestochasticnetworks(,).Bengioetal.2014\\n714'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 729}, page_content='CHAPTER20.DEEPGENERATIVEMODELSthefreeunitsxogivenxfandthesampledlatentvariables(ifany).Forexample,MP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareabletosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentinMP-DBMstoperformthesameoperation(,).()Bengioetal.2014Alainetal.2015identiﬁedamissingconditionfromProposition1of(),whichisBengioetal.2014thatthetransitionoperator(deﬁnedbythestochasticmappinggoingfromonestateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance,whichspeciﬁesthataMarkovChainatequilibriumwillremaininequilibriumwhetherthetransitionoperatorisruninforwardorreverse.Anexperimentinclampinghalfofthepixels(therightpartoftheimage)andrunningtheMarkovchainontheotherhalfisshowninFig..20.12\\nFigure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkovChainbyresamplingonlythelefthalfateachstep. ThesesamplescomefromaGSNtrainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure.20.11.3Walk-BackTrainingProcedureThewalk-backtrainingprocedurewasproposedby()asawayBengioetal.2013ctoacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders.Insteadofperformingaone-stepencode-decodereconstruction,thisprocedureconsistsinalternativemultiplestochasticencode-decodesteps(asinthegenerative715'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 730}, page_content='CHAPTER20.DEEPGENERATIVEMODELSMarkovchain)initializedatatrainingexample(justlikewiththecontrastivedivergencealgorithm,describedinSec.)andpenalizingthelastprobabilistic18.2reconstructions(orallofthereconstructionsalongtheway).Trainingwithkstepsisequivalent(inthesenseofachievingthesamestationarydistribution)astrainingwithonestep,butpracticallyhastheadvantagethatspuriousmodesfartherfromthedatacanberemovedmoreeﬃciently.20.12GenerativeStochasticNetworksGenerativestochasticnetworksGSNsor(,)aregeneralizationsofBengioetal.2014denoisingautoencodersthatincludelatentvariableshinthegenerativeMarkovchain,inadditiontothevisiblevariables(usuallydenoted).xAGSNisparametrizedbytwoconditionalprobabilitydistributionswhichspecifyonestepoftheMarkovchain:1. p(x()k|h()k)tellshowtogeneratethenextvisiblevariablegiventhecurrentlatentstate.Sucha“reconstructiondistribution”isalsofoundindenoisingautoencoders,RBMs,DBNsandDBMs.2. p(h()k|h(1)k−,x(1)k−)tellshowtoupdatethelatentstatevariable,giventhepreviouslatentstateandvisiblevariable.DenoisingautoencodersandGSNsdiﬀerfromclassicalprobabilisticmodels(directedorundirected)inthattheyparametrizethegenerativeprocessitselfratherthanthemathematicalspeciﬁcationofthejointdistributionofvisibleandlatentvariables.Instead,thelatterisdeﬁnedimplicitly,,asthestationaryifitexistsdistributionofthegenerativeMarkovchain.TheconditionsforexistenceofthestationarydistributionaremildandarethesameconditionsrequiredbystandardMCMCmethods(seeSec.).Theseconditionsarenecessarytoguarantee17.3thatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransitiondistributions(forexample,iftheyweredeterministic).OnecouldimaginediﬀerenttrainingcriteriaforGSNs.Theoneproposedandevaluatedby()issimplyreconstructionlog-probabilityontheBengioetal.2014visibleunits,justlikefordenoisingautoencoders.Thisisachievedbyclampingx(0)=xtotheobservedexampleandmaximizingtheprobabilityofgeneratingxatsomesubsequenttimesteps,i.e.,maximizinglogp(x()k=x|h()k),whereh()kissampledfromthechain,givenx(0)=x. Inordertoestimatethegradientoflogp(x()k=x|h()k)withrespecttotheotherpiecesofthemodel,Bengioetal.()usethereparametrizationtrick,introducedinSec..201420.9716'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 731}, page_content='CHAPTER20.DEEPGENERATIVEMODELSThewalk-backtrainingprotocol(describedinSec.)wasused(20.11.3Bengioetal.,)toimprovetrainingconvergenceofGSNs.201420.12.1DiscriminantGSNsTheoriginalformulationofGSNs(,)wasmeantforunsupervisedBengioetal.2014learningandimplicitlymodelingp(x)forobserveddatax,butitispossibletomodifytheframeworktooptimize.p()y|xForexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,byonlyback-propagatingthereconstructionlog-probabilityovertheoutputvariables,keep-ingtheinputvariablesﬁxed.Theyappliedthissuccessfullytomodelsequences(proteinsecondarystructure)andintroduceda(one-dimensional)convolutionalstructureinthetransitionoperatoroftheMarkovchain.Itisimportanttore-memberthat,foreachstepoftheMarkovchain,onegeneratesanewsequenceforeachlayer,andthatsequenceistheinputforcomputingotherlayervalues(saytheonebelowandtheoneabove)atthenexttimestep.HencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher-levelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain,withback-propagationallowingtolearnhowtheinputsequencecanconditiontheoutputdistributionimplicitlyrepresentedbytheMarkovchain.ItisthereforeacaseofusingtheGSNinthecontextofstructuredoutputs,wherep(y x|)doesnothaveasimpleparametricformbutinsteadthecomponentsofyarestatisticallydependentofeachother,given,incomplicatedways.xZöhrerandPernkopf2014()introducedahybridmodelthatcombinesasuper-visedobjective(asintheabovework)andanunsupervisedobjective(asintheoriginalGSNwork),bysimplyadding(withadiﬀerentweight)thesupervisedandunsupervisedcostsi.e.,thereconstructionlog-probabilitiesofyandxrespectively.SuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelleandBengio2008().Theyshowimprovedclassiﬁcationperformanceusingthisscheme.20.13OtherGenerationSchemesThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestralsampling,orsomemixtureofthetwotogeneratesamples. Whilethesearethemostpopularapproachestogenerativemodeling,theyarebynomeanstheonlyapproaches.717'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 732}, page_content='CHAPTER20.DEEPGENERATIVEMODELSSohl-Dickstein2015etal.()developedatrainingschemediﬀusioninversionforlearningagenerativemodel,basedonnon-equilibriumthermodynamics.Theapproachisbasedontheideathattheprobabilitydistributionswewishtosamplefromhavestructure.Thisstructurecangraduallybedestroyedbyadiﬀusionprocessthat incrementallychangestheprobabilitydistribution tohavemoreentropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytrainingamodelthatgraduallyrestoresthestructuretoanunstructureddistribution.Byiterativelyapplyingaprocessthatbringsadistributionclosertothetargetone,wecangraduallyapproachthattargetdistribution.ThisapproachresemblesMCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample.However,themodelisdeﬁnedtobetheprobabilitydistributionproducedbytheﬁnalstepofthechain.Inthissense,thereisnoapproximationinducedbytheiterativeprocedure.TheapproachintroducedbySohl-Dickstein2015etal.()isalsoveryclosetothegenerativeinterpretationofthedenoisingautoencoder(Sec.).20.11.1Likewiththedenoisingautoencoder,thetrainingobjectivetrainsatransitionoperatorwhichattemptstoprobabilisticallyundotheeﬀectofaddingsomenoise,tryingtoundoonestepofthediﬀusionprocess.Ifwecomparewiththewalkbacktrainingprocedure(Sec.)fordenoisingautoencodersandGSNs,themain20.11.3diﬀerenceisthatinsteadofreconstructingonlytowardstheobservedtrainingpointx,theobjectivefunctiononlytriestoreconstructtowardsthepreviouspointinthediﬀusiontrajectorythatstartedatx(whichshouldbeeasier).Thisaddressesthefollowingdilemmapresentwiththeordinaryreconstructionlog-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethelearneronlyseesconﬁgurationsnearthedatapoints,whilewithlargelevelsofnoiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistributionisgoingtobehighlycomplexandmulti-modal).Withthediﬀusioninversionobjective,thelearnercanlearnmorepreciselytheshapeofthedensityaroundthedatapointsaswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints.AnotherapproachtosamplegenerationistheapproximateBayesiancomputa-tion(ABC)framework(,).Inthisapproach,samplesarerejectedRubinetal.1984ormodiﬁedinordertomakethemomentsofselectedfunctionsofthesamplesmatchthoseofthedesireddistribution.Whilethisideausesthemomentsofthesampleslikeinmomentmatching,itisdiﬀerentfrommomentmatchingbecauseitmodiﬁesthesamplesthemselves,ratherthantrainingthemodeltoautomaticallyemitsampleswiththecorrectmoments.BachmanandPrecup2015()showedhowtouseideasfromABCinthecontextofdeeplearning,byusingABCtoshapetheMCMCtrajectoriesofGSNs.Weexpectthatmanyotherpossibleapproachestogenerativemodelingawaitdiscovery.718'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 733}, page_content='CHAPTER20.DEEPGENERATIVEMODELS20.14EvaluatingGenerativeModelsResearchersstudyinggenerativemodelsoftenneedtocompareonegenerativemodeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerativemodelisbetteratcapturingsomedistributionthanthepre-existingmodels.Thiscanbeadiﬃcultandsubtletask.Inmanycases,wecannotactuallyevaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation.Inthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhatisbeingmeasured.Forexample,supposewecanevaluateastochasticestimateofthelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihoodformodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwecareaboutdeterminingwhichmodelhasabetterinternalrepresentationofthedistribution,weactuallycannottell,unlesswehavesomewayofdetermininghowloosetheboundformodelBis.However,ifwecareabouthowwellwecanusethemodelinpractice,forexampletoperformanomalydetection,thenitisfairtosaythatamodelispreferablebasedonacriterionspeciﬁctothepracticaltaskofinterest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecisionandrecall.Anothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetricsareoftenhardresearchproblemsinandofthemselves.Itcanbeverydiﬃculttoestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuseAIStoestimatelogZinordertocomputelog ˜p(x)−logZforanewmodelwehavejustinvented.AcomputationallyeconomicalimplementationofAISmayfailtoﬁndseveralmodesofthemodeldistributionandunderestimateZ,whichwillresultinusoverestimatinglogp(x).ItcanthusbediﬃculttotellwhetherahighlikelihoodestimateisduetoagoodmodelorabadAISimplementation.Otherﬁeldsofmachinelearningusuallyallowforsomevariationinthepre-processingofthedata.Forexample,whencomparingtheaccuracyofobjectrecognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimagesslightlydiﬀerentlyforeachalgorithmbasedonwhatkindofinputrequirementsithas.Generativemodelingisdiﬀerentbecausechangesinpreprocessing,evenverysmallandsubtleones,arecompletelyunacceptable.Anychangetotheinputdatachangesthedistributiontobecapturedandfundamentallyaltersthetask.Forexample,multiplyingtheinputby0.1willartiﬁciallyincreaselikelihoodbyafactorof10.IssueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodelsontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks.MNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints719'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 734}, page_content='CHAPTER20.DEEPGENERATIVEMODELSinarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthegrayscalevaluesasprobabilitiesforabinarysamples.Itisessentialtocomparereal-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonlytootherbinary-valuedmodels. Otherwisethelikelihoodsmeasuredarenotonthesamespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,whileforreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofadensity.Amongbinarymodels,itisimportanttocomparemodelsusingexactlythesamekindofbinarization.Forexample,wemightbinarizeagraypixelto0or1bythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing1isgivenbythegraypixelintensity.Ifweusetherandombinarization,wemightbinarizethewholedatasetonce,orwemightdrawadiﬀerentrandomexampleforeachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthesethreeschemesyieldswildlydiﬀerentlikelihoodnumbers,andwhencomparingdiﬀerentmodelsitisimportantthatbothmodelsusethesamebinarizationschemefortrainingandforevaluation. Infact,researcherswhoapplyasinglerandombinarizationstepshareaﬁlecontainingtheresultsoftherandombinarization,sothatthereisnodiﬀerenceinresultsbasedondiﬀerentoutcomesofthebinarizationstep.Becausebeingabletogeneraterealisticsamplesfromthedatadistributionisoneofthegoalsofagenerativemodel,practitionersoftenevaluategenerativemodelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbytheresearchersthemselves,butbyexperimentalsubjectswhodonotknowthesourceofthesamples(Denton2015etal.,).Unfortunately,itispossibleforaverypoorprobabilisticmodeltoproduceverygoodsamples.AcommonpracticetoverifyifthemodelonlycopiessomeofthetrainingexamplesisillustratedinFig..16.1Theideaistoshowforsomeofthegeneratedsamplestheirnearestneighborinthetrainingset,accordingtoEuclideandistanceinthespaceofx. Thistestisintendedtodetectthecasewherethemodeloverﬁtsthetrainingsetandjustreproducestraininginstances.Itisevenpossibletosimultaneouslyunderﬁtandoverﬁtyetstillproducesamplesthatindividuallylookgood.Imagineagenerativemodeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethetrainingimagesofdogs.Suchamodelhasclearlyoverﬁt,becauseitdoesnotproducesimagesthatwerenotinthetrainingset,butithasalsounderﬁt,becauseitassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserverwouldjudgeeachindividualimageofadogtobehighquality.Inthissimpleexample,itwouldbeeasyforahumanobserverwhocaninspectmanysamplestodeterminethatthecatsareabsent.Inmorerealisticsettings,agenerativemodeltrainedondatawithtensofthousandsofmodesmayignoreasmallnumberofmodes,andahumanobserverwouldnoteasilybeabletoinspectorremember720'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 735}, page_content='CHAPTER20.DEEPGENERATIVEMODELSenoughimagestodetectthemissingvariation.Since thevisual quality ofsamples is not areliable guide, weoften alsoevaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisiscomputationallyfeasible.Unfortunately,insomecasesthelikelihoodseemsnottomeasureanyattributeofthemodelthatwereallycareabout.Forexample,real-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigningarbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsandalgorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,eventhoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacostapproaching negativeinﬁnity ispresent forany kindof maximum likelihoodproblemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsofMNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstronglysuggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels.Theis2015etal.()reviewmanyoftheissuesinvolvedinevaluatinggenerativemodels,includingmanyoftheideasdescribedabove.Theyhighlightthefactthattherearemanydiﬀerentusesofgenerativemodelsandthatthechoiceofmetricmustmatchtheintendeduseofthemodel.Forexample,somegenerativemodelsarebetteratassigninghighprobabilitytomostrealisticpointswhileothergenerativemodelsarebetteratrarelyassigninghighprobabilitytounrealisticpoints.ThesediﬀerencescanresultfromwhetheragenerativemodelisdesignedtominimizeDKL(pdata\\ue06bpmodel)orDKL(pmodel\\ue06bpdata),asillustratedinFig..3.6Unfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismostsuitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses.Oneofthemostimportantresearchtopicsingenerativemodelingisthereforenotjusthowtoimprovegenerativemodels,butinfact,designingnewtechniquestomeasureourprogress.20.15ConclusionTraininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodelsunderstandtheworldrepresentedinthegiventrainingdata.Bylearningamodelpmodel(x)andarepresentationpmodel(hx|),agenerativemodelcanprovideanswerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariablesinxandcanprovidemanydiﬀerentwaysofrepresentingxbytakingexpectationsofhatdiﬀerentlayersofthehierarchy. GenerativemodelsholdthepromisetoprovideAIsystemswithaframeworkforallofthemanydiﬀerentintuitiveconceptstheyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthefaceofuncertainty.Wehopethatourreaderswillﬁndnewwaystomakethese721'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 736}, page_content='CHAPTER20.DEEPGENERATIVEMODELSapproachesmorepowerfulandcontinuethejourneytounderstandingtheprinciplesthatunderlielearningandintelligence.\\n722'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 737}, page_content='BibliographyAbadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,A.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,Jia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Mané,D.,Monga,R.,Moore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Viégas,F.,Vinyals,O.,Warden,P.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scalemachinelearningonheterogeneoussystems.Softwareavailablefromtensorﬂow.org.,25212448,Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).AlearningalgorithmforBoltzmannmachines.CognitiveScience,,147–169.,9572656Alain,G.andBengio,Y.(2013). Whatregularizedauto-encoderslearnfromthedatageneratingdistribution.In.,,ICLR’2013,arXiv:1211.4246509515523Alain,G.,Bengio,Y.,Yao,L.,ÉricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015).GSNs:Generativestochasticnetworks.arXiv:1503.05571.,512715Anderson,E.(1935).TheIrisesoftheGaspéPeninsula.BulletinoftheAmericanIrisSociety,,2–5.5 921Ba,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisualattention..arXiv:1412.7755693Bachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswithcollaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,Lille,France,6-11July2015,pages1964–1972.718Bacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationinneuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConferenceonReinforcementLearningandDecisionMaking(RLDM2015).452Bagnell,J.A.andBradley,D.M.(2009).Diﬀerentiablesparsecoding.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems21(NIPS’08),pages113–120.500723'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 738}, page_content='BIBLIOGRAPHYBahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointlylearningtoalignandtranslate.In.,,,,,ICLR’2015,arXiv:1409.047325101398420421467477,Bahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987). Speechrecognitionwithcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage,2,219–234.460Baldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:Learningfromexampleswithoutlocalminima.NeuralNetworks,,53–58.2286Baldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthepastandthefutureinproteinsecondarystructureprediction.,Bioinformatics1 5(11),937–946.395Baldi, P., Sadowski, P., andWhiteson, D.(2014).Searchingforexoticparticlesinhigh-energyphysicswithdeeplearning.Naturecommunications,.526Ballard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation.Nature.454Barlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295–311.1146Barron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidalfunction.IEEETrans.onInformationTheory,,930–945.3 9198Bartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversityPress.492Basilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:TheoryandApplications.Wiley.492Bastien,F.,Lamblin,P., Pascanu,R.,Bergstra,J., Goodfellow,I.J.,Bergeron,A.,Bouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements.DeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582212222448,Basu,S.andChristensen,J.(2013). Teachingclassiﬁcationboundariestohumans. InAAAI’2013.328Baxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternationalConferenceonComputationalLearningTheory(COLT’95),pages311–320,SantaCruz,California.ACMPress.246Bayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXive-prints.265Becker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfacesinrandom-dotstereograms.Nature,,161–163.3 5 5543724'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 739}, page_content='BIBLIOGRAPHYBehnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstractionpyramid.Int.J.ComputationalIntelligenceandApplications,(4),427–438.1517Beiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthresholdlogic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson,1 4(5),1217–1243.453Belkin, M.and Niyogi, P.(2002).Laplacianeigenmapsandspectraltechniquesforembeddingandclustering. InT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.244Belkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.NeuralComputation,(6),1373–1396.,1 5163520Bengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationinneuralnetworksforfastermodels.arXiv:1511.06297.452Bengio, S. andBengio, Y. (2000a).Taking onthecurseofdimensionalityinjointdistributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,specialissueonDataMiningandKnowledgeDiscovery,(3),550–557.1 1709Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingforsequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099.384Bengio,Y.(1991).ArtiﬁcialNeuralNetworksandtheirApplicationtoSequenceRecognition.Ph.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.408Bengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,1 2(8),1889–1900.437Bengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,Dept.IRO,UniversitédeMontréal.469Bengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,200624Bengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatisticalLanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,pages1–37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.450Bengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation.TechnicalReportarXiv:1510.02777,UniversitedeMontreal.658Bengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-layerneuralnetworks.In,pages400–406.MITPress.,,,NIPS12707709710712Bengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence.NeuralComputation,(6),1601–1621.,2 1515613725'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 740}, page_content='BIBLIOGRAPHYBengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-foldcross-validation.InS.Thrun,L.Saul,andB.Schölkopf,editors,AdvancesinNeuralInformationProcessingSystems16(NIPS’03),Cambridge,MA.MITPress,Cambridge.122Bengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScaleKernelMachines.19Bengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),pages129–136.MITPress.,159521Bengio,Y.andSénécal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsbyimportancesampling.InProceedingsofAISTATS2003.472Bengio,Y.andSénécal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetrainingofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks,1 9(4),713–722.472Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivatedacousticparametersforcontinuousspeechrecognitionusingartiﬁcialneuralnetworks.InProceedingsofEuroSpeech’91.,27461Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussianmixturehybridforspeechrecognitionordensityestimation.In,pages175–182.NIPS4MorganKaufmann.461Bengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-termdependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeuralNetworks,pages1183–1195,SanFrancisco.IEEEPress.(invitedpaper).404Bengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswithgradientdescentisdiﬃcult.IEEETr.NeuralNets.,,,,18402404405413Bengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-parameters.LearningConference,Snowbird.437Bengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel.InT.K.Leen,T.G.Dietterich,andV.Tresp,editors,,pages932–938.MITNIPS’2000Press.,,,,,,18449465468474479484Bengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilisticlanguagemodel.,,1137–1155.,JMLR3468474Bengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convexneuralnetworks.In,pages123–130.NIPS’2005258Bengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctionsforlocalkernelmachines.In.NIPS’2005157726'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 741}, page_content='BIBLIOGRAPHYBengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows.In.MITPress.,NIPS’2005159522Bengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wisetrainingofdeepnetworks.In.,,,,,,NIPS’20061419200323324530532Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.InICML’09.328Bengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeeprepresentations.In.ICML’2013606Bengio,Y.,Léonard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradientsthroughstochasticneuronsforconditionalcomputation. arXiv:1308.3432.,,450452691693,Bengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-encodersasgenerativemodels.In.,,NIPS’2013509713715Bengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewandnewperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),3 5(8),1798–1828.557Bengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014). Deepgenerativestochasticnetworkstrainablebybackprop.In.,,,,ICML’2014713714715716717Bennett,C.(1976).EﬃcientestimationoffreeenergydiﬀerencesfromMonteCarlodata.JournalofComputationalPhysics,(2),245–268.2 2630Bennett,J.andLanning,S.(2007).TheNetﬂixprize.481Berger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropyapproachtonaturallanguageprocessing.,,39–71.ComputationalLinguistics2 2475Berglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastivedivergenceandpersistentcontrastivedivergence.,.CoRRa b s / 1 3 1 2 . 6 0 0 2616Bergstra, J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor PatternClassiﬁcation.Ph.D.thesis,UniversitédeMontréal.255Bergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplexcell-likenetworks.In.NIPS’2009496Bergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J.MachineLearningRes.,,281–305.,1 3436437Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,J.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpressioncompiler.InProc.SciPy.,,,,2582212222448727'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 742}, page_content='BIBLIOGRAPHYBergstra,J.,Bardenet,R.,Bengio,Y.,andKégl,B.(2011).Algorithmsforhyper-parameteroptimization.In.NIPS’2011438Berkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplexcellproperties.,(6),579–602.JournalofVision5497Bertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScientiﬁc.106Besag,J.(1975).Statisticalanalysisofnon-latticedata.,TheStatistician2 4(3),179–195.617Bishop,C.M.(1994).Mixturedensitynetworks.188Bishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks.InProceedingsInternationalConferenceonArtiﬁcialNeuralNetworksICANN’95,volume1,page141–148.,242250Bishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization.NeuralComputation,(1),108–116.7242Bishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,98145Blum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete.293Blumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).LearnabilityandtheVapnik–Chervonenkisdimension.,(4),929––865.JournaloftheACM3 6114Bonnet,G.(1964).Transformationsdessignauxaléatoiresàtraverslessystèmesnonlinéairessansmémoire.AnnalesdesTélécommunications,(9–10),203–220.1 9691Bordes, A., Weston, J., Collobert, R., andBengio, Y.(2011).Learningstructuredembeddingsofknowledgebases.In.AAAI2011486Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsandmeaningrepresentationsforopen-textsemanticparsing.AISTATS’2012.,402486Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergyfunctionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueonLearningSemantics.485Bordes,A.,Usunier, N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b).Translatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages2787–2795.CurranAssociates,Inc.486Bornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR’2015,arXiv:1406.2751.695728'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 743}, page_content='BIBLIOGRAPHYBornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).TrainingbidirectionalHelmholtzmachines.Technicalreport,arXiv:1506.03877.695Boser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-malmarginclassiﬁers.InCOLT’92:ProceedingsoftheﬁfthannualworkshoponComputationallearningtheory,pages144–152,NewYork,NY,USA.ACM.,18140Bottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,OnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.296Bottou, L.(2011).Frommachinelearning tomachinereasoning.Technicalreport,arXiv.1102.1808.,400402Bottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.442Bottou,L.andBousquet,O.(2008).Thetradeoﬀsoflargescalelearning.In.NIPS’2008282295,Boulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporaldependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgenerationandtranscription.In.,ICML’12687688Boureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolinginvisionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML’10).345Boureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011). Askthelocals:multi-waylocalpoolingforimagerecognition.InProc.InternationalConferenceonComputerVision(ICCV’11).IEEE.345Bourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsandsingularvaluedecomposition.BiologicalCybernetics,,291–294.5 9504Bourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layeredperceptrons.ComputerSpeechandLanguage,,1–19.3461Boyd,S.andVandenberghe,L.(2004)..CambridgeUniversityConvexOptimizationPress,NewYork,NY,USA.93Brady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparatewhereperceptronssucceed.IEEETransactionsonCircuitsandSystems,3 6,665–674.284Brakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfortime-seriesimputation.JournalofMachineLearningResearch,1 4,2771–2797.,676700Brand,M.(2003).Chartingamanifold.In,pages961–968.MITPress.,NIPS’2002163520729'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 744}, page_content='BIBLIOGRAPHYBreiman,L.(1994).Baggingpredictors.MachineLearning,(2),123–140.2 4256Breiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).ClassiﬁcationandRegressionTrees.WadsworthInternationalGroup,Belmont,CA.145Bridle,J.S.(1990).Alphanets:arecurrent‘neural’networkarchitecturewithahiddenMarkovmodelinterpretation.SpeechCommunication,(1),83–92.9185Briggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009).Maximinaﬃnitylearningofimagesegmentation.In,pages1865–1873.NIPS’2009359Brown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,Laﬀerty,J.D.,Mercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation.Computationallinguistics,(2),79–85.1 621Brown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-based-grammodelsofnaturallanguage.,nComputationalLinguistics1 8,467–479.465Bryson,A.andHo,Y.(1969). Appliedoptimalcontrol: optimization,estimation,andcontrol.BlaisdellPub.Co.224Bryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolvingoptimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,MissleandSpaceDivision.224Buciluˇa, C.,Caruana,R., and Niculescu-Mizil, A. (2006).Model compression.InProceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pages535–541.ACM.450Burda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders.arXivpreprintarXiv:1509.00519.700Cai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition.InAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshopon,pages291–296.IEEE.193Carreira-Perpiñan,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning.InR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternationalWorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS’05),pages33–40.SocietyforArtiﬁcialIntelligenceandStatistics.613Caruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModelsSummerSchool,pages372–379.244Cauchy,A.(1847).Méthodegénéralepourlarésolutiondesystèmesd’équationssimul-tanées.InCompterendudesséancesdel’académiedessciences,pages536–538.,83224730'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 745}, page_content='BIBLIOGRAPHYCayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,UCSD.163Chandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACMcomputingsurveys(CSUR),(3),15.4 1102Chapelle,O.,Weston,J.,andSchölkopf,B.(2003).Clusterkernelsforsemi-supervisedlearning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessingSystems15(NIPS’02),pages585–592,Cambridge,MA.MITPress.244Chapelle,O.,Schölkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MITPress,Cambridge,MA.,244543Chellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeuralNetworks forDocumentProcessing.In GuyLorette, editor, Tenth InternationalWorkshoponFrontiersinHandwritingRecognition,LaBaule(France).UniversitédeRennes1,Suvisoft.http://www.suvisoft.com.,,2427447Chen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariantspatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervisedFeatureLearningWorkshop.360Chen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesforlanguagemodeling.Computer,SpeechandLanguage,(4),359–393.,1 3464475Chen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:Asmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-ceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramminglanguagesandoperatingsystems,pages269–284.ACM.453Chen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,andZhang,Z.(2015).MXNet: Aﬂexibleandeﬃcientmachinelearninglibraryforheterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25Chen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,etal.Microarchitecture(2014b).DaDianNao:Amachine-learningsupercomputer.In(MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609–622.IEEE.453Chilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:Buildinganeﬃcientandscalabledeeplearningtrainingsystem.In11thUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI’14).449Cho,K.,Raiko,T.,andIlin,A.(2010).ParalleltemperingiseﬃcientforlearningrestrictedBoltzmannmachines.In.,IJCNN’2010605616731'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 746}, page_content='BIBLIOGRAPHYCho,K.,Raiko,T.,andIlin,A.(2011).EnhancedgradientandadaptivelearningratefortrainingrestrictedBoltzmannmachines.In,pages105–112.ICML’2011676Cho,K.,vanMerriënboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y.(2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatisticalmachinetranslation. InProceedingsoftheEmpiricialMethodsinNaturalLanguageProcessing(EMNLP2014).,,396476477Cho,K.,VanMerriënboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-ertiesofneuralmachinetranslation:Encoder-decoderapproaches.,ArXive-printsa b s / 1 4 0 9 . 1 2 5 9.413Choromanska,A.,Henaﬀ,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014). Thelosssurfaceofmultilayernetworks.,285286Chorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014). End-to-endcontinuousspeechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412.1602.462Christianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalofNumericalAnalysis,(2),135–150.1 2224Chrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures.arXiv1506.03694.413Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling.NIPS’2014DeepLearningworkshop,arXiv1412.3555.,413462Chung,J.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrentneuralnetworks.In.ICML’15413Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).Arecurrentlatentvariablemodelforsequentialdata.In.NIPS’2015700Ciresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneuralnetworkfortraﬃcsignclassiﬁcation.NeuralNetworks,,333–338.,3 223200Ciresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010). Deepbigsimpleneuralnetsforhandwrittendigitrecognition.NeuralComputation,2 2,1–14.2427448,,Coates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparsecodingandvectorquantization.In.,,ICML’201127256500Coates, A.,Lee, H.,andNg,A.Y. (2011).Ananalysisofsingle-layernetworksinunsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS2011).,,363364457732'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 747}, page_content='BIBLIOGRAPHYCoates,A.,Huval,B.,Wang, T.,Wu, D.,Catanzaro, B.,and Andrew,N. (2013).DeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,Proceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),volume28(3),pages1337–1345.JMLRWorkshopandConferenceProceedings.,,2427364449,Cohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:Atensoranalysis.arXiv:1509.05009.556Collobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,UniversitédeParisVI,LIP6.196Collobert,R.(2011).Deeplearningforeﬃcientdiscriminativeparsing.InAISTATS’2011.101479,Collobert,R.andWeston,J.(2008a).Auniﬁedarchitecturefornaturallanguageprocessing:Deepneuralnetworkswithmultitasklearning.In.,ICML’2008473479Collobert, R. and Weston,J. (2008b).A uniﬁed architecture fornatural languageprocessing:Deepneuralnetworkswithmultitasklearning.In.ICML’2008537Collobert,R.,Bengio,S.,andBengio,Y.(2001). AparallelmixtureofSVMsforverylargescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.452Collobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylargescaleproblems.NeuralComputation,(5),1105–1114.1 4452Collobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a).Naturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearningResearch,,2493–2537.,,,1 2328479537538Collobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-mentformachinelearning.InBigLearn,NIPSWorkshop.,,25210448Comon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,3 6,287–314.493Cortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning,2 0,273–297.,18140Couprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentationusingdepthinformation.InInternationalConferenceonLearningRepresentations(ICLR2013).,23200Courbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeeplearning.InArxiv:1412.7024,ICLR’2015Workshop.454Courville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesbyspike-and-slabRBMs.In.,ICML’11563683733'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 748}, page_content='BIBLIOGRAPHYCourville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slabRBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisandMachineIntelligence,IEEETransactionson,(9),1874–1887.3 6684Cover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition.Wiley-Interscience.73Cox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearchapproachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognitionandWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8–15.IEEE.363Cramér,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,135295Crick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature,3 0 4,111–114.611Cybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.MathematicsofControl,Signals,andSystems,,303–314.2197Dahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognitionwiththemean-covariancerestrictedBoltzmannmachine.In.NIPS’201023Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeepneuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,Speech,andLanguageProcessing,(1),33–42.2 0461Dahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).ImprovingdeepneuralnetworksforLVCSRusingrectiﬁedlinearunitsanddropout.In.ICASSP’2013461Dahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksforQSARpredictions.arXiv:1406.1231.26Dauphin, Y.andBengio, Y.(2013).Stochasticratiomatchingof RBMsforsparsehigh-dimensionalinputs.In.NIPSFoundation.NIPS26621Dauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswithreconstructionsampling.In.ICML’2011473Dauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014).Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization.In.,,NIPS’2014285286288Davis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T.(2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactionsonGraphics(Proc.SIGGRAPH),(4),79:1–79:10.3 3454734'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 749}, page_content='BIBLIOGRAPHYDayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsofthe1990ConnectionistSummerSchool,SanMateo,CA.693Dayan,P.andHinton,G.E.(1996).VarietiesofHelmholtzmachine.NeuralNetworks,9(8),1385–1403.695Dayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtzmachine.Neuralcomputation,(5),889–904.7695Dean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,Senior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeepnetworks.In.,NIPS’201225449Dean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation.ComputationalIntelligence,(3),142–150.5664Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990).Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformationScience,(6),391–407.,4 1478484Delalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS19556,Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet: ALarge-ScaleHierarchicalImageDatabase.In.CVPR0921Deng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceonComputerVision:PartV,ECCV’10,pages71–84,Berlin,Heidelberg.Springer-Verlag.21Deng,L.andYu,D.(2014).Deeplearning–methodsandapplications.FoundationsandTrendsinSignalProcessing.462Deng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binarycodingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,Chiba,Japan.23Denil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattendwithdeeparchitecturesforimagetracking.NeuralComputation,2 4(8),2151–2184.367Denton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).DeepgenerativeimagemodelsusingaLaplacianpyramidofadversarialnetworks..,,NIPS703704720Desjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsforvision. TechnicalReport1327,Départementd’InformatiqueetdeRechercheOpéra-tionnelle,UniversitédeMontréal.685735'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 750}, page_content='BIBLIOGRAPHYDesjardins, G., Courville, A.C., Bengio, Y., Vincent, P., andDelalleau, O.(2010).TemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages145–152.,605616Desjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction.In.NIPS’2011631Desjardins,G.,Simonyan,K.,Pascanu,R.,(2015). Naturalneuralnetworks. Inetal.AdvancesinNeuralInformationProcessingSystems,pages2062–2070.320Devlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fastandrobustneuralnetworkjointmodelsforstatisticalmachinetranslation. InProc.ACL’2014.475Devroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:Bücher.SpringerNewYork.696DiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs.neuronsvs.machines.NIPSTutorial.,26366Dinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponentsestimation.arXiv:1410.8516.495Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,K.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisualrecognitionanddescription.arXiv:1411.4389.102Donoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembeddingtechniquesforhigh-dimensional data.TechnicalReport2003-08, Dept.Statistics,StanfordUniversity.,163521Dosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswithconvolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages1538–1546.,,697706707Doya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning.IEEETransactionsonNeuralNetworks,,75–80.,1402405Dreyfus, S. E.(1962).The numerical solutionofvariational problems.JournalofMathematicalAnalysisandApplications,,30–45.5 ( 1 )224Dreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtimelag.IEEETransactionsonAutomaticControl,,383–385.1 8 ( 4 )224Drucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdoubleback-propagation.IEEETransactionsonNeuralNetworks,(6),991–997.3271736'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 751}, page_content='BIBLIOGRAPHYDuchi,J.,Hazan,E.,andSinger,Y.(2011). Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.JournalofMachineLearningResearch.307Dudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning.InProceedingsofthe28thInternationalConferenceonMachinelearning,ICML’11.484Dugas,C.,Bengio,Y.,Bélisle,F.,andNadeau,C.(2001). Incorporatingsecond-orderfunctionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,editors, AdvancesinNeural InformationProcessingSystems 13(NIPS’00), pages472–478.MITPress.,68196Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-worksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906.705ElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-termdependencies.In.,NIPS’1995400409Elkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachforcrossdomainusermodelinginrecommendationsystems. InProceedingsofthe24thInternationalConferenceonWorldWideWeb,pages278–288.482Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceofstartingsmall.Cognition,,781–799.4 8328Erhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thediﬃcultyoftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InProceedingsofAISTATS’2009.200Erhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010).Whydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes.531535536,,Fahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitecturesfor AI:NETL,thistle, andBoltzmann machines.In Proceedings ofthe NationalConferenceonArtiﬁcialIntelligenceAAAI-83.,572656Fang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Dollár,P.,Gao,J.,He,X.,Mitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisualconceptsandback.arXiv:1411.4952.102Farabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,andTalay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,M.Bilenko,andJ.Langford, editors,ScalingupMachineLearning:ParallelandDistributedApproaches.CambridgeUniversityPress.525737'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 752}, page_content='BIBLIOGRAPHYFarabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeaturesforscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,3 5(8),1915–1929.,,23200359Fei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories.IEEETransactionsonPatternAnalysisandMachineIntelligence,2 8(4),594–611.540Finn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learningvisualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXivpreprintarXiv:1509.06113.25Fisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.AnnalsofEugenics,,179–188.,721105Földiák,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternationalJointConferenceonNeuralNetworks(IJCNN),volume1,pages401–405,Washington1989.IEEE,NewYork.496Franzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,head-direction,andspatial-viewcells.497Franzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslowfeatureanalysis.InArtiﬁcialNeuralNetworks-ICANN2008,pages961–970.Springer.498Frasconi,P.,Gori,M.,andSperduti,A.(1997).Ontheeﬃcientclassiﬁcationofdatastructuresbyneuralnetworks.InProc.Int.JointConf.onArtiﬁcialIntelligence.,400402Frasconi, P., Gori, M., andSperduti, A.(1998).Ageneralframeworkforadaptiveprocessingofdatastructures.IEEETransactionsonNeuralNetworks,9(5),768–786.400402,Freund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.InMachineLearning:ProceedingsofThirteenthInternationalConference,pages148–156,USA.ACM.258Freund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.InProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages325–332.258Frey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication.MITPress.,707708Frey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngooddensityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95),pages661–670.MITPress,Cambridge,MA.653738'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 753}, page_content='BIBLIOGRAPHYFrobenius,G.(1908).Übermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss.Berlin,Germany.599Fukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.BiologicalCybernetics,,121–136.,,2 016225530Fukushima, K.(1980).Neocognitron:Aself-organizingneuralnetworkmodelforamechanismofpatternrecognitionunaﬀectedbyshiftinposition.BiologicalCybernetics,3 6,193–202.,,,,162427225367Gal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulliapproximatevariationalinference.arXivpreprintarXiv:1506.02158.264Gallinari,P.,LeCun,Y.,Thiria,S.,andFogelman-Soulie,F.(1987).Memoiresassociativesdistribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.517Garcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwoandthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprintarXiv:1506.00999.486Garofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993).Darpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1.NASASTI/ReconTechnicalReportN,,27403.9 3461Garson,J.(1900).Themetricsystemofidentiﬁcationofcriminals,asusedinGreatBritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainandIreland,(2),177–227.21Gers,F.A.,Schmidhuber,J.,andCummins,F.(2000). Learningtoforget:ContinualpredictionwithLSTM.Neuralcomputation,(10),2451–2471.,1 2410414Ghahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactoranalyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.491Gillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguageprocessingfrombytes.arXivpreprintarXiv:1512.00103.479Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutionalnetworksforaccurateobjectdetectionandsegmentation.428Giudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogenyofmirrorneurons.,(2),350––363.Dev.Sci.1 2658Glorot,X.andBengio,Y.(2010).Understandingthediﬃcultyoftrainingdeepfeedforwardneuralnetworks.InAISTATS’2010.303Glorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserectiﬁerneuralnetworks.InAISTATS’2011.,,,16173196226739'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 754}, page_content='BIBLIOGRAPHYGlorot, X.,Bordes, A.,andBengio, Y.(2011b).Domainadaptationforlarge-scalesentimentclassiﬁcation:Adeeplearningapproach.In.,ICML’2011509539Goldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhoodcomponentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04).MITPress.115Gong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFaceRecognition.ImperialCollegePress.,164521Goodfellow,I.,Le,Q.,Saxe,A., andNg,A.(2009).Measuringinvariancesindeepnetworks.In,pages646–654.NIPS’2009255Goodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010).Helpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction(HRI),Osaka,Japan.ACMPress,ACMPress.100Goodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolutionforautoencoders.Technicalreport,UniversitédeMontréal.357Goodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels.InInternationalConferenceonLearningRepresentations,WorkshopsTrack.,,624702703Goodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecodingforunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearningHierarchicalModels.,534540Goodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a).Maxoutnetworks.InS.DasguptaandD.McAllester,editors,,pages1319–ICML’131327.,,,,192264344365457Goodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeepBoltzmannmachines.In.NIPSFoundation.,,,,,,,NIPS26100619673674675676677700Goodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,Bergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearchlibrary.arXivpreprintarXiv:1308.4214.,25448Goodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodelsforunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachineIntelligence,(8),1902–1914.,,,,3 5499500501652685Goodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempiricalinvestigationofcatastrophicforgetingingradient-basedneuralnetworks.In.ICLR’2014193740'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 755}, page_content='BIBLIOGRAPHYGoodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-sarialexamples.,.,,,,CoRRa b s / 1 4 1 2 . 6 5 7 2268269271557558Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In.NIPS’2014546691702703706,,,,Goodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digitnumberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks.InInternationalConferenceonLearningRepresentations.,,,,,,25101200201202390424451,Goodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneuralnetworkoptimizationproblems.InInternationalConferenceonLearningRepresenta-tions.,,,285286287291Goodman,J.(2001).Classes forfast maximumentropytraining.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.469Gori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEETransactionsonPatternAnalysisandMachineIntelligence,PA M I - 1 4(1),76–86.284Gosset,W.S.(1908).Theprobableerrorofamean.,Biometrika6(1),1–25.Originallypublishedunderthepseudonym“Student”.21Gouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributedrepresentationswithoutwordalignments.Technicalreport,arXiv:1410.2455.,478541Graf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.CircuitsandDevicesMagazine,IEEE,(4),44–49.5453Graves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In.NIPS’2011242Graves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.StudiesinComputationalIntelligence.Springer.,,,374395413462Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,arXiv:1308.0850.,,,189410417421Graves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrentneuralnetworks.In.ICML’2014410Graves,A.andSchmidhuber,J.(2005).Framewisephonemeclassiﬁcationwithbidirec-tionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks,1 8(5),602–610.395Graves,A.andSchmidhuber,J.(2009).Oﬄinehandwritingrecognitionwithmultidi-mensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,,pages545–552.NIPS’2008395741'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 756}, page_content='BIBLIOGRAPHYGraves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporalclassiﬁcation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.InICML’2006,pages369–376,Pittsburgh,USA.462Graves,A.,Liwicki,M.,Bunke,H.,Schmidhuber,J.,andFernández,S.(2008).Uncon-strainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,,pages577–584.NIPS’2007395Graves,A.,Liwicki,M.,Fernández,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J.(2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.PatternAnalysisandMachineIntelligence,IEEETransactionson,(5),855–868.3 1410Graves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrentneuralnetworks.In,pages6645–6649.,,,,,,ICASSP’2013395398400410412413462Graves,A.,Wayne,G.,andDanihelka,I.(2014a).NeuralTuringmachines.arXiv:1410.5401.25Graves,A.,Wayne,G.,andDanihelka,I.(2014b).NeuralTuringmachines.arXivpreprintarXiv:1410.5401.,418420Grefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningtotransducewithunboundedmemory.In.NIPS’2015420Greﬀ,K.,Srivastava,R.K.,Koutník,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015).LSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.414Gregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproductnetworkwithlocalreceptiveﬁelds.Technicalreport,arXiv:1006.0448.352Gregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceonMachineLearning(ICML-10).ACM.654Gregor, K.,Danihelka, I.,Mnih,A., Blundell,C.,and Wierstra, D. (2014).Deepautoregressivenetworks.InInternationalConferenceonMachineLearning(ICML’2014).695Gregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneuralnetworkforimagegeneration.arXivpreprintarXiv:1502.04623.700Gretton,A.,Borgwardt,K.M.,Rasch,M.J.,Schölkopf,B.,andSmola,A.(2012).Akerneltwo-sampletest.TheJournalofMachineLearningResearch,1 3(1),723–773.705Gülçehre,Ç.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformationforoptimization.InInternationalConferenceonLearningRepresentations(ICLR’2013).25742'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 757}, page_content='BIBLIOGRAPHYGuo,H.andGelfand,S.B.(1992).Classiﬁcationtreeswithneuralnetworkfeatureextraction.NeuralNetworks,IEEETransactionson,(6),923–933.3452Gupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearningwithlimitednumericalprecision.,.CoRRa b s / 1 5 0 2 . 0 2 5 5 1454Gutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation: Anewestima-tionprincipleforunnormalizedstatisticalmodels. InProceedingsofTheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10).622Hadsell, R., Sermanet, P., Ben, J.,Erkan,A., Han, J., Muller,U., andLeCun, Y.(2007).Onlinelearningforoﬀroadrobots:Spatiallabelpropagationtolearnlong-rangetraversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.455Hajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuitsofboundeddepth.,,129–154.J.Comput.System.Sci.4 6198Håstad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedingsofthe18thannualACMSymposiumonTheoryofComputing,pages6–20,Berkeley,California.ACMPress.198Håstad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits.ComputationalComplexity,,113–129.1198Hastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:datamining,inferenceandprediction. SpringerSeriesinStatistics.SpringerVerlag.145He,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorectiﬁers:Surpassinghuman-levelperformanceonImageNetclassiﬁcation.arXivpreprintarXiv:1502.01852.28192,Hebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1417658Henaﬀ,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearningofsparsefeaturesforscalableaudioclassiﬁcation.In.ISMIR’11525Henderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatisticalparsing.InHLT-NAACL,pages103–110.479Henderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.InProceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,page95.479Henniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andLücke,J.(2010).Binarysparsecoding.InLatentVariableAnalysisandSignalSeparation,pages450–457.Springer.642743'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 758}, page_content='BIBLIOGRAPHYHerault,J.andAns,B.(1984).Circuitsneuronauxàsynapsesmodiﬁables:Décodagedemessagescompositesparapprentissagenonsupervisé.ComptesRendusdel’AcadémiedesSciences,,525––528.2 9 9 ( III - 1 3 )493Hinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.307Hinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,Nguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacousticmodelinginspeechrecognition.IEEESignalProcessingMagazine,2 9(6),82–97.,23462Hinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531.450Hinton,G.E.(1989).Connectionistlearningprocedures.ArtiﬁcialIntelligence,4 0,185–234.496Hinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.ArtiﬁcialIntelligence,(1),47–75.4 6420Hinton,G.E.(1999).Productsofexperts.In.ICANN’1999572Hinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence.TechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,612678Hinton,G.E.(2006).Torecognizeshapes,ﬁrstlearntogenerateimages.TechnicalReportUTMLTR2006-003,UniversityofToronto.,530597Hinton,G.E.(2007a).Howtodobackpropagationinabrain.InvitedtalkattheNIPS’2007DeepLearningWorkshop.658Hinton,G.E.(2007b). Learningmultiplelayersofrepresentation. Trendsincognitivesciences,(10),428–434.1 1662Hinton, G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines.TechnicalReportUTMLTR2010-003,DepartmentofComputerScience,UniversityofToronto.612Hinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparsedistributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon.146Hinton,G.E.andMcClelland,J.L.(1988).Learningrepresentationsbyrecirculation.InNIPS’1987,pages358–366.504Hinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In.NIPS’2002521744'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 759}, page_content='BIBLIOGRAPHYHinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawithneuralnetworks.Science,(5786),504–507.,,,,3 1 3511526530531536Hinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter7,pages282–317.MITPress,Cambridge.,572656Hinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneuralcomputation.MITpress.543Hinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsofacquireddyslexia.Psychologicalreview,(1),74.9 813Hinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,andHelmholtzfreeenergy.In.NIPS’1993504Hinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraintsatisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-MellonUniversity,Dept.ofComputerScience.,572656Hinton,G.E.,McClelland,J.,andRumelhart,D.(1986). Distributedrepresentations.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing:ExplorationsintheMicrostructureofCognition,volume1,pages77–109.MITPress,Cambridge.,,17225528Hinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusingmixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,AdvancesinNeuralInformationProcessingSystems7(NIPS’94),pages1015–1022.MITPress,Cambridge,MA.491Hinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithmforunsupervisedneuralnetworks.Science,,1558–1161.,2 6 8506653Hinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesofhandwrittendigits.IEEETransactionsonNeuralNetworks,,65–74.8501Hinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.InProceedingsof3rdInternationalConferenceonIndependentComponentAnalysisandBlindSignalSeparation(ICA’01),pages746–751,SanDiego,CA.493Hinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbeliefnets.NeuralComputation,,1527–1554.,,,,,,,1 8141927142530531662663Hinton,G.E., Deng,L.,Yu, D.,Dahl,G.E.,Mohamed, A.,Jaitly, N.,Senior,A.,Vanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups.IEEESignalProcess.Mag.,(6),82–97.2 9101745'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 760}, page_content='BIBLIOGRAPHYHinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c).Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technicalreport,arXiv:1207.0580.,,238263267Hinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge. InvitedtalkattheBayLearnBayAreaMachineLearningSymposium.450Hochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diplomathesis,T.U.München.,,18402404Hochreiter,S.andSchmidhuber,J.(1995). Simplifyingneuralnetsbydiscoveringﬂatminima.InAdvancesinNeuralInformationProcessingSystems7,pages529–536.MITPress.243Hochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,9(8),1735–1780.,,18410413Hochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradientﬂowinrecurrentnets:thediﬃcultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,FieldGuidetoDynamicalRecurrentNetworks.IEEEPress.413Holi,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetworkhardwareimplementations.Computers,IEEETransactionson,(3),281–290.4 2453Holt,J.L.andBaker,T.E.(1991). Backpropagationsimulationsusinglimitedpreci-sioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJointConferenceon,volume2,pages121–126.IEEE.453Hornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksareuniversalapproximators.NeuralNetworks,,359–366.2197Hornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofanunknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neuralnetworks,(5),551–560.3197Hsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorldChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2Huang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkovrandomﬁeldsonlattice.AnnalsoftheInstituteofStatisticalMathematics,5 4(1),1–18.618Huang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeepstructuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsofthe22ndACMinternationalconferenceonConferenceoninformation&knowledgemanagement,pages2333–2338.ACM.482Hubel,D.andWiesel,T.(1968).Receptiveﬁeldsandfunctionalarchitectureofmonkeystriatecortex.JournalofPhysiology(London),,215–243.1 9 5364746'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 761}, page_content='BIBLIOGRAPHYHubel,D.H.andWiesel,T.N.(1959).Receptiveﬁeldsofsingleneuronsinthecat’sstriatecortex.JournalofPhysiology,,574–591.1 4 8364Hubel,D.H.andWiesel, T.N.(1962).Receptiveﬁelds, binocularinteraction,andfunctionalarchitectureinthecat’svisualcortex.JournalofPhysiology(London),1 6 0,106–154.364Huszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,adversary?.arXiv:1511.05101699Hutter,F.,Hoos,H.,andLeyton-Brown,K.(2011). Sequentialmodel-basedoptimizationforgeneralalgorithmconﬁguration.In.ExtendedversionasUBCTechreportLION-5TR-2010-10.438Hyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP’96,pages13–24.379Hyvärinen,A.(1999). Surveyonindependentcomponentanalysis.NeuralComputingSurveys,,94–128.2493Hyvärinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching.JournalofMachineLearningResearch,,695–709.,6515619Hyvärinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,andpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeuralNetworks,,1529–1531.1 8620Hyvärinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsandDataAnalysis,,2499–2512.5 1620Hyvärinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcellpropertiesfromnaturalimagesusingextensionsofica.In,pages827–833.NIPS495Hyvärinen, A.andPajunen, P.(1999).Nonlinearindependentcomponentanalysis:Existenceanduniquenessresults.NeuralNetworks,(3),429–439.1 2495Hyvärinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis.Wiley-Interscience.493Hyvärinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponentanalysis.NeuralComputation,(7),1527–1558.1 3495Hyvärinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilisticapproachtoearlycomputationalvision.Springer-Verlag.370Iba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,C 1 2,623–656.605747'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 762}, page_content='BIBLIOGRAPHYInayoshi, H. and Kurita, T. (2005).Improved generalizationbyadding both auto-associationandhidden-layernoisetoneural-network-based-classiﬁers.IEEEWorkshoponMachineLearningforSignalProcessing,pages141—-146.517Ioﬀe,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.,,100317320Jacobs,R.A.(1988). Increasedratesofconvergencethroughlearningrateadaptation.Neuralnetworks,(4),295–307.1307Jacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.(1991).Adaptivemixturesoflocalexperts.NeuralComputation,,79–87.,3188452Jaeger,H.(2003).Adaptivenonlinearsystemidentiﬁcationwithechostatenetworks.InAdvancesinNeuralInformationProcessingSystems15.405Jaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostatenetworks.Technicalreport,JacobsUniversity.400Jaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330.2405Jaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulationstudy.Technicalreport,Technicalreport,JacobsUniversityBremen.406Jaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsandsavingenergyinwirelesscommunication.Science,(5667),78–80.,3 0 427405Jaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationandapplicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,2 0(3),335–352.409Jain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,M.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestorationwithconvolutionalnetworks.InComputer Vision,2007.ICCV2007.IEEE11thInternationalConferenceon,pages1–8.IEEE.359Jaitly,N.andHinton,G.(2011).LearningabetterrepresentationofspeechsoundwavesusingrestrictedBoltzmannmachines.InAcoustics, SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConferenceon,pages5884–5887.IEEE.460Jaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improvesspeechrecognition.In.ICML’2013241Jarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebestmulti-stagearchitectureforobjectrecognition?In.,,,,,,ICCV’09162427173192226363364525,,Jarzynski,C.(1997).Nonequilibriumequalityforfreeenergydiﬀerences.Phys.Rev.Lett.,7 8,2690–2693.,627630748'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 763}, page_content='BIBLIOGRAPHYJaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversityPress.53Jean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetargetvocabularyforneuralmachinetranslation.arXiv:1412.2007.476Jelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparametersfromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitioninPractice.North-Holland,Amsterdam.,464475Jia,Y.(2013).Caﬀe:Anopensourceconvolutionalarchitectureforfastfeatureembedding.http://caffe.berkeleyvision.org/.,25210Jia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptiveﬁeldlearningforpooledimagefeatures.InComputerVisionandPatternRecognition(CVPR),2012IEEEConferenceon,pages3370–3377.IEEE.345Jim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneuralnetworks: convergenceandgeneralization.IEEETransactionsonNeuralNetworks,7(6),1424–1438.242Jordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.18Joulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmentedrecurrentnets.arXivpreprintarXiv:1503.01007.420Jozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrentnetworkarchitectures.In.,,ICML’2015306413414Judd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress.293Jutten, C.andHerault, J.(1991).Blindseparationofsources, partI:anadaptivealgorithmbasedonneuromimeticarchitecture.SignalProcessing,,1–10.2 4493Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gülçehre,c.,Memisevic,R.,Vincent,P.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,Y.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer, J.,Lamblin,P.,Raymond,J.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,E.,Côté,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeciﬁcdeepneuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMonInternationalConferenceonMultimodalInteraction.200Kalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.InEMNLP’2013.476Kalchbrenner,N.,Danihelka,I.,andGraves,A.(2015). Gridlongshort-termmemory.arXivpreprintarXiv:1507.01526.396749'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 764}, page_content='BIBLIOGRAPHYKamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder.IEEETransactionsonPatternAnalysisandMachineIntelligence.517Karpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimagedescriptions.In.arXiv:1412.2306.CVPR’2015102Karpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014).Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.In.CVPR21Karush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSideConstraints.Master’sthesis,Dept.ofMathematics,Univ.ofChicago.95Katz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodelcomponentofaspeechrecognizer.IEEETransactionsonAcoustics,Speech,andSignalProcessing,(3),400–401.,A S S P - 3 5464475Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008). Fastinferenceinsparsecodingalgorithmswithapplicationstoobjectrecognition.Technicalreport,ComputationalandBiologicalLearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01.525Kavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariantfeaturesthroughtopographicﬁltermaps.In.CVPR’2009525Kavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y.(2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In.NIPS’2010364525,Kelley,H.J.(1960).Gradienttheoryofoptimalﬂightpaths.,ARSJournal3 0(10),947–954.224Khan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearningandteachingdimension.InAdvancesinNeuralInformationProcessingSystems24(NIPS’11),pages1449–1457.328Kim,S.K.,McAfee,L.C.,McMahon,P.L.,andOlukotun,K.(2009).AhighlyscalablerestrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogicandApplications,2009.FPL2009.InternationalConferenceon,pages367–372.IEEE.453Kindermann,R.(1980).MarkovRandomFieldsandTheirApplications(ContemporaryMathematics;V.1).AmericanMathematicalSociety.568Kingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.308Kingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscorematching.In.,NIPS’2010515622750'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 765}, page_content='BIBLIOGRAPHYKingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearningwithdeepgenerativemodels.In.NIPS’2014428Kingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariablemodelsinauxiliaryform.Technicalreport,arxiv:1306.0733.,,654691698Kingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR).,691701Kingma, D.P.andWelling, M.(2014b).Eﬃcientgradient-basedinferencethroughtransformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480.691Kirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulatedannealing.Science,,671–680.2 2 0327Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels.In.ICML’2014102Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddingswithmultimodalneurallanguagemodels..,arXiv:1411.2539[cs.LG]102410Klementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributedrepresentationsofwords.InProceedingsofCOLING2012.,478541Knowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,andPﬁster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26Koller,D.andFriedman, N.(2009).ProbabilisticGraphicalModels:PrinciplesandTechniques.MITPress.,,585597647Konig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationandmaximizationofaposterioriprobabilities–applicationtotransition-basedconnectionistspeechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.461Koren,Y.(2009).TheBellKorsolutiontotheNetﬂixgrandprize.,258481Kotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividuallabelsusingdeepfeatures.In.ACMSIGKDD106Koutnik,J.,Greﬀ,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.InICML’2014.409Kočiský,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-sentationsbyMarginalizingAlignments.InProceedingsofACL.478Krause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).ApproximationpropertiesofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In.ICML’2013555751'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 766}, page_content='BIBLIOGRAPHYKrizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,UniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-cifar10-aug2010.pdf.448Krizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport,UniversityofToronto.,21563Krizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-basedimageretrieval.In.ESANN527Krizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassiﬁcationwithdeepconvolutionalneuralnetworks.In.,,,,,,,NIPS’2012232427100200371456460Krueger,K.A.andDayan,P.(2009).Flexibleshaping:howlearninginsmallstepshelps.Cognition,,380–394.1 1 0328Kuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsoftheSecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481–492,Berkeley,Calif.UniversityofCaliforniaPress.95Kumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,M.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworksfornaturallanguageprocessing..,arXiv:1506.07285420487Kumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariablemodels.In.NIPS’2010328Lang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetworkarchitectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-MellonUniversity.,,367374408Lang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetworkarchitectureforisolatedwordrecognition.Neuralnetworks,(1),23–43.3374Langford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armedbandits.In,pages1096––1103.NIPS’2008482Lappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinearindependentcomponentanalysisusingensemblelearning:Experimentsanddiscussion.InProc.ICA.Citeseer.495Larochelle, H. and Bengio, Y.(2008).Classiﬁcation usingdiscriminative restrictedBoltzmannmachines.In.,,,,ICML’2008244255532688717Larochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswithathird-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems23,pages1243–1251.367752'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 767}, page_content='BIBLIOGRAPHYLarochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator.InAISTATS’2011.,707710Larochelle,H.,Erhan,D.,andBengio,Y.(2008). Zero-datalearningofnewtasks.InAAAIConferenceonArtiﬁcialIntelligence.541Larochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfortrainingdeepneuralnetworks.JournalofMachineLearningResearch,,1–40.1 0537Lasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeanddiscriminativemodels.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’06),pages87–94,Washington,DC,USA.IEEEComputerSociety.244253,Le,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiledconvolutionalneuralnetworks.InJ.Laﬀerty,C.K.I.Williams,J.Shawe-Taylor,R.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems23(NIPS’10),pages1279–1287.352Le,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimizationmethodsfordeeplearning.InProc.ICML’2011.ACM.316Le,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,A.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.InICML’2012.,2427LeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmannmachinesanddeepbeliefnetworks.NeuralComputation,(6),1631–1649.,2 0555657LeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-mators.NeuralComputation,(8),2192–2207.2 2555LeCun,Y.(1985).Uneprocédured’apprentissagepourRéseauàseuilassymétrique.InCognitiva85:AlaFrontièredel’IntelligenceArtiﬁcielle,desSciencesdelaConnaissanceetdesNeurosciences,pages599–604,Paris1985.CESTA,Paris.224LeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-Soulié,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiologicalOrganization,pages233–240.Springer-Verlag,LesHouches,France.350LeCun,Y.(1987).Modèlesconnexionistesdel’apprentissage.Ph.D.thesis,UniversitédeParisVI.,,18504517LeCun, Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReportCRG-TR-89-4,UniversityofToronto.,330350753'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 768}, page_content='BIBLIOGRAPHYLeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,Howard,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applicationsofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,2 7(11),41–46.368LeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K.-R.(1998a).Eﬃcientbackprop.InNeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524.SpringerVerlag.,310431LeCun,Y.,Bottou,L.,Bengio,Y.,andHaﬀner,P.(1998b).Gradientbasedlearningappliedtodocumentrecognition.Proc.IEEE.,,,,,,16182127371460462LeCun, Y., Kavukcuoglu, K., andFarabet, C.(2010).Convolutionalnetworksandapplicationsinvision. InCircuitsandSystems(ISCAS),Proceedingsof2010IEEEInternationalSymposiumon,pages253–256.IEEE.371L’Ecuyer,P.(1994).Eﬃciencyimprovementandvariancereduction.InProceedingsofthe1994WinterSimulationConference,pages122––132.692Lee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets.arXivpreprintarXiv:1409.5185.326Lee,H.,Battle,A.,Raina,R.,andNg,A.(2007).Eﬃcientsparsecodingalgorithms.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformationProcessingSystems19(NIPS’06),pages801–808.MITPress.639Lee,H.,Ekanadham,C.,andNg,A.(2008).SparsedeepbeliefnetmodelforvisualareaV2.In.NIPS’07255Lee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbeliefnetworksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09).ACM,Montreal,Canada.,,363685686Lee,Y.J.andGrauman,K.(2011).Learningtheeasythingsﬁrst:self-pacedvisualcategorydiscovery.In.CVPR’2011328Leibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,2010).224Lenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-tionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc.2Leshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforwardnetworkswithanonpolynomialactivationfunctioncanapproximateanyfunction.NeuralNetworks,,861––867.,6197198754'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 769}, page_content='BIBLIOGRAPHYLevenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleastsquares.QuarterlyJournalofAppliedMathematics,(2),164–168.I  I312L’Hôpital,G.F.A.(1696).Analysedesinﬁnimentpetits,pourl’intelligencedeslignescourbes.Paris:L’ImprimerieRoyale.224Li,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks.CoRR,.a b s / 1 5 0 2 . 0 2 7 6 1705Lin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependenciesisnotasdiﬃcultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeuralNetworks,(6),1329–1338.7408Lin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelationembeddingsforknowledgegraphcompletion.InProc.AAAI’15.486Linde,N.(1992).Themachinethatchangedtheworld,episode3.Documentaryminiseries.2Lindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser’sperspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHighEnergyPhysics,pages195––202,Isolad’Elba,Italy.453Linnainmaa, S. (1976).Taylorexpansionofthe accumulated roundingerror.BITNumericalMathematics,(2),146–160.1 6224LISA(2008).Deeplearningtutorials:RestrictedBoltzmannmachines.Technicalreport,LISALab,UniversitédeMontréal.590Long,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardtoapproximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML’10).660Lotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructureusingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,546547Lovelace,A.(1842).NotesuponL.F.Menabrea’s“SketchoftheAnalyticalEngineinventedbyCharlesBabbage”.1Lu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetworkencoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.462Lu,T.,Pál,D.,andPál,M.(2010).Contextualmulti-armedbandits.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages485–492.482Luenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.316Lukoševičius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrentneuralnetworktraining.ComputerScienceReview,(3),127–149.3405755'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 770}, page_content='BIBLIOGRAPHYLuo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesandclass-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages470–478.688Luo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwithconvolutionalspike-and-slabRBMsanddeepextensions.InAISTATS’2013.102Lyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsoftheTwenty-ﬁfthConferenceinUncertaintyinArtiﬁcialIntelligence(UAI’09).620Ma,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnetsasamethodforquantitativestructure–activityrelationships.J.Chemicalinformationandmodeling.532Maas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Rectiﬁernonlinearitiesimproveneuralnetworkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,andLanguageProcessing.192Maass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalogneuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,pages335–344.198Maass,W.,Schnitger,G.,andSontag,E.D.(1994).AcomparisonofthecomputationalpowerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeuralComputationandLearning,pages127–151.198Maass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithoutstablestates:Anewframeworkforneuralcomputationbasedonperturbations.NeuralComputation,(11),2531–2560.1 4405MacKay,D.(2003). InformationTheory,InferenceandLearningAlgorithms.CambridgeUniversityPress.73Maclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameteroptimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.437Mao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioningwithmultimodalrecurrentneuralnetworks.In.arXiv:1410.1090.ICLR’2015102Marcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem.ZeitschriftfürOperationsResearch(Theory),,517–545.3 6276Marlin,B.anddeFreitas,N.(2011).Asymptoticeﬃciencyofdeterministicestimatorsfordiscreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In.,UAI’2011619621756'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 771}, page_content='BIBLIOGRAPHYMarlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).InductiveprinciplesforrestrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10),volume9,pages509–516.,,615620621Marquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-eters.JournaloftheSocietyofIndustrialandAppliedMathematics,1 1(2),431–441.312Marr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,1 9 4.367Martens, J.(2010).DeeplearningviaHessian-freeoptimization.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceonMachineLearning(ICML-10),pages735–742.ACM.304Martens,J.andMedabalimi,V.(2014).Ontheexpressiveeﬃciencyofsumproductnetworks..arXiv:1411.7717556Martens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-freeoptimization.InProc.ICML’2011.ACM.414Mase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuousstatespaceGibbsianprocesses.TheAnnalsofAppliedProbability,5(3),pp.603–612.618McClelland,J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributedprocessing.InComputation&intelligence,pages305–341.AmericanAssociationforArtiﬁcialIntelligence.17McCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervousactivity.BulletinofMathematicalBiophysics,,115–133.,51415Mead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80.SpringerScience&BusinessMedia.453Melchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmannmachines.arXivpreprintarXiv:1311.1354.675Memisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).688Memisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformationswithfactoredhigher-orderBoltzmannmachines.NeuralComputation,2 2(6),1473–1492.688757'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 772}, page_content='BIBLIOGRAPHYMesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,Muller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,J.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.InJMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,200534540Mesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Surﬁngonthemanifold.LearningWorkshop,Snowbird.713Miikkulainen,R.andDyer,M.G.(1991).NaturallanguageprocessingwithmodularPDPnetworksanddistributedlexicon.CognitiveScience,,343–399.1 5479Mikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,BrnoUniversityofTechnology.416Mikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empiricalevaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-nualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH2011).474Mikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfortraininglargescaleneuralnetworklanguagemodels.InProc.ASRU’2011.,328474Mikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).Eﬃcientestimationofwordrep-resentationsinvectorspace.InInternationalConferenceonLearningRepresentations:WorkshopsTrack.538Mikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguagesformachinetranslation.Technicalreport,arXiv:1309.4168.541Minka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridgeUKTechRepMSRTR2005173,(TR-2005-173).7 2626Minsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.15Mirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprintarXiv:1411.1784.703Mishkin,D.and Matas,J.(2015).Allyouneedisagoodinit.arXivpreprintarXiv:1511.06422.305Misra,J.andSaha,I.(2010). Artiﬁcialneuralnetworksinhardware:Asurveyoftwodecadesofprogress.Neurocomputing,(1),239–255.7 4453Mitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.99Miyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributionalsmoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507.00677.ICLR269758'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 773}, page_content='BIBLIOGRAPHYMnih,A.andGregor, K.(2014).Neuralvariationalinferenceandlearninginbeliefnetworks.In.,ICML’2014693695Mnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguagemodelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternationalConferenceonMachineLearning(ICML’07),pages641–648.ACM.466Mnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems21(NIPS’08),pages1081–1088.469Mnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingseﬃcientlywithnoise-contrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages2265–2273.CurranAssociates,Inc.,474624Mnih, A.andTeh, Y. W.(2012).Afastandsimple algorithmfortrainingneuralprobabilisticlanguagemodels.In,pages1751–1758.ICML’2012474Mnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages.InProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).102Mnih,V.,Larochelle,H., andHinton,G.(2011).ConditionalrestrictedBoltzmannmachinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArtiﬁcialIntelligence(UAI).687Mnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013).PlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312.5602.106Mnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisualattention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,editors,,pages2204–2212.NIPS’2014693Mnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,Antonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015).Human-levelcontrolthroughdeepreinforcementlearning.Nature,,529–533.5 1 825Mobahi,H.andFisher, III,J.W.(2015).AtheoreticalanalysisofoptimizationbyGaussiancontinuation.In.AAAI’2015327Mobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherenceinvideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternationalConferenceonMachineLearning,pages737–744,Montreal.Omnipress.496Mohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition.461759'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 774}, page_content='BIBLIOGRAPHYMohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,M.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.InAcoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConferenceon,pages5060–5063.IEEE.461Mohamed,A.,Dahl,G.,andHinton,G.(2012a). Acousticmodelingusingdeepbeliefnetworks.IEEETrans.onAudio,SpeechandLanguageProcessing,2 0(1),14–22.461Mohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworksperformacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),2012IEEEInternationalConferenceon,pages4273–4276.IEEE.461Moller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning.NeuralNetworks,,525–533.6316Montavon,G.andMuller,K.-R.(2012). DeepBoltzmannmachinesandthecenteringtrick.InG.Montavon,G.Orr,andK.-R.Müller,editors,NeuralNetworks:TricksoftheTrade,volume7700ofLectureNotesinComputerScience,pages621–637.Preprint:http://arxiv.org/abs/1203.3783.675Montúfar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworkswithdiscreteunits.NeuralComputation,.2 6555Montúfar,G.andAy,N.(2011).ReﬁnementsofuniversalapproximationresultsfordeepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation,2 3(5),1306–1319.555Montufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinearregionsofdeepneuralnetworks.In.,NIPS’201419199Mor-Yosef,S.,Samueloﬀ,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Rankingtheriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.ObstetGynecol,(6),944–7.7 53Morin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguagemodel.InAISTATS’2005.,469471Mozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.HansonandR.Lippmann, editors, AdvancesinNeural InformationProcessingSystems4(NIPS’91),pages275–282,SanMateo,CA.MorganKaufmann.409Murphy,K. P.(2012).MachineLearning:a Probabilistic Perspective.MIT Press,Cambridge,MA,USA.,,6298145Murray,B.U.I.andLarochelle,H.(2014).Adeepandtractabledensityestimator.InICML’2014.,189712Nair,V.andHinton,G.(2010).RectiﬁedlinearunitsimproverestrictedBoltzmannmachines.In.,,ICML’201016173196760'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 775}, page_content='BIBLIOGRAPHYNair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22,pages1339–1347.CurranAssociates,Inc.688Narayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis.In.NIPS’2010163Naumann,U.(2008).OptimalJacobianaccumulationisNP-complete.MathematicalProgramming,(2),427–441.1 1 2221Navigli,R.andVelardi,P.(2005). Structuralsemanticinterconnections:aknowledge-basedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisandMachineIntelligence,(7),1075––1086.2 7486Neal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjustiﬁesincremental,sparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MITPress,Cambridge,MA.636Neal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.694Neal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods.TechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.682Neal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions.TechnicalReport9421,Dept.ofStatistics,UniversityofToronto.605Neal,R.M.(1996).BayesianLearningforNeuralNetworks.LectureNotesinStatistics.Springer.265Neal,R.M.(2001).Annealedimportancesampling.,StatisticsandComputing1 1(2),125–139.,,627629630Neal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportancesampling.631Nesterov,Y.(1983).AmethodofsolvingaconvexprogrammingproblemwithconvergencerateO/k(12).,,372–376.SovietMathematicsDoklady2 7300Nesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Appliedoptimization.KluwerAcademicPubl.,Boston,Dordrecht,London.300Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Readingdigits in naturalimages withunsupervised feature learning.Deep Learning andUnsupervisedFeatureLearningWorkshop,NIPS.21Ney,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatisticallanguagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology(Eurospeech),pages973–976,Berlin.465761'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 776}, page_content='BIBLIOGRAPHYNg,A.(2015).Adviceforapplyingmachinelearning.https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.423Niesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-speechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages177–180.465Ning,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005).Towardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,IEEETransactionson,(9),1360–1371.1 4360Nocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9295Norouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.InICML’2011.527Nowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociativemixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.452Nowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing.NeuralComputation,(4),473–493.4139Olshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?NeuralComputation,,1665–1699.1 716Olshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptiveﬁeldpropertiesbylearningasparsecodefornaturalimages. Nature,3 8 1,607–609.,,,146255370498Olshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiologicalmodelofvisualattentionandinvariantpatternrecognitionbasedondynamicroutingofinformation.J.Neurosci.,(11),4700–4719.1 3452Opper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited.Neuralcomputation,(3),786–792.2 1691Oquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-levelimagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionandPatternRecognition(CVPR),2014IEEEConferenceon,pages1717–1724.IEEE.538Osindero,S.andHinton,G.E.(2008).ModelingimagepatcheswithadirectedhierarchyofMarkovrandomﬁelds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1121–1128,Cambridge,MA.MITPress.634OvidandMartin,C.(2004)..W.W.Norton.Metamorphoses1762'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 777}, page_content='BIBLIOGRAPHYPaccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconceptsandrelationsfrompositiveandnegativepropositions.InInternationalJointConferenceonNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.486Paine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervisedpre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.534Palatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shotlearningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22,pages1410–1418.CurranAssociates,Inc.541Parker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.ResearchinEconomicsandManagementSci.,MIT.224Pascanu,R.,Mikolov,T.,andBengio,Y.(2013a).Onthediﬃcultyoftrainingrecurrentneuralnetworks.In.,,,,,ICML’2013289402405409416418Pascanu,R.,Montufar,G.,andBengio,Y.(2013b).Onthenumberofinferenceregionsofdeepfeedforwardnetworkswithpiece-wiselinearactivations.Technicalreport,U.Montreal,arXiv:1312.6098.198Pascanu,R.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeeprecurrentneuralnetworks.In.,,,,,,,ICLR’201419199265398399400412462Pascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregionsofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In.ICLR’2014552Pati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:Recursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-ceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,pages40–44.255Pearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidentialreasoning.In Proceedingsofthe7thConferenceofthe CognitiveScience Society,UniversityofCalifornia,Irvine,pages329–334.565Pearl,J.(1988). ProbabilisticReasoninginIntelligentSystems: NetworksofPlausibleInference.MorganKaufmann.54Perron,O.(1907).Zurtheoriedermatrices.,MathematischeAnnalen6 4(2),248–263.599Petersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003.31Peterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner’sdiscoveryofshaping.JournaloftheExperimentalAnalysisofBehavior,(3),317–328.8 2328Pham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependentsourcesthroughamaximumlikelihoodapproach.In,pages771–774.EUSIPCO493763'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 778}, page_content='BIBLIOGRAPHYPham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012).NeuFlow:dataﬂowvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-CAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044–1047.IEEE.453Pinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksforscenelabeling.In.ICML’2014359Pinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwithconvolutionalnetworks.InConferenceonComputerVisionandPatternRecognition(CVPR).359Pinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognitionhard?PLoSComputBiol,.4458Pinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologically-inspiredcomputervision:Acasestudyinunconstrainedfacerecognitiononfacebook.InComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputerSocietyConferenceon,pages35–42.IEEE.363Pollack,J.B.(1990).Recursivedistributedrepresentations.ArtiﬁcialIntelligence,4 6(1),77–105.400Polyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging.SIAMJ.ControlandOptimization,,838–855.3 0 ( 4 )322Polyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods.USSRComputationalMathematicsandMathematicalPhysics,(5),1–17.4296Poole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014). Analyzingnoiseinautoencodersanddeepnetworks.,.CoRRa b s / 1 4 0 6 . 1 8 3 1241Poon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.InProceedingsoftheTwenty-seventhConferenceinUncertaintyinArtiﬁcialIntelligence(UAI),Barcelona,Spain.556Presley,R.K.andHaggard,R.L.(1994).Aﬁxedpointimplementationofthebackpropa-gationlearningalgorithm.InSoutheastcon’94.CreativeTechnologyTransfer-AGlobalAﬀair.,Proceedingsofthe1994IEEE,pages136–138.IEEE.453Price,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEETransactionsonInformationTheory,(2),69–72.4691Quiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisualrepresentationbysingleneuronsinthehumanbrain.Nature,4 3 5(7045),1102–1107.366764'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 779}, page_content='BIBLIOGRAPHYRadford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434.554703704,,Raiko,T.,Yao,L.,Cho,K.,andBengio, Y.(2014).Iterativeneuralautoregressivedistributionestimator(NADE-k).Technicalreport,arXiv:1406.1485.,678711Raina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearningusinggraphicsprocessors. InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages873–880,NewYork,NY,USA.ACM.,27448Ramsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundationsofMathematicsandotherLogicalEssays,chapter7,pages156–198.McMasterUniversityArchivefortheHistoryofEconomicThought.56Ranzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusingfactorizedthird-orderBoltzmannmachines.In,pages2551–2558.CVPR’2010682Ranzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).Eﬃcientlearningofsparserepresentationswithanenergy-basedmodel.In.,,,,NIPS’20061419509530532Ranzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningofinvariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).IEEEPress.364Ranzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbeliefnetworks.In.NIPS’2007509Ranzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestrictedBoltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010.680681,Ranzato,M.,Mnih,V.,andHinton,G.(2010b).GeneratingmorerealisticimagesusinggatedMRFs.In.NIPS’2010682Rao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatisticalparameters.BulletinoftheCalcuttaMathematicalSociety,,81–89.,3 7135295Rasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervisedlearningwithladdernetwork.arXivpreprintarXiv:1507.02672.,428532Recht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachtoparallelizingstochasticgradientdescent.In.NIPS’2011449Reichert,D.P.,Seriès,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-basedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformationProcessingSystems,pages2357–2365.668765'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 780}, page_content='BIBLIOGRAPHYRezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagationand approximateinferencein deepgenerative models.In .Preprint:ICML’2014arXiv:1401.4082.,,654691698Rifai, S., Vincent, P., Muller,X., Glorot, X.,andBengio,Y.(2011a).Contractiveauto-encoders:Explicitinvarianceduringfeatureextraction.In.,,ICML’2011523524525Rifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X.(2011b).Higherordercontractiveauto-encoder.In.,ECMLPKDD523524Rifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c). Themanifoldtangentclassiﬁer.In.,NIPS’2011271272Rifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessforsamplingcontractiveauto-encoders.In.ICML’2012713Ringach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.CognitiveScience,(2),147–166.2 8368Roberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesandpractice.CambridgeUniversityPress.495Robinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeechrecognitionsystem.ComputerSpeechandLanguage,(3),259–274.,527461Rockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.93Romero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y.(2015).Fitnets:Hintsforthindeepnets.In.ICLR’2015,arXiv:1412.6550325Rosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti.linearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics,8(1),pp.181–217.93Rosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageandorganizationinthebrain.PsychologicalReview,,386–408.,,6 5141527Rosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1527Roweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinearembedding.Science,(5500).,2 9 0163520Roweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.InT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.491Rubin,D.B.(1984).Bayesianlyjustiﬁableandrelevantfrequencycalculationsforetal.theappliedstatistician.,(4),1151–1172.TheAnnalsofStatistics1 2718766'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 781}, page_content='BIBLIOGRAPHYRumelhart, D., Hinton, G., andWilliams, R.(1986a).Learningrepresentationsbyback-propagatingerrors.Nature,,533–536.,,,,,,,3 2 3141823203225373478484Rumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.(1986b).Learninginternalrepresen-tationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter8,pages318–362.MITPress,Cambridge.,2127225,Rumelhart,D.E.,McClelland,J.L.,andthePDPResearchGroup(1986c).ParallelDistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,Cambridge.17Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLargeScaleVisualRecognitionChallenge.21Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognitionetal.challenge.arXivpreprintarXiv:1409.0575.28Russel,S.J.andNorvig,P.(2003).ArtiﬁcialIntelligence:aModernApproach.PrenticeHall.86Rust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).SpatiotemporalelementsofmacaqueV1receptiveﬁelds.Neuron,(6),945–956.4 6367Sainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-tionalneuralnetworksforLVCSR.In.ICASSP2013462Salakhutdinov,R.(2010).LearninginMarkovrandomﬁeldsusingtemperedtransitions.InY.Bengio,D.Schuurmans,C.Williams,J.Laﬀerty,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22(NIPS’09).605Salakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsoftheInternationalConferenceonArtiﬁcialIntelligenceandStatistics,volume5,pages448–455.,,,,,,2427531665668673674Salakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalofApproximateReasoning.527Salakhutdinov, R. andHinton, G. E.(2007a).Learning a nonlinearembedding bypreservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’07),SanJuan,PortoRico.Omnipress.529Salakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In.SIGIR’2007527767'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 782}, page_content='BIBLIOGRAPHYSalakhutdinov,R.andHinton,G.E.(2008).UsingdeepbeliefnetstolearncovariancekernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1249–1256,Cambridge,MA.MITPress.244Salakhutdinov,R.andLarochelle,H.(2010).EﬃcientlearningofdeepBoltzmannmachines.InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS2010),JMLRW&CP,volume9,pages693–700.654Salakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In.NIPS’2008481Salakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbeliefnetworks.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,ProceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),volume25,pages872–879.ACM.,630664Salakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesforcollaborativeﬁltering.In.ICML481Sanger, T.D. (1994).Neuralnetworklearningcontrolofrobotmanipulatorsusinggraduallyincreasingtaskdiﬃculty.IEEETransactionsonRoboticsandAutomation,1 0(3).328Saul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractablenetworks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.640Saul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Meanﬁeldtheoryforsigmoidbeliefnetworks.JournalofArtiﬁcialIntelligenceResearch,,61–76.,427695Savich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentationonimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,1 8(1),240–252.453Saxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandomweightsandunsupervisedfeaturelearning.InProc.ICML’2011.ACM.363Saxe,A.M.,McClelland,J.L.,andGanguli,S.(2013).Exactsolutionstothenonlineardynamicsoflearningindeeplinearneuralnetworks.In.,,ICLR285286303Schaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization.InInternationalConferenceonLearningRepresentations.309Schmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleofhistorycompression.NeuralComputation,(2),234–242.4400Schmidhuber,J.(1996).Sequentialneuraltextcompression.IEEETransactionsonNeuralNetworks,(1),142–146.7479768'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 783}, page_content='BIBLIOGRAPHYSchmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118.390Schölkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,regularization,optimization,andbeyond.MITpress.705Schölkopf,B.,Smola,A.,andMüller,K.-R.(1998).Nonlinearcomponentanalysisasakerneleigenvalueproblem.NeuralComputation,,1299–1319.,1 0163520Schölkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods—SupportVectorLearning.MITPress,Cambridge,MA.,18142Schölkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).Oncausalandanticausallearning.In,pages1255–1262.ICML’2012547Schuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsforspeechrecognition.189Schuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEETransactionsonSignalProcessing,(11),2673–2681.4 5395Schwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,2 1,492–518.468Schwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation.ThePragueBulletinofMathematicalLinguistics,,137–146.9 3475Schwenk,H.(2014).CleanedsubsetofWMT’14dataset.21Schwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-works.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformationProcessingSystems10(NIPS’97),pages647–653.MITPress.258Schwenk, H.andGauvain, J.-L.(2002).Connectionistlanguagemodeling forlargevocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages765–768,Orlando,Florida.468Schwenk,H.,Costa-jussà,M.R.,andFonollosa,J.A.R.(2006).ContinuousspacelanguagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpokenLanguageTranslation,pages166–173.475Seide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-dependentdeepneuralnetworks.InInterspeech2011,pages437–440.23Sejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings151onNeuralNetworksforComputing,pages398–403.AmericanInstituteofPhysicsInc.688769'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 784}, page_content='BIBLIOGRAPHYSeries,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnetsyndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesinNeuralInformationProcessingSystems,pages2020–2028.668Sermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksappliedtohousenumbersdigitclassiﬁcation.,.CoRRa b s / 1 2 0 4 . 3 9 6 8458Sermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetectionwithunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceonComputerVisionandPatternRecognition(CVPR’13).IEEE.,23200Shilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications.31Siegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science,2 6 8(5210),545–548.379Siegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.AppliedMathematicsLetters,(6),77–80.4379Siegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets.JournalofComputerandSystemsSciences,(1),132–150.,5 0379405Sietsma,J.andDow,R.(1991).Creatingartiﬁcialneuralnetworksthatgeneralize.NeuralNetworks,(1),67–79.4241Simard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutionalneuralnetworks.In.ICDAR’2003371Simard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvancesinNeuralInformationProcessingSystems,pages232–239.453Simard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalismforspecifyingselectedinvariancesinanadaptivenetwork.In.,,,NIPS’1991270271272356Simard,P.Y.,LeCun,Y.,andDenker,J.(1993).Eﬃcientpatternrecognitionusinganewtransformationdistance.In.NIPS’92270Simard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformationinvarianceinpatternrecognition—tangentdistanceandtangentpropagation.LectureNotesinComputerScience,.1 5 2 4270Simons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringareal-worldinteraction.PsychonomicBulletin&Review,(4),644–649.5545Simonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.In.ICLR323770'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 785}, page_content='BIBLIOGRAPHYSjöberg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,withapplicationtoneuralnetworks.InternationalJournalofControl,6 2(6),1391–1407.250Skinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94–99.1 3328Smolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsofharmonytheory.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter6,pages194–281.MITPress,Cambridge.,,573589658Snoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationofmachinelearningalgorithms.In.NIPS’2012438Socher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamicpoolingandunfoldingrecursiveautoencodersforparaphrasedetection.In.NIPS’2011400402,Socher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-guagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternationalConferenceonMachineLearning(ICML’2011).400Socher, R., Pennington, J., Huang, E.H., Ng, A.Y.,andManning, C.D.(2011c).Semi-supervisedrecursiveautoencoders forpredictingsentimentdistributions.InEMNLP’2011.400Socher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,C.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.In.,EMNLP’2013400402Socher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthroughcross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessingSystems(NIPS2013).541Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deepunsupervisedlearningusingnonequilibriumthermodynamics.,717718Sohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywithpoint-wisegatedBoltzmannmachines.In.ICML’2013689Solomonoﬀ,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-bility.328Sontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputerandSystemsSciences,,69–96.,1 6 8549553Sontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocalminimaevenfornetworkswithouthiddenlayers.,,91–106.ComplexSystems3284771'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 786}, page_content='BIBLIOGRAPHYSparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1Spitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how“lessismore”inunsuperviseddependencyparsing.InHLT’10.328Squire,W.andTrapp,G.(1998). Usingcomplexvariablestoestimatederivativesofrealfunctions.SIAMRev.,(1),110––112.4 0441Srebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsofthe18thAnnualConferenceonLearningTheory,pages545–560.Springer-Verlag.238Srivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master’sthesis,U.Toronto.537Srivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmannmachines.In.NIPS’2012543Srivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).ModelingdocumentswithdeepBoltzmannmachines.arXivpreprintarXiv:1309.6865.665Srivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014).Dropout:Asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachineLearningResearch,,1929–1958.,,,1 5258265267674Srivastava,R.K.,Greﬀ,K.,andSchmidhuber,J.(2015).Highwaynetworks.arXiv:1505.00387.326Steinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearningalgorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,0,1115–1119.447Stoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphicalmodelparametersgivenapproximateinference,decoding,andmodelstructure.InProceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS)JMLRWorkshopandConferenceProceedings,volume15of,pages725–733,FortLauderdale.Supplementarymaterial(4pages)alsoavailable.,676700Sukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemorynetworks.arXivpreprintarXiv:1503.08895.420Supancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.InCVPR’2013.328Sussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworkswithsmartinitialization.,.,,,CoRRa b s / 1 4 1 2 . 6 5 5 8290303305404Sutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentofcomputerscience,UniversityofToronto.,407414772'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 787}, page_content='BIBLIOGRAPHYSutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversalapproximators.NeuralComputation,(11),2629–2636.2 0695Sutskever,I.andTieleman,T.(2010).OntheConvergencePropertiesofContrastiveDivergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),volume9,pages789–795.614Sutskever,I.,Hinton,G.,andTaylor,G.(2009).TherecurrenttemporalrestrictedBoltzmannmachine.In.NIPS’2008687Sutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrentneuralnetworks.In,pages1017–1024.ICML’2011479Sutskever, I.,Martens,J.,Dahl, G.,andHinton,G.(2013).Ontheimportanceofinitializationandmomentumindeeplearning.In.,,ICML300407414Sutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwithneuralnetworks.In.,,,,,,NIPS’2014,arXiv:1409.321525101396410413476477Sutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress.106Sutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethodsforreinforcementlearningwithfunctionapproximation.In,pages1057–NIPS’1999–1063.MITPress.693Swersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).Onautoencodersandscorematchingforenergybasedmodels.In.ACM.ICML’2011515Swersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization.arXivpreprintarXiv:1406.3896.438Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,V.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,arXiv:1409.4842.,,,,,,2427200258269326347Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,andFergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLRa b s / 1 3 1 2 . 6 1 9 9.268271,Szegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,andWojna,Z.(2015).RethinkingtheInceptionArchitectureforComputerVision..,ArXive-prints243322Taigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegaptohuman-levelperformanceinfaceveriﬁcation.In.CVPR’2014100Tandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocialSciences.UniversityofCaliforniaPress.1773'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 788}, page_content='BIBLIOGRAPHYTang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.InProceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,2010,Haifa,Israel.241Tang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers.arXivpreprintarXiv:1206.4635.491Taylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachinesformodelingmotionstyle.InL.BottouandM.Littman, editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages1025–1032,Montreal,Quebec,Canada.ACM.687Taylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinarylatentvariables.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformationProcessingSystems19(NIPS’06),pages1345–1352.MITPress,Cambridge,MA.687Teh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodelsforsparseovercompleterepresentations.JournalofMachineLearningResearch,4,1235–1260.493Tenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframeworkfornonlineardimensionalityreduction.Science,(5500),2319–2323.,,2 9 0163520535Theis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerativemodels.arXiv:1511.01844.,699721Thompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutionalnetworkandagraphicalmodelforhumanposeestimation.In.NIPS’2014360Thrun,S.(1995).Learningtoplaythegameofchess.In.NIPS’1994271Tibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatisticalSocietyB,,267–288.5 8236Tieleman,T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationstothelikelihoodgradient.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,Pro-ceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),pages1064–1071.ACM.614Tieleman,T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastivedivergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages1033–1040.ACM.616Tipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis.JournaloftheRoyalStatisticalSocietyB,(3),611–622.6 1493774'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 789}, page_content='BIBLIOGRAPHYTorralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesforrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’08),pages1–8.527Touretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsofaconnectionistinferencearchitecture. InProceedingsofthe9thInternationalJointConferenceonArtiﬁcialIntelligence-Volume1,IJCAI’85,pages238–243,SanFrancisco,CA,USA.MorganKaufmannPublishersInc.17Tu,K.andHonavar,V.(2011). Ontheutilityofcurriculainunsupervisedlearningofprobabilisticgrammars.In.IJCAI’2011328Turaga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,W.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogenerateaﬃnitygraphsforimagesegmentation.NeuralComputation,(2),511–538.2 2359Turian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleandgeneralmethodforsemi-supervisedlearning.InProc.ACL’2010,pages384–394.537Töscher,A.,Jahrer,M.,andBell,R.M.(2009). TheBigChaossolutiontotheNetﬂixgrandprize.481Uria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-sivedensity-estimator.In.NIPS’2013711vandenOörd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusicrecommendation.In.NIPS’2013482vanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.MachineLearningRes.,.,9479521Vanhoucke,V.,Senior,A.,andMao,M.Z.(2011).ImprovingthespeedofneuralnetworksonCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop.446454,Vapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-Verlag,Berlin.114Vapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork.114Vapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelativefrequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,1 6,264–280.114Vincent,P.(2011). Aconnectionbetweenscorematchinganddenoisingautoencoders.NeuralComputation,(7).,,2 3515517714775'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 790}, page_content='BIBLIOGRAPHYVincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In.MITPress.NIPS’2002522Vincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingandcomposingrobustfeatureswithdenoisingautoencoders.In.,ICML2008241517Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stackeddenoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocaldenoisingcriterion.J.MachineLearningRes.,.1 1517Vincent,P.,deBrébisson,A.,andBouthillier,X.(2015).Eﬃcientexactgradientupdatefortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems28,pages1108–1116.CurranAssociates,Inc.467Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., andHinton, G.(2014a).Grammarasaforeignlanguage.Technicalreport,arXiv:1412.7449.410Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimagecaptiongenerator.arXiv1411.4555.410Vinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprintarXiv:1506.03134.420Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimagecaptiongenerator.In.arXiv:1411.4555.CVPR’2015102Viola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternationalJournalofComputerVision.451Visin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015).ReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXivpreprintarXiv:1505.00393.396VonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinalprojectionsdirectedtotheauditorypathway.Nature,(6780),871–876.4 0 416Wager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization.InAdvancesinNeuralInformationProcessingSystems26,pages351–359.265Waibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phonemerecognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,andSignalProcessing,,328–339.,,3 7374455461Wan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneuralnetworksusingdropconnect.In.ICML’2013266Wang,S.andManning,C.(2013).Fastdropouttraining.In.ICML’2013266776'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 791}, page_content='BIBLIOGRAPHYWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointlyembedding.InProc.EMNLP’2014.486Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b). Knowledgegraphembeddingbytranslatingonhyperplanes.InProc.AAAI’2014.486Warde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempiricalanalysisofdropoutinpiecewiselinearnetworks.In.,,ICLR’2014262266267Wawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N.(1996).Spert-II:Avectormicroprocessorsystem.,(3),79–86.Computer2 9453Weaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-mentlearning.InProc.UAI’2001,pages538–545.693Weinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsbysemideﬁniteprogramming.In,pages988–995.,CVPR’2004163521Weiss, Y., Torralba, A., andFergus, R.(2008).Spectral hashing.In,pagesNIPS1753–1760.527Welling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvancesinNeuralInformationProcessingSystems,pages665–672.705Welling, M.,Hinton,G.E., andOsindero, S.(2003a).LearningsparsetopographicrepresentationswithproductsofStudent-tdistributions.In.NIPS’2002682Welling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessingSystems15(NIPS’02),pages665–672.MITPress.624Welling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniumswithanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),volume17,Cambridge,MA.MITPress.678Werbos, P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.InProceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762–770.224Weston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningtorankwithjointword-imageembeddings.MachineLearning,(1),21–35.8 1402Weston, J., Chopra, S., andBordes, A.(2014).Memorynetworks.arXivpreprintarXiv:1410.3916.,420487Widrow,B.andHoﬀ,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCONConventionRecord,volume4,pages96–104.IRE,NewYork.,,,15212427777'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 792}, page_content='BIBLIOGRAPHYWikipedia(2015).Listofanimalsbynumberofneurons—Wikipedia,thefreeencyclopedia.[Online;accessed4-March-2015].,2427Williams,C.K.I.andAgakov,F.V.(2002). ProductsofGaussiansandProbabilisticMinorComponentAnalysis.NeuralComputation,,1169–1182.1 4 ( 5 )684Williams,C.K.I.andRasmussen,C.E.(1996). Gaussianprocessesforregression. InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95),pages514–520.MITPress,Cambridge,MA.142Williams,R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionistreinforcementlearning.MachineLearning,,229–256.,8690691Williams,R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks.NeuralComputation,,270–280.1222Wilson,D.R.andMartinez,T.R.(2003).Thegeneralineﬃciencyofbatchtrainingforgradientdescentlearning.NeuralNetworks,(10),1429–1451.1 6279Wilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.AmericanJournalofMathematicalandManagementSciences,(3),277––312.4692Wiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningofinvariances.NeuralComputation,(4),715–770.1 4496Wolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEETransactionsonEvolutionaryComputation,,67–82.1293Wolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.NeuralComputation,(7),1341–1390.8116Wu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimagerecognition.arXiv:1501.02876.449Wu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalofOptimization,,814–836.7327Xiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulatedsplicingusingRNAsequenceandcellularcontext.,Bioinformatics2 7(18),2554–2562.265Xu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,andBengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.In.,,ICML’2015,arXiv:1502.03044102410693Yildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty.Neuralnetworks,,1–9.3 5406778'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 793}, page_content='BIBLIOGRAPHYYosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeaturesindeepneuralnetworks?In.,NIPS’2014323538Younes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidlydecreasingergodicityrates.InStochasticsandStochasticsModels,pages177–228.614Yu,D.,Wang,S.,and Deng, L. (2010).Sequential labeling using deep-structuredconditionalrandomﬁelds.IEEEJournalofSelectedTopicsinSignalProcessing.323Zaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615.329Zaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines.arXiv:1505.00521.421Zaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitionsofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematicalSociety.AmericanMathematicalSociety.552Zeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks.In.ECCV’146Zeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,A.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013). Onrectiﬁedlinearunitsforspeechprocessing.In.ICASSP2013461Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015). ObjectdetectorsemergeindeepsceneCNNs.ICLR’2015,arXiv:1412.6856.553Zhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerativestochasticnetworkforproteinsecondarystructureprediction.In.ICML’2014717Zhou,Y.andChellappa,R.(1988).Computationofopticalﬂowusinganeuralnetwork.InNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71–78.IEEE.339Zöhrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassiﬁcation.InNIPS’2014.717\\n779'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 794}, page_content='Index0-1loss,,104276Absolutevaluerectiﬁcation,192Accuracy,425Activationfunction,170Activeconstraint,95AdaGrad,307ADALINE,seeadaptivelinearelementAdam,,308427Adaptivelinearelement,,,152427Adversarialexample,268Adversarialtraining,,,268271532Aﬃne,110AIS,seeannealedimportancesamplingAlmosteverywhere,71Almostsureconvergence,130Ancestralsampling,,582597ANN,seeArtiﬁcialneuralnetworkAnnealedimportancesampling, ,,627670719ApproximateBayesiancomputation,718Approximateinference,585Artiﬁcialintelligence,1Artiﬁcialneuralnetwork, seeNeuralnet-workASR,seeautomaticspeechrecognitionAsymptoticallyunbiased,124Audio,,,102360460Autoencoder,,,4356504Automaticspeechrecognition,460Back-propagation,203Back-propagationthroughtime,384Backprop,seeback-propagationBagofwords,473Bagging,256Batchnormalization,,268427Bayeserror,117Bayes’rule,70Bayesianhyperparameteroptimization,438Bayesian network, seedirected graphicalmodelBayesianprobability,55Bayesianstatistics,135Beliefnetwork,seedirectedgraphicalmodelBernoullidistribution,62BFGS,316Bias,,124229Biasparameter,110Biasedimportancesampling,595Bigram,464Binaryrelation,484BlockGibbssampling,601Boltzmanndistribution,572Boltzmannmachine,,572656BPTT,seeback-propagationthroughtimeBroadcasting,34Burn-in,599CAE,seecontractiveautoencoderCalculusofvariations,179Categoricaldistribution,seemultinoullidis-tributionCD,seecontrastivedivergenceCenteringtrick(DBM),675Centrallimittheorem,63Chainrule(calculus),206Chainruleofprobability,59780'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 795}, page_content='INDEXChess,2Chord,581Chordalgraph,581Class-basedlanguagemodels,465Classicaldynamicalsystem,375Classiﬁcation,100Cliquepotential,seefactor(graphicalmodel)CNN,seeconvolutionalneuralnetworkCollaborativeFiltering,480Collider,seeexplainingawayColorimages,360Complexcell,365Computationalgraph,204Computervision,454Conceptdrift,540Conditionnumber,279Conditionalcomputation,seedynamicstruc-tureConditionalindependence,,xiii60Conditionalprobability,59ConditionalRBM,687Connectionism,,17445Connectionisttemporalclassiﬁcation,462Consistency,,130515Constrainedoptimization,,93237Content-basedaddressing,421Content-basedrecommendersystems,482Context-speciﬁcindependence,575Contextualbandits,482Continuationmethods,327Contractiveautoencoder,523Contrast,456Contrastivedivergence,,,291612674Convexoptimization,141Convolution,,330685Convolutionalnetwork,16Convolutionalneuralnetwork,,254330,,427462Coordinatedescent,,321673Correlation,61Costfunction,seeobjectivefunctionCovariance,,xiii61Covariancematrix,62Coverage,426Criticaltemperature,605Cross-correlation,332Cross-entropy,,75132Cross-validation,122CTC,seeconnectionisttemporalclassiﬁca-tionCurriculumlearning,328Curseofdimensionality,154Cyc,2D-separation,574DAE,seedenoisingautoencoderDatageneratingdistribution,,111131Datageneratingprocess,111Dataparallelism,449Dataset,105Datasetaugmentation,,271459DBM,seedeepBoltzmannmachineDCGAN,,,553554703Decisiontree,,145550Decoder,4Deepbeliefnetwork,,,,,,27531633659662686694,DeepBlue,2DeepBoltzmannmachine,,,,,2427531633654659665674686,,,,Deepfeedforwardnetwork,,167427Deeplearning,,25Denoisingautoencoder,,512691Denoisingscorematching,621Densityestimation,103Derivative,,xiii83Designmatrix,106Detectorlayer,339Determinant,xiiDiagonalmatrix,41Diﬀerentialentropy,,74648Diracdeltafunction,65Directedgraphicalmodel,,,,77509565694Directionalderivative,85Discriminativeﬁne-tuning,seesupervisedﬁne-tuningDiscriminativeRBM,688Distributedrepresentation,,,17150548Domainadaptation,538781'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 796}, page_content='INDEXDotproduct,,34141Doublebackprop,271Doublyblockcirculantmatrix,333Dreamsleep,,611654DropConnect,266Dropout,,,,,,258427432433674691Dynamicstructure,,450451E-step,636Earlystopping,,,,,246247249250427EBM,seeenergy-basedmodelEchostatenetwork,,,2427405Eﬀectivecapacity,114Eigendecomposition,42Eigenvalue,42Eigenvector,42ELBO,seeevidencelowerboundElement-wiseproduct,seeHadamardprod-uct,seeHadamardproductEM,seeexpectationmaximizationEmbedding,518Empiricaldistribution,66Empiricalrisk,276Empiricalriskminimization,276Encoder,4Energyfunction,571Energy-basedmodel,,,,571597656665Ensemblemethods,256Epoch,246Equalityconstraint,94Equivariance,338Errorfunction,seeobjectivefunctionESN,seeechostatenetworkEuclideannorm,39Euler-Lagrangeequation,648Evidencelowerbound,,635663Example,99Expectation,60Expectationmaximization,636Expectedvalue,seeexpectationExplainingaway,,,576633646Exploitation,483Exploration,483Exponentialdistribution,65F-score,425Factor(graphicalmodel),569Factoranalysis,492Factorgraph,581Factorsofvariation,4Feature,99Featureselection,236Feedforwardneuralnetwork,167Fine-tuning,323Finitediﬀerences,441Forgetgate,306Forwardpropagation,203Fouriertransform,,360362Fovea,366FPCD,616Freeenergy,,573682Freebase,485Frequentistprobability,55Frequentiststatistics,135Frobeniusnorm,46Fully-visibleBayesnetwork,707Functionalderivatives,647FVBN,seefully-visibleBayesnetworkGaborfunction,368GANs,seegenerativeadversarialnetworksGatedrecurrentunit,427Gaussiandistribution,seenormaldistribu-tionGaussiankernel,142Gaussianmixture,,67188GCN,seeglobalcontrastnormalizationGeneOntology,485Generalization,110GeneralizedLagrangefunction,seegeneral-izedLagrangianGeneralizedLagrangian,94Generativeadversarialnetworks,,691702Generativemomentmatchingnetworks,705Generatornetwork,695Gibbsdistribution,570Gibbssampling,,583601Globalcontrastnormalization,456GPU,seegraphicsprocessingunitGradient,84782'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 797}, page_content='INDEXGradientclipping,,289416Gradientdescent,,8385Graph,xiiGraphicalmodel,seestructuredprobabilis-ticmodelGraphicsprocessingunit,446Greedyalgorithm,323Greedylayer-wiseunsupervisedpretraining,530Greedysupervisedpretraining,323Gridsearch,434Hadamardproduct,,xii34Hard,tanh196Harmonium,seerestrictedBoltzmannma-chineHarmonytheory,573Helmholtzfreeenergy,seeevidencelowerboundHessian,223Hessianmatrix,,xiii87Heteroscedastic,187Hiddenlayer,,6167Hillclimbing,86Hyperparameteroptimization,434Hyperparameters,,120432Hypothesisspace,,112118i.i.d.assumptions,,,111122268Identitymatrix,36ILSVRC,seeImageNetLarge-ScaleVisualRecognitionChallengeImageNetLarge-ScaleVisualRecognitionChallenge,23Immorality,579Importancesampling,,,594626700Importanceweightedautoencoder,700Independence,,xiii60Independentandidenticallydistributed,seei.i.d.assumptionsIndependentcomponentanalysis,493Independentsubspaceanalysis,495Inequalityconstraint,94Inference,,,,,,,,564585633635637640650653Informationretrieval,527Initialization,301Integral,xiiiInvariance,342Isotropic,65Jacobianmatrix,,,xiii7286Jointprobability,57k-means,,364548k-nearestneighbors,,143550Karush-Kuhn-Tuckerconditions,,95237Karush–Kuhn–Tucker,94Kernel(convolution),,331332Kernelmachine,550Kerneltrick,141KKT,seeKarush–Kuhn–TuckerKKTconditions,seeKarush-Kuhn-TuckerconditionsKLdivergence,seeKullback-Leiblerdiver-genceKnowledgebase,,2485Krylovmethods,223Kullback-Leiblerdivergence,,xiii74Labelsmoothing,243Lagrangemultipliers,,94648Lagrangian,seegeneralizedLagrangianLAPGAN,704Laplacedistribution,,,65498499Latentvariable,67Layer(neuralnetwork),167LCN,seelocalcontrastnormalizationLeakyReLU,192Leakyunits,408Learningrate,85Linesearch,,,858693Linearcombination,37Lineardependence,38Linearfactormodels,491Linearregression,,,107110140Linkprediction,486Lipschitzconstant,92Lipschitzcontinuous,92Liquidstatemachine,405783'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 798}, page_content='INDEXLocalconditionalprobabilitydistribution,566Localcontrastnormalization,458Logisticregression,,,3140140Logisticsigmoid,,767Longshort-termmemory,,,,1825306410,427Loop,581Loopybeliefpropagation,587Lossfunction,seeobjectivefunctionLpnorm,39LSTM,seelongshort-termmemoryM-step,636Machinelearning,2Machinetranslation,101Maindiagonal,33Manifold,160Manifoldhypothesis,161Manifoldlearning,161Manifoldtangentclassiﬁer,272MAPapproximation,,138507Marginalprobability,58Markovchain,597MarkovchainMonteCarlo,597Markovnetwork,seeundirectedmodelMarkovrandomﬁeld,seeundirectedmodelMatrix,,,xixii32Matrixinverse,36Matrixproduct,34Maxnorm,40Maxpooling,339Maximumlikelihood,131Maxout,,192427MCMC,seeMarkovchainMonteCarloMeanﬁeld,,,640641674Meansquarederror,108Measuretheory,71Measurezero,71Memorynetwork,,418420Methodof steepestdescent, seegradientdescentMinibatch,279Missinginputs,100Mixing(Markovchain),603Mixturedensitynetworks,188Mixturedistribution,66Mixturemodel,,188512Mixtureofexperts,,452550MLP,seemultilayerperceptionMNIST,,,2122674Modelaveraging,256Modelcompression,450Modelidentiﬁability,284Modelparallelism,449Momentmatching,705Moore-Penrosepseudoinverse,,45239Moralizedgraph,579MP-DBM,seemulti-predictionDBMMRF(Markov RandomField),seeundi-rectedmodelMSE,seemeansquarederrorMulti-modallearning,541Multi-predictionDBM,676Multi-tasklearning,,244540Multilayerperception,5Multilayerperceptron,27Multinomialdistribution,62Multinoullidistribution,62n-gram,463NADE,710NaiveBayes,3Nat,73Naturalimage,561Naturallanguageprocessing,463Nearestneighborregression,115Negativedeﬁnite,89Negativephase,,,472608610Neocognitron,,,,162427367Nesterovmomentum,300NetﬂixGrandPrize,,258481Neurallanguagemodel,,465478Neuralnetwork,13NeuralTuringmachine,420Neuroscience,15Newton’smethod,,89310NLM,seeneurallanguagemodelNLP,seenaturallanguageprocessingNofreelunchtheorem,116784'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 799}, page_content='INDEXNoise-contrastiveestimation,622Non-parametricmodel,114Norm,,xiv39Normaldistribution,,,6364125Normalequations,,,,109109112234Normalizedinitialization,303Numericaldiﬀerentiation,seeﬁnitediﬀer-encesObjectdetection,455Objectrecognition,455Objectivefunction,82OMP-,kseeorthogonalmatchingpursuitOne-shotlearning,540Operation,204Optimization,,8082Orthodoxstatistics,seefrequentiststatisticsOrthogonalmatchingpursuit,,27255Orthogonalmatrix,42Orthogonality,41Outputlayer,167Paralleldistributedprocessing,17Parameterinitialization,,301407Parametersharing,,,,,253335373375389Parametertying,seeParametersharingParametricmodel,114ParametricReLU,192Partialderivative,84Partitionfunction,,,570607671PCA,seeprincipalcomponentsanalysisPCD,seestochasticmaximumlikelihoodPerceptron,,1527Persistentcontrastivedivergence,seestochas-ticmaximumlikelihoodPerturbationanalysis,seereparametrizationtrickPointestimator,122Policy,482Pooling,,330685Positivedeﬁnite,89Positivephase,,,,,472608610658670Precision,425Precision(ofanormaldistribution),,6365Predictivesparsedecomposition,525Preprocessing,455Pretraining,,323530Primaryvisualcortex,365Principalcomponentsanalysis,,–,48146148492633,Priorprobabilitydistribution,135Probabilisticmaxpooling,685ProbabilisticPCA,,,492493634Probabilitydensityfunction,58Probabilitydistribution,56Probabilitymassfunction,56Probabilitymassfunctionestimation,103Productofexperts,572Productruleofprobability,seechainruleofprobabilityPSD,seepredictivesparsedecompositionPseudolikelihood,617Quadraturepair,369Quasi-Newtonmethods,316Radialbasisfunction,196Randomsearch,436Randomvariable,56Ratiomatching,620RBF,196RBM,seerestrictedBoltzmannmachineRecall,425Receptiveﬁeld,337RecommenderSystems,480Rectiﬁedlinearunit,,,,171192427509Recurrentnetwork,27Recurrentneuralnetwork,378Regression,101Regularization,,,,,120120177228432Regularizer,119REINFORCE,691Reinforcementlearning,,,,25106482691Relationaldatabase,485Reparametrizationtrick,690Representationlearning,3Representationalcapacity,114RestrictedBoltzmannmachine, , ,356461481589633658659674678,,,,,,,680682685,,785'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 800}, page_content='INDEXRidgeregression,seeweightdecayRisk,275RNN-RBM,687Saddlepoints,285Samplemean,125Scalar,,,xixii31Scorematching,,515619Secondderivative,86Secondderivativetest,89Self-information,73Semantichashing,527Semi-supervisedlearning,243Separableconvolution,362Separation(probabilisticmodeling),574Set,xiiSGD,seestochasticgradientdescentShannonentropy,,xiii73Shortlist,468Sigmoid,,xivseelogisticsigmoidSigmoidbeliefnetwork,27Simplecell,365Singularvalue,seesingularvaluedecompo-sitionSingularvaluedecomposition,,,44148481Singularvector,seesingularvaluedecom-positionSlowfeatureanalysis,495SML,seestochasticmaximumlikelihoodSoftmax,,,183420452Softplus,,,xiv68196Spamdetection,3Sparsecoding,,,,,321356498633694Sparseinitialization,,304407Sparserepresentation,,,,,146226254507558Spearmint,438Spectralradius,406Speechrecognition,see automaticspeechrecognitionSphering,seewhiteningSpikeand slabrestricted Boltzmannma-chine,682SPN,seesum-productnetworkSquarematrix,38ssRBM,seespikeandslabrestrictedBoltz-mannmachineStandarddeviation,61Standarderror,127Standarderrorofthemean,,128278Statistic,122Statisticallearningtheory,110Steepestdescent,seegradientdescentStochasticback-propagation,seereparametriza-tiontrickStochasticgradientdescent,,,,15150279294,674Stochasticmaximumlikelihood,,614674Stochasticpooling,266Structurelearning,584Structuredoutput,,101687Structuredprobabilisticmodel,,77560Sumruleofprobability,58Sum-productnetwork,555Supervisedﬁne-tuning,,531664Supervisedlearning,105Supportvectormachine,140Surrogatelossfunction,276SVD,seesingularvaluedecompositionSymmetricmatrix,,4143Tangentdistance,270Tangentplane,518Tangentprop,270TDNN,seetime-delayneuralnetworkTeacherforcing,,382383Tempering,605Templatematching,141Tensor,,,xixii33Testset,110Tikhonovregularization,seeweightdecayTiledconvolution,352Time-delayneuralnetwork,,368374Toeplitzmatrix,333TopographicICA,495Traceoperator,46Trainingerror,110Transcription,101Transferlearning,538Transpose,,xii33786'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 801}, page_content='INDEXTriangleinequality,39Triangulatedgraph,seechordalgraphTrigram,464Unbiased,124Undirectedgraphicalmodel,,77509Undirectedmodel,568Uniformdistribution,57Unigram,464Unitnorm,41Unitvector,41Universalapproximationtheorem,197Universalapproximator,555Unnormalizedprobabilitydistribution,569Unsupervisedlearning,,105146Unsupervisedpretraining,,461530V-structure,seeexplainingawayV1,365VAE,seevariationalautoencoderVapnik-Chervonenkisdimension,114Variance,,,xiii61229Variationalautoencoder,,691698Variationalderivatives,seefunctionalderiva-tivesVariationalfreeenergy,seeevidencelowerboundVCdimension,seeVapnik-Chervonenkisdi-mensionVector,,,xixii32Virtualadversarialexamples,269Visiblelayer,6Volumetricdata,360Wake-sleep,,653663Weightdecay,,,,118177231433Weightspacesymmetry,284Weights,,15107Whitening,457Wikibase,485Wikibase,485Wordembedding,466Word-sensedisambiguation,486WordNet,485Zero-datalearning,seezero-shotlearningZero-shotlearning,540\\n787')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating chunks from PDF"
      ],
      "metadata": {
        "id": "Qvy80tlqwuup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
        "                                               chunk_overlap = 200,\n",
        "                                               length_function = len)"
      ],
      "metadata": {
        "id": "TNYfD8CvwsSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "WSYqML1Tw8Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiXa3_tNxui7",
        "outputId": "af960d4d-3845-49f4-efa2-9d4d4ea57b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 0}, page_content='DeepLearningIanGoodfellowYoshuaBengioAaronCourville'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 1}, page_content='ContentsWebsiteviiAcknowledgmentsviiiNotationxi1Introduction11.1WhoShouldReadThisBook?. . . . . .. . . . . . . . .. . . . .81.2HistoricalTrendsinDeepLearning. . . . . . . . .. . . . . . . .11IAppliedMathandMachineLearningBasics292LinearAlgebra312.1Scalars,Vectors,MatricesandTensors. . . . . . . . .. . . . . .312.2MultiplyingMatricesandVectors. . . . . . . . .. . . . . . . ..342.3IdentityandInverseMatrices. . . . . . . . .. . . . . . . .. . .362.4LinearDependenceandSpan. . . . . . . . .. . . . . . . .. . .372.5Norms. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .392.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . .402.7Eigendecomposition. . . . . . . . . .. . . . . . . .. . . . . . . .422.8SingularValueDecomposition. . . . . . . .. . . . . . . .. . . .442.9TheMoore-PenrosePseudoinverse. . . . . . . . .. . . . . . . ..452.10TheTraceOperator. . . . . . . . .. . . . . . . .. . . . . . . .462.11TheDeterminant. .. . . . . . . .. . . . . . . .. . . . . . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 1}, page_content='. . . . . . . .. . . . . . . ..452.10TheTraceOperator. . . . . . . . .. . . . . . . .. . . . . . . .462.11TheDeterminant. .. . . . . . . .. . . . . . . .. . . . . . . . .472.12Example:PrincipalComponentsAnalysis. . . . . . . . .. . . .483ProbabilityandInformationTheory533.1WhyProbability?. . . . .. . . . . . . . .. . . . . . . .. . . . .54i'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 2}, page_content='CONTENTS3.2RandomVariables. . . . .. . . . . . . .. . . . . . . . .. . . .563.3ProbabilityDistributions. . . . . . . . .. . . . . . . .. . . . . .563.4MarginalProbability. . . . . . . . .. . . . . . . . .. . . . . . .583.5ConditionalProbability. .. . . . . . . .. . . . . . . .. . . . .593.6TheChainRuleofConditionalProbabilities. . . . . . . . .. . .593.7IndependenceandConditionalIndependence. . . . . . . . .. . .603.8Expectation,VarianceandCovariance. . . . . . . . . .. . . . .603.9CommonProbabilityDistributions. . . . . . . . . . . . . . .. .623.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . .. .673.11Bayes’Rule. . . . . . . . . .. . . . . . . .. . . . . . . .. . . .703.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . .713.13InformationTheory. . . . . . . . . .. . . . . . . .. . . . . . . .723.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .754NumericalComputation804.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 2}, page_content='. . . . . .. . . . . . . .. . . . . . . .723.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .754NumericalComputation804.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . . .804.2PoorConditioning. . . . . . . . .. . . . . . . .. . . . . . . . .824.3Gradient-BasedOptimization. . . . . . .. . . . . . . .. . . . .824.4ConstrainedOptimization. . . . . . . . . . . . .. . . . . . . ..934.5Example:LinearLeastSquares. . . . . . .. . . . . . . . .. . .965MachineLearningBasics985.1LearningAlgorithms. . . . . . . . . . .. . . . . . . .. . . . . .995.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . .. . . . .1105.3HyperparametersandValidationSets.. . . . . . . .. . . . . . .1205.4Estimators,BiasandVariance. . . . . .. . . . . . . .. . . . . .1225.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . .. . .1315.6BayesianStatistics. . . . . . . . . . .. . . . . . . .. . . . . . .1355.7SupervisedLearningAlgorithms. . .. . . . . . . .. . . . . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 2}, page_content='. . . . .. . . . . . . . .. . .1315.6BayesianStatistics. . . . . . . . . . .. . . . . . . .. . . . . . .1355.7SupervisedLearningAlgorithms. . .. . . . . . . .. . . . . . . .1395.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . .. .1455.9StochasticGradientDescent. . . .. . . . . . . . .. . . . . . . .1505.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . ..1525.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . .. .154IIDeepNetworks:ModernPractices1656DeepFeedforwardNetworks1676.1Example:LearningXOR.. . . . . . . . .. . . . . . . .. . . . .1706.2Gradient-BasedLearning.. . . . . . . .. . . . . . . .. . . . . .176ii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 3}, page_content='CONTENTS6.3HiddenUnits. . . . . .. . . . . . . .. . . . . . . . .. . . . . .1906.4ArchitectureDesign. . . . . . . . .. . . . . . . .. . . . . . . ..1966.5Back-PropagationandOtherDiﬀerentiationAlgorithms. . . . .2036.6HistoricalNotes. . . . . . .. . . . . . . .. . . . . . . . .. . . .2247RegularizationforDeepLearning2287.1ParameterNormPenalties. . . . .. . . . . . . . .. . . . . . . .2307.2NormPenaltiesasConstrainedOptimization. . . . . . . .. . . .2377.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .2397.4DatasetAugmentation. . . . . . . . . .. . . . . . . . .. . . . .2407.5NoiseRobustness. . . . . . . . .. . . . . . . .. . . . . . . .. .2427.6Semi-SupervisedLearning. . . . . . . . . . . . . . . .. . . . . .2437.7Multi-TaskLearning. . . . . . . . . . . . . .. . . . . . . . .. .2447.8EarlyStopping. . . . . . . . .. . . . . . . .. . . . . . . .. . .2467.9ParameterTyingandParameterSharing . . . . . . . . . . . . . .2537.10SparseRepresentations. . . . . . . . .. . . . . . . .. .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 3}, page_content='. . . . . . . .. . . . . . . .. . . . . . . .. . .2467.9ParameterTyingandParameterSharing . . . . . . . . . . . . . .2537.10SparseRepresentations. . . . . . . . .. . . . . . . .. . . . . . .2547.11BaggingandOtherEnsembleMethods.. . . . . . . . .. . . . .2567.12Dropout. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .2587.13AdversarialTraining. . . . . . . .. . . . . . . . .. . . . . . . .2687.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer2708OptimizationforTrainingDeepModels2748.1HowLearningDiﬀersfromPureOptimization. . . . . . . . . . .2758.2ChallengesinNeuralNetworkOptimization. . . . .. . . . . . .2828.3BasicAlgorithms. . . . . . . . . . . . .. . . . . . . .. . . . . .2948.4ParameterInitializationStrategies.. . . . . . . . .. . . . . . .3018.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .3068.6ApproximateSecond-OrderMethods. . . .. . . . . . . . .. . .3108.7OptimizationStrategiesandMeta-Algorithms. . . . .. . . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 3}, page_content='. . .3018.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .3068.6ApproximateSecond-OrderMethods. . . .. . . . . . . . .. . .3108.7OptimizationStrategiesandMeta-Algorithms. . . . .. . . . . .3179ConvolutionalNetworks3309.1TheConvolutionOperation. . . . . . . . . . . . . . . .. . . . .3319.2Motivation. .. . . . . . . .. . . . . . . . .. . . . . . . .. . . .3359.3Pooling. . . . . . . . . . . . .. . . . . . . .. . . . . . . . .. . .3399.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .3459.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . .3479.6StructuredOutputs.. . . . . . . .. . . . . . . . .. . . . . . . .3589.7DataTypes. . . . . .. . . . . . . .. . . . . . . .. . . . . . . .3609.8EﬃcientConvolutionAlgorithms. . . . . . . .. . . . . . . .. .3629.9RandomorUnsupervisedFeatures. . . . . . . .. . . . . . . ..363iii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 4}, page_content='CONTENTS9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks. . . . . ..3649.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .37110 SequenceModeling:RecurrentandRecursiveNets37310.1UnfoldingComputationalGraphs. . . . . . . . . . . . .. . . . .37510.2RecurrentNeuralNetworks. . .. . . . . . . . .. . . . . . . ..37810.3BidirectionalRNNs . . . . . . . . . . . . . .. . . . . . . . .. . .39510.4Encoder-DecoderSequence-to-SequenceArchitectures. . . . . ..39610.5DeepRecurrentNetworks. . . . . . . .. . . . . . . . .. . . . .39810.6RecursiveNeuralNetworks. . . . .. . . . . . . . .. . . . . . . .40010.7TheChallengeofLong-TermDependencies. . . . . . . . . .. . .40210.8EchoStateNetworks. . . . . . . . . .. . . . . . . . .. . . . . .40510.9LeakyUnitsandOtherStrategiesforMultipleTimeScales. . ..40810.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .41010.11 OptimizationforLong-TermDependencies. . . . . . . .. . . . .41410.12 ExplicitMemory. . . . . . . . . .. . . . . . . . .. . . . . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 4}, page_content='TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .41010.11 OptimizationforLong-TermDependencies. . . . . . . .. . . . .41410.12 ExplicitMemory. . . . . . . . . .. . . . . . . . .. . . . . . . .41811 PracticalMethodology42311.1PerformanceMetrics. . . . . . . . . .. . . . . . . .. . . . . . .42411.2DefaultBaselineModels. . . . . . . .. . . . . . . .. . . . . . .42711.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . .42811.4SelectingHyperparameters. . . . . . . . .. . . . . . . .. . . . .42911.5DebuggingStrategies. . . . .. . . . . . . .. . . . . . . . .. . .43811.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . .44212 Applications44512.1LargeScaleDeepLearning.. . . . . . . .. . . . . . . . .. . . .44512.2ComputerVision. . . . . . . . .. . . . . . . .. . . . . . . .. .45412.3SpeechRecognition . . . . . .. . . . . . . .. . . . . . . . .. . .46012.4NaturalLanguageProcessing. . .. . . . . . . .. . . . . . . ..46312.5OtherApplications. . . . . . . . .. . . . . . . .. . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 4}, page_content='. . . . . .. . . . . . . .. . . . . . . . .. . .46012.4NaturalLanguageProcessing. . .. . . . . . . .. . . . . . . ..46312.5OtherApplications. . . . . . . . .. . . . . . . .. . . . . . . ..479IIIDeepLearningResearch48813 LinearFactorModels49113.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . .49213.2IndependentComponentAnalysis(ICA). . . . . . . . . . . .. .49313.3SlowFeatureAnalysis. . . . . .. . . . . . . . .. . . . . . . ..49513.4SparseCoding. . . . . .. . . . . . . .. . . . . . . . .. . . . . .498iv'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 5}, page_content='CONTENTS13.5ManifoldInterpretationofPCA. . . . . . . . . . . . . . . . .. .50114 Autoencoders50414.1UndercompleteAutoencoders. . . . . . . . . .. . . . . . . .. .50514.2RegularizedAutoencoders. . . . . . . . .. . . . . . . .. . . . .50614.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .51014.4StochasticEncodersandDecoders. . . . . . . . . . .. . . . . . .51114.5DenoisingAutoencoders. .. . . . . . . .. . . . . . . . .. . . .51214.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . .51714.7ContractiveAutoencoders.. . . . . . . .. . . . . . . .. . . . .52314.8PredictiveSparseDecomposition. . . . . . . .. . . . . . . . ..52514.9ApplicationsofAutoencoders. . . . .. . . . . . . . .. . . . . .52615 RepresentationLearning52815.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .53015.2TransferLearningandDomainAdaptation. . . .. . . . . . . ..53815.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .54315.4DistributedRepresentation. . . . . . . . . . . .. .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 5}, page_content='.. . . . .53015.2TransferLearningandDomainAdaptation. . . .. . . . . . . ..53815.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .54315.4DistributedRepresentation. . . . . . . . . . . .. . . . . . . . ..54815.5ExponentialGainsfromDepth. . . . . . . . . .. . . . . . . ..55515.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .55616 StructuredProbabilisticModelsforDeepLearning56016.1TheChallengeofUnstructuredModeling.. . . . . . . .. . . . .56116.2UsingGraphstoDescribeModelStructure. .. . . . . . . .. . .56516.3SamplingfromGraphicalModels. . .. . . . . . . .. . . . . . .58216.4AdvantagesofStructuredModeling .. . . . . . . . .. . . . . . .58416.5LearningaboutDependencies. . . .. . . . . . . .. . . . . . . .58416.6InferenceandApproximateInference. . . . . . . . .. . . . . . .58516.7TheDeepLearningApproachtoStructuredProbabilisticModels58617 MonteCarloMethods59217.1SamplingandMonteCarloMethods. . . . . . . .. . . . . . . .59217.2ImportanceSampling. . . . . . . . . . . ..'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 5}, page_content='.58516.7TheDeepLearningApproachtoStructuredProbabilisticModels58617 MonteCarloMethods59217.1SamplingandMonteCarloMethods. . . . . . . .. . . . . . . .59217.2ImportanceSampling. . . . . . . . . . . .. . . . . . . .. . . . .59417.3MarkovChainMonteCarloMethods. . . . .. . . . . . . .. . .59717.4GibbsSampling . . . . . . .. . . . . . . . .. . . . . . . .. . . .60117.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..60118 ConfrontingthePartitionFunction60718.1TheLog-LikelihoodGradient.. . . . . . . .. . . . . . . . .. .60818.2StochasticMaximumLikelihoodandContrastiveDivergence. . .609v'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 6}, page_content='CONTENTS18.3Pseudolikelihood. . . . . . . . . . .. . . . . . . . .. . . . . . .61718.4ScoreMatchingandRatioMatching. . . . . . . .. . . . . . . .61918.5DenoisingScoreMatching. . . . . . . . .. . . . . . . .. . . . .62118.6Noise-ContrastiveEstimation. . . . .. . . . . . . .. . . . . . .62218.7EstimatingthePartitionFunction. . . . . . . . . . .. . . . . . .62519 ApproximateInference63319.1InferenceasOptimization.. . . . . . . . .. . . . . . . .. . . .63519.2ExpectationMaximization. .. . . . . . . .. . . . . . . . .. . .63619.3MAPInferenceandSparseCoding.. . . . . . . . .. . . . . . .63719.4VariationalInferenceandLearning. . . . . . . . . . . . . . .. .64019.5LearnedApproximateInference. . .. . . . . . . . .. . . . . . .65320 DeepGenerativeModels65620.1BoltzmannMachines. . . . . . . . . . .. . . . . . . . .. . . . .65620.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . .. . .65820.3DeepBeliefNetworks.. . . . . . . .. . . . . . . .. . . . . . . .66220.4DeepBoltzmannMachines. . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 6}, page_content='. . . . . . . .. . . . .65620.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . .. . .65820.3DeepBeliefNetworks.. . . . . . . .. . . . . . . .. . . . . . . .66220.4DeepBoltzmannMachines. . . . . . . . . .. . . . . . . .. . . .66520.5BoltzmannMachinesforReal-ValuedData. . . . . . . . .. . . .67820.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . ..68520.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .68720.8OtherBoltzmannMachines. . . . .. . . . . . . .. . . . . . . .68820.9Back-PropagationthroughRandomOperations. . . . . .. . . .68920.10 DirectedGenerativeNets. . . . . . . . . . . .. . . . . . . . .. .69420.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . .. .71220.12 GenerativeStochasticNetworks. . .. . . . . . . .. . . . . . . .71620.13 OtherGenerationSchemes. . . . . . . . . . . . .. . . . . . . . .71720.14 EvaluatingGenerativeModels . . . . . . . . . . . . .. . . . . . .71920.15 Conclusion. . . . . . . .. . . . . . . . .. . . . . . . .. . . .'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 6}, page_content='. . . . . . . . . . . .. . . . . . . . .71720.14 EvaluatingGenerativeModels . . . . . . . . . . . . .. . . . . . .71920.15 Conclusion. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . .721Bibliography723Index780'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 6}, page_content='vi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 7}, page_content='Websitewww.deeplearningbook.orgThisbookisaccompaniedbytheabovewebsite.Thewebsiteprovidesavarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsofmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\\nvii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 8}, page_content='AcknowledgmentsThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.Wewouldliketothankthosewhocommentedonourproposalforthebookandhelpedplanitscontentsandorganization:GuillaumeAlain,KyunghyunCho,ÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomasRohée.Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthebookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,GuillaumeAlain,IonAndroutsopoulos,FredBertsch,OlexaBilaniuk,UfukCanBiçici,MatkoBošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLucCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,LaurentDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,Nando deFreitas,Çağlar Gülçehre, Jurgen VanGael,JavierAlonso'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 8}, page_content='deFreitas,Çağlar Gülçehre, Jurgen VanGael,JavierAlonso García,JonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,RudolfMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,RomanNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,KariPulli,RousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,HalisSak,CésarSalgado,GrigorySapunov,YoshinoriSasaki,MikeSchuster,JulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,DavidSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,AnTran,ShubhenduTrivedi,AlexeyUmnov,VincentVanhoucke,MarcoVisentini-Scarzanella,DavidWarde-Farley,DustinWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackonindividualchapters:•Notation:ZhangYuanhang.•Chapter,:YusufAkgul,SebastienBratieres,SamiraEbrahimi,1IntroductionCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescuviii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 9}, page_content='CONTENTSandAlfredoSolano.•Chapter,:AmjadAlmahairi,NikolaBanić,KevinBennett,2LinearAlgebraPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,SergeyOreshkov, IstvánPetrás,DennisPrangle, ThomasRohée, ColbyToland,MassimilianoTomassoli,AlessandroVitaleandBobWelland.•Chapter,:JohnPhilipAnderson,Kai3ProbabilityandInformationTheoryArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,AnttiRasmus,AlexeySurkovandVolkerTresp.•Chapter,:TranLamAn,IanFischer,andHu4NumericalComputationYuhuang.•Chapter, :DzmitryBahdanau, Nikhil Garg,5MachineLearning'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 9}, page_content=':DzmitryBahdanau, Nikhil Garg,5MachineLearning BasicsMakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,Kee-BongSong,ZhengSunandAndyWu.•Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,ElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKruegerandAdityaKumarPraharaj.•Chapter,:KshitijLauria,InkyuLee,7RegularizationforDeepLearningSunilMohanandJoshuaSalisbury.•Chapter,8OptimizationforTrainingDeepModels:MarcelAckermann,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,KlausStroblandMartinVita.•Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-stantinDivilov,EricJensen,AsifullahKhan,MehdiMirza,AlexPaino,EddiePierce,MarjorieSayer,RyanStoutandWentaoWu.•Chapter,10SequenceModeling:RecurrentandRecursiveNets:GökçenEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,DmitriySerdyuk,DongyuShiandKaiyuYang.•Chapter,:DanielBeckstein.11PracticalMethodology•Chapter,:GeorgeDahlandRibanaRoscher.12Applications•Chapter,:KunalGhosh.15RepresentationLearningix'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 10}, page_content='CONTENTS•Chapter,: MinhLê16StructuredProbabilisticModelsforDeepLearningandAntonVarfolom.•Chapter,18ConfrontingthePartitionFunction:SamBowman.•Chapter,:YujiaBao.19ApproximateInference•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,WenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.•Bibliography:LukasMichelbacherandLeslieN.Smith.Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresordatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptionsthroughoutthetext.WewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedtomakethewebversionofthebook,andforoﬀeringsupporttoimprovethequalityoftheresultingHTML.We would liketothank Ian’swifeDaniela'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 10}, page_content='would liketothank Ian’swifeDaniela FloriGoodfellowforpatientlysupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.WewouldliketothanktheGoogleBrainteamforprovidinganintellectualenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthisbookandreceivefeedbackandguidancefromcolleagues.WewouldespeciallyliketothankIan’sformermanager,GregCorrado,andhiscurrentmanager,SamyBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀreyHintonforencouragementwhenwritingwasdiﬃcult.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 10}, page_content='x'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 11}, page_content='NotationThissectionprovidesaconcisereferencedescribingthenotationusedthroughoutthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematicalconcepts,thisnotationreferencemayseemintimidating.However,donotdespair,wedescribemostoftheseideasinchapters2-4.NumbersandArraysaAscalar(integerorreal)aAvectorAAmatrixAAtensorInIdentitymatrixwithrowsandcolumnsnnIIdentitymatrixwithdimensionalityimpliedbycontexte()iStandardbasisvector[0,...,0,1,0,...,0]witha1atpositionidiag()aAsquare,diagonalmatrixwithdiagonalentriesgivenbyaaAscalarrandomvariableaAvector-valuedrandomvariableAAmatrix-valuedrandomvariablexi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 12}, page_content='CONTENTSSetsandGraphsAAsetRThesetofrealnumbers{}01,Thesetcontaining0and1{}01,,...,nThesetofallintegersbetweenand0n[]a,bTherealintervalincludingandab(]a,bTherealintervalexcludingbutincludingabAB\\\\Setsubtraction,i.e., thesetcontainingtheele-mentsofthatarenotinABGAgraphPaG(xi)TheparentsofxiinGIndexingaiElementiofvectora,withindexingstartingat1a−iAllelementsofvectorexceptforelementaiAi,jElementofmatrixi,jAAi,:RowofmatrixiAA:,iColumnofmatrixiAAi,j,kElementofa3-Dtensor()i,j,kAA::,,i2-Dsliceofa3-DtensoraiElementoftherandomvectoriaLinearAlgebraOperationsA\\ue03eTransposeofmatrixAA+Moore-PenrosepseudoinverseofAAB\\ue00cElement-wise(Hadamard)productofandABdet()ADeterminantofAxii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 13}, page_content='CONTENTSCalculusdydxDerivativeofwithrespecttoyx∂y∂xPartialderivativeofwithrespecttoyx∇xyGradientofwithrespecttoyx∇XyMatrixderivativesofwithrespecttoyX∇XyTensorcontainingderivativesofywithrespecttoX∂f∂xJacobianmatrixJ∈Rmn×off: Rn→Rm∇2xfff()(xorH)()xTheHessianmatrixofatinputpointx\\ue05afd()xxDeﬁniteintegralovertheentiredomainofx\\ue05aSfd()xxDeﬁniteintegralwithrespecttooverthesetxSProbabilityandInformationTheoryabTherandomvariablesaandbareindependent⊥abcTheyareareconditionallyindependentgivenc⊥|P()aAprobabilitydistributionoveradiscretevariablep()aAprobabilitydistributionoveracontinuousvari-able,oroveravariablewhosetypehasnotbeenspeciﬁedaRandomvariableahasdistribution∼PPEx∼P[()]()()()fxorEfxExpectationoffxwithrespecttoPxVar(())fxVarianceofunderxfx()P()Cov(()())fx,gxCovarianceofandunderxfx()gx()P()H()xShannonentropyoftherandomvariablexDKL()PQ\\ue06bKullback-LeiblerdivergenceofPandQN(;)xµ,ΣGaussiandistributionoverxwithmeanµandcovarianceΣxiii'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 14}, page_content='CONTENTSFunctionsff: AB→ThefunctionwithdomainandrangeABfgfg◦Compositionofthefunctionsandf(;)xθAfunctionofxparametrizedbyθ.Sometimeswejustwritef(x)andignoretheargumentθtolightennotation.logxxNaturallogarithmofσx()Logisticsigmoid,11+exp()−xζxx()log(1+exp(Softplus,))||||xpLpnormofx||||xL2normofxx+Positivepartof,i.e.,xmax(0),x1conditionis1iftheconditionistrue,0otherwiseSometimesweuseafunctionfwhoseargumentisascalar,butapplyittoavector,matrix,ortensor:f(x),f(X),orf(X).Thismeanstoapplyftothearrayelement-wise.Forexample,ifC=σ(X),thenCi,j,k=σ(Xi,j,k)forallvalidvaluesof,and.ijkDatasetsanddistributionspdataThedatageneratingdistributionˆpdataTheempiricaldistributiondeﬁnedbythetrainingsetXAsetoftrainingexamplesx()iThe-thexample(input)fromadatasetiy()iory()iThetargetassociatedwithx()iforsupervisedlearn-ingXThemn×matrixwithinputexamplex()iinrowXi,:xiv'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 15}, page_content='Chapter1IntroductionInventorshavelongdreamedofcreatingmachinesthatthink.ThisdesiredatesbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,andGalatea,Talos,andPandoramayallberegardedasartiﬁciallife(,OvidandMartin2004Sparkes1996Tandy1997;,;,).Whenprogrammablecomputerswereﬁrstconceived,peoplewonderedwhethertheymightbecomeintelligent,overahundredyearsbeforeonewasbuilt(Lovelace,1842).Today,artiﬁcialintelligence(AI)isathrivingﬁeldwithmanypracticalapplicationsandactiveresearchtopics.Welooktointelligentsoftwaretoautomateroutinelabor, understandspeechorimages, makediagnosesinmedicineandsupportbasicscientiﬁcresearch.Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolvedproblemsthatareintellectuallydiﬃcultforhumanbeingsbutrelativelystraight-forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-ematicalrules.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 15}, page_content='Thetruechallengetoartiﬁcialintelligenceprovedtobesolvingthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribeformally—problemsthatwesolveintuitively,thatfeelautomatic,likerecognizingspokenwordsorfacesinimages.Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionistoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofahierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimplerconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneedforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputerneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconceptsbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese1'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 16}, page_content='CHAPTER1.INTRODUCTIONconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.Forthisreason,wecallthisapproachtoAIdeeplearning.ManyoftheearlysuccessesofAItookplaceinrelativelysterileandformalenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabouttheworld. Forexample,IBM’sDeepBluechess-playingsystemdefeatedworldchampionGarryKasparovin1997(,).ChessisofcourseaverysimpleHsu2002world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmoveinonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis atremendousaccomplishment,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 16}, page_content='butthechallengeisnotduetothediﬃcultyofdescribingthesetofchesspiecesandallowablemovestothecomputer.Chesscanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easilyprovidedaheadoftimebytheprogrammer.Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmentalundertakingsforahumanbeingareamongtheeasiestforacomputer.Computershavelongbeenabletodefeateventhebesthumanchessplayer,butareonlyrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjectsorspeech.Aperson’severydayliferequiresanimmenseamountofknowledgeabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andthereforediﬃculttoarticulateinaformalway.Computersneedtocapturethissameknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesinartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabouttheworldinformallanguages.Acomputercanreasonaboutstatementsintheseformallanguagesautomaticallyusinglogicalinferencerules.Thisis'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 16}, page_content='r.Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabouttheworldinformallanguages.Acomputercanreasonaboutstatementsintheseformallanguagesautomaticallyusinglogicalinferencerules.Thisisknownastheknowledgebaseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasledtoamajorsuccess.OneofthemostfamoussuchprojectsisCyc(,LenatandGuha1989).CycisaninferenceengineandadatabaseofstatementsinalanguagecalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisanunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexitytoaccuratelydescribetheworld.Forexample,CycfailedtounderstandastoryaboutapersonnamedFredshavinginthemorning(,).ItsinferenceLinde1992enginedetectedaninconsistencyinthestory:'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 16}, page_content='itknewthatpeopledonothaveelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedtheentity“FredWhileShaving”containedelectricalparts.ItthereforeaskedwhetherFredwasstillapersonwhilehewasshaving.Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggestthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextractingpatternsfromrawdata.Thiscapabilityisknownasmachinelearning.Theintroduction2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 17}, page_content='CHAPTER1.INTRODUCTIONofmachinelearningallowedcomputerstotackleproblemsinvolvingknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimplemachinelearningalgorithmcalledlogisticregressioncandeterminewhethertorecommendcesareandelivery(Mor-Yosef1990etal.,).Asimplemachinelearningalgorithmcalledcanseparatelegitimatee-mailfromspame-mail.naiveBayesTheperformanceofthesesimplemachinelearningalgorithmsdependsheavilyontherepresentationofthedatatheyaregiven.Forexample,whenlogisticregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexaminethepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevantinformation,suchasthepresenceorabsenceofauterinescar.Eachpieceofinformationincludedintherepresentationofthepatientisknownasafeature.Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswithvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesaredeﬁnedinanyway.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 17}, page_content='IflogisticregressionwasgivenanMRIscanofthepatient,ratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeusefulpredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithanycomplicationsthatmightoccurduringdelivery.Thisdependenceonrepresentationsisageneralphenomenonthatappearsthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-tionssuchassearchingacollectionofdatacanproceedexponentiallyfasterifthecollectionisstructuredandindexedintelligently.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 17}, page_content='PeoplecaneasilyperformarithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuchmoretime-consuming.Itisnotsurprisingthatthechoiceofrepresentationhasanenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimplevisualexample,seeFig..1.1Manyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetoffeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachinelearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfromsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrongclueastowhetherthespeakerisaman,woman,orchild.However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.Forexample,supposethatwewouldliketowriteaprogramtodetectcarsinphotographs.Weknowthatcarshavewheels,sowemightliketousethepresenceofawheelasafeature.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 17}, page_content='Unfortunately,itisdiﬃculttodescribeexactlywhatawheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebutitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀthemetalpartsofthewheel,thefenderofthecaroranobjectintheforegroundobscuringpartofthewheel,andsoon.3'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 18}, page_content='CHAPTER1.INTRODUCTION\\n\\ue078\\ue079\\ue043\\ue061\\ue072\\ue074\\ue065\\ue073\\ue069\\ue061\\ue06e\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073\\n\\ue072\\ue0b5\\ue050\\ue06f\\ue06c\\ue061\\ue072\\ue020\\ue063\\ue06f\\ue06f\\ue072\\ue064\\ue069\\ue06e\\ue061\\ue074\\ue065\\ue073'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 18}, page_content='Figure1.1:Exampleof diﬀerentrepresentations:supposewewanttoseparate twocategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplotontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpletosolvewithaverticalline.(FigureproducedincollaborationwithDavidWarde-Farley)Onesolutiontothisproblemistousemachinelearningtodiscovernotonlythemappingfromrepresentationtooutputbutalsotherepresentationitself.Thisapproachisknownasrepresentationlearning.Learnedrepresentationsoftenresultinmuchbetterperformance thancanbeobtained'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 18}, page_content='withhand-designedrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,withminimalhumanintervention.Arepresentationlearningalgorithmcandiscoveragoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhourstomonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealofhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.Thequintessentialexampleofarepresentationlearningalgorithmistheau-toencoderencoder.Anautoencoderisthecombinationofanfunctionthatconvertstheinputdataintoadiﬀerentrepresentation,andadecoderfunctionthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencodersaretrainedtopreserveasmuchinformationaspossiblewhenaninputisrunthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenewrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimtoachievediﬀerentkindsofproperties.Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusuallytoseparatethethatexplaintheobserveddata.Inthiscontext,factorsofvariationweusethew'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 18}, page_content='autoencodersaimtoachievediﬀerentkindsofproperties.Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusuallytoseparatethethatexplaintheobserveddata.Inthiscontext,factorsofvariationweusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;thefactorsareusuallynotcombinedbymultiplication.Suchfactorsareoftennotquantities4'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 19}, page_content='CHAPTER1.INTRODUCTIONthataredirectlyobserved.Instead,theymayexisteitherasunobservedobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifyingexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofasconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’sage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzinganimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,andtheangleandbrightnessofthesun.Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplicationsisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweareabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclosetoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.Mostapplicationsrequireustothefactorsofvariationanddiscardthedisentangleonesthatwedonotcareabout.Ofcourse,itcanbeverydiﬃc'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 19}, page_content='beveryclosetoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.Mostapplicationsrequireustothefactorsofvariationanddiscardthedisentangleonesthatwedonotcareabout.Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeaturesfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,canbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingofthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvetheoriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.Deeplearningsolvesthiscentralprobleminrepresentationlearningbyintroduc-ingrepresentationsthatareexpressedintermsofother,simplerrepresentations.Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-cepts.Fig.showshowadeeplearningsystemcanrepresenttheconceptofan1.2imageofapersonbycombiningsimplerconcepts,suchascornersandcontours,whichareinturndeﬁnedintermsofedges.Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeepnetworkormultilayerperceptron(MLP).Amultilay'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 19}, page_content='ersonbycombiningsimplerconcepts,suchascornersandcontours,whichareinturndeﬁnedintermsofedges.Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeepnetworkormultilayerperceptron(MLP).Amultilayerperceptronisjustamathe-maticalfunctionmappingsomesetofinputvaluestooutputvalues.Thefunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeachapplicationofadiﬀerentmathematicalfunctionasprovidinganewrepresentationoftheinput.Theideaoflearningtherightrepresentationforthedataprovidesoneperspec-tiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthecomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentationcanbethoughtofasthestateofthecomputer’smemoryafterexecutinganothersetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemoreinstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselaterinstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis5'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 20}, page_content='CHAPTER1.INTRODUCTION\\nVisible layer(input pixels)1st hidden layer(edges)2nd hidden layer(corners andcontours)3rd hidden layer(object parts)CARPERSONANIMALOutput(object identity)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 20}, page_content='Figure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstandthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollectionofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisverycomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoaseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.Theinputispresentedatthe,sonamedbecauseitcontainsthevariablesthatwevisiblelayerareabletoobserve.Thenaseriesofextractsincreasinglyabstractfeatureshiddenlayersfromtheimage.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 20}, page_content='Theselayersarecalled“hidden”becausetheirvaluesarenotgiveninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplainingtherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekindoffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasilyidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthiddenlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersandextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhiddenlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayercandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursandcorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscanbeusedtorecognizetheobjectspresentintheimage.ImagesreproducedwithpermissionfromZeilerandFergus2014().6'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 21}, page_content='CHAPTER1.INTRODUCTION\\nx1x1σ'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 21}, page_content='w1w1×x2x2w2w2×+ElementSet+×σx xw'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 21}, page_content='wElementSetLogisticRegressionLogisticRegressionFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhereeachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputtooutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,σ(wTx),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationandlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepththree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.viewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarilyencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostoresstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditionalcomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,butithelpsthemodeltoorganizeitsprocessing.Therearetwomainwaysofmeasuringthedepthofamodel.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 21}, page_content='eanalogoustoacounterorpointerinatraditionalcomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,butithelpsthemodeltoorganizeitsprocessing.Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewisbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluatethearchitecture.Wecanthinkofthisasthelengthofthelongestpaththroughaﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgivenitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengthsdependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmaybedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionsweallowtobeusedasindividualstepsintheﬂowchart.Fig.illustrateshowthischoice1.3oflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofamodelasbeingnotthedepthofthecomputationalgraphbutthedepthofthegraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepthoftheﬂowchartofthecomputationsneededtocomputetherepresentationof7'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 22}, page_content='CHAPTER1.INTRODUCTIONeachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁnedgiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystemobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobablypresentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers—alayerforeyesandalayerforfaces—butthegraphofcomputationsincludes2nlayersifwereﬁneourestimateofeachconceptgiventheothertimes.nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthecomputationalgraph,orthedepthoftheprobabilisticmodelinggraph—ismostrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelementsfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthedepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthofacomputerprogram.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 22}, page_content='Noristhereaconsensusabouthowmuchdepthamodelrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthestudyofmodelsthateitherinvolveagreateramountofcompositionoflearnedfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputersystemstoimprovewithexperienceanddata.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 22}, page_content='Accordingtotheauthorsofthisbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthatcanoperateincomplicated,real-worldenvironments.Deeplearningisaparticularkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningtorepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedinrelationtosimplerconcepts,andmoreabstractrepresentationscomputedintermsoflessabstractones.Fig.illustratestherelationshipbetweenthesediﬀerent1.4AIdisciplines.Fig.givesahigh-levelschematicofhoweachworks.1.51.1WhoShouldReadThisBook?Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomaintargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents(undergraduateorgraduate)learningaboutmachinelearning,includingthosewhoarebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 22}, page_content='Theothertargetaudienceissoftwareengineerswhodonothaveamachinelearningorstatisticsbackground,butwanttorapidlyacquireoneandbeginusingdeeplearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulinmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,8'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 23}, page_content='CHAPTER1.INTRODUCTION\\nAIMachine learningRepresentation learningDeep learningExample:KnowledgebasesExample:LogisticregressionExample:ShallowautoencodersExample:MLPs\\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproachestoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\\n9'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 24}, page_content='CHAPTER1.INTRODUCTION\\nInputHand-designed programOutput\\nInputHand-designed featuresMapping from featuresOutput\\nInputFeaturesMapping from featuresOutput\\nInputSimple featuresMapping from featuresOutput\\nAdditional layers of more abstract features\\nRule-basedsystemsClassicmachinelearningRepresentationlearningDeeplearningFigure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeachotherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareabletolearnfromdata.10'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 25}, page_content='CHAPTER1.INTRODUCTIONnaturallanguageprocessing,robotics,bioinformaticsandchemistry,videogames,searchengines,onlineadvertisingandﬁnance.Thisbookhasbeenorganizedintothreepartsinordertobestaccommodateavarietyofreaders.PartintroducesbasicmathematicaltoolsandmachinelearningIconcepts.PartdescribesthemostestablisheddeeplearningalgorithmsthatareIIessentiallysolvedtechnologies.PartdescribesmorespeculativeideasthatareIIIwidelybelievedtobeimportantforfutureresearchindeeplearning.Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterestsorbackground.Readersfamiliarwithlinearalgebra,probability,andfundamentalmachinelearningconceptscanskipPart,forexample,whilereaderswhojustwantItoimplementaworkingsystemneednotreadbeyondPart.TohelpchoosewhichIIchapterstoread,Fig.providesaﬂowchartshowingthehigh-levelorganization1.6ofthebook.Wedoassumethatallreaderscomefromacomputersciencebackground.Weassumefamiliaritywithprogramming,abasicunderstandingofcomputationalperformanceissues,complexitytheory,introd'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 25}, page_content='-levelorganization1.6ofthebook.Wedoassumethatallreaderscomefromacomputersciencebackground.Weassumefamiliaritywithprogramming,abasicunderstandingofcomputationalperformanceissues,complexitytheory,introductorylevelcalculusandsomeoftheterminologyofgraphtheory.1.2HistoricalTrendsinDeepLearningItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthanprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:•Deeplearninghashadalongandrichhistory,buthasgonebymanynamesreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedinpopularity.•Deeplearninghasbecomemoreusefulastheamountofavailabletrainingdatahasincreased.•Deeplearningmodelshavegrowninsizeovertimeascomputerhardwareandsoftwareinfrastructurefordeeplearninghasimproved.•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasingaccuracyovertime.11'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 26}, page_content='CHAPTER1.INTRODUCTION1. IntroductionPart I: Applied Math and Machine Learning Basics2. Linear Algebra3. Probability and Information Theory4. Numerical Computation5. Machine Learning BasicsPart II: Deep Networks: Modern Practices6. Deep Feedforward Networks7. Regularization8. Optimization9.  CNNs10.  RNNs11. Practical Methodology12. ApplicationsPart III: Deep Learning Research13. Linear Factor Models14. Autoencoders15. Representation Learning16. Structured Probabilistic Models17. Monte Carlo Methods18. Partition Function19. Inference20. Deep Generative ModelsFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanotherindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.12'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 27}, page_content='CHAPTER1.INTRODUCTION1.2.1TheManyNamesandChangingFortunesofNeuralNet-worksWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasanexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabookaboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deeplearningonlyappearstobenew,becauseitwasrelativelyunpopularforseveralyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmanydiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeldhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchersanddiﬀerentperspectives.Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.However,somebasiccontextisusefulforunderstandingdeeplearning.Broadlyspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deeplearn-ingknownascyberneticsconnectionisminthe1940s–1960s,deeplearningknownasinthe1980s–1990s,andthecurrentresurgenceunderthenamedeeplearningbeginningin2006.ThisisquantitativelyillustratedinFig..1.7Someoftheearliestlearningalgorithmswerecognizetoda'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 27}, page_content='40s–1960s,deeplearningknownasinthe1980s–1990s,andthecurrentresurgenceunderthenamedeeplearningbeginningin2006.ThisisquantitativelyillustratedinFig..1.7Someoftheearliestlearningalgorithmswerecognizetodaywereintendedtobecomputationalmodelsofbiologicallearning,i.e.modelsofhowlearninghappensorcouldhappeninthebrain.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 27}, page_content='Asaresult,oneofthenamesthatdeeplearninghasgonebyisartiﬁcialneuralnetworks(ANNs).Thecorrespondingperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspiredbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).Whilethekindsofneuralnetworksusedformachinelearninghavesometimesbeenusedtounderstandbrainfunction(,),theyareHintonandShallice1991generallynotdesignedtoberealisticmodelsofbiologicalfunction.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 27}, page_content='Theneuralperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthatthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,andaconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthecomputationalprinciplesbehindthebrainandduplicateitsfunctionality.Anotherperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandtheprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshedlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolveengineeringapplications.Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspectiveonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneralprincipleoflearningmultiplelevelsofcomposition,whichcanbeappliedinmachinelearningframeworksthatarenotnecessarilyneurallyinspired.13'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 28}, page_content='CHAPTER1.INTRODUCTION\\n1940195019601970198019902000Year0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrasecybernetics(connectionism+neuralnetworks)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 28}, page_content='Figure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnetsresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).Theﬁrstwavestartedwithcyberneticsinthe1940s–1960s,withthedevelopmentoftheoriesofbiologicallearning(,;,)andimplementationsofMcCullochandPitts1943Hebb1949theﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingleneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,withback-propagation(,)totrainaneuralnetworkwithoneortwoRumelhartetal.1986ahiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hintonetal.etal.etal.,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook2007aformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthanthecorrespondingscientiﬁcactivityoccurred.\\n14'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 29}, page_content='CHAPTER1.INTRODUCTIONTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodelsmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedtotakeasetofninputvaluesx1,...,xnandassociatethemwithanoutputy.Thesemodelswouldlearnasetofweightsw1,...,wnandcomputetheiroutputf(xw,)=x1w1+···+xnwn.Thisﬁrstwaveofneuralnetworksresearchwasknownascybernetics,asillustratedinFig..1.7TheMcCulloch-PittsNeuron(,)wasanearlymodelMcCullochandPitts1943ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesofinputsbytestingwhetherf(xw,)ispositiveornegative.Ofcourse,forthemodeltocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobesetcorrectly. Theseweightscouldbesetbythehumanoperator.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 29}, page_content='Inthe1950s,theperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.Theadaptivelinearelement(ADALINE),whichdatesfromaboutthesametime,simplyreturnedthevalueoff(x)itselftopredictarealnumber(WidrowandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofmachinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADA-LINEwasaspecialcaseofanalgorithmcalledstochasticgradientdescent.Slightlymodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominanttrainingalgorithmsfordeeplearningmodelstoday.Modelsbasedonthef(xw,)usedbytheperceptronandADALINEarecalledlinearmodels.Thesemodelsremainsomeofthemostwidelyusedmachinelearningmodels,thoughinmanycasestheyaretrainedindiﬀerentwaysthantheoriginalmodelsweretrained.Linearmodelshavemanylimitations.Mostfamously,theycannotlearntheXORfunction,wheref([0,1],w)=1andf([1,0],w)=1butf([1,1],w)=0andf([0,0],w)=0'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 29}, page_content='estheyaretrainedindiﬀerentwaysthantheoriginalmodelsweretrained.Linearmodelshavemanylimitations.Mostfamously,theycannotlearntheXORfunction,wheref([0,1],w)=1andf([1,0],w)=1butf([1,1],w)=0andf([0,0],w)=0.Criticswhoobservedtheseﬂawsinlinearmodelscausedabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.Today,neuroscienceisregardedasanimportantsourceofinspirationfordeeplearningresearchers,butitisnolongerthepredominantguidefortheﬁeld.Themainreasonforthediminishedrole'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 29}, page_content='ofneuroscienceindeeplearningresearchtodayisthatwesimplydonothaveenoughinformationaboutthebraintouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsusedbythebrain,wewouldneedtobeabletomonitortheactivityof(attheveryleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenotabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand15'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 30}, page_content='CHAPTER1.INTRODUCTIONwell-studiedpartsofthebrain(,).OlshausenandField2005Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithmcansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewiredtosendvisualsignalstothatarea(VonMelchner2000etal.,).Thissuggeststhatmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthediﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearningresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudyingnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearningresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaofhavingmanycomputationalunitsthatbecomeintelligentonlyviatheirinteractionswitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)introducedapowerfulmodelarc'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 30}, page_content='inesfromneuroscience.Thebasicideaofhavingmanycomputationalunitsthatbecomeintelligentonlyviatheirinteractionswitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspiredbythestructureofthemammalianvisualsystemandlaterbecamethebasisforthemodernconvolutionalnetwork(,),aswewillseeinSec..LeCunetal.1998b9.10Mostneuralnetworkstodayarebasedonamodelneuroncalledtherectiﬁedlinearunit.TheoriginalCognitron(Fukushima1975,)introducedamorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrainfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrommanyviewpoints,withNairandHinton2010Glorot2011a()andetal.()citingneuroscienceasaninﬂuence,andJarrett2009etal.()citingmoreengineering-orientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,itneednotbetakenasarigidguide.Weknowthatactualneuronscomputeverydiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealismhasnotyetledtoanimprovementinmachin'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 30}, page_content='animportantsourceofinspiration,itneednotbetakenasarigidguide.Weknowthatactualneuronscomputeverydiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealismhasnotyetledtoanimprovementinmachinelearningperformance.Also,whileneurosciencehassuccessfullyinspiredseveralneuralnetworkarchitectures,wedonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuchguidanceforthelearningalgorithmsweusetotrainthesearchitectures.Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.WhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasaninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernelmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempttosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,especiallyappliedmathfundamentalslikelinearalgebra,probability,informationtheory,andnumericaloptimization.Whilesomedeeplearningresearchersciteneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith16'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 31}, page_content='CHAPTER1.INTRODUCTIONneuroscienceatall.Itis worth notingthat theeﬀorttounderstandhowthe brainworksonan algorithmic level is alive andwell.This endeavor isprimarily'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 31}, page_content='knownas“computationalneuroscience”andisaseparateﬁeldofstudyfromdeeplearning.Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.Theﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystemsthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldofcomputationalneuroscienceisprimarilyconcernedwithbuildingmoreaccuratemodelsofhowthebrainactuallyworks.Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreatpartviaamovementcalledconnectionismparalleldistributedprocessingor(Rumelhartetal.etal.,;1986cMcClelland,).Connectionismaroseinthecontextof1995cognitivescience.Cognitivescienceisaninterdisciplinaryapproachtounderstand-ingthemind,combiningmultiplediﬀerentlevelsofanalysis.Duringtheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsofhowthebraincouldactuallyimplementthemusingneurons.Theconnectionistsbegantostudymodelsofcognitionthatcouldactuallybegroundedinneuralimplementations(T'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 31}, page_content='pularity,symbolicmodelswerediﬃculttoexplainintermsofhowthebraincouldactuallyimplementthemusingneurons.Theconnectionistsbegantostudymodelsofcognitionthatcouldactuallybegroundedinneuralimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingbacktotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949Thecentralideainconnectionismisthatalargenumberofsimplecomputationalunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsightappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsincomputationalmodels.Severalkeyconceptsaroseduringtheconnectionismmovementofthe1980sthatremaincentraltotoday’sdeeplearning.Oneoftheseconceptsisthatofdistributedrepresentation(,).Hintonetal.1986Thisistheideathateachinputtoasystemshouldberepresentedbymanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmanypossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognizecars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Onewayofrepresentingtheseinputswouldbetohavease'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 31}, page_content='ntherepresentationofmanypossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognizecars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Onewayofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunitthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,redbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuronmustindependentlylearntheconceptofcolorandobjectidentity.Onewaytoimproveonthissituationistouseadistributedrepresentation,withthreeneuronsdescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequiresonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisabletolearnaboutredness17'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 32}, page_content='CHAPTER1.INTRODUCTIONfromimagesofcars,trucksandbirds,notonlyfromimagesofonespeciﬁccategoryofobjects.Theconceptofdistributedrepresentationiscentraltothisbook,andwillbedescribedingreaterdetailinChapter.15Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc-cessfuluseofback-propagationtotraindeepneuralnetworkswithinternalrepre-sentationsandthepopularizationoftheback-propagationalgorithm(Rumelhartetal.,;,).Thisalgorithmhaswaxedandwanedinpopularity1986aLeCun1987butasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.Duringthe1990s,researchersmadeimportantadvancesinmodelingsequenceswithneuralnetworks.()and()identiﬁedsomeHochreiter1991Bengioetal.1994ofthefundamentalmathematicaldiﬃcultiesinmodelinglongsequences,describedinSec..10.7HochreiterandSchmidhuber1997()introducedthelongshort-termmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTMiswidelyusedformanysequencemodelingtasks,includingmanynaturallanguageprocessingtasksatGoogle.Thesecondwaveofneuralnetworksres'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 32}, page_content='ngshort-termmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTMiswidelyusedformanysequencemodelingtasks,includingmanynaturallanguageprocessingtasksatGoogle.Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁlltheseunreasonableexpectations,investorsweredisappointed.Simultaneously,otherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boseretal.1992CortesandVapnik1995Schölkopf1999Jor-;,;etal.,)andgraphicalmodels(dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactorsledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformanceonsometasks(,;,).TheCanadianInstituteLeCunetal.1998bBengioetal.2001forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchaliveviaitsNeuralComputationandAdaptivePerception(NCAP)researchinitiative.Thisprogramunitedmach'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 32}, page_content='nadianInstituteLeCunetal.1998bBengioetal.2001forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchaliveviaitsNeuralComputationandAdaptivePerception(NCAP)researchinitiative.ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHintonatUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYannLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehadamulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhumanandcomputervision.Atthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃculttotrain.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 32}, page_content='Wenowknowthatalgorithmsthathaveexistedsincethe1980sworkquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythatthesealgorithmsweretoocomputationallycostlytoallowmuchexperimentationwiththehardwareavailableatthetime.Thethirdwaveofneuralnetworksresearchbeganwithabreakthroughin18'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 33}, page_content='CHAPTER1.INTRODUCTION2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbeliefnetworkcould be eﬃcientlytrainedusinga strategycalled'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 33}, page_content='greedylayer-wisepretraining(Hinton2006etal.,),whichwillbedescribedinmoredetailinSec.15.1.TheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategycouldbeusedtotrainmanyotherkindsofdeepnetworks(,Bengioetal.2007Ranzato2007a;etal.,)andsystematicallyhelpedtoimprovegeneralizationontestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthetermdeeplearningtoemphasizethatresearcherswerenowabletotraindeeperneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthetheoreticalimportanceofdepth(,;,BengioandLeCun2007DelalleauandBengio2011Pascanu2014aMontufar2014;etal.,;etal.,).Atthistime,deepneuralnetworksoutperformedcompetingAIsystemsbasedonothermachinelearningtechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularityofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeeplearningresearchhaschangeddramaticallywithinthetimeofthiswave.Thethirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandtheabilityofdeepmodelstogeneralizewellfromsmallda'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 33}, page_content='ing,thoughthefocusofdeeplearningresearchhaschangeddramaticallywithinthetimeofthiswave.Thethirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandtheabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereismoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeepmodelstoleveragelargelabeleddatasets.1.2.2IncreasingDatasetSizesOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasacrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswereconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercialapplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthanatechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistruethatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.Fortunately,theamountofskillrequiredreducesastheamountoftrainingdataincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextaskstodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoyproblemsinthe1980s,thoughthemod'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 33}, page_content='educesastheamountoftrainingdataincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextaskstodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoyproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshaveundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themostimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswiththeresourcestheyneedtosucceed.Fig.showshowthesizeofbenchmark1.8datasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasingdigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasinglynetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem19'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 34}, page_content='CHAPTER1.INTRODUCTIONintoadatasetappropriateformachinelearningapplications.Theageof“BigData”hasmademachinelearningmucheasierbecausethekeyburdenofstatisticalestimation—generalizingwelltonewdataafterobservingonlyasmallamountofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumbisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptableperformancewitharound5,000labeledexamplespercategory,andwillmatchorexceedhumanperformancewhentrainedwithadatasetcontainingatleast10millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisisanimportantresearcharea,focusinginparticularonhowwecantakeadvantageoflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervisedlearning.1.2.3IncreasingModelSizesAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoyingcomparativelylittlesuccesssincethe1980sisthatwehavethecomputationalresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 34}, page_content='arativelylittlesuccesssincethe1980sisthatwehavethecomputationalresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.Biologicalneuronsarenotespeciallydenselyconnected.AsseeninFig.,1.10ourmachinelearningmodelshavehadanumberofconnectionsperneuronthatwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishinglysmalluntilquiterecently,asshowninFig..Sincetheintroductionofhidden1.11units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Thisgrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailabilityoflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmorecomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologiesallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberofneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuron'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 34}, page_content='ecomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologiesallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberofneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmayrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiologicalneuralnetworksmaybeevenlargerthanthisplotportrays.Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewerneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-lems.Eventoday’snetworks,whichweconsiderquitelargefromacomputationalsystemspointofview,aresmallerthanthenervoussystemofevenrelativelyprimitivevertebrateanimalslikefrogs.Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,20'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 35}, page_content='CHAPTER1.INTRODUCTION\\n\\ue031\\ue039\\ue030\\ue030\\ue031\\ue039\\ue035\\ue030\\ue031\\ue039\\ue038\\ue035\\ue032\\ue030\\ue030\\ue030\\ue032\\ue030\\ue031\\ue035\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue031\\ue030\\ue036\\ue031\\ue030\\ue037\\ue031\\ue030\\ue038\\ue031\\ue030\\ue039\\ue044\\ue061\\ue074\\ue061\\ue073\\ue065\\ue074\\ue020\\ue073\\ue069\\ue07a\\ue065\\ue020\\ue028\\ue06e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue029\\ue049\\ue072\\ue069\\ue073\\ue04d\\ue04e\\ue049\\ue053\\ue054\\ue050\\ue075\\ue062\\ue06c\\ue069\\ue063\\ue020\\ue053\\ue056\\ue048\\ue04e\\ue049\\ue06d\\ue061\\ue067\\ue065\\ue04e\\ue065\\ue074\\ue043\\ue049\\ue046\\ue041\\ue052\\ue02d\\ue031\\ue030\\ue049\\ue06d\\ue061\\ue067\\ue065\\ue04e\\ue065\\ue074\\ue031\\ue030\\ue06b\\ue049\\ue04c\\ue053\\ue056\\ue052\\ue043\\ue020\\ue032\\ue030\\ue031\\ue034\\ue053\\ue070\\ue06f\\ue072\\ue074\\ue073\\ue02d\\ue031\\ue04d\\n\\ue052\\ue06f\\ue074\\ue061\\ue074\\ue065\\ue064\\ue020\\ue054\\ue020\\ue076\\ue073\\ue020\\ue043\\ue054\\ue020\\ue076\\ue073\\ue020\\ue047\\ue020\\ue076\\ue073\\ue020\\ue046\\ue043\\ue072\\ue069\\ue06d\\ue069\\ue06e\\ue061\\ue06c\\ue073\\ue043\\ue061\\ue06e\\ue061\\ue064\\ue069\\ue061\\ue06e\\ue020\\ue048\\ue061\\ue06e\\ue073\\ue061\\ue072\\ue064\\ue057\\ue04d\\ue054\\ue049\\ue06e\\ue063\\ue072\\ue065\\ue061\\ue073\\ue069\\ue06e\\ue067\\ue020\\ue064\\ue061\\ue074\\ue061\\ue073\\ue065\\ue074\\ue020\\ue073\\ue069\\ue07a\\ue065\\ue020\\ue06f\\ue076\\ue065\\ue072\\ue020\\ue074\\ue069\\ue06d\\ue065'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 35}, page_content='Figure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticiansstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson1900Gosset1908Anderson1935Fisher1936;,;,;,).Inthe1950sthrough1980s,thepioneersofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,suchaslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostanddemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(WidrowandHoﬀ1960Rumelhart1986b,;etal.,).Inthe1980sand1990s,machinelearningbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtensofthousandsofexamplessuchastheMNISTdataset(showninFig.)ofscansof1.9handwrittennumbers(,).Intheﬁrstdecadeofthe2000s,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 35}, page_content='moreLeCunetal.1998bsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(KrizhevskyandHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughouttheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousandstotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset(,Netzeretal.2011),variousversionsoftheImageNetdataset(,,;Dengetal.20092010aRussakovskyetal.etal.,),andtheSports-1Mdataset(2014aKarpathy,).Atthetopofthe2014graph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructedfromtheCanadianHansard(,)andtheWMT2014EnglishtoFrenchBrownetal.1990dataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.21'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 36}, page_content='CHAPTER1.INTRODUCTION\\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNationalInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewithmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigitsandassociatedlabelsdescribingwhichdigit0-9iscontainedineachimage.Thissimpleclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearningresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.GeoﬀreyHintonhasdescribeditas“thedrosophilaofmachinelearning,”meaningthatitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratoryconditions,muchasbiologistsoftenstudyfruitﬂies.22'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 37}, page_content='CHAPTER1.INTRODUCTIONtheadventofgeneralpurposeGPUs(describedinSec.),fasternetwork12.1.2connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneofthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerallyexpectedtocontinuewellintothefuture.1.2.4IncreasingAccuracy,ComplexityandReal-WorldImpactSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovideaccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeenappliedwithsuccesstobroaderandbroadersetsofapplications.Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightlycropped,extremelysmallimages(,).SincethentherehasRumelhartetal.1986abeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modernobjectrecognitionnetworksprocessrichhigh-resolutionphotographsanddonothavearequirementthatthephotobecroppedneartheobjecttoberecognized(,).Similarly,theearliestnetworkscouldonlyrecognizeKrizhevskyetal.2012twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindofobject),'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 37}, page_content='irementthatthephotobecroppedneartheobjecttoberecognized(,).Similarly,theearliestnetworkscouldonlyrecognizeKrizhevskyetal.2012twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindofobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerentcategoriesofobjects.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 37}, page_content='ThelargestcontestinobjectrecognitionistheImageNetLarge-ScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramaticmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetworkwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthestate-of-the-arttop-5errorratefrom26.1%to15.3%(,),Krizhevskyetal.2012meaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategoriesforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthislistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsareconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesindeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,asshowninFig..1.12Deeplearninghasalsohadadramaticimpactonspeechrecognition.Afterimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnatedstartinginabout2000.Theintroductionofdeeplearning(,;Dahletal.2010Dengetal.etal.,;2010bSeide,;2011Hinton2012aetal.,)tospeechrecognitionresultedinasuddendropoferrorrates,withsomeerrorratescutinha'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 37}, page_content='agnatedstartinginabout2000.Theintroductionofdeeplearning(,;Dahletal.2010Dengetal.etal.,;2010bSeide,;2011Hinton2012aetal.,)tospeechrecognitionresultedinasuddendropoferrorrates,withsomeerrorratescutinhalf.WewillexplorethishistoryinmoredetailinSec..12.3Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionandimagesegmentation(,;Sermanetetal.2013Farabet2013Couprieetal.,;etal.,2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan23'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 38}, page_content='CHAPTER1.INTRODUCTION\\n\\ue031\\ue039\\ue035\\ue030\\ue031\\ue039\\ue038\\ue035\\ue032\\ue030\\ue030\\ue030\\ue032\\ue030\\ue031\\ue035\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue043\\ue06f\\ue06e\\ue06e\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue073\\ue020\\ue070\\ue065\\ue072\\ue020\\ue06e\\ue065\\ue075\\ue072\\ue06f\\ue06e\\ue031\\ue032\\ue033\\ue034\\ue035\\ue036\\ue037\\ue038\\ue039\\ue031\\ue030\\ue046\\ue072\\ue075\\ue069\\ue074\\ue020\\ue066\\ue06c\\ue079\\ue04d\\ue06f\\ue075\\ue073\\ue065\\ue043\\ue061\\ue074\\ue048\\ue075\\ue06d\\ue061\\ue06e\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue063\\ue06f\\ue06e\\ue06e\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue073\\ue020\\ue070\\ue065\\ue072\\ue020\\ue06e\\ue065\\ue075\\ue072\\ue06f\\ue06e\\ue020\\ue06f\\ue076\\ue065\\ue072\\ue020\\ue074\\ue069\\ue06d\\ue065'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 38}, page_content='Figure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneuralnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetweenneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyasmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworkstohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehumanbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneuralnetworksizesfrom().Wikipedia20151.Adaptivelinearelement(,)WidrowandHoﬀ19602.Neocognitron(Fukushima1980,)3.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.20064.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)5.Unsupervisedconvolutionalnetwork(,)Jarrettetal.20096.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.20107.Distributedautoencoder(,)Leetal.20128.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.20129.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201310.GoogLeNet(,)Szegedyetal.2014a\\n24'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 39}, page_content='CHAPTER1.INTRODUCTIONetal.,).2012Atthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,sohasthecomplexityofthetasksthattheycansolve.()Goodfellowetal.2014dshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacterstranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividualelementsofthesequence(,).Recurrentneuralnetworks,GülçehreandBengio2013suchastheLSTMsequencemodelmentionedabove,arenowusedtomodelrelationshipsbetweensequencessequencesandotherratherthanjustﬁxedinputs.Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizinganotherapplication:machinetranslation(Sutskever2014Bahdanauetal.,;etal.,2015).ThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusionwiththeintroductionofneuralTuringmachines(Graves2014aetal.,)thatlearntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Suchneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.Forexample,the'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 39}, page_content='ductionofneuralTuringmachines(Graves2014aetal.,)thatlearntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Suchneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.Forexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledandsortedsequences.Thisself-programmingtechnologyisinitsinfancy,butinthefuturecouldinprinciplebeappliedtonearlyanytask.Anothercrowningachievementofdeeplearningisitsextensiontothedomainofreinforcementlearning.Inthecontextofreinforcementlearning,anautonomousagentmustlearntoperformataskbytrialanderror,withoutanyguidancefromthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystembasedondeeplearningiscapableoflearningtoplayAtarivideogames,reachinghuman-levelperformanceonmanytasks(,).DeeplearninghasMnihetal.2015alsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics(,).Finnetal.2015Manyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearningisnowused'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 39}, page_content='bymanytoptechnologycompanies includingGoogle, Microsoft,Facebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftwareinfrastructure.SoftwarelibrariessuchasTheano(,;Bergstraetal.2010Bastienetal.etal.,),PyLearn2(2012Goodfellow,),Torch(,),2013cCollobertetal.2011bDistBelief(,),Caﬀe(,),MXNet(,),andDeanetal.2012Jia2013Chenetal.2015TensorFlow(,)haveallsupportedimportantresearchprojectsorAbadietal.2015commercialproducts.Deeplearninghasalsomadecontributionsbacktoothersciences.Modernconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing25'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 40}, page_content='CHAPTER1.INTRODUCTIONthatneuroscientistscanstudy(,).DeeplearningalsoprovidesusefulDiCarlo2013toolsforprocessingmassiveamountsofdataandmakingusefulpredictionsinscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteractinordertohelppharmaceuticalcompaniesdesignnewdrugs(,),Dahletal.2014tosearchforsubatomicparticles(,),andtoautomaticallyparseBaldietal.2014microscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-Barley2014etal.,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁcﬁeldsinthefuture.Insummary,deeplearningisanapproachtomachinelearningthathasdrawnheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasitdevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendousgrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-puters,largerdatasetsandtechniquestotraindeepernetworks.Theyearsaheadarefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherandbringittonewfrontiers.\\n26'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 41}, page_content='CHAPTER1.INTRODUCTION\\n1950198520002015205610−210−110010110210310410510610710810910101011Numberofneurons(logarithmicscale)\\n1234567891011121314151617181920\\nSpongeRoundwormLeechAntBeeFrogOctopusHumanIncreasingneuralnetworksizeovertime'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 41}, page_content='Figure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom().Wikipedia20151.Perceptron(,,)Rosenblatt195819622.Adaptivelinearelement(,)WidrowandHoﬀ19603.Neocognitron(Fukushima1980,)4.Earlyback-propagationnetwork(,)Rumelhartetal.1986b5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)6.Multilayerperceptronforspeechrecognition(,)Bengioetal.19917.Meanﬁeldsigmoidbeliefnetwork(,)Sauletal.19968.LeNet-5(,)LeCunetal.1998b9.Echostatenetwork(,)JaegerandHaas200410.Deepbeliefnetwork(,)Hintonetal.200611.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.200612.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)13.GPU-accelerateddeepbeliefnetwork(,)Rainaetal.200914.Unsupervisedconvolutionalnetwork(,)Jarrettetal.200915.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.201016.OMP-1network(,)CoatesandNg201117.Distributedautoencoder(,)Leetal.201218.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.201219.CO'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 41}, page_content='ettetal.200915.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.201016.OMP-1network(,)CoatesandNg201117.Distributedautoencoder(,)Leetal.201218.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.201219.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201320.GoogLeNet(,)Szegedyetal.2014a'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 41}, page_content='27'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 42}, page_content='CHAPTER1.INTRODUCTION\\n201020112012201320142015000.005.010.015.020.025.030.ILSVRC classiﬁcationerrorrateDecreasingerrorrateovertime\\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNetLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetitioneveryyear,andyieldedlowerandlowererrorrateseachtime. DatafromRussakovskyetal.etal.()and2014bHe().2015\\n28'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 43}, page_content='PartIAppliedMathandMachineLearningBasics\\n29'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 44}, page_content='Thispartofthebookintroducesthebasicmathematicalconceptsneededtounderstanddeeplearning.Webeginwithgeneralideasfromappliedmaththatallowustodeﬁnefunctionsofmanyvariables,ﬁndthehighestandlowestpointsonthesefunctionsandquantifydegreesofbelief.Next,wedescribethefundamentalgoalsofmachinelearning.Wedescribehowtoaccomplishthesegoalsbyspecifyingamodelthatrepresentscertainbeliefs,designingacostfunctionthatmeasureshowwellthosebeliefscorrespondwithrealityandusingatrainingalgorithmtominimizethatcostfunction.Thiselementaryframeworkisthebasisforabroadvarietyofmachinelearningalgorithms,includingapproachestomachinelearningthatarenotdeep. Inthesubsequentpartsofthebook,wedevelopdeeplearningalgorithmswithinthisframework.\\n30'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 45}, page_content='Chapter2LinearAlgebraLinearalgebraisabranchofmathematicsthatiswidelyusedthroughoutscienceandengineering.However,becauselinearalgebraisaformofcontinuousratherthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.Agoodunderstandingoflinearalgebraisessentialforunderstandingandworkingwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.Wethereforeprecedeourintroductiontodeeplearningwithafocusedpresentationofthekeylinearalgebraprerequisites.Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.Ifyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreferencesheettoreviewkeyformulas,werecommendTheMatrixCookbook(PetersenandPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapterwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualsoconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchasShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebratopicsthatarenotessentialforunderstandingdeeplearning.2.1S'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 45}, page_content='alsoconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchasShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebratopicsthatarenotessentialforunderstandingdeeplearning.2.1Scalars,Vectors,MatricesandTensorsThestudyoflinearalgebrainvolvesseveraltypesofmathematicalobjects:•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheotherobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.Whenweintroducethem,wespecifywhatkindofnumbertheyare.For31'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 46}, page_content='CHAPTER2.LINEARALGEBRAexample,wemightsay“Lets∈Rbetheslopeoftheline,”whiledeﬁningareal-valuedscalar,or“Letn∈Nbethenumberofunits,”whiledeﬁninganaturalnumberscalar.•Vectors:Avectorisanarrayofnumbers.Thenumbersarearrangedinorder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.Typicallywegivevectorslowercasenameswritteninboldtypeface,suchasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalictypeface,withasubscript.Theﬁrstelementofxisx1,thesecondelementisx2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredinthevector.IfeachelementisinR,andthevectorhasnelements,thenthevectorliesinthesetformedbytakingtheCartesianproductofRntimes,denotedasRn.Whenweneedtoexplicitlyidentifytheelementsofavector,wewritethemasacolumnenclosedinsquarebrackets:x=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1x2...xn\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fb.(2.1)Wecanthinkofvectorsasidentifyingpointsinspace,witheachelementgivingthecoordinatealongadiﬀerentaxis.Sometimesweneedtoindexasetofelementsofavector.Inthiscase,wedeﬁneasetcontainingtheindicesandwritethesetasasubscript.Forex'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 46}, page_content='asidentifyingpointsinspace,witheachelementgivingthecoordinatealongadiﬀerentaxis.Sometimesweneedtoindexasetofelementsofavector.Inthiscase,wedeﬁneasetcontainingtheindicesandwritethesetasasubscript.Forexample,toaccessx1,x3andx6,wedeﬁnethesetS={1,3,6}andwritexS.Weusethe−signtoindexthecomplementofaset.Forexamplex−1isthevectorcontainingallelementsofxexceptforx1,andx−Sisthevectorcontainingalloftheelementsofexceptforxx1,x3andx6.•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁedbytwoindicesinsteadofjustone.Weusuallygivematricesupper-casevariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhasaheightofmandawidthofn,thenwesaythatA∈Rmn×.Weusuallyidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,andtheindicesarelistedwithseparatingcommas.Forexample,A11,istheupperleftentryofAandAm,nisthebottomrightentry.Wecanidentifyallofthenumberswithverticalcoordinateibywritinga“”forthehorizontal:coordinate.Forexample,Ai,:denotesthehorizontalcrosssectionofAwithverticalcoordinatei.Thi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 46}, page_content='ofAandAm,nisthebottomrightentry.Wecanidentifyallofthenumberswithverticalcoordinateibywritinga“”forthehorizontal:coordinate.Forexample,Ai,:denotesthehorizontalcrosssectionofAwithverticalcoordinatei.Thisisknownasthei-throwofA.Likewise,A:,iis32'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 47}, page_content='CHAPTER2.LINEARALGEBRA'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 47}, page_content='A=\\uf8ee\\uf8f0A11,A12,A21,A22,A31,A32,\\uf8f9\\uf8fb⇒A\\ue021=\\ue025A11,A21,A31,A12,A22,A32,\\ue026Figure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthemaindiagonal.thei-thcolumnofA.Whenweneedtoexplicitlyidentifytheelementsofamatrix,wewritethemasanarrayenclosedinsquarebrackets:\\ue014A11,A12,A21,A22,\\ue015.(2.2)Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjustasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdonotconvertanythingtolowercase.Forexample,f(A)i,jgiveselement(i,j)ofthematrixcomputedbyapplyingthefunctionto.fA•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwithavariablenumberofaxesisknownasaWedenoteatensornamed“A”tensor.withthistypeface:A.WeidentifytheelementofAatcoordinates(i,j,k)bywritingAi,j,k.Oneimportantoperationonmatricesisthetranspose.Thetransposeofamatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemaindiagonal,runningdownandtotheright,startingfromitsupperleftcorner.SeeFig.foragraphicaldepictionofthis'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 47}, page_content='icesisthetranspose.Thetransposeofamatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemaindiagonal,runningdownandtotheright,startingfromitsupperleftcorner.SeeFig.foragraphicaldepictionofthisoperation.Wedenotethetransposeofa2.1matrixasAA\\ue03e,anditisdeﬁnedsuchthat(A\\ue03e)i,j='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 47}, page_content='Aj,i.(2.3)Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.Thetransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe33'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 48}, page_content='CHAPTER2.LINEARALGEBRAdeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,thenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,x= [x1,x2,x3]\\ue03e.Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,wecanseethatascalarisitsowntranspose:aa= \\ue03e.Wecanaddmatricestoeachother,aslongastheyhavethesameshape,justbyaddingtheircorrespondingelements:whereCAB= +Ci,j= Ai,j+Bi,j.Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,justbyperformingthatoperationoneachelementofamatrix:D=a·B+cwhereDi,j='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 48}, page_content='+Ci,j= Ai,j+Bi,j.Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,justbyperformingthatoperationoneachelementofamatrix:D=a·B+cwhereDi,j= aB·i,j+c.Inthecontextofdeeplearning,wealsousesomelessconventionalnotation.Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,whereCi,j=Ai,j+bj.Inotherwords,thevectorbisaddedtoeachrowofthematrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedintoeachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocationsiscalledbroadcasting.2.2MultiplyingMatricesandVectorsOneofthemostimportantoperationsinvolvingmatricesismultiplicationoftwomatrices.ThematrixproductofmatricesAandBisathirdmatrixC.Inorderforthisproducttobedeﬁned,AmusthavethesamenumberofcolumnsasBhasrows.IfAisofshapemn×andBisofshapenp×,thenCisofshapemp×.Wecanwritethematrixproductjustbyplacingtwoormorematricestogether,e.g.CAB='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 48}, page_content='.(2.4)TheproductoperationisdeﬁnedbyCi,j=\\ue058kAi,kBk,j.(2.5)Notethatthestandardproductoftwomatricesisjustamatrixcontainingnottheproductoftheindividualelements.Suchanoperationexistsandiscalledtheelement-wiseproductHadamardproductor,andisdenotedas.AB\\ue00cThedotproductbetweentwovectorsxandyofthesamedimensionalityisthematrixproductx\\ue03ey.WecanthinkofthematrixproductC=ABascomputingCi,jasthedotproductbetweenrowofandcolumnof.iAjB34'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 49}, page_content='CHAPTER2.LINEARALGEBRAMatrixproductoperationshavemanyusefulpropertiesthatmakemathematicalanalysis ofmatrices moreconvenient.For example, matrix multiplication isdistributive:ABCABAC(+) = +.(2.6)Itisalsoassociative:ABCABC() = ().(2.7)Matrixmultiplicationiscommutative(theconditionnotAB=BAdoesnotalwayshold),unlikescalarmultiplication.However,thedotproductbetweentwovectorsiscommutative:x\\ue03eyy= \\ue03ex.(2.8)Thetransposeofamatrixproducthasasimpleform:()AB\\ue03e= B\\ue03eA\\ue03e.(2.9)ThisallowsustodemonstrateEq.,byexploitingthefactthatthevalueof2.8suchaproductisascalarandthereforeequaltoitsowntranspose:x\\ue03ey=\\ue010x\\ue03ey\\ue011\\ue03e= y\\ue03ex.(2.10)Sincethefocusofthistextbookisnotlinearalgebra,wedonotattempttodevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,butthereadershouldbeawarethatmanymoreexist.Wenowknowenoughlinearalgebranotationtowritedownasystemoflinearequations:Axb='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 49}, page_content='(2.11)whereA∈Rmn×isaknownmatrix,b∈Rmisaknownvector,andx∈Rnisavectorofunknownvariableswewouldliketosolvefor.Eachelementxiofxisoneoftheseunknownvariables.EachrowofAandeachelementofbprovideanotherconstraint.WecanrewriteEq.as:2.11A1:,x= b1(2.12)A2:,x= b2(2.13)...(2.14)Am,:x= bm(2.15)or,evenmoreexplicitly,as:A11,x1+A12,x2++···A1,nxn= b1(2.16)35'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 50}, page_content='CHAPTER2.LINEARALGEBRA\\uf8ee\\uf8f0100010001\\uf8f9\\uf8fbFigure2.2::ThisisExampleidentitymatrixI3.A21,x1+A22,x2++···A2,nxn= b2(2.17)...(2.18)Am,1x1+Am,2x2++···Am,nxn= bm.(2.19)Matrix-vectorproductnotationprovidesamorecompactrepresentationforequationsofthisform.2.3IdentityandInverseMatricesLinearalgebraoﬀersapowerfultoolcalledthatallowsustomatrixinversionanalyticallysolveEq.formanyvaluesof.2.11ATodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentitymatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwemultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreservesn-dimensionalvectorsasIn.Formally,In∈Rnn×,and∀∈xRn,Inxx= .(2.20)Thestructureoftheidentitymatrixissimple:alloftheentriesalongthemaindiagonalare1,whilealloftheotherentriesarezero.SeeFig.foranexample.2.2TheofmatrixinverseAisdenotedasA−1,anditisdeﬁnedasthematrixsuchthatA−1AI= n.(2.21)WecannowsolveEq.bythefollowingsteps:2.11Axb= (2.22)A−1AxA= −1b(2.23)InxA= −1b(2.24)36'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 51}, page_content='CHAPTER2.LINEARALGEBRAxA= −1b.(2.25)Ofcourse,thisdependsonitbeingpossibletoﬁndA−1.WediscusstheconditionsfortheexistenceofA−1inthefollowingsection.WhenA−1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmanytimesfordiﬀerentvaluesofb.However,A−1isprimarilyusefulasatheoreticaltool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.BecauseA−1canberepresentedwithonlylimitedprecisiononadigitalcomputer,algorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurateestimatesof.x2.4LinearDependenceandSpanInorderforA−1toexist,Eq.musthaveexactlyonesolutionforeveryvalue2.11ofb.However,itisalsopossibleforthesystemofequationstohavenosolutionsorinﬁnitelymanysolutionsforsomevaluesofb.Itisnotpossibletohavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;ifbothxandyaresolutionsthenzxy='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 51}, page_content='α+(1)−α(2.26)isalsoasolutionforanyreal.αToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumnsofAasspecifyingdiﬀerentdirectionswecantravelfromthe(thepointoriginspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareofreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelineachofthesedirections,withxispecifyinghowfartomoveinthedirectionofcolumn:iAx=\\ue058ixiA:,i.(2.27)Ingeneral,thiskindofoperationiscalledalinearcombination.Formally,alinearcombinationofsomesetofvectors{v(1),...,v()n}isgivenbymultiplyingeachvectorv()ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\\ue058iciv()i.(2.28)Thespanofasetofvectorsisthesetofallpointsobtainablebylinearcombinationoftheoriginalvectors.37'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 52}, page_content='CHAPTER2.LINEARALGEBRADeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherbisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumnspacerangeortheof.AInorderforthesystemAx=btohaveasolutionforallvaluesofb∈Rm,wethereforerequirethatthecolumnspaceofAbeallofRm.IfanypointinRmisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathasnosolution.TherequirementthatthecolumnspaceofAbeallofRmimpliesimmediatelythatAmusthaveatleastmcolumns,i.e.,nm≥.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 52}, page_content='Otherwise,thedimensionalityofthecolumnspacewouldbelessthanm.Forexample,considera3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofxatbestallowsustotraceouta2-DplanewithinR3.Theequationhasasolutionifandonlyifliesonthatplane.bHavingnm≥isonlyanecessaryconditionforeverypointtohaveasolution.Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnstoberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.Thishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthereplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailstoencompassallofR2,eventhoughtherearetwocolumns.Formally,thiskindofredundancyisknownaslineardependence.Asetofvectorsislinearlyindependentifnovectorinthesetisalinearcombinationoftheothervectors.Ifweaddavectortoasetthatisalinearcombinationoftheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’sspan.ThismeansthatforthecolumnspaceofthematrixtoencompassallofRm,thematrixmustcontainatleastonesetofmlinearlyindependentcolumns.Thiscon'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 52}, page_content='theothervectorsintheset,thenewvectordoesnotaddanypointstotheset’sspan.ThismeansthatforthecolumnspaceofthematrixtoencompassallofRm,thematrixmustcontainatleastonesetofmlinearlyindependentcolumns.ThisconditionisbothnecessaryandsuﬃcientforEq.tohaveasolutionforeveryvalueof2.11b.Notethattherequirementisforasettohaveexactlymlinearindependentcolumns,notatleastm.Nosetofm-dimensionalvectorscanhavemorethanmmutuallylinearlyindependentcolumns,butamatrixwithmorethancolumnsmmayhavemorethanonesuchset.Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethatEq.hasonesolutionforeachvalueof2.11atmostb.Todoso,weneedtoensurethatthematrixhasatmostmcolumns.Otherwisethereismorethanonewayofparametrizingeachsolution.Together,thismeansthatthematrixmustbesquare,thatis,werequirethatm=nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrixwithlinearlydependentcolumnsisknownas.singularIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe38'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 53}, page_content='CHAPTER2.LINEARALGEBRAequation.However,wecannotusethemethodofmatrixinversiontoﬁndthesolution.Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itisalsopossibletodeﬁneaninversethatismultipliedontheright:AA−1= I.(2.29)Forsquarematrices,theleftinverseandrightinverseareequal.2.5NormsSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusuallymeasurethesizeofvectorsusingafunctioncalleda.Formally,thenormLpnormisgivenby||||xp=\\ue020\\ue058i|xi|p\\ue0211p(2.30)forp,p.∈R≥1Norms,includingtheLpnorm,arefunctionsmappingvectorstonon-negativevalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefromtheorigintothepointx.Morerigorously,anormisanyfunctionfthatsatisﬁesthefollowingproperties:•⇒f() = 0 xx= 0•≤f(+) xyff()+x()y(thetriangleinequality)•∀∈||αR,fα(x) = αf()xTheL2norm,withp= 2,isknownastheEuclideannorm.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 53}, page_content='= 0 xx= 0•≤f(+) xyff()+x()y(thetriangleinequality)•∀∈||αR,fα(x) = αf()xTheL2norm,withp= 2,isknownastheEuclideannorm. ItissimplytheEuclideandistancefromtheorigintothepointidentiﬁedbyx.TheL2normisusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,withthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing2thesquaredL2norm,whichcanbecalculatedsimplyasx\\ue03ex.ThesquaredL2normismoreconvenienttoworkwithmathematicallyandcomputationallythantheL2normitself.Forexample,thederivativesofthesquaredL2normwithrespecttoeachelementofxeachdependonlyonthecorrespondingelementofx,whileallofthederivativesoftheL2normdependontheentirevector.Inmanycontexts,thesquaredL2normmaybeundesirable39'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 54}, page_content='CHAPTER2.LINEARALGEBRAbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearningapplications,itisimportanttodiscriminatebetweenelementsthatareexactlyzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunctionthatgrowsatthesamerateinalllocations,butretainsmathematicalsimplicity:theL1norm.TheL1normmaybesimpliﬁedto||||x1=\\ue058i|xi|.(2.31)TheL1normiscommonlyusedinmachinelearningwhenthediﬀerencebetweenzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmovesawayfrom0by,the\\ue00fL1normincreasesby.\\ue00fWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzeroelements.Someauthorsrefertothisfunctionasthe“L0norm,”butthisisincorrectterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,becausescalingthevectorbyαdoesnotchangethenumberofnonzeroentries. TheL1normisoftenusedasasubstituteforthenumberofnonzeroentries.OneothernormthatcommonlyarisesinmachinelearningistheL∞norm,alsoknownasthe.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 54}, page_content='TheL1normisoftenusedasasubstituteforthenumberofnonzeroentries.OneothernormthatcommonlyarisesinmachinelearningistheL∞norm,alsoknownasthe. Thisnormsimpliﬁestotheabsolutevalueofthemaxnormelementwiththelargestmagnitudeinthevector,||||x∞= maxi|xi|.(2.32)Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontextofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscureFrobeniusnorm||||AF=\\ue073\\ue058i,jA2i,j,(2.33)whichisanalogoustotheL2normofavector.Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,x\\ue03eyx= ||||2||||y2cosθ(2.34)whereistheanglebetweenand.θxy2.6SpecialKindsofMatricesandVectorsSomespecialkindsofmatricesandvectorsareparticularlyuseful.40'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 55}, page_content='CHAPTER2.LINEARALGEBRADiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalongthemaindiagonal. Formally,amatrixDisdiagonalifandonlyifDi,j=0foralli\\ue036=j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentitymatrix,whereallofthediagonalentriesare1.Wewritediag(v)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 55}, page_content='todenoteasquarediagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrixisverycomputationallyeﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeachelementxibyvi.Inotherwords,diag(v)x=vx\\ue00c.Invertingasquarediagonalmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,andinthatcase,diag(v)−1=diag([1/v1,...,1/vn]\\ue03e).Inmanycases,wemayderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsomematricestobediagonal.Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangulardiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstillpossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,theproductDxwillinvolvescalingeachelementofx,andeitherconcatenatingsomezerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelastelementsofthevectorifiswiderthanitistall.DAmatrixisanymatrixthatisequaltoitsowntr'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 55}, page_content='nvolvescalingeachelementofx,andeitherconcatenatingsomezerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelastelementsofthevectorifiswiderthanitistall.DAmatrixisanymatrixthatisequaltoitsowntranspose:symmetricAA='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 55}, page_content='\\ue03e.(2.35)Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionoftwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,ifAisamatrixofdistancemeasurements,withAi,jgivingthedistancefrompointitopoint,thenjAi,j= Aj,ibecausedistancefunctionsaresymmetric.Aunitvectorunitnormisavectorwith:||||x2= 1.(2.36)Avectorxandavectoryareorthogonaltoeachotherifx\\ue03ey=0. Ifbothvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeachother.InRn,atmostnvectorsmaybemutuallyorthogonalwithnonzeronorm.Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthemorthonormal.Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonormalandwhosecolumnsaremutuallyorthonormal:A\\ue03eAAA= \\ue03e= I.(2.37)41'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 56}, page_content='CHAPTER2.LINEARALGEBRAThisimpliesthatA−1= A\\ue03e,(2.38)soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,theirrowsarenotmerelyorthogonalbutfullyorthonormal.Thereisnospecialtermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.2.7EigendecompositionManymathematicalobjectscanbeunderstoodbetterbybreakingthemintoconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcausedbythewaywechoosetorepresentthem.Forexample,integerscanbedecomposedintoprimefactors.Thewaywerepresentthenumberwillchangedependingonwhetherwewriteitinbaseten12orinbinary,butitwillalwaysbetruethat12 ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 56}, page_content='= 2×2×3.Fromthisrepresentationwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany125integermultipleofwillbedivisibleby.123Muchaswecandiscoversomethingaboutthetruenatureofanintegerbydecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthatshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromtherepresentationofthematrixasanarrayofelements.Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-decomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsandeigenvalues.AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmultipli-cationbyaltersonlythescaleof:AvAvv= λ.(2.39)Thescalarλisknownasthecorrespondingtothiseigenvector.(Oneeigenvaluecanalsoﬁndalefteigenvectorsuchthatv\\ue03eA=λv\\ue03e,butweareusuallyconcernedwithrighteigenvectors).IfvisaneigenvectorofA,thensoisanyrescaledvectorsvfors,s∈R\\ue036='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 56}, page_content='0.Moreover,svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylookforuniteigenvectors.SupposethatamatrixAhasnlinearlyindependenteigenvectors,{v(1),...,v()n},withcorrespondingeigenvalues{λ1,...,λn}.Wemayconcatenateallofthe42'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 57}, page_content='CHAPTER2.LINEARALGEBRA\\n\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue031\\ue076\\ue028\\ue031\\ue029\\ue076\\ue028\\ue032\\ue029\\ue042\\ue065\\ue066\\ue06f\\ue072\\ue065\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\n\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\ue030\\U000f0913\\ue033\\U000f0913\\ue032\\U000f0913\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\ue031\\ue076\\ue028\\ue031\\ue029\\ue0b8\\ue031\\ue076\\ue028\\ue031\\ue029\\ue076\\ue028\\ue032\\ue029\\ue0b8\\ue032\\ue076\\ue028\\ue032\\ue029\\ue041\\ue066\\ue074\\ue065\\ue072\\ue020\\ue06d\\ue075\\ue06c\\ue074\\ue069\\ue070\\ue06c\\ue069\\ue063\\ue061\\ue074\\ue069\\ue06f\\ue06e\\ue045\\ue066\\ue066\\ue065\\ue063\\ue074\\ue020\\ue06f\\ue066\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue065\\ue063\\ue074\\ue06f\\ue072\\ue073\\ue020\\ue061\\ue06e\\ue064\\ue020\\ue065\\ue069\\ue067\\ue065\\ue06e\\ue076\\ue061\\ue06c\\ue075\\ue065\\ue073'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 57}, page_content='Figure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehaveamatrixAwithtwoorthonormaleigenvectors,v(1)witheigenvalueλ1andv(2)witheigenvalueλ2.(Left)Weplotthesetofallunitvectorsu∈R2asaunitcircle.(Right)WeplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,wecanseethatitscalesspaceindirectionv()ibyλi.eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v(1),...,v()n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [λ1,...,λn]\\ue03e.TheeigendecompositionofisthengivenbyAAVλV= diag()−1.(2.40)Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-torsallowsustostretchspaceindesireddirections. However,weoftenwanttodecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelpustoanalyzecertainpropertiesofthematrix,muchasdecomposinganintegerintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome43'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 58}, page_content='CHAPTER2.LINEARALGEBRAcases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassofmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetricmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectorsandeigenvalues:AQQ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 58}, page_content='Λ\\ue03e,(2.41)whereQisanorthogonalmatrixcomposedofeigenvectorsofA,andΛisadiagonalmatrix.TheeigenvalueΛi,iisassociatedwiththeeigenvectorincolumniofQ,denotedasQ:,i.BecauseQisanorthogonalmatrix,wecanthinkofAasscalingspacebyλiindirectionv()i.SeeFig.foranexample.2.3WhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectorssharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspanarealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesofΛindescendingorder.Underthisconvention,theeigendecompositionisuniqueonlyifalloftheeigenvaluesareunique.Theeigendecompositionof amatrix tellsus many usefulfactsabout thematrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.Theeigendecompositionofarealsymmetricmatrixcanalsobeusedtooptimizequadraticexpressionsoftheformf(x) =x\\ue03eAxsubjectto||||x2='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 58}, page_content='thematrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.Theeigendecompositionofarealsymmetricmatrixcanalsobeusedtooptimizequadraticexpressionsoftheformf(x) =x\\ue03eAxsubjectto||||x2= 1.WheneverxisequaltoaneigenvectorofA,ftakesonthevalueofthecorrespondingeigenvalue.Themaximumvalueoffwithintheconstraintregionisthemaximumeigenvalueanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.Amatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.Amatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,andifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positivesemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx,\\ue03eAx≥0.Positivedeﬁnitematricesadditionallyguaranteethatx\\ue03eAxx= 0 ⇒='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 58}, page_content='0 ⇒= 0.2.8SingularValueDecompositionInSec.,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues.2.7Thesingularvaluedecomposition(SVD)providesanotherwaytofactorizeamatrix,intosingularvectorssingularvaluesand.TheSVDallowsustodiscoversomeofthesamekindofinformationastheeigendecomposition.However,theSVDis44'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 59}, page_content='CHAPTER2.LINEARALGEBRAmoregenerallyapplicable.Everyrealmatrixhasasingularvaluedecomposition,butthesameisnottrueoftheeigenvaluedecomposition.Forexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwemustuseasingularvaluedecompositioninstead.RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscoveramatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewriteAasAVλV= diag()−1.(2.42)Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteAasaproductofthreematrices:AUDV='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 59}, page_content='\\ue03e.(2.43)SupposethatAisanmn×matrix.ThenUisdeﬁnedtobeanmm×matrix,DVtobeanmatrix,andmn×tobeanmatrix.nn×Eachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesUandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobeadiagonalmatrix.Notethatisnotnecessarilysquare.DTheelementsalongthediagonalofDareknownastheofthesingularvaluesmatrixA.ThecolumnsofUareknownastheleft-singularvectors.ThecolumnsofareknownasastheVright-singularvectors.WecanactuallyinterpretthesingularvaluedecompositionofAintermsoftheeigendecompositionoffunctionsofA.Theleft-singularvectorsofAaretheeigenvectorsofAA\\ue03e.Theright-singularvectorsofAaretheeigenvectorsofA\\ue03eA.Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofA\\ue03eA.ThesameistrueforAA\\ue03e.PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartiallygeneralizematrixinversiontonon-squarematrices,aswewillseeinthenextsection.2.9TheMoore-PenrosePseudoinverseMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewanttomakealeft-inverseofamatrix,sothatwe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 59}, page_content='matrixinversiontonon-squarematrices,aswewillseeinthenextsection.2.9TheMoore-PenrosePseudoinverseMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewanttomakealeft-inverseofamatrix,sothatwecansolvealinearequationBAAxy='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 59}, page_content='(2.44)45'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 60}, page_content='CHAPTER2.LINEARALGEBRAbyleft-multiplyingeachsidetoobtainxBy= .(2.45)Dependingonthestructureoftheproblem,itmaynotbepossibletodesignauniquemappingfromto.ABIfAistallerthanitiswide, thenitispossibleforthisequationtohavenosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossiblesolutions.TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayinthesecases.ThepseudoinverseofisdeﬁnedasamatrixAA+=limα\\ue0260(A\\ue03eAI+α)−1A\\ue03e.(2.46)Practicalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-tion,butrathertheformulaA+='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 60}, page_content='VD+U\\ue03e,(2.47)whereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverseD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zeroelementsthentakingthetransposeoftheresultingmatrix.WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthepseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovidesthesolutionx=A+ywithminimalEuclideannorm||||x2amongallpossiblesolutions.WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseaspossibletointermsofEuclideannormy||−||Axy2.2.10TheTraceOperatorThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:Tr() =A\\ue058iAi,i.(2.48)Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatarediﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing46'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 61}, page_content='CHAPTER2.LINEARALGEBRAmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovidesanalternativewayofwritingtheFrobeniusnormofamatrix:||||AF=\\ue071Tr(AA\\ue03e).(2.49)Writinganexpressionintermsofthetraceoperatoropensupopportunitiestomanipulatetheexpressionusingmanyusefulidentities. Forexample,thetraceoperatorisinvarianttothetransposeoperator:Tr() = Tr(AA\\ue03e).(2.50)Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvarianttomovingthelastfactorintotheﬁrstposition,iftheshapesofthecorrespondingmatricesallowtheresultingproducttobedeﬁned:Tr() = Tr() = Tr()ABCCABBCA(2.51)ormoregenerally,Tr(n\\ue059i=1F()i) = Tr(F()nn−1\\ue059i=1F()i).(2.52)Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasadiﬀerentshape.Forexample,forA∈Rmn×andB∈Rnm×,wehaveTr() = Tr()ABBA(2.53)eventhoughAB∈Rmm×andBA∈Rnn×.Anotherusefulfacttokeepinmindisthatascalarisitsowntrace:a=Tr(a).2.11TheDeterminantThedeterminantofa squarematrix, denoteddet(A), isa functionmappingmatricesto realscalars.Thedeterminantisequal totheproductof'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 61}, page_content='squarematrix, denoteddet(A), isa functionmappingmatricesto realscalars.Thedeterminantisequal totheproductof alltheeigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethoughtofasameasureofhowmuchmultiplicationbythematrixexpandsorcontractsspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleastonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,thenthetransformationisvolume-preserving.47'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 62}, page_content='CHAPTER2.LINEARALGEBRA2.12Example:PrincipalComponentsAnalysisOnesimplemachinelearningalgorithm,principalcomponentsanalysisPCAorcanbederivedusingonlyknowledgeofbasiclinearalgebra.Supposewehaveacollectionofmpoints{x(1),...,x()m}inRn.Supposewewouldliketoapplylossycompressiontothesepoints.Lossycompressionmeansstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.Wewouldliketoloseaslittleprecisionaspossible.Onewaywecanencodethesepointsistorepresentalower-dimensionalversionofthem.Foreachpointx()i∈Rnwewillﬁndacorrespondingcodevectorc()i∈Rl.Iflissmallerthann,itwilltakelessmemorytostorethecodepointsthantheoriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecodeforaninput,f(x) =c,andadecodingfunctionthatproducesthereconstructedinputgivenitscode,.xx≈gf(())PCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethedecoderverysimple,wechoosetousematrixmultiplicationtomapthecodebackintoRn.Let,whereg() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 62}, page_content='= cDcD∈Rnl×isthematrixdeﬁningthedecoding.Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.Tokeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonaltoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unlessln= )Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewecanincreasethescaleofD:,iifwedecreaseciproportionallyforallpoints.Togivetheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitDnorm.Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrstthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗foreachinputpointx.Onewaytodothisistominimizethedistancebetweentheinputpointxanditsreconstruction,g(c∗).Wecanmeasurethisdistanceusinganorm.Intheprincipalcomponentsalgorithm,weusetheL2norm:c∗= argminc||−||xg()c2.(2.54)WecanswitchtothesquaredL2norminsteadoftheL2normitself,becausebothareminimizedbythesamevalueofc.ThisisbecausetheL2normisnon-negativeandthesquaringoperationismonotonicallyincreasingfornon-negative48'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 63}, page_content='CHAPTER2.LINEARALGEBRAarguments.c∗= argminc||−||xg()c22.(2.55)Thefunctionbeingminimizedsimpliﬁesto(())x−gc\\ue03e(())x−gc(2.56)(bythedeﬁnitionoftheL2norm,Eq.)2.30= x\\ue03exx−\\ue03egg()c−()c\\ue03exc+(g)\\ue03eg()c(2.57)(bythedistributiveproperty)= x\\ue03exx−2\\ue03egg()+c()c\\ue03eg()c(2.58)(becausethescalarg()x\\ue03exisequaltothetransposeofitself).Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,sincethistermdoesnotdependon:cc∗= argminc−2x\\ue03egg()+c()c\\ue03eg.()c(2.59)Tomakefurtherprogress,wemustsubstituteinthedeﬁnitionof:g()cc∗= argminc−2x\\ue03eDcc+\\ue03eD\\ue03eDc(2.60)= argminc−2x\\ue03eDcc+\\ue03eIlc(2.61)(bytheorthogonalityandunitnormconstraintson)D= argminc−2x\\ue03eDcc+\\ue03ec(2.62)Wecansolvethisoptimizationproblemusingvectorcalculus(seeSec.if4.3youdonotknowhowtodothis):∇c(2−x\\ue03eDcc+\\ue03ec) = 0(2.63)−2D\\ue03exc+2= 0(2.64)cD= \\ue03ex.(2.65)Thismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusingamatrix-vectoroperation.Toencodeavector,weapplytheencoderfunctionf() = xD\\ue03ex.(2.66)49'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 64}, page_content='CHAPTER2.LINEARALGEBRAUsingafurthermatrixmultiplication,wecanalsodeﬁnethePCAreconstructionoperation:rgf() = x(()) = xDD\\ue03ex.(2.67)Next,weneedtochoosetheencodingmatrixD.Todoso,werevisittheideaofminimizingtheL2distancebetweeninputsandreconstructions.However,sincewewillusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthepointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrixoferrorscomputedoveralldimensionsandallpoints:D∗= argminD\\ue073\\ue058i,j\\ue010x()ij−r(x()i)j\\ue0112subjecttoD\\ue03eDI= l(2.68)ToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecasewherel= 1.Inthiscase,Disjustasinglevector,d.SubstitutingEq.into2.67Eq.andsimplifyinginto,theproblemreducesto2.68Ddd∗= argmind\\ue058i||x()i−dd\\ue03ex()i||22subjectto||||d2='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 64}, page_content='1.Inthiscase,Disjustasinglevector,d.SubstitutingEq.into2.67Eq.andsimplifyinginto,theproblemreducesto2.68Ddd∗= argmind\\ue058i||x()i−dd\\ue03ex()i||22subjectto||||d2= 1.(2.69)Theaboveformulationisthemostdirectwayofperformingthesubstitution,butisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthescalarvalued\\ue03ex()iontherightofthevectord.Itismoreconventionaltowritescalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywritesuchaformulaasd∗= argmind\\ue058i||x()i−d\\ue03ex()id||22subjectto||||d2= 1,(2.70)or,exploitingthefactthatascalarisitsowntranspose,asd∗= argmind\\ue058i||x()i−x()i\\ue03edd||22subjectto||||d2= 1.(2.71)Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements.Atthispoint,itcanbehelpfultorewritetheproblemintermsofasingledesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.Thiswillallowustousemorecompactnotation.LetX∈Rmn×bethematrixdeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatXi,:=x()i\\ue03e.Wecannowrewritetheproblemasd∗= argmind||−XXdd\\ue03e||2Fsubjecttod\\ue03ed='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 64}, page_content='argmind||−XXdd\\ue03e||2Fsubjecttod\\ue03ed= 1.(2.72)50'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 65}, page_content='CHAPTER2.LINEARALGEBRADisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnormportionasfollows:argmind||−XXdd\\ue03e||2F(2.73)= argmindTr\\ue012\\ue010XXdd−\\ue03e\\ue011\\ue03e\\ue010XXdd−\\ue03e\\ue011\\ue013(2.74)(byEq.)2.49= argmindTr(X\\ue03eXX−\\ue03eXdd\\ue03e−dd\\ue03eX\\ue03eXdd+\\ue03eX\\ue03eXdd\\ue03e)(2.75)= argmindTr(X\\ue03eX)Tr(−X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.76)= argmind−Tr(X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.77)(becausetermsnotinvolvingdonotaﬀectthe)dargmin= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.78)(becausewecancycletheorderofthematricesinsideatrace,Eq.)2.52= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)(2.79)(usingthesamepropertyagain)Atthispoint,were-introducetheconstraint:argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)subjecttod\\ue03ed= 1(2.80)= argmind−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.81)(duetotheconstraint)= argmind−Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.82)= argmaxdTr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.83)= argmaxdTr(d\\ue03eX\\ue03eXdd)subjectto\\ue03ed= 1(2.84)51'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 66}, page_content='CHAPTER2.LINEARALGEBRAThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,theoptimaldisgivenbytheeigenvectorofX\\ue03eXcorrespondingtothelargesteigenvalue.Inthegeneralcase,wherel>1,thematrixDisgivenbytheleigenvectorscorrespondingtothelargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommendwritingthisproofasanexercise.Linearalgebraisoneofthefundamentalmathematicaldisciplinesthatisnecessarytounderstanddeeplearning.Anotherkeyareaofmathematicsthatisubiquitousinmachinelearningisprobabilitytheory,presentednext.\\n52'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 67}, page_content='Chapter3ProbabilityandInformationTheoryInthischapter,wedescribeprobabilitytheoryandinformationtheory.Probabilitytheoryisamathematicalframeworkforrepresentinguncertainstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderivingnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobabilitytheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystemsshouldreason,sowedesignouralgorithmstocomputeorapproximatevariousexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityandstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceandengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundisprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycanunderstandthematerialinthisbook.Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasoninthepresenceofuncertainty,informationallowsustoquantifytheamountofuncertaintyinaprobabilitydistribution.Ifyouarealreadyfamili'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 67}, page_content='nthisbook.Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasoninthepresenceofuncertainty,informationallowsustoquantifytheamountofuncertaintyinaprobabilitydistribution.Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,youmaywishtoskipallofthischapterexceptforSec.,whichdescribesthe3.14graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.Ifyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershouldbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedosuggestthatyouconsultanadditionalresource,suchasJaynes2003().53'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 68}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.1WhyProbability?Manybranchesofcomputersciencedealmostlywithentitiesthatareentirelydeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwillexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butarerareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccountforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkinarelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearningmakesheavyuseofprobabilitytheory.Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities,andsometimesmayalsoneedtodealwithstochastic(non-deterministic)quantities.Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemadecompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleastthe1980s.ManyoftheargumentspresentedherearesummarizedfromorinspiredbyPearl1988().Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.Infact,beyondmathematicalstatementsthataretruebydeﬁnition,itisdi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 68}, page_content='eargumentspresentedherearesummarizedfromorinspiredbyPearl1988().Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.Infact,beyondmathematicalstatementsthataretruebydeﬁnition,itisdiﬃculttothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutelyguaranteedtooccur.Therearethreepossiblesourcesofuncertainty:1.Inherentstochasticityinthesystembeingmodeled.Forexample,mostinterpretationsofquantummechanicsdescribethedynamicsofsubatomicparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthatwepostulatetohaverandomdynamics,suchasahypotheticalcardgamewhereweassumethatthecardsaretrulyshuﬄedintoarandomorder.2.Incompleteobservability.Evendeterministicsystemscanappearstochasticwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthesystem.Forexample,intheMontyHallproblem,agameshowcontestantisaskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosendoor.Twodoorsleadtoagoatwhileathirdleadstoacar.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 68}, page_content='Theoutcomegiventhecontestant’schoiceisdeterministic,butfromthecontestant’spointofview,theoutcomeisuncertain.3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeofthe informationwehave observed, the discarded informationresults inuncertaintyinthemodel’spredictions.Forexample,supposewebuildarobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe54'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 69}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,thenthediscretizationmakestherobotimmediatelybecomeuncertainabouttheprecisepositionofobjects: eachobjectcouldbeanywherewithinthediscretecellthatitwasobservedtooccupy.Inmanycases,itismorepracticaltouseasimplebutuncertainruleratherthanacomplexbutcertainone,evenifthetrueruleisdeterministicandourmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,thesimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearuleoftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedtoﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirdsincludingthecassowary,ostrichandkiwi...”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 69}, page_content='isexpensivetodevelop,maintainandcommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.Giventhatweneedameansofrepresentingandreasoningaboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovideallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheorywasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytoseehowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandofcardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.Whenwesaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeatedtheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportionpoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnotseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctoranalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasofthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatientwouldpresentwiththesamesymptomsye'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 69}, page_content='enthasa40%chanceofhavingtheﬂu,thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasofthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatientwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.Inthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresentadegreeofbelief,with1indicatingabsolutecertaintythatthepatienthastheﬂuand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 69}, page_content='Theformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,isknownasfrequentistprobability,whilethelatter,relatedtoqualitativelevelsofcertainty,isknownasBayesianprobability.Ifwelistseveralpropertiesthatweexpectcommonsensereasoningaboutuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreatBayesianprobabilitiesasbehavingexactlythesameasfrequentistprobabilities.Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapokergamegiventhatshehasacertainsetofcards,weuseexactlythesameformulasaswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe55'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 70}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsenseassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,see().Ramsey1926Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logicprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedtobetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrueorfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthelikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.3.2RandomVariablesArandomvariableisavariablethatcantakeondiﬀerentvaluesrandomly.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 70}, page_content='Wetypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,andthevaluesitcantakeonwithlowercasescriptletters.Forexample,x1andx2arebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valuedvariables,wewouldwritetherandomvariableasxandoneofitsvaluesasx.Onitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;itmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachofthesestatesare.Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariableisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthesestatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthatarenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableisassociatedwitharealvalue.3.3ProbabilityDistributionsAprobabilitydistributionis adescription ofhowlikelyarandomvariable'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 70}, page_content='adescription ofhowlikelyarandomvariable orsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywedescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteorcontinuous.3.3.1DiscreteVariablesandProbabilityMassFunctionsAprobabilitydistributionoverdiscretevariablesmaybedescribedusingaproba-bilitymassfunction(PMF).WetypicallydenoteprobabilitymassfunctionswithacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability56'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 71}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmassfunctionandthereadermustinferwhichprobabilitymassfunctiontousebasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;PP()xisusuallynotthesameas()y.Theprobabilitymassfunctionmapsfromastateofarandomvariabletotheprobabilityofthatrandomvariabletakingonthatstate.Theprobabilitythatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xiscertainandaprobabilityof0indicatingthatx=xisimpossible.SometimestodisambiguatewhichPMFtouse,wewritethenameoftherandomvariableexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationtospecifywhichdistributionitfollowslater:xx.∼P()Probabilitymassfunctionscanactonmanyvariablesatthesametime.Suchaprobabilitydistributionovermanyvariablesisknownasajointprobabilitydistribution.P(x=x,y=y)denotestheprobabilitythatx=xandy=ysimultaneously.Wemayalsowriteforbrevity.Px,y()Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.P'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 71}, page_content='tythatx=xandy=ysimultaneously.Wemayalsowriteforbrevity.Px,y()Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.P•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan0'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 71}, page_content='belessprobablethanthat.Likewise,aneventthatisguaranteedtohappenhasprobability,andnostatecanhaveagreaterchanceofoccurring.1•\\ue050x∈xP(x) = 1.Werefertothispropertyasbeingnormalized.Withoutthisproperty,wecouldobtainprobabilitiesgreaterthanonebycomputingtheprobabilityofoneofmanyeventsoccurring.Forexample,considerasinglediscreterandomvariablexwithkdiﬀerentstates.Wecanplaceaonuniformdistributionx—thatis,makeeachofitsstatesequallylikely—bysettingitsprobabilitymassfunctiontoPx(= xi) =1k(3.1)foralli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.Thevalue1kispositivebecauseisapositiveinteger.Wealsoseethatk\\ue058iPx(= xi) =\\ue058i1k=kk= 1,(3.2)sothedistributionisproperlynormalized.57'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 72}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.3.2ContinuousVariablesandProbabilityDensityFunctionsWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydis-tributionsusingaprobabilitydensityfunction(PDF)ratherthanaprobabilitymassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.p•∀∈≥≤xx,px() 0() .pNotethatwedonotrequirex1.•\\ue052pxdx()='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 72}, page_content='0() .pNotethatwedonotrequirex1.•\\ue052pxdx()= 1.Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁcstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwithvolumeisgivenby.δxpxδx()Wecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofasetofpoints.Speciﬁcally,theprobabilitythatxliesinsomesetSisgivenbytheintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatxliesintheintervalisgivenby[]a,b\\ue052[]a,bpxdx().Foranexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁcprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-tiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),whereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans“parametrizedby”;weconsiderxtobetheargumentofthefunction,whileaandbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobabilitymassoutsidetheinterval,wesayu(x;a,b)=0forallx\\ue036∈[a,b][.Withina,b],uxa,b(;)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 72}, page_content='=1ba−.Wecanseethatthisisnonnegativeeverywhere.Additionally,itintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]bywritingx.∼Ua,b()3.4MarginalProbabilitySometimesweknowtheprobabilitydistributionoverasetofvariablesandwewanttoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobabilitydistributionoverthesubsetisknownasthemarginalprobabilitydistribution.Forexample,supposewehavediscreterandomvariablesxandy,andweknowP,(xy.Wecanﬁndxwiththe:)P()sumrule∀∈xxx,P(= ) =x\\ue058yPx,y.(= xy= )(3.3)58'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 73}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThename“marginalprobability”comesfromtheprocessofcomputingmarginalprobabilitiesonpaper.WhenthevaluesofP(xy,)arewritteninagridwithdiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturaltosumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjusttotherightoftherow.Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:px() =\\ue05apx,ydy.()(3.4)3.5ConditionalProbabilityInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsomeothereventhashappened.Thisiscalledaconditionalprobability.Wedenotetheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).ThisconditionalprobabilitycanbecomputedwiththeformulaPyx(= y|x= ) =Py,x(= yx= )Px(='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 73}, page_content='y|x= ) =Py,x(= yx= )Px(= x).(3.5)TheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcomputetheconditionalprobabilityconditionedonaneventthatneverhappens.Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhatwouldhappenifsomeactionwereundertaken.TheconditionalprobabilitythatapersonisfromGermanygiventhattheyspeakGermanisquitehigh,butifarandomlyselectedpersonistaughttospeakGerman,theircountryoforigindoesnotchange.Computingtheconsequencesofanactioniscalledmakinganinterventionquery.Interventionqueriesarethedomainofcausalmodeling,whichwedonotexploreinthisbook.3.6TheChainRuleofConditionalProbabilitiesAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposedintoconditionaldistributionsoveronlyonevariable:P(x(1),...,x()n) = (Px(1))Πni=2P(x()i|x(1),...,x(1)i−).(3.6)Thisobservationisknownastheorchainruleproductruleofprobability.ItfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinEq..For3.559'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 74}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYexample,applyingthedeﬁnitiontwice,wegetP,,P,P,(abc)=(ab|c)(bc)P,PP(bc)=()bc|()cP,,P,PP.(abc)=(ab|c)()bc|()c3.7IndependenceandConditionalIndependenceTworandomvariablesxandyareindependentiftheirprobabilitydistributioncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolvingonlyy:∀∈∈xx,yyxyxy(3.7),p(= x,= ) = (yp= )(xp= )y.Tworandomvariablesxandyareconditionallyindependentgivenarandomvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthiswayforeveryvalueofz:∀∈∈∈|||xx,yy,zzxy,p(= x,= yzx= ) = (zp= xzy= )(zp= yz= )z.(3.8)We candenoteindependence andconditionalindependence withcompactnotation:xy⊥meansthatxandyareindependent,whilexyz⊥|meansthatxandyareconditionallyindependentgivenz.3.8Expectation,VarianceandCovarianceTheexpectationexpectedvalueorofsomefunctionf(x) withrespecttoaprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenxisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation:PEx∼P[()]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 74}, page_content='withrespecttoaprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenxisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation:PEx∼P[()] =fx\\ue058xPxfx,()()(3.9)whileforcontinuousvariables,itiscomputedwithanintegral:Ex∼p[()] =fx\\ue05apxfxdx.()()(3.10)60'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 75}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYWhentheidentityofthedistributionisclearfromthecontext,wemaysimplywritethenameoftherandomvariablethattheexpectationisover,asinEx[f(x)].Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthesubscriptentirely,asinE[f(x)].Bydefault,wecanassumethatE[·]averagesoverthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereisnoambiguity,wemayomitthesquarebrackets.Expectationsarelinear,forexample,Ex[()+()] = αfxβgxαEx[()]+fxβEx[()]gx,(3.11)whenandarenotdependenton.αβxThevariancegivesameasureofhowmuchthevaluesofafunctionofarandomvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:Var(()) = fxE\\ue068(()[()])fx−Efx2\\ue069.(3.12)Whenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.Thesquarerootofthevarianceisknownasthestandarddeviation.Thecovariancegivessomesenseofhowmuchtwovaluesarelinearlyrelatedtoeachother,aswellasthescaleofthesevariables:Cov(()()) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 75}, page_content='[(()[()])(()[()])]fx,gyEfx−Efxgy−Egy.(3.13)Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuchandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthecovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvaluessimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendstotakeonarelativelyhighvalueatthetimesthattheothertakesonarelativelylowvalueandviceversa.Othermeasuressuchascorrelationnormalizethecontributionofeachvariableinordertomeasureonlyhowmuchthevariablesarerelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.Thenotionsofcovarianceanddependencearerelated,butareinfactdistinctconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezerocovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-ever,independenceisadistinctpropertyfromcovariance.Fortwovariablestohavezerocovariance,theremustbenolineardependencebetweenthem.Independenceisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludesnonlin'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 75}, page_content='sadistinctpropertyfromcovariance.Fortwovariablestohavezerocovariance,theremustbenolineardependencebetweenthem.Independenceisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludesnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthavezerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfromauniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable61'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 76}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYs.Withprobability12,wechoosethevalueofstobe.Otherwise,wechoose1thevalueofstobe−1.Wecanthengeneratearandomvariableybyassigningy=sx.Clearly,xandyarenotindependent,becausexcompletelydeterminesthemagnitudeof.However,yCov() = 0x,y.Thecovariancematrixofarandomvectorx∈Rnisannn×matrix,suchthatCov()xi,j= Cov(xi,xj).(3.14)Thediagonalelementsofthecovariancegivethevariance:Cov(xi,xi) = Var(xi).(3.15)3.9CommonProbabilityDistributionsSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachinelearning.3.9.1BernoulliDistributionThedistributionisadistributionoverasinglebinaryrandomvariable.BernoulliItiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityoftherandomvariablebeingequalto1.Ithasthefollowingproperties:Pφ(= 1) = x(3.16)Pφ(= 0) = 1x−(3.17)Pxφ(= x) = x(1)−φ1−x(3.18)Ex[] = xφ(3.19)Varx() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 76}, page_content='1) = x(3.16)Pφ(= 0) = 1x−(3.17)Pxφ(= x) = x(1)−φ1−x(3.18)Ex[] = xφ(3.19)Varx() = (1)xφ−φ(3.20)3.9.2MultinoulliDistributionTheormultinoullicategoricaldistributionisadistributionoverasinglediscretevariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedbyMurphy2012().Themultinoullidistributionisaspecialcaseofthedistribution.Amultinomialmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmanytimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifyingthattheyreferonlytothecase.n= 162'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 77}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYparametrizedbyavectorp∈[0,1]k−1,wherepigivestheprobabilityofthei-thstate.Theﬁnal,k-thstate’sprobabilityisgivenby1−1\\ue03ep.Notethatwemustconstrain1\\ue03ep≤1.Multinoullidistributionsareoftenusedtorefertodistributionsovercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumericalvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectationorvarianceofmultinoulli-distributedrandomvariables.TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-butionovertheirdomain.Thisisbecausetheymodeldiscretevariablesforwhichitisfeasibletosimplyenumerateallofthestates.Whendealingwithcontinuousvariables,thereareuncountablymanystates,soanydistributiondescribedbyasmallnumberofparametersmustimposestrictlimitsonthedistribution.3.9.3GaussianDistributionThemostcommonlyuseddistributionoverrealnumbersisthe,normaldistributionalsoknownasthe:GaussiandistributionN(;xµ,σ2)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 77}, page_content='=\\ue07212πσ2exp\\ue012−12σ2()xµ−2\\ue013.(3.21)SeeFig.foraplotofthedensityfunction.3.1Thetwoparametersµ∈Randσ∈(0,∞)controlthenormaldistribution.Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanofthedistribution:E[x] =µ.Thestandarddeviationofthedistributionisgivenbyσ,andthevariancebyσ2.WhenweevaluatethePDF,weneedtosquareandinvertσ.WhenweneedtofrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientwayofparametrizingthedistributionistouseaparameterβ∈(0,∞)tocontroltheprecisionorinversevarianceofthedistribution:N(;xµ,β−1) =\\ue072β2πexp\\ue012−12βxµ(−)2\\ue013.(3.22)Normaldistributionsareasensiblechoiceformanyapplications.Intheabsenceofpriorknowledgeaboutwhatformadistributionovertherealnumbersshouldtake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.First,manydistributionswewishtomodelaretrulyclosetobeingnormaldistributions.Thecentrallimittheoremshowsthatthesumofmanyindependentrandomvariablesisapproximatelynormallydistributed.Thismeansthatin63'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 78}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue030\\ue02e\\ue030\\ue035\\ue030\\ue02e\\ue031\\ue030\\ue030\\ue02e\\ue031\\ue035\\ue030\\ue02e\\ue032\\ue030\\ue030\\ue02e\\ue032\\ue035\\ue030\\ue02e\\ue033\\ue030\\ue030\\ue02e\\ue033\\ue035\\ue030\\ue02e\\ue034\\ue030\\ue070\\ue028\\ue078\\ue029\\ue04d\\ue061\\ue078\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue061\\ue074\\ue020\\ue078\\ue0b9\\ue03d\\ue049\\ue06e\\ue066\\ue06c\\ue065\\ue063\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue073\\ue020\\ue061\\ue074\\ue020\\ue020\\ue020\\ue020\\ue020\\ue020\\ue078\\ue0b9\\ue0be\\ue03d\\ue0a7\\ue054\\ue068\\ue065\\ue020\\ue06e\\ue06f\\ue072\\ue06d\\ue061\\ue06c\\ue020\\ue064\\ue069\\ue073\\ue074\\ue072\\ue069\\ue062\\ue075\\ue074\\ue069\\ue06f\\ue06e'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 78}, page_content='Figure3.1::ThenormaldistributionThenormaldistributionN(x;µ,σ2) exhibitsaclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,andthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormaldistribution,withand.µ= 0σ= 1practice, manycomplicatedsystemscanbemodeledsuccessfullyasnormallydistributednoise,evenifthesystemcanbedecomposedintopartswithmorestructuredbehavior.Second,outofallpossibleprobabilitydistributionswiththesamevariance,thenormaldistributionencodesthemaximumamountofuncertaintyovertherealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheonethatinsertstheleastamountofpriorknowledgeintoamodel. Fullydevelopingandjustifyingthisidearequiresmoremathematicaltools,andispostponedtoSec.19.4.2.ThenormaldistributiongeneralizestoRn,inwhichcaseitisknownasthemultivariatenormaldistribution.Itmaybeparametrizedwithapositivedeﬁnitesymmetricmatrix:ΣN(;)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 78}, page_content='=xµ,Σ\\ue0731(2)πndet()Σexp\\ue012−12()xµ−\\ue03eΣ−1()xµ−\\ue013.(3.23)Theparameterµstillgivesthemeanofthedistribution,thoughnowitisvector-valued.TheparameterΣgivesthecovariancematrixofthedistribution.Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor64'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 79}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationallyeﬃcientwaytoparametrizethedistribution,sinceweneedtoinvertΣtoevaluatethePDF.Wecaninsteaduseaprecisionmatrixβ:N(;xµβ,−1) =\\ue073det()β(2)πnexp\\ue012−12()xµ−\\ue03eβxµ(−)\\ue013.(3.24)Weoftenﬁxthecovariancematrixtobeadiagonalmatrix.AnevensimplerversionistheisotropicGaussiandistribution,whosecovariancematrixisascalartimestheidentitymatrix.3.9.4ExponentialandLaplaceDistributionsInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistributionwithasharppointatx=0.Toaccomplishthis,wecanusetheexponentialdistribution:pxλλ(;) = 1x≥0exp()−λx.(3.25)Theexponentialdistributionusestheindicatorfunction1x≥0toassignprobabilityzerotoallnegativevaluesof.xAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeakofprobabilitymassatanarbitrarypointistheµLaplacedistributionLaplace(;)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 79}, page_content='=xµ,γ12γexp\\ue012−|−|xµγ\\ue013.(3.26)3.9.5TheDiracDistributionandEmpiricalDistributionInsomecases,wewishtospecifythatallofthemassinaprobabilitydistributionclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusingtheDiracdeltafunction,:δx()pxδxµ.() = (−)(3.27)TheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthatassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindofmathematicalobjectcalledageneralizedfunctionthatisdeﬁnedintermsofitspropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthelimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsotherthan.µ65'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 80}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYBydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowandinﬁnitelyhighpeakofprobabilitymasswhere.xµ= AcommonuseoftheDiracdeltadistributionisasacomponentofanempiricaldistribution,ˆp() =x1mm\\ue058i=1δ(xx−()i)(3.28)whichputsprobabilitymass1moneachofthempointsx(1),...,x()mformingagivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessarytodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,thesituationissimpler:anempiricaldistributioncanbeconceptualizedasamultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvaluethatissimplyequaltotheempiricalfrequencyofthatvalueinthetrainingset.Wecanviewtheempiricaldistributionformedfromadatasetoftrainingexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodelonthisdataset.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 80}, page_content='Anotherimportantperspectiveontheempiricaldistributionisthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata(seeSec.).5.53.9.6MixturesofDistributionsItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimplerprobabilitydistributions.Onecommon wayof combining distributionsis toconstructamixturedistribution.Amixturedistributionismadeupofseveralcomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistributiongeneratesthesampleisdeterminedbysamplingacomponentidentityfromamultinoullidistribution:P() =x\\ue058iPiPi(= c)(= xc|)(3.29)wherecisthemultinoullidistributionovercomponentidentities.P()Wehavealreadyseenoneexampleofamixturedistribution:theempiricaldistributionoverreal-valuedvariablesisamixturedistributionwithoneDiraccomponentforeachtrainingexample.Themixturemodelisonesimplestrategyforcombiningprobabilitydistributionstocreatearicherdistribution.InChapter,weexploretheartofbuildingcomplex16probabilitydistributionsfromsimpleonesinmoredetail.66'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 81}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThemixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeofparamountimportancelater—the.Alatentvariableisarandomlatentvariablevariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthemixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthroughthejointdistribution,inthiscase,P(xc,)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 81}, page_content='=P(xc|)P(c).ThedistributionP(c)overthelatentvariableandthedistributionP(xc|)relatingthelatentvariablestothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhoughitispossibletodescribeP(x)withoutreferencetothelatentvariable.LatentvariablesarediscussedfurtherinSec..16.5AverypowerfulandcommontypeofmixturemodelistheGaussianmixturemodel,inwhichthecomponentsp(x|c=i)areGaussians.Eachcomponenthasaseparatelyparametrizedmeanµ()iandcovarianceΣ()i.Somemixturescanhavemoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponentsviatheconstraintΣ()i=Σ∀i.AswithasingleGaussiandistribution,themixtureofGaussiansmightconstrainthecovariancematrixforeachcomponenttobediagonalorisotropic.Inadditiontothemeansandcovariances,theparametersofaGaussianmixturespecifythepriorprobabilityαi=P(c=i)giventoeachcomponenti.Theword“prior”indicatesthatitexpressesthemodel’sbeliefsaboutcbeforeithasobservedx.Bycomparison,P(c|x)isaposteriorprobability,becauseitiscomputedafterobservation ofx.AGaussian'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 81}, page_content='ofx.AGaussian mixturemodel isa universalapproximatorofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithanyspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenoughcomponents.Fig.showssamplesfromaGaussianmixturemodel.3.23.10UsefulPropertiesofCommonFunctionsCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especiallytheprobabilitydistributionsusedindeeplearningmodels.Oneofthesefunctionsisthelogisticsigmoid:σx() =11+exp()−x.(3.30)ThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoullidistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvaluesfortheφparameter.SeeFig.foragraphofthesigmoidfunction.Thesigmoid3.367'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 82}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nx1x2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 82}, page_content='Figure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethreecomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonalcovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligneddirection.Thisexamplehasmorevariancealongthex2axisthanalongthex1axis.Thethirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevarianceseparatelyalonganarbitrarybasisofdirections.functionsaturateswhenitsargumentisverypositiveorverynegative,meaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinitsinput.Anothercommonlyencounteredfunctionisthefunction(,softplusDugasetal.2001):ζxx.() = log(1+exp())(3.31)Thesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormaldistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulatingexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthefactthatitisasmoothedor“softened”versionofx+='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 82}, page_content='max(0),x.(3.32)SeeFig.foragraphofthesoftplusfunction.3.4Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorizethem:σx() =exp()xexp()+exp(0)x(3.33)ddxσxσxσx() = ()(1−())(3.34)68'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 83}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\U000f0913\\ue031\\ue030\\U000f0913\\ue035\\ue030\\ue035\\ue031\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue038\\ue031\\ue02e\\ue030\\ue0be\\ue078\\ue028\\ue029\\ue054\\ue068\\ue065\\ue020\\ue06c\\ue06f\\ue067\\ue069\\ue073\\ue074\\ue069\\ue063\\ue020\\ue073\\ue069\\ue067\\ue06d\\ue06f\\ue069\\ue064\\ue020\\ue066\\ue075\\ue06e\\ue063\\ue074\\ue069\\ue06f\\ue06e\\nFigure3.3:Thelogisticsigmoidfunction.\\n\\U000f0913\\ue031\\ue030\\U000f0913\\ue035\\ue030\\ue035\\ue031\\ue030\\ue030\\ue032\\ue034\\ue036\\ue038\\ue031\\ue030\\ue0b3\\ue078\\ue028\\ue029\\ue054\\ue068\\ue065\\ue020\\ue073\\ue06f\\ue066\\ue074\\ue070\\ue06c\\ue075\\ue073\\ue020\\ue066\\ue075\\ue06e\\ue063\\ue074\\ue069\\ue06f\\ue06e\\nFigure3.4:Thesoftplusfunction.69'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 84}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY1() = ()−σxσ−x(3.35)log() = ()σx−ζ−x(3.36)ddxζxσx() = ()(3.37)∀∈x(01),,σ−1() = logx\\ue012x1−x\\ue013(3.38)∀x>,ζ0−1() = log(exp()1)xx−(3.39)ζx() =\\ue05ax−∞σydy()(3.40)ζxζxx()−(−) = (3.41)Thefunctionσ−1(x)iscalledthelogitinstatistics,butthistermismorerarelyusedinmachinelearning.Eq.providesextrajustiﬁcationforthename“softplus.”Thesoftplus3.41functionisintendedasasmoothedversionofthepositivepartfunction,x+=max{0,x}.Thepositivepartfunctionisthecounterpartofthenegativepartfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothenegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepartandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverxusingthesamerelationshipbetweenand,asshowninEq..ζx()ζx(−)3.413.11Bayes’RuleWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknowP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantityusing:Bayes’ruleP()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 84}, page_content='=xy|PP()x()yx|P()y.(3.42)NotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocomputeP() =y\\ue050xPxPxP(y|)(),sowedonotneedtobeginwithknowledgeof()y.Bayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditionalprobability,butitisusefultoknowthenameofthisformulasincemanytextsrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrstdiscoveredaspecialcaseoftheformula.ThegeneralversionpresentedherewasindependentlydiscoveredbyPierre-SimonLaplace.70'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 85}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.12TechnicalDetailsofContinuousVariablesAproperformalunderstandingofcontinuousrandomvariablesandprobabilitydensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchofmathematicsknownasmeasuretheory.Measuretheoryisbeyondthescopeofthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryisemployedtoresolve.InSec.,wesawthattheprobabilityofacontinuousvector-valued3.3.2xlyinginsomesetSisgivenbytheintegralofp(x)overthesetS.SomechoicesofsetScanproduceparadoxes.Forexample,itispossibletoconstructtwosetsS1andS2suchthatp(x∈S1)+p(x∈S2)>1butS1∩S2=∅.Thesesetsaregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofrealnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedbytransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasuretheoryistoprovideacharacterizationofthesetofsetsthatwecancomputetheprobabilityofwithoutencounteringparadoxes.Inthisbook,weonlyintegrateoversetswithrelativelysimpledescriptions,sothisaspec'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 85}, page_content='ionsofmeasuretheoryistoprovideacharacterizationofthesetofsetsthatwecancomputetheprobabilityofwithoutencounteringparadoxes.Inthisbook,weonlyintegrateoversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheoryneverbecomesarelevantconcern.Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthatapplytomostpointsinRnbutdonotapplytosomecornercases.Measuretheoryprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Suchasetissaidtohave“measurezero.”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 85}, page_content='Wedonotformallydeﬁnethisconceptinthistextbook.However,itisusefultounderstandtheintuitionthatasetofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,withinR2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysetsthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherationalnumbershasmeasurezero,forinstance).Anotherusefultermfrommeasuretheoryis“almosteverywhere.”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 85}, page_content='Apropertythatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetofmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,theycanbesafelyignoredformanyapplications.Someimportantresultsinprobabilitytheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuousvalues.Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuousrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehavetworandomvariables,xandy,suchthaty=g(x),wheregisaninvertible,con-2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.71'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 86}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYtinuous,diﬀerentiabletransformation.Onemightexpectthatpy(y) =px(g−1(y)).Thisisactuallynotthecase.Asasimpleexample,supposewehavescalarrandomvariablesxandy.Supposey=x2andx∼U(0,1).Ifweusetherulepy(y)=px(2y)thenpywillbe0everywhereexcepttheinterval[0,12]1,anditwillbeonthisinterval.Thismeans\\ue05apy()=ydy12,(3.43)whichviolatesthedeﬁnitionofaprobabilitydistribution.Thiscommonmistakeiswrongbecauseitfailstoaccountforthedistortionofspaceintroducedbythefunctiong. Recallthattheprobabilityofxlyinginaninﬁnitesimallysmallregionwithvolumeδxisgivenbyp(x)δx. Sincegcanexpandorcontractspace,theinﬁnitesimalvolumesurroundingxinxspacemayhavediﬀerentvolumeinspace.yToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedtopreservetheproperty|py(())= gxdy||px()xdx.|(3.44)Solvingfromthis,weobtainpy() = ypx(g−1())y\\ue00c\\ue00c\\ue00c\\ue00c∂x∂y\\ue00c\\ue00c\\ue00c\\ue00c(3.45)orequivalentlypx() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 86}, page_content='gxdy||px()xdx.|(3.44)Solvingfromthis,weobtainpy() = ypx(g−1())y\\ue00c\\ue00c\\ue00c\\ue00c∂x∂y\\ue00c\\ue00c\\ue00c\\ue00c(3.45)orequivalentlypx() = xpy(())gx\\ue00c\\ue00c\\ue00c\\ue00c∂gx()∂x\\ue00c\\ue00c\\ue00c\\ue00c.(3.46)Inhigherdimensions,thederivativegeneralizestothedeterminantoftheJacobianmatrix—thematrixwithJi,j=∂xi∂yj.Thus,forreal-valuedvectorsand,xypx() = xpy(())gx\\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g()x∂x\\ue013\\ue00c\\ue00c\\ue00c\\ue00c.(3.47)3.13InformationTheoryInformationtheory isa branchof appliedmathematics thatrevolvesaroundquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinventedtostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchascommunicationviaradiotransmission.Inthiscontext,informationtheorytellshowtodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom72'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 87}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextofmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariableswheresomeofthesemessagelengthinterpretationsdonotapply.Thisﬁeldisfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthistextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterizeprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.Formoredetailoninformationtheory,seeCoverandThomas2006MacKay()or().2003Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikelyeventhas occurredismoreinformativethanlearningthata likely'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 87}, page_content='occurredismoreinformativethanlearningthata likely eventhasoccurred.Amessagesaying“thesunrosethismorning”issouninformativeastobeunnecessarytosend,butamessagesaying“therewasasolareclipsethismorning”isveryinformative.Wewouldliketoquantifyinformationinawaythatformalizesthisintuition.Speciﬁcally,•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,eventsthatareguaranteedtohappenshouldhavenoinformationcontentwhatsoever.•Lesslikelyeventsshouldhavehigherinformationcontent.•Independenteventsshouldhaveadditiveinformation.Forexample,ﬁndingoutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceasmuchinformationasﬁndingoutthatatossedcoinhascomeupasheadsonce.Inordertosatisfyallthreeoftheseproperties,wedeﬁnetheself-informationofaneventxtobe= xIxPx.() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 87}, page_content='xIxPx.() = log−()(3.48)Inthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.OurdeﬁnitionofI(x)isthereforewritteninunitsof.Onenatistheamountofnatsinformationgainedbyobservinganeventofprobability1e.Othertextsusebase-2logarithmsandunitscalledor;informationmeasuredinbitsisjustbitsshannonsarescalingofinformationmeasuredinnats.Whenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,butsomeofthepropertiesfromthediscretecasearelost.Forexample,aneventwithunitdensitystillhaszeroinformation,despitenotbeinganeventthatisguaranteedtooccur.73'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 88}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue038\\ue031\\ue02e\\ue030\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue031\\ue030\\ue02e\\ue032\\ue030\\ue02e\\ue033\\ue030\\ue02e\\ue034\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue036\\ue030\\ue02e\\ue037\\ue053\\ue068\\ue061\\ue06e\\ue06e\\ue06f\\ue06e\\ue020\\ue065\\ue06e\\ue074\\ue072\\ue06f\\ue070\\ue079\\ue020\\ue069\\ue06e\\ue020\\ue06e\\ue061\\ue074\\ue073\\ue053\\ue068\\ue061\\ue06e\\ue06e\\ue06f\\ue06e\\ue020\\ue065\\ue06e\\ue074\\ue072\\ue06f\\ue070\\ue079\\ue020\\ue06f\\ue066\\ue020\\ue061\\ue020\\ue062\\ue069\\ue06e\\ue061\\ue072\\ue079\\ue020\\ue072\\ue061\\ue06e\\ue064\\ue06f\\ue06d\\ue020\\ue076\\ue061\\ue072\\ue069\\ue061\\ue062\\ue06c\\ue065'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 88}, page_content='Figure3.5:ThisplotshowshowdistributionsthatareclosertodeterministichavelowShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequalto.Theentropyisgivenby1(p−1)log(1−p)−pplog.Whenpisnear0,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwooutcomes.Self-informationdealsonlywithasingleoutcome.WecanquantifytheamountofuncertaintyinanentireprobabilitydistributionusingtheShannonentropy:H() = xEx∼P[()] ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 88}, page_content='= xEx∼P[()] = Ix−Ex∼P[log()]Px.(3.49)alsodenotedH(P).Inotherwords,theShannonentropyofadistributionistheexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgivesalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunitsarediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)havelowentropy;distributionsthatareclosertouniformhavehighentropy.SeeFig.forademonstration.When3.5xiscontinuous,theShannonentropyisknownasthediﬀerentialentropy.IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesamerandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusingtheKullback-Leibler(KL)divergence:DKL() = PQ\\ue06bEx∼P\\ue014logPx()Qx()\\ue015= Ex∼P[log()log()]Px−Qx.(3.50)74'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 89}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYInthecaseofdiscretevariables,itistheextraamountofinformation(measuredinbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats2andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawnfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimizethelengthofmessagesdrawnfromprobabilitydistribution.QTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributioninthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuousvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerencebetweentwodistributions,itisoftenconceptualizedasmeasuringsomesortofdistancebetweenthesedistributions.However,itisnotatruedistancemeasurebecauseitisnotsymmetric:DKL(PQ\\ue06b)\\ue036=DKL(QP\\ue06b)forsomePandQ.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 89}, page_content='ThisasymmetrymeansthatthereareimportantconsequencestothechoiceofwhethertouseDKL()PQ\\ue06borDKL()QP\\ue06b.SeeFig.formoredetail.3.6AquantitythatiscloselyrelatedtotheKLdivergenceisthecross-entropyH(P,Q) =H(P)+DKL(PQ\\ue06b),whichissimilartotheKLdivergencebutlackingthetermontheleft:HP,Q() = −Ex∼Plog()Qx.(3.51)Minimizingthecross-entropywithrespecttoQisequivalenttominimizingtheKLdivergence,becausedoesnotparticipateintheomittedterm.QWhencomputingmanyofthesequantities,itiscommontoencounterexpres-sionsoftheform0log0.Byconvention,inthecontextofinformationtheory,wetreattheseexpressionsaslimx→0xxlog='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 89}, page_content='0.3.14StructuredProbabilisticModelsMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveraverylargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolvedirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctiontodescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(bothcomputationallyandstatistically).Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,wecansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.Forexample,supposewehavethreerandomvariables:a,bandc.Supposethatainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcareindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree75'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 90}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nxProbability Densityq∗= argminqDKL()pq\\ue06bpx()q∗()x\\nxProbability Densityq∗= argminqDKL()qp\\ue06bp()xq∗()x'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 90}, page_content='Figure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)andwishtoapproximateitwithanotherdistributionq(x).WehavethechoiceofminimizingeitherDKL(pq\\ue06b)orDKL(qp\\ue06b).WeillustratetheeﬀectofthischoiceusingamixtureoftwoGaussiansforp,andasingleGaussianforq.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 90}, page_content='ThechoiceofwhichdirectionoftheKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximationthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshighprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshighprobabilityanywherethatthetruedistributionplaceslowprobability.ThechoiceofthedirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeachapplication.(Left)TheeﬀectofminimizingDKL(pq\\ue06b).Inthiscase,weselectaqthathashighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosestoblurthemodestogether,inordertoputhighprobabilitymassonallofthem.The(Right)eﬀectofminimizingDKL(qp\\ue06b).Inthiscase,weselectaqthathaslowprobabilitywherephaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,asinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inordertoavoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,weillustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohaveachie'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 90}, page_content='ergenceisminimizedbychoosingasinglemode,inordertoavoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,weillustratetheoutcomewhenqischosentoemphasizetheleftmode.WecouldalsohaveachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodesarenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionoftheKLdivergencecanstillchoosetoblurthemodes.76'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 91}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYvariablesasaproductofprobabilitydistributionsovertwovariables:p,,ppp.(abc) = ()a()ba|()cb|(3.52)Thesefactorizationscangreatlyreducethenumberofparametersneededtodescribethedistribution.Eachfactorusesanumberofparametersthatisexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatlyreducethecostofrepresentingadistributionifweareabletoﬁndafactorizationintodistributionsoverfewervariables.Wecandescribethesekindsoffactorizationsusinggraphs. Hereweusetheword“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeachotherwithedges.Whenwerepresentthefactorizationofaprobabilitydistributionwithagraph,wecallitastructuredprobabilisticmodelgraphicalormodel.Therearetwomainkindsofstructuredprobabilisticmodels:directedandundirected.BothkindsofgraphicalmodelsuseagraphGinwhicheachnodeinthegraphcorrespondstoarandomvariable,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 91}, page_content='andanedgeconnectingtworandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirectinteractionsbetweenthosetworandomvariables.Directedmodelsusegraphswithdirectededges,andtheyrepresentfactoriza-tionsintoconditionalprobabilitydistributions,asintheexampleabove.Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablexiinthedistribution,andthatfactorconsistsoftheconditionaldistributionoverxigiventheparentsofxi,denotedPaG(xi):p()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 91}, page_content='=x\\ue059ip(xi|PaG(xi)).(3.53)SeeFig.foranexampleofadirectedgraphandthefactorizationofprobability3.7distributionsitrepresents.Undirectedmodelsusegraphswithundirectededges,andtheyrepresentfac-torizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctionsareusuallynotprobabilitydistributionsofanykind.AnysetofnodesthatareallconnectedtoeachotherinGiscalledaclique.EachcliqueC()iinanundirectedmodelisassociatedwithafactorφ()i(C()i).Thesefactorsarejustfunctions,notprobabilitydistributions.Theoutputofeachfactormustbenon-negative,butthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobabilitydistribution.Theprobabilityofaconﬁgurationofrandomvariablesisproportionaltotheproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare77'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 92}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 92}, page_content='e ed dFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,ppp,pp.(abcde) = ()a()ba|(ca|b)()db|()ec|(3.54)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.WethereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegraloverallstatesoftheproductoftheφfunctions,inordertoobtainanormalizedprobabilitydistribution:p() =x1Z\\ue059iφ()i\\ue010C()i\\ue011.(3.55)See Fig.foran exampleof anundirected graph andthe factorizationof3.8 probabilitydistributionsitrepresents.Keep inmind thatthese graphicalrepresentationsof factorizationsare alanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusivefamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotapropertyofaprobabilitydistribution; itisapropertyofa'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 92}, page_content='alanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusivefamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotapropertyofaprobabilitydistribution; itisapropertyofa particularofadescriptionprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinbothways.ThroughoutPartandPartofthisbook,wewillusestructuredprobabilisticIIImodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationshipsdiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstandingofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,inPart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreaterIIIdetail.78'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 93}, page_content='CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b\\ne ed dFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,(abcde) =1Zφ(1)()abc,,φ(2)()bd,φ(3)()ce,.(3.56)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.Thischapterhasreviewedthebasicconceptsofprobabilitytheorythataremostrelevanttodeeplearning.Onemoresetoffundamentalmathematicaltoolsremains:numericalmethods.\\n79'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 94}, page_content='Chapter4NumericalComputationMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-tation.Thistypicallyreferstoalgorithmsthatsolvemathematicalproblemsbymethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthananalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-lution.Commonoperationsincludeoptimization(ﬁndingthevalueofanargumentthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.Evenjustevaluatingamathematicalfunctiononadigitalcomputercanbediﬃcultwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedpreciselyusingaﬁniteamountofmemory.4.1OverﬂowandUnderﬂowThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputeristhatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumberofbitpatterns.Thismeansthatforalmostallrealnumbers,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 94}, page_content='weincursomeapproximationerrorwhenwerepresentthenumberinthecomputer.Inmanycases,thisisjustroundingerror.Roundingerrorisproblematic,especiallywhenitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkintheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationofroundingerror.Oneformofroundingerrorthatisparticularlydevastatingis.Under-underﬂowﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctionsbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmallpositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(somesoftware80'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 95}, page_content='CHAPTER4.NUMERICALCOMPUTATIONenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturnaresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(thisisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformanyfurtherarithmeticoperations).Anotherhighlydamagingformofnumericalerroris.Overﬂowoccursoverﬂowwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Furtherarithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowandoverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredicttheprobabilitiesassociatedwithamultinoullidistribution.Thesoftmaxfunctionisdeﬁnedtobesoftmax()xi=exp(xi)\\ue050nj=1exp(xj).(4.1)Considerwhathappenswhenallofthexiareequaltosomeconstantc.Analytically,wecanseethatalloftheoutputsshouldbeequalto1n.Numerically,thismaynotoccurwhenchaslargemagnitude.Ifcisverynegative,thenexp(c)willunderﬂow.Thismeansthedenominatorofthesoftmaxwillbecome0,sotheﬁnalresultisundeﬁned.Whencisverylargeandpositi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 95}, page_content='eequalto1n.Numerically,thismaynotoccurwhenchaslargemagnitude.Ifcisverynegative,thenexp(c)willunderﬂow.Thismeansthedenominatorofthesoftmaxwillbecome0,sotheﬁnalresultisundeﬁned.Whencisverylargeandpositive,exp(c)willoverﬂow,againresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃcultiescanberesolvedbyinsteadevaluatingsoftmax(z)wherez=x−maxixi.Simplealgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallybyaddingorsubtractingascalarfromtheinputvector.Subtractingmaxixiresultsinthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.Likewise,atleastoneterminthedenominatorhasavalueof1,whichrulesoutthepossibilityofunderﬂowinthedenominatorleadingtoadivisionbyzero.Thereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcausetheexpressionasawholetoevaluatetozero.Thismeansthatifweimplementlogsoftmax(x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresulttothelogfunction,wecoulderroneouslyobtain−∞.Instead,wemustimplementaseparatefunctionthatcalculateslogsoftmaxinan'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 95}, page_content='meansthatifweimplementlogsoftmax(x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresulttothelogfunction,wecoulderroneouslyobtain−∞.Instead,wemustimplementaseparatefunctionthatcalculateslogsoftmaxinanumericallystableway.Thelogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilizethefunction.softmaxForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderationsinvolvedinimplementingthevariousalgorithmsdescribedinthisbook.Developersoflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementingdeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-levellibrariesthatprovidestableimplementations.Insomecases,itispossibletoimplementanewalgorithmandhavethenewimplementationautomatically81'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 96}, page_content='CHAPTER4.NUMERICALCOMPUTATIONstabilized.Theano(,;,)isanexampleBergstraetal.2010Bastienetal.2012ofasoftwarepackagethatautomaticallydetectsandstabilizesmanycommonnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.4.2PoorConditioningConditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchangesinitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightlycanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputscanresultinlargechangesintheoutput.Considerthefunctionf(x)=A−1x.WhenA∈Rnn×hasaneigenvaluedecomposition,itsconditionnumberismaxi,j\\ue00c\\ue00c\\ue00c\\ue00cλiλj\\ue00c\\ue00c\\ue00c\\ue00c.(4.2)Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.Whenthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresultofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplifypre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,theerrorwillbecompoundedfurtherbynumericalerrorsinth'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 96}, page_content=',nottheresultofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplifypre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,theerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.4.3Gradient-BasedOptimizationMostdeeplearningalgorithmsinvolveoptimizationofsomesort.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 96}, page_content='Optimizationreferstothetaskofeitherminimizingormaximizingsomefunctionf(x) byalteringx. Weusuallyphrasemostoptimizationproblemsintermsofminimizingf(x).Maximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing−f()x.Thefunctionwewanttominimizeormaximizeiscalledtheobjectivefunctionor.Whenweareminimizingit,wemayalsocallitthecriterioncostfunction,lossfunctionerrorfunction,or.Inthisbook,weusethesetermsinterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaningtosomeoftheseterms.Weoftendenotethevaluethatminimizesormaximizesafunctionwithasuperscript.Forexample,wemightsay∗x∗= argmin()fx.82'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 97}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue078\\U000f0913\\ue032\\ue02e\\ue030\\U000f0913\\ue031\\ue02e\\ue035\\U000f0913\\ue031\\ue02e\\ue030\\U000f0913\\ue030\\ue02e\\ue035\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue047\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue061\\ue074\\ue020\\ue02e\\ue078\\ue03d\\ue030\\ue053\\ue069\\ue06e\\ue063\\ue065\\ue020\\ue066\\ue030\\ue028\\ue029\\ue03d\\ue030\\ue078\\ue02c\\ue020\\ue067\\ue072\\ue061\\ue064\\ue069\\ue065\\ue06e\\ue074\\ue064\\ue065\\ue073\\ue063\\ue065\\ue06e\\ue074\\ue020\\ue068\\ue061\\ue06c\\ue074\\ue073\\ue020\\ue068\\ue065\\ue072\\ue065\\ue02e\\ue046\\ue06f\\ue072\\ue020\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue068\\ue061\\ue076\\ue065\\ue020\\ue078\\ue03c\\ue030\\ue066\\ue030\\ue028\\ue029\\ue030\\ue078\\ue03c\\ue02c\\ue073\\ue06f\\ue020\\ue077\\ue065\\ue020\\ue063\\ue061\\ue06e\\ue020\\ue064\\ue065\\ue063\\ue072\\ue065\\ue061\\ue073\\ue065\\ue020\\ue066\\ue020\\ue062\\ue079\\ue06d\\ue06f\\ue076\\ue069\\ue06e\\ue067\\ue020\\ue072\\ue069\\ue067\\ue068\\ue074\\ue077\\ue061\\ue072\\ue064\\ue02e\\ue046\\ue06f\\ue072\\ue020\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue068\\ue061\\ue076\\ue065\\ue020\\ue078\\ue03e\\ue030\\ue066\\ue030\\ue028\\ue029\\ue030\\ue078\\ue03e\\ue02c\\ue073\\ue06f\\ue020\\ue077\\ue065\\ue020\\ue063\\ue061\\ue06e\\ue020\\ue064\\ue065\\ue063\\ue072\\ue065\\ue061\\ue073\\ue065\\ue020\\ue066\\ue020\\ue062\\ue079\\ue06d\\ue06f\\ue076\\ue069\\ue06e\\ue067\\ue020\\ue06c\\ue065\\ue066\\ue074\\ue077\\ue061\\ue072\\ue064\\ue02e\\ue047\\ue072\\ue061\\ue064\\ue069\\ue065\\ue06e\\ue074\\ue020\\ue064\\ue065\\ue073\\ue063\\ue065\\ue06e\\ue074'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 97}, page_content='\\ue066\\ue078\\ue028\\ue029\\ue03d\\ue031\\ue032\\ue078\\ue032\\ue066\\ue030\\ue028\\ue029\\ue03d\\ue078\\ue078Figure4.1:Anillustrationofhowthederivativesofafunctioncanbeusedtofollowthefunctiondownhilltoaminimum.Thistechniqueiscalledgradientdescent.Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabriefreviewofhowcalculusconceptsrelatetooptimizationhere.Supposewehaveafunctiony=f(x),wherebothxandyarerealnumbers.Theofthisfunctionisdenotedasderivativef\\ue030(x)orasdydx.Thederivativef\\ue030(x)givestheslopeoff(x)atthepointx.Inotherwords,itspeciﬁeshowtoscaleasmallchangeintheinputinordertoobtainthecorrespondingchangeintheoutput:fx\\ue00ffx\\ue00ff(+) ≈()+\\ue030()x.Thederivativeisthereforeusefulforminimizingafunctionbecauseittellsushowtochangexinordertomakeasmallimprovementiny.Forexample,weknowthatf(x\\ue00f−sign(f\\ue030(x)))islessthanf(x)forsmallenough\\ue00f.Wecanthusreducef(x)bymovingxinsmallstepswithoppositesignofthederivative.Thistechniqueiscalledgradientdescent(Cauchy1847,).SeeFig.foranexampleof4.1thistechnique.Whenf\\ue030(x) = 0,thederivativeprovidesnoinformationaboutwhichdirectiontomove.Pointswheref\\ue030(x) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 97}, page_content='= 0,thederivativeprovidesnoinformationaboutwhichdirectiontomove.Pointswheref\\ue030(x) = 0areknownascriticalpointsstationarypointsor.Alocalminimumisapointwheref(x)islowerthanatallneighboringpoints,soitisnolongerpossibletodecreasef(x)bymakinginﬁnitesimalsteps.Alocalmaximumisapointwheref(x)ishigherthanatallneighboringpoints,soitis83'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 98}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\ue04d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue04d\\ue061\\ue078\\ue069\\ue06d\\ue075\\ue06d\\ue053\\ue061\\ue064\\ue064\\ue06c\\ue065\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue054\\ue079\\ue070\\ue065\\ue073\\ue020\\ue06f\\ue066\\ue020\\ue063\\ue072\\ue069\\ue074\\ue069\\ue063\\ue061\\ue06c\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue073'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 98}, page_content='Figure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointisapointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthantheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,orasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.notpossibletoincreasef(x)bymakinginﬁnitesimalsteps.Somecriticalpointsareneithermaximanorminima.Theseareknownassaddlepoints.SeeFig.4.2forexamplesofeachtypeofcriticalpoint.Apointthatobtainstheabsolutelowestvalueoff(x)isaglobalminimum.Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaofthefunction.Itisalsopossiblefortheretobelocalminimathatarenotgloballyoptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhavemanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedbyveryﬂatregions.Allofthismakesoptimizationverydiﬃcult,especiallywhentheinputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndingavalueoffthatisverylow,butnotnecessarilyminimalinanyformalsense.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 98}, page_content='ﬂatregions.Allofthismakesoptimizationverydiﬃcult,especiallywhentheinputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndingavalueoffthatisverylow,butnotnecessarilyminimalinanyformalsense.SeeFig.foranexample.4.3Weoftenminimizefunctionsthathavemultipleinputs:f:Rn→R.Fortheconceptof“minimization”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 98}, page_content='tomakesense,theremuststillbeonlyone(scalar)output.Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptofpartialderivatives.Thepartialderivative∂∂xif(x)measureshowfchangesasonlythevariablexiincreasesatpointx.Thegradientgeneralizesthenotionofderivativetothecasewherethederivativeiswithrespecttoavector:thegradientoffisthevectorcontainingallofthepartialderivatives,denoted∇xf(x).Elementiofthegradientisthepartialderivativeoffwithrespecttoxi.Inmultipledimensions,84'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 99}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue066\\ue078\\ue028\\ue029\\ue049\\ue064\\ue065\\ue061\\ue06c\\ue06c\\ue079\\ue02c\\ue020\\ue077\\ue065\\ue020\\ue077\\ue06f\\ue075\\ue06c\\ue064\\ue020\\ue06c\\ue069\\ue06b\\ue065\\ue020\\ue074\\ue06f\\ue020\\ue061\\ue072\\ue072\\ue069\\ue076\\ue065\\ue020\\ue061\\ue074\\ue020\\ue074\\ue068\\ue065\\ue020\\ue067\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue02c\\ue020\\ue062\\ue075\\ue074\\ue020\\ue074\\ue068\\ue069\\ue073\\ue020\\ue06d\\ue069\\ue067\\ue068\\ue074\\ue020\\ue06e\\ue06f\\ue074\\ue020\\ue062\\ue065\\ue020\\ue070\\ue06f\\ue073\\ue073\\ue069\\ue062\\ue06c\\ue065\\ue02e\\ue054\\ue068\\ue069\\ue073\\ue020\\ue06c\\ue06f\\ue063\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue070\\ue065\\ue072\\ue066\\ue06f\\ue072\\ue06d\\ue073\\ue020\\ue06e\\ue065\\ue061\\ue072\\ue06c\\ue079\\ue020\\ue061\\ue073\\ue020\\ue077\\ue065\\ue06c\\ue06c\\ue020\\ue061\\ue073\\ue074\\ue068\\ue065\\ue020\\ue067\\ue06c\\ue06f\\ue062\\ue061\\ue06c\\ue020\\ue06f\\ue06e\\ue065\\ue02c\\ue073\\ue06f\\ue020\\ue069\\ue074\\ue020\\ue069\\ue073\\ue020\\ue061\\ue06e\\ue020\\ue061\\ue063\\ue063\\ue065\\ue070\\ue074\\ue061\\ue062\\ue06c\\ue065\\ue068\\ue061\\ue06c\\ue074\\ue069\\ue06e\\ue067\\ue020\\ue070\\ue06f\\ue069\\ue06e\\ue074\\ue02e\\ue054\\ue068\\ue069\\ue073\\ue020\\ue06c\\ue06f\\ue063\\ue061\\ue06c\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue075\\ue06d\\ue020\\ue070\\ue065\\ue072\\ue066\\ue06f\\ue072\\ue06d\\ue073\\ue070\\ue06f\\ue06f\\ue072\\ue06c\\ue079\\ue02c\\ue020\\ue061\\ue06e\\ue064\\ue020\\ue073\\ue068\\ue06f\\ue075\\ue06c\\ue064\\ue020\\ue062\\ue065\\ue020\\ue061\\ue076\\ue06f\\ue069\\ue064\\ue065\\ue064\\ue02e\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue078\\ue069\\ue06d\\ue061\\ue074\\ue065\\ue020\\ue06d\\ue069\\ue06e\\ue069\\ue06d\\ue069\\ue07a\\ue061\\ue074\\ue069\\ue06f\\ue06e'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 99}, page_content='Figure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhentherearemultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerallyacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespondtosigniﬁcantlylowvaluesofthecostfunction.criticalpointsarepointswhereeveryelementofthegradientisequaltozero.Thedirectionalderivativeindirectionu(aunitvector)istheslopeofthefunctionfindirectionu.Inotherwords,thedirectionalderivativeisthederivativeofthefunctionf(x+αu)withrespecttoα,evaluatedatα= 0.Usingthechainrule,wecanseethat∂∂αfα(+xuu) = \\ue03e∇xf()x.Tominimizef,wewouldliketoﬁndthedirectioninwhichfdecreasesthefastest.Wecandothisusingthedirectionalderivative:minuu,\\ue03eu=1u\\ue03e∇xf()x(4.3)=minuu,\\ue03eu=1||||u2||∇xf()x||2cosθ(4.4)whereθistheanglebetweenuandthegradient.Substitutingin||||u2='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 99}, page_content='1andignoringfactorsthatdonotdependonu,thissimpliﬁestominucosθ.Thisisminimizedwhenupointsintheoppositedirectionasthegradient.Inotherwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectlydownhill.Wecandecreasefbymovinginthedirectionofthenegativegradient.Thisisknownasthemethodofsteepestdescentgradientdescentor.Steepestdescentproposesanewpointx\\ue030= x−∇\\ue00fxf()x(4.5)85'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 100}, page_content='CHAPTER4.NUMERICALCOMPUTATIONwhere\\ue00fisthelearningrate,apositivescalardeterminingthesizeofthestep.Wecanchoose\\ue00finseveraldiﬀerentways.Apopularapproachistoset\\ue00ftoasmallconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectionalderivativevanish.Anotherapproachistoevaluatef\\ue00f(x−∇xf())xforseveralvaluesof\\ue00fandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.Thislaststrategyiscalledalinesearch.Steepestdescentconvergeswheneveryelementofthegradientiszero(or,inpractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthisiterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingtheequation∇xf() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 100}, page_content='0xfor.xAlthoughgradientdescentislimitedtooptimizationincontinuousspaces,thegeneralconceptofmakingsmallmoves(thatareapproximatelythebestsmallmove)towardsbetterconﬁgurationscanbegeneralizedtodiscretespaces.Ascendinganobjectivefunctionofdiscreteparametersiscalled(,hillclimbingRusselandNorvig2003).4.3.1BeyondtheGradient:JacobianandHessianMatricesSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinputandoutputarebothvectors.ThematrixcontainingallsuchpartialderivativesisknownasaJacobianmatrix.Speciﬁcally,ifwehaveafunctionf:Rm→Rn,thentheJacobianmatrixJ∈Rnm×ofisdeﬁnedsuchthatfJi,j=∂∂xjf()xi.Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknownasasecondderivative.Forexample,forafunctionf:Rn→R,thederivativewithrespecttoxiofthederivativeoffwithrespecttoxjisdenotedas∂2∂xi∂xjf.Inasingledimension,wecandenoted2dx2fbyf\\ue030\\ue030(x).Thesecondderivativetellsushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportantbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 100}, page_content='.Inasingledimension,wecandenoted2dx2fbyf\\ue030\\ue030(x).Thesecondderivativetellsushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportantbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovementaswewouldexpectbasedonthegradientalone.Wecanthinkofthesecondderivativeasmeasuringcurvature.Supposewehaveaquadraticfunction(manyfunctionsthatariseinpracticearenotquadraticbutcanbeapproximatedwellasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,thenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredictedusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize1\\ue00falongthenegativegradient,andthecostfunctionwilldecreaseby\\ue00f.Ifthesecondderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwillactuallydecreasebymorethan\\ue00f.Finally,ifthesecondderivativeispositive,thefunctioncurvesupward,sothecostfunctioncandecreasebylessthan\\ue00f.SeeFig.86'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 101}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\nxfx()Negativecurvature\\nxfx()Nocurvature\\nxfx()Positivecurvature'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 101}, page_content='Figure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshowquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecostfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradientstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfasterthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecreasecorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpectedandeventuallybeginstoincrease,sotoolargeofstepsizescanactuallyincreasethefunctioninadvertently.4.4toseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetweenthevalueofthecostfunctionpredictedbythegradientandthetruevalue.Whenourfunctionhasmultipleinputdimensions,therearemanysecondderivatives.ThesederivativescanbecollectedtogetherintoamatrixcalledtheHessianmatrix.TheHessianmatrixisdeﬁnedsuchthatHx()(f)Hx()(f)i,j=∂2∂xi∂xjf.()x(4.6)Equivalently,theHessianistheJacobianofthegradient.Anywherethatthesecondpartialderivativesarecontinuous,thediﬀe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 101}, page_content='alledtheHessianmatrix.TheHessianmatrixisdeﬁnedsuchthatHx()(f)Hx()(f)i,j=∂2∂xi∂xjf.()x(4.6)Equivalently,theHessianistheJacobianofthegradient.Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerentialoperatorsarecommutative,i.e.theirordercanbeswapped:∂2∂xi∂xjf()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 101}, page_content='=x∂2∂xj∂xif.()x(4.7)ThisimpliesthatHi,j=Hj,i,sotheHessianmatrixissymmetricatsuchpoints.MostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetricHessianalmosteverywhere. BecausetheHessianmatrixisrealandsymmetric,wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof87'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 102}, page_content='CHAPTER4.NUMERICALCOMPUTATIONeigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunitvectordisgivenbyd\\ue03eHd.WhendisaneigenvectorofH,thesecondderivativeinthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsofd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,withweightsbetween0and1,andeigenvectorsthathavesmalleranglewithdreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecondderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.The(directional)secondderivativetellsushowwellwecanexpectagradientdescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximationtothefunctionaroundthecurrentpointf()xx(0):ff() x≈(x(0))+(xx−(0))\\ue03eg+12(xx−(0))\\ue03eHxx(−(0)).(4.8)wheregisthegradientandHistheHessianatx(0). Ifweusealearningrateof\\ue00f,thenthenewpointxwillbegivenbyx(0)−\\ue00fg.Substitutingthisintoourapproximation,weobtainf(x(0)−≈\\ue00fg) f(x(0))−\\ue00fg\\ue03eg+12\\ue00f2g\\ue03eHg.(4.9)Therearethree termshere:theoriginalvalue ofthefunction,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 102}, page_content='theexpectedimprovementduetotheslopeofthefunction,andthecorrectionwemustapplytoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,thegradientdescentstepcanactuallymoveuphill.Wheng\\ue03eHgiszeroornegative,theTaylorseriesapproximationpredictsthatincreasing\\ue00fforeverwilldecreasefforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge\\ue00f,soonemustresorttomoreheuristicchoicesof\\ue00finthiscase.Wheng\\ue03eHgispositive,solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximationofthefunctionthemostyields\\ue00f∗=g\\ue03egg\\ue03eHg.(4.10)Intheworstcase,whengalignswiththeeigenvectorofHcorrespondingtothemaximaleigenvalueλmax,thenthisoptimalstepsizeisgivenby1λmax.Totheextentthatthefunctionweminimizecanbeapproximatedwellbyaquadraticfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearningrate.Thesecondderivativecanbeusedtodeterminewhetheracriticalpointisalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacriticalpoint,f\\ue030(x)=0.Whenf\\ue030\\ue030(x)>0,thismeansthatf\\ue030(x)increasesaswemovetotheright,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 102}, page_content='e.Thesecondderivativecanbeusedtodeterminewhetheracriticalpointisalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacriticalpoint,f\\ue030(x)=0.Whenf\\ue030\\ue030(x)>0,thismeansthatf\\ue030(x)increasesaswemovetotheright,andf\\ue030(x)decreasesaswemovetotheleft.Thismeansf\\ue030(x\\ue00f−)<0and88'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 103}, page_content='CHAPTER4.NUMERICALCOMPUTATIONf\\ue030(x+\\ue00f)>0 forsmallenough\\ue00f.Inotherwords,aswemoveright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslopebeginstopointuphilltotheleft.Thus,whenf\\ue030(x) = 0andf\\ue030\\ue030(x)>0,wecanconcludethatxisalocalminimum.Similarly,whenf\\ue030(x) = 0andf\\ue030\\ue030(x)<0,wecanconcludethatxisalocalmaximum.Thisisknownasthesecondderivativetest.Unfortunately,whenf\\ue030\\ue030(x) = 0,thetestisinconclusive.Inthiscasexmaybeasaddlepoint,orapartofaﬂatregion.Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthefunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralizethesecondderivativetesttomultipledimensions.Atacriticalpoint,where∇xf(x) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 103}, page_content='= 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhetherthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.WhentheHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocalminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivativeinanydirectionmustbepositive,andmakingreferencetotheunivariatesecondderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvaluesarenegative),thepointisalocalmaximum.Inmultipledimensions,itisactuallypossibletoﬁndpositiveevidenceofsaddlepointsinsomecases.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 103}, page_content='Whenatleastoneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthatxisalocalmaximumononecrosssectionoffbutalocalminimumonanothercrosssection.SeeFig.foranexample.Finally,themultidimensionalsecond4.5derivativetestcanbeinconclusive,justliketheunivariateversion.Thetestisinconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butatleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestisinconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.Inmultipledimensions,therecanbeawidevarietyofdiﬀerentsecondderivativesatasinglepoint,becausethereisadiﬀerentsecondderivativeforeachdirection.TheconditionnumberoftheHessianmeasureshowmuchthesecondderivativesvary.WhentheHessianhasapoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinonedirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreasesslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnotknowthatitneedstoexplorepreferentiallyinthedirectionwherethederivativeremains'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 103}, page_content='veincreasesrapidly,whileinanotherdirection,itincreasesslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnotknowthatitneedstoexplorepreferentiallyinthedirectionwherethederivativeremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoinguphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthestepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithlesscurvature.SeeFig.foranexample.4.6ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixto89'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 104}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue031\\U000f0913\\ue031\\ue035\\ue030\\ue031\\ue035\\ue078\\ue032\\U000f0913\\ue031\\ue035\\ue030\\ue031\\ue035\\ue066\\ue078\\ue028\\ue031\\ue03b\\ue078\\ue032\\ue029\\U000f0913\\ue035\\ue030\\ue030\\ue030\\ue035\\ue030\\ue030\\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunctioninthisexampleisf(x)=x21−x22.Alongtheaxiscorrespondingtox1,thefunctioncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.Alongtheaxiscorrespondingtox2,thefunctioncurvesdownward.ThisdirectionisaneigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfromthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunctionwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalueof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegativeeigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocalmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\\n90'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 105}, page_content='CHAPTER4.NUMERICALCOMPUTATION\\n−−−30201001020x1−30−20−1001020x2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 105}, page_content='Figure4.6:GradientdescentfailstoexploitthecurvatureinformationcontainedintheHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunctionf(x) whoseHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvaturehasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themostcurvatureisinthedirection[1,1]\\ue03eandtheleastcurvatureisinthedirection[1,−1]\\ue03e.Theredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadraticfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescendingcanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhattoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedstodescendtheoppositecanyonwallonthenextiteration.ThelargepositiveeigenvalueoftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthatthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedontheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearchdirectioninthiscontext.\\n91'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 106}, page_content='CHAPTER4.NUMERICALCOMPUTATIONguidethesearch.ThesimplestmethodfordoingsoisknownasNewton’smethod.Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansiontoapproximatenearsomepointf()xx(0):ff() x≈(x(0))+(xx−(0))\\ue03e∇xf(x(0))+12(xx−(0))\\ue03eHx()(f(0))(xx−(0)).(4.11)Ifwethensolveforthecriticalpointofthisfunction,weobtain:x∗='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 106}, page_content='x(0)−Hx()(f(0))−1∇xf(x(0)).(4.12)Whenfisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsofapplyingEq.oncetojumptotheminimumofthefunctiondirectly.When4.12fisnottrulyquadraticbutcanbelocallyapproximatedasapositivedeﬁnitequadratic,Newton’smethodconsistsofapplyingEq.multipletimes.Iterativelyupdating4.12theapproximationandjumpingtotheminimumoftheapproximationcanreachthecriticalpointmuchfasterthangradientdescentwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmfulpropertynearasaddlepoint.AsdiscussedinSec.,Newton’smethodisonlyappropriatewhenthenearby8.2.3criticalpointisaminimum(alltheeigenvaluesoftheHessianarepositive),whereasgradientdescentisnotattractedtosaddlepointsunlessthegradientpointstowardthem.Optimizationalgorithmssuchasgradientdescentthatuseonlythegradientarecalledﬁrst-orderoptimizationalgorithms.OptimizationalgorithmssuchasNew-ton’smethodthatalsousetheHessianmatrixarecalledsecond-orderoptimizationalgorithms(NocedalandWright2006,).The optimizationalgorithms'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 106}, page_content='optimizationalgorithms employedin mostcontextsin thisbook areapplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.Thisisbecausethefamilyoffunctionsusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominantapproachtooptimizationistodesignoptimizationalgorithmsforalimitedfamilyoffunctions.Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-ingourselvestofunctionsthatareeitherLipschitzcontinuousorhaveLipschitzcontinuousderivatives.ALipschitzcontinuousfunctionisafunctionfwhoserateofchangeisboundedbyaLipschitzconstantL:∀∀|−|≤L||−||x,y,f()xf()yxy2.(4.13)Thispropertyisusefulbecauseitallowsustoquantifyourassumptionthatasmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhaveasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,92'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 107}, page_content='CHAPTER4.NUMERICALCOMPUTATIONandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuouswithrelativelyminormodiﬁcations.Perhapsthemostsuccessfulﬁeldofspecializedoptimizationisconvexoptimiza-tion.Convexoptimizationalgorithmsareabletoprovidemanymoreguaranteesbymakingstrongerrestrictions.Convexoptimizationalgorithmsareapplicableonlytoconvexfunctions—functionsforwhichtheHessianispositivesemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddlepointsandalloftheirlocalminimaarenecessarilyglobalminima.However,mostproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.Convexoptimizationisusedonlyasasubroutineofsomedeeplearningalgorithms.Ideasfromtheanalysisofconvexoptimizationalgorithmscanbeusefulforprovingtheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportanceofconvexoptimizationisgreatlydiminishedinthecontextofdeeplearning.Formoreinformationaboutconvexoptimization,seeBoydandVandenberghe2004()orRockafellar1997().4.4ConstrainedOptimizationSo'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 107}, page_content='al,theimportanceofconvexoptimizationisgreatlydiminishedinthecontextofdeeplearning.Formoreinformationaboutconvexoptimization,seeBoydandVandenberghe2004()orRockafellar1997().4.4ConstrainedOptimizationSometimeswewishnotonlytomaximizeorminimizeafunctionf(x)overallpossiblevaluesofx.Insteadwemaywishtoﬁndthemaximalorminimalvalueoff(x)forvaluesofxinsomesetS.Thisisknownasconstrainedoptimization.PointsxthatliewithinthesetSarecalledfeasiblepointsinconstrainedoptimizationterminology.Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommonapproachinsuchsituationsistoimposeanormconstraint,suchas.||||≤x1Onesimpleapproachtoconstrainedoptimizationissimplytomodifygradientdescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize\\ue00f,wecanmakegradientdescentsteps,thenprojecttheresultbackintoS.Ifweusealinesearch,wecansearchonlyoverstepsizes\\ue00fthatyieldnewxpointsthatarefeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradienti'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 107}, page_content='nesearch,wecansearchonlyoverstepsizes\\ue00fthatyieldnewxpointsthatarefeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradientintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginningthelinesearch(,).Rosen1960Amoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,constrainedoptimizationproblem.Forexample,ifwewanttominimizef(x)forx∈R2withxconstrainedtohaveexactlyunitL2norm,wecaninsteadminimize93'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 108}, page_content='CHAPTER4.NUMERICALCOMPUTATIONg(θ) =f([cossinθ,θ]\\ue03e)withrespecttoθ,thenreturn[cossinθ,θ]asthesolutiontotheoriginalproblem.Thisapproachrequirescreativity;thetransformationbetweenoptimizationproblemsmustbedesignedspeciﬁcallyforeachcaseweencounter.TheKarush–Kuhn–Tucker(KKT)approach1providesaverygeneralsolutiontoconstrainedoptimization.WiththeKKTapproach,weintroduceanewfunctioncalledthegeneralizedLagrangiangeneralizedLagrangefunctionor.TodeﬁnetheLagrangian,weﬁrstneedtodescribeSintermsofequationsandinequalities. WewantadescriptionofSintermsofmfunctionsg()iandnfunctionsh()jsothatS={|∀xi,g()i(x) = 0and∀j,h()j(x)≤0}.Theequationsinvolvingg()iarecalledtheequalityconstraintsandtheinequalitiesinvolvingh()jarecalledinequalityconstraints.Weintroducenewvariablesλiandαjforeachconstraint,thesearecalledtheKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedasL,,f(xλα) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 108}, page_content='= ()+x\\ue058iλig()i()+x\\ue058jαjh()j()x.(4.14)WecannowsolveaconstrainedminimizationproblemusingunconstrainedoptimizationofthegeneralizedLagrangian.Observethat,solongasatleastonefeasiblepointexistsandisnotpermittedtohavevalue,thenf()x∞minxmaxλmaxαα,≥0L,,.(xλα)(4.15)hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsasxminx∈Sf.()x(4.16)Thisfollowsbecauseanytimetheconstraintsaresatisﬁed,maxλmaxαα,≥0L,,f,(xλα) = ()x(4.17)whileanytimeaconstraintisviolated,maxλmaxαα,≥0L,,.(xλα) = ∞(4.18)Thesepropertiesguaranteethatnoinfeasiblepointwilleverbeoptimal,andthattheoptimumwithinthefeasiblepointsisunchanged.1TheKKTapproachgeneralizesthemethodofLagrangemultiplierswhichallowsequalityconstraintsbutnotinequalityconstraints.94'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 109}, page_content='CHAPTER4.NUMERICALCOMPUTATIONToperformconstrainedmaximization,wecanconstructthegeneralizedLa-grangefunctionof,whichleadstothisoptimizationproblem:−f()xminxmaxλmaxαα,≥0−f()+x\\ue058iλig()i()+x\\ue058jαjh()j()x.(4.19)Wemayalsoconvertthistoaproblemwithmaximizationintheouterloop:maxxminλminαα,≥0f()+x\\ue058iλig()i()x−\\ue058jαjh()j()x.(4.20)Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneitwithadditionorsubtractionaswewish,becausetheoptimizationisfreetochooseanysignforeachλi.Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstrainth()i(x)isifactiveh()i(x∗) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 109}, page_content='= 0.Ifaconstraintisnotactive,thenthesolutiontotheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionifthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludesothersolutions.Forexample,aconvexproblemwithanentireregionofgloballyoptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthisregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocalstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,thepointfoundatconvergenceremainsastationarypointwhetherornottheinactiveconstraintsareincluded.Becauseaninactiveh()ihasnegativevalue,thenthesolutiontominxmaxλmaxαα,≥0L(xλα,,)willhaveαi=0.Wecanthusobservethatatthesolution,αh(x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 109}, page_content='=0.Inotherwords,foralli,weknowthatatleastoneoftheconstraintsαi≥0andh()i(x)≤0mustbeactiveatthesolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolutionisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultipliertoinﬂuencethesolutiontox,ortheinequalityhasnoinﬂuenceonthesolutionandwerepresentthisbyzeroingoutitsKKTmultiplier.ThepropertiesthatthegradientofthegeneralizedLagrangianiszero,allconstraintsonbothxandtheKKTmultipliersaresatisﬁed,andαh\\ue00c(x) =0arecalledtheKarush-Kuhn-Tucker(KKT)conditions(,;Karush1939KuhnandTucker1951,).Together,thesepropertiesdescribetheoptimalpointsofconstrainedoptimizationproblems.FormoreinformationabouttheKKTapproach,seeNocedalandWright2006().95'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 110}, page_content='CHAPTER4.NUMERICALCOMPUTATION4.5Example:LinearLeastSquaresSupposewewanttoﬁndthevalueofthatminimizesxf() =x12||−||Axb22.(4.21)Therearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.However,wecanalsoexplorehowtosolveitusinggradient-basedoptimizationasasimpleexampleofhowthesetechniqueswork.First,weneedtoobtainthegradient:∇xf() = xA\\ue03e() = Axb−A\\ue03eAxA−\\ue03eb.(4.22)Wecanthenfollowthisgradientdownhill,takingsmallsteps.SeeAlgorithm4.1fordetails.Algorithm4.1Analgorithmtominimizef(x) =12||−||Axb22withrespecttoxusinggradientdescent.Setthestepsize()andtolerance()tosmall,positivenumbers.\\ue00fδwhile||A\\ue03eAxA−\\ue03eb||2>δdoxx←−\\ue00f\\ue000A\\ue03eAxA−\\ue03eb\\ue001endwhileOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,becausethetruefunctionisquadratic,thequadraticapproximationemployedbyNewton’smethodisexact,andthealgorithmconvergestotheglobalminimuminasinglestep.Nowsuppose we wishto minimizethesame function,butsubject totheconstraintx\\ue03ex≤1.Todoso,weintroducetheLagrangianL,λfλ(x) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 110}, page_content='we wishto minimizethesame function,butsubject totheconstraintx\\ue03ex≤1.Todoso,weintroducetheLagrangianL,λfλ(x) = ()+x\\ue010x\\ue03ex−1\\ue011.(4.23)Wecannowsolvetheproblemminxmaxλ,λ≥0L,λ.(x)(4.24)Thesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybefoundusingtheMoore-Penrosepseudoinverse:x=A+b.Ifthispointisfeasible,thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda96'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 111}, page_content='CHAPTER4.NUMERICALCOMPUTATIONsolutionwheretheconstraintisactive.BydiﬀerentiatingtheLagrangianwithrespectto,weobtaintheequationxA\\ue03eAxA−\\ue03ebx+2λ= 0.(4.25)ThistellsusthatthesolutionwilltaketheformxA= (\\ue03eAI+2λ)−1A\\ue03eb.(4.26)Themagnitudeofλmustbechosensuchthattheresultobeystheconstraint.Wecanﬁndthisvaluebyperforminggradientascenton.Todoso,observeλ∂∂λL,λ(x) = x\\ue03ex−1.(4.27)Whenthenormofxexceeds1,thisderivativeispositive,sotofollowthederivativeuphillandincreasetheLagrangianwithrespecttoλ,weincreaseλ.Becausethecoeﬃcientonthex\\ue03expenaltyhasincreased,solvingthelinearequationforxwillnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequationandadjustingλcontinuesuntilxhasthecorrectnormandthederivativeonλis0.Thisconcludesthemathematicalpreliminariesthatweusetodevelopmachinelearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedgedlearningsystems.\\n97'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 112}, page_content='Chapter5MachineLearningBasicsDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstanddeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesofmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneralprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersorthosewhowantawiderperspectiveareencouragedtoconsidermachinelearningtextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy2012Bishop2006()or().Ifyouarealreadyfamiliarwithmachinelearningbasics,feelfreetoskipaheadtoSec..Thatsectioncoverssomeper-5.11spectivesontraditionalmachinelearningtechniquesthathavestronglyinﬂuencedthedevelopmentofdeeplearningalgorithms.Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentanexample:thelinearregressionalgorithm.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 112}, page_content='Wethenproceedtodescribehowthechallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatternsthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettingscalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithmitself;wediscusshowtosettheseusingadditionaldata.Machinelearningisessentiallyaformofappliedstatisticswithincreasedemphasisontheuseofcomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasisonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthetwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervisedlearningandunsupervisedlearning;wedescribethesecategoriesandgivesomeexamplesofsimplelearningalgorithmsfromeachcategory.Mostdeeplearningalgorithmsare basedonan optimizationalgorithmcalled stochasticgradientdescent.Wedescribehowtocombinevariousalgorithmcomponentssuchasan98'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 113}, page_content='CHAPTER5.MACHINELEARNINGBASICSoptimizationalgorithm,acostfunction,amodel,andadatasettobuildamachinelearningalgorithm.Finally,inSec.,wedescribesomeofthefactorsthathave5.11limitedtheabilityoftraditionalmachinelearningtogeneralize.Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthatovercometheseobstacles.5.1LearningAlgorithmsAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.Butwhatdowemeanbylearning?()providesthedeﬁnition“AcomputerMitchell1997programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksTandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,improveswithexperienceE.”OnecanimagineaverywidevarietyofexperiencesE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthisbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthediﬀerentkindsoftasks,performancemeasuresandexperiencesthatcanbeusedtoconstructmachinelearningalgorithms.5.1.1TheTask,TMachinel'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 113}, page_content='s.Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthediﬀerentkindsoftasks,performancemeasuresandexperiencesthatcanbeusedtoconstructmachinelearningalgorithms.5.1.1TheTask,TMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewithﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcandphilosophicalpointofview,machinelearningisinterestingbecausedevelopingourunderstandingofmachinelearningentailsdevelopingourunderstandingoftheprinciplesthatunderlieintelligence.Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearningitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthetask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywriteaprogramthatspeciﬁeshowtowalkmanually.Machinelearningtasksareusuallydescribedintermsofhowthemachinelearningsystemshouldprocessan.Anexampleisacollectionofexamplefeaturesthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewantthem'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 113}, page_content='.Machinelearningtasksareusuallydescribedintermsofhowthemachinelearningsystemshouldprocessan.Anexampleisacollectionofexamplefeaturesthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewantthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasavectorx∈Rnwhereeachentryxiofthevectorisanotherfeature.Forexample,thefeaturesofanimageareusuallythevaluesofthepixelsintheimage.99'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 114}, page_content='CHAPTER5.MACHINELEARNINGBASICSManykindsoftaskscanbesolvedwithmachinelearning.Someofthemostcommonmachinelearningtasksincludethefollowing:•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecifywhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearningalgorithmisusuallyaskedtoproduceafunctionf:Rn→{1,...,k}.Wheny=f(x),themodelassignsaninputdescribedbyvectorxtoacategoryidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcationtask,forexample,wherefoutputsaprobabilitydistributionoverclasses.Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinputisanimage(usuallydescribedasasetofpixelbrightnessvalues),andtheoutputisanumericcodeidentifyingtheobjectintheimage.Forexample,theWillowGaragePR2robotisabletoactasawaiterthatcanrecognizediﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwithdeeplearning(,;,).ObjectKrizhevskyetal.2012IoﬀeandSzegedy2015recognitionisthesamebasictechnologythatallowscomputerstoreco'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 114}, page_content='eoncommand(Good-fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwithdeeplearning(,;,).ObjectKrizhevskyetal.2012IoﬀeandSzegedy2015recognitionisthesamebasictechnologythatallowscomputerstorecognizefaces(Taigman2014etal.,),whichcanbeusedtoautomaticallytagpeopleinphotocollectionsandallowcomputerstointeractmorenaturallywiththeirusers.•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechallengingifthecomputerprogramisnotguaranteedthateverymeasurementinitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcationtask,thelearningalgorithmonlyhastodeﬁneafunctionmappingfromavectorsingleinputtoacategoricaloutput.Whensomeoftheinputsmaybemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearningalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassifyingsetxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituationarisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltestsareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargesetoffunctionsistolearnaprobabili'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 114}, page_content='entsubsetofitsinputsmissing.Thiskindofsituationarisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltestsareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargesetoffunctionsistolearnaprobabilitydistributionoveralloftherelevantvariables,thensolvetheclassiﬁcationtaskbymarginalizingoutthemissingvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁcationfunctionsneededforeachpossiblesetofmissinginputs,butweonlyneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.SeeGoodfellowetal.()foranexampleofadeepprobabilisticmodelappliedtosuchatask2013binthisway.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 114}, page_content='Manyoftheothertasksdescribedinthissectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcationwithmissinginputsisjustoneexampleofwhatmachinelearningcando.100'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 115}, page_content='CHAPTER5.MACHINELEARNINGBASICS•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredictanumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithmisaskedtooutputafunctionf:Rn→R.Thistypeoftaskissimilartoclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleofaregressiontaskisthepredictionoftheexpectedclaimamountthataninsuredpersonwillmake(usedtosetinsurancepremiums),orthepredictionoffuturepricesofsecurities.Thesekindsofpredictionsarealsousedforalgorithmictrading.•Transcription:Inthistypeoftask,themachinelearningsystemisaskedtoobservearelativelyunstructuredrepresentationofsomekindofdataandtranscribeitintodiscrete,textualform.Forexample,inopticalcharacterrecognition,thecomputerprogramisshownaphotographcontaininganimageoftextandisaskedtoreturnthistextintheformofasequenceofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewusesdeeplearningtoprocessaddressnumbersinthisway(Goodfellowetal.,2014d).Anotherexampleisspeechrecognition,wherethecomputerprogramisprovidedanaudi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 115}, page_content='haracters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewusesdeeplearningtoprocessaddressnumbersinthisway(Goodfellowetal.,2014d).Anotherexampleisspeechrecognition,wherethecomputerprogramisprovidedanaudiowaveformandemitsasequenceofcharactersorwordIDcodesdescribingthewordsthatwerespokenintheaudiorecording.DeeplearningisacrucialcomponentofmodernspeechrecognitionsystemsusedatmajorcompaniesincludingMicrosoft,IBMandGoogle(Hintonetal.,2012b).•Machinetranslation:Inamachinetranslationtask,theinputalreadyconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogrammustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisiscommonlyappliedtonaturallanguages,suchastotranslatefromEnglishtoFrench.Deeplearninghasrecentlybeguntohaveanimportantimpactonthiskindoftask(Sutskever2014Bahdanau2015etal.,;etal.,).•Structuredoutput:Structuredoutputtasksinvolveanytaskwheretheoutputisavector(orotherdatastructurecontainingmultiplevalues)withimportantrelationshipsbetweenthediﬀerentelements.Thisisabroadcategory,an'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 115}, page_content=').•Structuredoutput:Structuredoutputtasksinvolveanytaskwheretheoutputisavector(orotherdatastructurecontainingmultiplevalues)withimportantrelationshipsbetweenthediﬀerentelements.Thisisabroadcategory,andsubsumesthetranscriptionandtranslationtasksdescribedabove,butalsomanyothertasks.Oneexampleisparsing—mappinganaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructureandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.See()Collobert2011foranexampleofdeeplearningappliedtoaparsingtask.Anotherexampleispixel-wisesegmentationofimages,wherethecomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.Forexample,deeplearningcan101'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 116}, page_content='CHAPTER5.MACHINELEARNINGBASICSbeusedtoannotatethelocationsofroadsinaerialphotographs(MnihandHinton2010,). Theoutputneednothaveitsformmirrorthestructureoftheinputascloselyasintheseannotation-styletasks.Forexample,inimagecaptioning,thecomputerprogramobservesanimageandoutputsanaturallanguagesentencedescribingtheimage(,,;,Kirosetal.2014abMaoetal.2015Vinyals2015bDonahue2014KarpathyandLi2015;etal.,;etal.,;,;Fang2015Xu2015etal.,;etal.,).Thesetasksarecalledstructuredoutputtasksbecausetheprogrammustoutputseveralvaluesthatarealltightlyinter-related.Forexample,thewordsproducedbyanimagecaptioningprogrammustformavalidsentence.•Anomalydetection:Inthistypeoftask,thecomputerprogramsiftsthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusualoratypical.Anexampleofananomalydetectiontaskiscreditcardfrauddetection.Bymodelingyourpurchasinghabits,acreditcardcompanycandetectmisuseofyourcards.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 116}, page_content='Ifathiefstealsyourcreditcardorcreditcardinformation,thethief’spurchaseswilloftencomefromadiﬀerentprobabilitydistributionoverpurchasetypesthanyourown.Thecreditcardcompanycanpreventfraudbyplacingaholdonanaccountassoonasthatcardhasbeenusedforanuncharacteristicpurchase.See()forasurveyofChandolaetal.2009anomalydetectionmethods.•Synthesisandsampling:Inthistypeoftask,themachinelearningalgorithmisaskedtogeneratenewexamplesthataresimilartothoseinthetrainingdata.Synthesisandsamplingviamachinelearningcanbeusefulformediaapplicationswhereitcanbeexpensiveorboringforanartisttogeneratelargevolumesofcontentbyhand.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 116}, page_content='Forexample,videogamescanautomaticallygeneratetexturesforlargeobjectsorlandscapes,ratherthanrequiringanartisttomanuallylabeleachpixel(,).Insomecases,weLuoetal.2013wantthesamplingorsynthesisproceduretogeneratesomespeciﬁckindofoutputgiventheinput.Forexample,inaspeechsynthesistask,weprovideawrittensentenceandasktheprogramtoemitanaudiowaveformcontainingaspokenversionofthatsentence.Thisisakindofstructuredoutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrectoutputforeachinput,andweexplicitlydesirealargeamountofvariationintheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.•Imputationofmissingvalues:Inthistypeoftask,themachinelearningalgorithmisgivenanewexamplex∈Rn,butwithsomeentriesxiofxmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissingentries.102'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 117}, page_content='CHAPTER5.MACHINELEARNINGBASICS•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenininputacorruptedexample˜x∈Rnobtainedbyanunknowncorruptionprocessfromacleanexamplex∈Rn.Thelearnermustpredictthecleanexamplexfromitscorruptedversion˜x,ormoregenerallypredicttheconditionalprobabilitydistributionp(x|˜x).•Densityestimationprobabilitymassfunctionestimationor:Inthedensityestimationproblem, themachinelearningalgorithmisaskedtolearnafunctionpmodel:Rn→R,wherepmodel(x) canbeinterpretedasaprobabilitydensityfunction(ifxiscontinuous)oraprobabilitymassfunction(ifxisdiscrete)onthespacethattheexamplesweredrawnfrom.Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwediscussperformancemeasuresP), the algorithmneeds tolearn thestructure ofthe dataithasseen.Itmustknowwhereexamplesclustertightlyandwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequirethatthelearningalgorithmhasat leastimplicitlycaptured thestructure'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 117}, page_content='oftheprobabilitydistribution.Densityestimationallowsustoexplicitlycapturethatdistribution.Inprinciple,wecanthenperformcomputationsonthatdistributioninordertosolvetheothertasksaswell.Forexample,ifwehaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),wecanusethatdistributiontosolvethemissingvalueimputationtask.Ifavaluexiismissingandalloftheothervalues,denotedx−i,aregiven,thenweknowthedistributionoveritisgivenbyp(xi|x−i).Inpractice,densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,becauseinmanycasestherequiredoperationsonp(x)arecomputationallyintractable.Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftaskswelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcando,nottodeﬁnearigidtaxonomyoftasks.5.1.2ThePerformanceMeasure,PInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesignaquantitativemeasureofitsperformance.UsuallythisperformancemeasurePisspeciﬁctothetaskbeingcarriedoutbythesystem.TFortaskssuchasclassiﬁcation,class'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 117}, page_content='tetheabilitiesofamachinelearningalgorithm,wemustdesignaquantitativemeasureofitsperformance.UsuallythisperformancemeasurePisspeciﬁctothetaskbeingcarriedoutbythesystem.TFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtranscrip-tion,weoftenmeasuretheaccuracyofthemodel.Accuracyisjusttheproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecanalsoobtain103'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 118}, page_content='CHAPTER5.MACHINELEARNINGBASICSequivalentinformationbymeasuringtheerrorrate,theproportionofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenrefertotheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0ifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1loss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodelacontinuous-valuedscoreforeachexample.Themostcommonapproachistoreporttheaveragelog-probabilitythemodelassignstosomeexamples.Usuallyweareinterestedinhowwellthemachinelearningalgorithmperformsondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhendeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusingaofdatathatisseparatefromthedatausedfortrainingthemachinetestsetlearningsystem.Thechoiceofperformancemeasuremayseemstraightforwardandobjective,butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswelltothedesiredbehaviorofthesystem.Insomecase'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 118}, page_content='machinetestsetlearningsystem.Thechoiceofperformancemeasuremayseemstraightforwardandobjective,butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswelltothedesiredbehaviorofthesystem.Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracyofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grainedperformancemeasurethatgivespartialcreditforgettingsomeelementsofthesequencecorrect?Whenperformingaregressiontask,shouldwepenalizethesystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakesverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.Inothercases,weknowwhatquantitywewouldideallyliketomeasure,butmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextofdensityestimation.Manyofthebestprobabilisticmodelsrepresentprobabilitydistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedtoaspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,onem'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 118}, page_content='timation.Manyofthebestprobabilisticmodelsrepresentprobabilitydistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedtoaspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,onemustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,ordesignagoodapproximationtothedesiredcriterion.5.1.3TheExperience,EMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedsu-orpervisedbywhatkindofexperiencetheyareallowedtohaveduringthelearningprocess.Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowedtoexperienceanentire.Adatasetisacollectionofmanyexamples,asdataset104'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 119}, page_content='CHAPTER5.MACHINELEARNINGBASICSdeﬁnedinSec..Sometimeswewillalsocallexamples5.1.1datapoints.Oneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-searchersistheIrisdataset(,).ItisacollectionofmeasurementsofFisher1936diﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.Thefeatureswithineachexamplearethemeasurementsofeachofthepartsoftheplant:thesepallength,sepalwidth,petallengthandpetalwidth.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 119}, page_content='Thedatasetalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesarerepresentedinthedataset.Unsupervisedlearningalgorithmsexperienceadatasetcontainingmanyfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontextofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthatgeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfortaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithmsperformotherroles,likeclustering,whichconsistsofdividingthedatasetintoclustersofsimilarexamples.Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures,buteachexampleisalsoassociatedwithalabeltargetor.Forexample,theIrisdatasetisannotatedwiththespeciesofeachirisplant.AsupervisedlearningalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothreediﬀerentspeciesbasedontheirmeasurements.Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamplesofarandomvectorx,andattemptingtoimplicitlyorexplicitlylearntheproba-bilitydistrib'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 119}, page_content='tsintothreediﬀerentspeciesbasedontheirmeasurements.Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamplesofarandomvectorx,andattemptingtoimplicitlyorexplicitlylearntheproba-bilitydistributionp(x),orsomeinterestingpropertiesofthatdistribution,whilesupervisedlearninginvolvesobservingseveralexamplesofarandomvectorxandanassociatedvalueorvectory,andlearningtopredictyfromx,usuallybyestimatingp(yx|).Thetermsupervisedlearningoriginatesfromtheviewofthetargetybeingprovidedbyaninstructororteacherwhoshowsthemachinelearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructororteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescanbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystatesthatforavectorx∈Rn,thejointdistributioncanbedecomposedasp()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 119}, page_content='=xn\\ue059i=1p(xi|x1,...,xi−1).(5.1)Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemofmodelingp(x) bysplittingitintonsupervisedlearningproblems.Alternatively,we105'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 120}, page_content='CHAPTER5.MACHINELEARNINGBASICScansolvethesupervisedlearningproblemoflearningp(y|x)byusingtraditionalunsupervised learningtechnologiesto learn thejointdistributionp(x,y)andinferringpy(|x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 120}, page_content='=p,y(x)\\ue050y\\ue030p,y(x\\ue030).(5.2)Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalordistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowithmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcationandstructuredoutputproblemsassupervisedlearning.Densityestimationinsupportofothertasksisusuallyconsideredunsupervisedlearning.Othervariantsofthelearningparadigmarepossible.Forexample,insemi-supervisedlearning,someexamplesincludeasupervisiontargetbutothersdonot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledascontainingornotcontaininganexampleofaclass,buttheindividualmembersofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearningwithdeepmodels,see().Kotziasetal.2015Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.Forexample,reinforcementlearningalgorithmsinteractwithanenvironment,sothereisafeedbackloopbetweenthelearningsystemanditsexperiences.Suchalgorithmsarebeyondthescopeofthisbook.Pleasesee()orSuttonandBart'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 120}, page_content='Forexample,reinforcementlearningalgorithmsinteractwithanenvironment,sothereisafeedbackloopbetweenthelearningsystemanditsexperiences.Suchalgorithmsarebeyondthescopeofthisbook.Pleasesee()orSuttonandBarto1998BertsekasandTsitsiklis1996Mnih()forinformationaboutreinforcementlearning,andetal.()forthedeeplearningapproachtoreinforcementlearning.2013Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcanbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,whichareinturncollectionsoffeatures.Onecommonwayofdescribingadatasetiswitha.Adesigndesignmatrixmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthematrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains150exampleswithfourfeaturesforeachexample.ThismeanswecanrepresentthedatasetwithadesignmatrixX∈R1504×,whereXi,1isthesepallengthofplanti,Xi,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearningalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.Ofcourse,todescribeadatasetasades'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 120}, page_content='04×,whereXi,1isthesepallengthofplanti,Xi,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearningalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibletodescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographswithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerentnumbersofpixels,sonotallofthephotographsmaybedescribedwiththesamelengthofvector.Sec.andChapterdescribehowtohandlediﬀerenttypes9.710106'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 121}, page_content='CHAPTER5.MACHINELEARNINGBASICSofsuchheterogeneousdata.Incaseslikethese,ratherthandescribingthedatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:{x(1),x(2),...,x()m}.Thisnotationdoesnotimplythatanytwoexamplevectorsx()iandx()jhavethesamesize.Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetaswellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithmtoperformobjectrecognitionfromphotographs,weneedtospecifywhichobjectappearsineachofthephotos.Wemightdothiswithanumericcode,with0signifyingaperson,1signifyingacar,2signifyingacat,etc.OftenwhenworkingwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealsoprovideavectoroflabels,withyyiprovidingthelabelforexample.iOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.Forexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentiresentences,thenthelabelforeachexamplesentenceisasequenceofwords.Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,thereisnorigidtaxonomyofdatasetso'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 121}, page_content='echrecognitionsystemtotranscribeentiresentences,thenthelabelforeachexamplesentenceisasequenceofwords.Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedherecovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.5.1.4Example:LinearRegressionOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapableofimprovingacomputerprogram’sperformanceatsometaskviaexperienceissomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofasimplemachinelearningalgorithm:linearregression.Wewillreturntothisexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelptounderstanditsbehavior.Asthenameimplies,linearregressionsolvesaregressionproblem.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 121}, page_content='Inotherwords,thegoalistobuildasystemthatcantakeavectorx∈Rnasinputandpredictthevalueofascalary∈Rasitsoutput.Inthecaseoflinearregression,theoutputisalinearfunctionoftheinput. Letˆybethevaluethatourmodelpredictsshouldtakeon.Wedeﬁnetheoutputtobeyˆy= w\\ue03ex(5.3)wherew∈Rnisavectorofparameters.Parametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,wiisthecoeﬃcientthatwemultiplybyfeaturexibeforesummingupthecontributionsfromallthefeatures.Wecanthinkofwasasetofthatdeterminehowweightseachfeatureaﬀectstheprediction.Ifafeaturexireceivesapositiveweightwi,107'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 122}, page_content='CHAPTER5.MACHINELEARNINGBASICSthenincreasingthevalueofthatfeatureincreasesthevalueofourpredictionˆy.Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeaturedecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,thenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasnoeﬀectontheprediction.WethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputtingˆy= w\\ue03ex.Nextweneedadeﬁnitionofourperformancemeasure,.PSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnotusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohaveavectorofregressiontargetsprovidingthecorrectvalueofyforeachoftheseexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetestset.WerefertothedesignmatrixofinputsasX()testandthevectorofregressiontargetsasy()test.Onewayofmeasuringtheperformanceofthemodelistocomputethemeansquarederrorofthemodelonthetestset.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 122}, page_content='Ifˆy()testgivesthepredictionsofthemodelonthetestset,thenthemeansquarederrorisgivenbyMSEtest=1m\\ue058i(ˆy()test−y()test)2i.(5.4)Intuitively,onecanseethatthiserrormeasuredecreasesto0whenˆy()test=y()test.WecanalsoseethatMSEtest=1m||ˆy()test−y()test||22,(5.5)sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictionsandthetargetsincreases.Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthatwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithmisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).Oneintuitivewayofdoingthis(whichwewilljustifylater,inSec.)isjustto5.5.1minimizethemeansquarederroronthetrainingset,MSEtrain.TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis:0∇wMSEtrain= 0(5.6)⇒∇w1m||ˆy()train−y()train||22= 0(5.7)⇒1m∇w||X()trainwy−()train||22= 0(5.8)108'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 123}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n−−10.05000510....x1−3−2−10123yLinearregressionexample\\n051015...w1020.025.030.035.040.045.050.055.MSE(train)Optimizationofw'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 123}, page_content='Figure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorwcontainsonlyasingleparametertolearn,w1.(Left)Observethatlinearregressionlearnstosetw1suchthattheliney=w1xcomesascloseaspossibletopassingthroughallthetrainingpoints.Theplottedpointindicatesthevalueof(Right)w1foundbythenormalequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.⇒∇w\\ue010X()trainwy−()train\\ue011\\ue03e\\ue010X()trainwy−()train\\ue011= 0(5.9)⇒∇w\\ue010w\\ue03eX()train\\ue03eX()trainww−2\\ue03eX()train\\ue03ey()train+y()train\\ue03ey()train\\ue011= 0(5.10)⇒2X()train\\ue03eX()trainwX−2()train\\ue03ey()train='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 123}, page_content='0(5.9)⇒∇w\\ue010w\\ue03eX()train\\ue03eX()trainww−2\\ue03eX()train\\ue03ey()train+y()train\\ue03ey()train\\ue011= 0(5.10)⇒2X()train\\ue03eX()trainwX−2()train\\ue03ey()train= 0(5.11)⇒w=\\ue010X()train\\ue03eX()train\\ue011−1X()train\\ue03ey()train(5.12)ThesystemofequationswhosesolutionisgivenbyEq.isknownasthe5.12normalequations.EvaluatingEq.constitutesasimplelearningalgorithm.5.12Foranexampleofthelinearregressionlearningalgorithminaction,seeFig..5.1Itisworthnotingthatthetermlinearregressionisoftenusedtorefertoaslightlymoresophisticatedmodelwithoneadditionalparameter—aninterceptterm.Inthismodelbˆy= w\\ue03ex+b(5.13)sothemappingfromparameterstopredictionsisstillalinearfunctionbutthemappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensiontoaﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikealine,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameterb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan109'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 124}, page_content='CHAPTER5.MACHINELEARNINGBASICSextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry11playstheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”whenreferringtoaﬃnefunctionsthroughoutthisbook.Theintercepttermbisoftencalledtheparameteroftheaﬃnetransfor-biasmation.Thisterminologyderivesfromthepointofviewthattheoutputofthetransformationisbiasedtowardbeingbintheabsenceofanyinput.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 124}, page_content='Thistermisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimationalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,butitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequentsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithmdesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicatedlearningalgorithms.5.2Capacity,OverﬁttingandUnderﬁttingThecentralchallengeinmachinelearningisthatwemustperformwellonnew,previouslyunseeninputs—notjustthoseonwhichourmodelwastrained.Theabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.Typically,whentrainingamachinelearningmodel,wehaveaccesstoatrainingset,wecancomputesomeerrormeasureonthetrainingsetcalledthetrainingerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimplyanoptimizationproblem.Whatseparatesmachinelearningfromoptimizationisthatwewantthegeneralizationerrortesterror,alsocalled'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 124}, page_content='dthetrainingerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimplyanoptimizationproblem.Whatseparatesmachinelearningfromoptimizationisthatwewantthegeneralizationerrortesterror,alsocalledthe,tobelowaswell.Thegeneralizationerrorisdeﬁnedastheexpectedvalueoftheerroronanewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawnfromthedistributionofinputsweexpectthesystemtoencounterinpractice.Wetypicallyestimatethegeneralizationerrorofamachinelearningmodelbymeasuringitsperformanceonaofexamplesthatwerecollectedseparatelytestsetfromthetrainingset.Inourlinearregressionexample,wetrainedthemodelbyminimizingthetrainingerror,1m()train||X()trainwy−()train||22,(5.14)butweactuallycareaboutthetesterror,1m()test||X()testwy−()test||22.Howcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythetrainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.Ifthe110'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 125}, page_content='CHAPTER5.MACHINELEARNINGBASICStrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecando.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtestsetarecollected,thenwecanmakesomeprogress.Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasetscalledthedatageneratingprocess.WetypicallymakeasetofassumptionsknowncollectivelyastheTheseassumptionsarethattheexamplesi.i.d.assumptionsineachdatasetareindependentfromeachother,andthatthetrainsetandtestsetareidenticallydistributed,drawnfromthesameprobabilitydistributionaseachother.Thisassumptionallowsustodescribethedatageneratingprocesswithaprobabilitydistributionoverasingleexample.Thesamedistributionisthenusedtogenerateeverytrainexampleandeverytestexample.Wecallthatsharedunderlyingdistributionthedatageneratingdistribution,denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowustomathematicallystudytherelationshipbetweentrainingerrorandtesterror.Oneimmediateconnectionwecanobservebetweenthetrainingandtest'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 125}, page_content='on,denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowustomathematicallystudytherelationshipbetweentrainingerrorandtesterror.Oneimmediateconnectionwecanobservebetweenthetrainingandtesterroristhattheexpectedtrainingerrorofarandomlyselectedmodelisequaltotheexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetestset.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameastheexpectedtestseterror,becausebothexpectationsareformedusingthesamedatasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthenameweassigntothedatasetwesample.Ofcourse,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 125}, page_content='when weuseamachinelearningalgorithm, wedo notﬁxtheparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethetestset. Underthisprocess,theexpectedtesterrorisgreaterthanorequaltotheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachinelearningalgorithmwillperformareitsabilityto:1. Makethetrainingerrorsmall.2. Makethegapbetweentrainingandtesterrorsmall.Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:underﬁttingoverﬁttingand.Underﬁttingoccurswhenthemodelisnotabletoobtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhenthegapbetweenthetrainingerrorandtesterroristoolarge.Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyalteringitscapacity. Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof111'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 126}, page_content='CHAPTER5.MACHINELEARNINGBASICSfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Modelswithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdonotservethemwellonthetestset.Onewaytocontrolthecapacityofalearningalgorithmisbychoosingitshypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedtoselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthesetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralizelinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,initshypothesisspace.Doingsoincreasesthemodel’scapacity.Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwearealreadyfamiliar,withpredictionˆybwx.= +(5.15)Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,wecanlearnamodelthatisquadraticasafunctionof:xˆybw='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 126}, page_content='+(5.15)Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,wecanlearnamodelthatisquadraticasafunctionof:xˆybw= +1xw+2x2.(5.16)Thoughthismodelimplementsaquadraticfunctionofitsinput,theoutputisstillalinearfunctionoftheparameters,sowecanstillusethenormalequationstotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxasadditionalfeatures,forexampletoobtainapolynomialofdegree9:ˆyb='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 126}, page_content='+9\\ue058i=1wixi.(5.17)Machinelearningalgorithmswillgenerallyperformbestwhentheircapacityisappropriateinregardtothetruecomplexityofthetasktheyneedtoperformandtheamountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacityareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplextasks,butwhentheircapacityishigherthanneededtosolvethepresenttasktheymayoverﬁt.Fig.showsthisprincipleinaction.Wecomparealinear,quadraticand5.2degree-9predictorattemptingtoﬁtaproblemwherethetrueunderlyingfunctionisquadratic.Thelinearfunctionisunabletocapturethecurvatureinthetrueun-derlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableofrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitelymanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewehavemore112'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 127}, page_content='CHAPTER5.MACHINELEARNINGBASICSparametersthantrainingexamples.Wehavelittlechanceofchoosingasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.Inthisexample,thequadraticmodelisperfectlymatchedtothetruestructureofthetasksoitgeneralizeswelltonewdata.\\n\\ue078\\ue030\\ue079\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\n\\ue078\\ue030\\ue079\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\n\\ue078\\ue030\\ue079\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 127}, page_content='Figure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawasgeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministicallybyevaluatingaquadraticfunction.(Left)Alinearfunctionﬁttothedatasuﬀersfromunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata.()ACenterquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfromasigniﬁcantamountofoverﬁttingorunderﬁtting.()Apolynomialofdegree9ﬁttoRightthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolvetheunderdeterminednormalequations.Thesolutionpassesthroughallofthetrainingpointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrueunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetruefunctiondecreasesinthisarea.Sofarwehaveonlydescribedchangingamodel’scapacitybychangingthenumber ofinput features ithas (andsimultaneouslyadding'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 127}, page_content='ofinput features ithas (andsimultaneouslyadding newparametersassociatedwiththosefeatures).Thereareinfactmanywaysofchangingamodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.Themodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefromwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalledtherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebestfunctionwithinthisfamilyisaverydiﬃcultoptimizationproblem.Inpractice,thelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyonethatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchastheimperfection113'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 128}, page_content='CHAPTER5.MACHINELEARNINGBASICSoftheoptimizationalgorithm,meanthatthelearningalgorithm’seﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodelfamily.OurmodernideasaboutimprovingthegeneralizationofmachinelearningmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearlyasPtolemy.ManyearlyscholarsinvokeaprincipleofparsimonythatisnowmostwidelyknownasOccam’srazor(c. 1287-1347).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 128}, page_content='Thisprinciplestatesthatamongcompetinghypothesesthatexplainknownobservationsequallywell,oneshouldchoosethe“simplest”one.Thisideawasformalizedandmademorepreciseinthe20thcenturybythefoundersofstatisticallearningtheory(VapnikandChervonenkis1971Vapnik1982Blumer1989Vapnik1995,;,;etal.,;,).Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,orVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.TheVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthereexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.Quantifyingthecapacityofthemodelallowsstatisticallearningtheorytomakequantitativepredictions.Themostimportantresultsinstatisticallearningtheoryshowthatthediscrepancybetweentrainingerrorandgeneralizationerrorisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbutshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,1971Vapnik1982Blumer1989Vapnik1995;,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 128}, page_content='iningerrorandgeneralizationerrorisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbutshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,1971Vapnik1982Blumer1989Vapnik1995;,;etal.,;,).Theseboundsprovideintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyarerarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisinpartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequitediﬃculttodeterminethecapacityofdeeplearningalgorithms.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 128}, page_content='Theproblemofdeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausetheeﬀectivecapacityislimitedbythecapabilitiesoftheoptimizationalgorithm,andwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimizationproblemsinvolvedindeeplearning.Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize(tohaveasmallgapbetweentrainingandtesterror)wemuststillchooseasuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,trainingerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodelcapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,generalizationerrorhasaU-shapedcurveasafunctionofmodelcapacity.ThisisillustratedinFig..5.3Toreachthemostextremecaseofarbitrarilyhighcapacity,weintroducetheconceptofnon-parametricmodels.Sofar, wehaveseenonlyparametric114'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 129}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n0OptimalCapacityCapacityErrorUnderﬁttingzoneOverﬁttingzoneGeneralizationgapTrainingerrorGeneralizationerror'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 129}, page_content='Figure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterrorbehavediﬀerently. Attheleftendofthegraph,trainingerrorandgeneralizationerrorarebothhigh.Thisistheunderﬁttingregime.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 129}, page_content='Asweincreasecapacity,trainingerrordecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁttingregime,wherecapacityistoolarge,abovetheoptimalcapacity.models,suchaslinearregression.Parametricmodelslearnafunctiondescribedbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.Non-parametricmodelshavenosuchlimitation.Sometimes,non-parametricmodelsarejusttheoreticalabstractions(suchasanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannotbeimplementedinpractice.However,wecanalsodesignpracticalnon-parametricmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexampleofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,whichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodelsimplystorestheXandyfromthetrainingset.Whenaskedtoclassifyatestpointx,themodellooksupthenearestentryinthetrainingsetandreturnstheassociatedregressiontarget.Inotherword'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 129}, page_content='thenearestneighborregressionmodelsimplystorestheXandyfromthetrainingset.Whenaskedtoclassifyatestpointx,themodellooksupthenearestentryinthetrainingsetandreturnstheassociatedregressiontarget.Inotherwords,ˆy=yiwherei=argmin||Xi,:−||x22.ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,suchaslearneddistancemetrics(,).IfthealgorithmisallowedGoldbergeretal.2005tobreaktiesbyaveragingtheyivaluesforallXi,:thataretiedfornearest,thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(whichmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerentoutputs)onanyregressiondataset.Finally,wecanalsocreateanon-parametriclearningalgorithmbywrappingaparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber115'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 130}, page_content='CHAPTER5.MACHINELEARNINGBASICSofparametersasneeded.Forexample,wecouldimagineanouterloopoflearningthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofapolynomialexpansionoftheinput.Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistributionthatgeneratesthedata.Evensuchamodelwillstillincursomeerroronmanyproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecaseofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,orymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthoseincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetruedistributioniscalledthep,y(x)Bayeserror.Trainingandgeneralizationerrorvaryasthesizeofthetrainingsetvaries.Expectedgeneralizationerrorcanneverincreaseasthenumberoftrainingexamplesincreases.Fornon-parametricmodels,moredatayieldsbettergeneralizationuntilthebestpossibleerrorisachieved.AnyﬁxedparametricmodelwithlessthanoptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.SeeFig.foranillustration'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 130}, page_content='dels,moredatayieldsbettergeneralizationuntilthebestpossibleerrorisachieved.AnyﬁxedparametricmodelwithlessthanoptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.SeeFig.foranillustration.Notethatitispossibleforthemodeltohaveoptimal5.4capacityandyetstillhavealargegapbetweentrainingandgeneralizationerror.Inthissituation,wemaybeabletoreducethisgapbygatheringmoretrainingexamples.5.2.1TheNoFreeLunchTheoremLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfromaﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesoflogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,isnotlogicallyvalid.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 130}, page_content='Tologicallyinferaruledescribingeverymemberofaset,onemusthaveinformationabouteverymemberofthatset.Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,ratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machinelearningpromisestoﬁndrulesthatareprobablymostcorrectaboutmembersofthesettheyconcern.Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofreelunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedoverallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthesameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,insomesense,nomachinelearningalgorithmisuniversallyanybetterthananyother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverageperformance(overallpossibletasks)asmerelypredictingthateverypointbelongstothesameclass.116'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 131}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue030\\ue02e\\ue030\\ue030\\ue02e\\ue035\\ue031\\ue02e\\ue030\\ue031\\ue02e\\ue035\\ue032\\ue02e\\ue030\\ue032\\ue02e\\ue035\\ue033\\ue02e\\ue030\\ue033\\ue02e\\ue035\\ue045\\ue072\\ue072\\ue06f\\ue072\\ue020\\ue028\\ue04d\\ue053\\ue045\\ue029\\ue042\\ue061\\ue079\\ue065\\ue073\\ue020\\ue065\\ue072\\ue072\\ue06f\\ue072\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue071\\ue075\\ue061\\ue064\\ue072\\ue061\\ue074\\ue069\\ue063\\ue029\\ue054\\ue065\\ue073\\ue074\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue020\\ue028\\ue06f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue029'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 131}, page_content='\\ue031\\ue030\\ue030\\ue031\\ue030\\ue031\\ue031\\ue030\\ue032\\ue031\\ue030\\ue033\\ue031\\ue030\\ue034\\ue031\\ue030\\ue035\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue074\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue065\\ue078\\ue061\\ue06d\\ue070\\ue06c\\ue065\\ue073\\ue030\\ue035\\ue031\\ue030\\ue031\\ue035\\ue032\\ue030\\ue04f\\ue070\\ue074\\ue069\\ue06d\\ue061\\ue06c\\ue020\\ue063\\ue061\\ue070\\ue061\\ue063\\ue069\\ue074\\ue079\\ue020\\ue028\\ue070\\ue06f\\ue06c\\ue079\\ue06e\\ue06f\\ue06d\\ue069\\ue061\\ue06c\\ue020\\ue064\\ue065\\ue067\\ue072\\ue065\\ue065\\ue029Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellasontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedonaddingmoderateamountofnoisetoadegree5polynomial,generatedasingletestset,andthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40diﬀerenttrainingsetsinordertoploterrorbarsshowing95%conﬁdenceintervals.(Top)TheMSEonthetrainandtestsetfortwodiﬀerentmodels:'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 131}, page_content='aquadraticmodel,andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.Forthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadraticmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotestoahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.ThetrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithmtomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,thetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoatleasttheBayeserror.Asthetrainingsetsizeincreases,theoptimalcapacity(Bottom)(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimalcapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.117'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 132}, page_content='CHAPTER5.MACHINELEARNINGBASICSFortunately,theseresultsholdonlywhenweaverageoverpossibledataallgeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobabilitydistributionsweencounterinreal-worldapplications,thenwecandesignlearningalgorithmsthatperformwellonthesedistributions.Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversallearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalistounderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAIagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellondatadrawnfromthekindsofdatageneratingdistributionswecareabout.5.2.2RegularizationThenofreelunchtheoremimpliesthatwemustdesignourmachinelearningalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetofpreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwiththelearningproblemsweaskthealgorithmtosolve,itperformsbetter.Sofar,theonlymethodofmodifyingalearningalgorithmwehavediscussedistoincreaseordecreasethemodel’scapacitybyaddin'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 132}, page_content='hesepreferencesarealignedwiththelearningproblemsweaskthealgorithmtosolve,itperformsbetter.Sofar,theonlymethodofmodifyingalearningalgorithmwehavediscussedistoincreaseordecreasethemodel’scapacitybyaddingorremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithmisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthedegreeofapolynomialforaregressionproblem.Theviewwehavedescribedsofarisoversimpliﬁed.Thebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewemakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentityofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.Theselinearfunctionscanbeveryusefulforproblemswheretherelationshipbetweeninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblemsthatbehaveinaverynonlinearfashion.Forexample,linearregressionwouldnotperformverywellifwetriedtouseittopredictsin(x)fromx.Wecanthuscontroltheperformanceofouralgorithmsbyc'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 132}, page_content='ar.Theyarelessusefulforproblemsthatbehaveinaverynonlinearfashion.Forexample,linearregressionwouldnotperformverywellifwetriedtouseittopredictsin(x)fromx.Wecanthuscontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionsweallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthesefunctions.Wecanalsogivealearningalgorithmapreferenceforonesolutioninitshypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butoneispreferred.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 132}, page_content='Theunpreferredsolutionbechosenonlyifitﬁtsthetrainingdatasigniﬁcantlybetterthanthepreferredsolution.Forexample, wecanmodifythetrainingcriterionforlinearregressiontoincludeweightdecay.Toperformlinearregressionwithweightdecay,weminimize118'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 133}, page_content='CHAPTER5.MACHINELEARNINGBASICSasumcomprisingboththemeansquarederroronthetrainingandacriterionJ(w)thatexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,J() = wMSEtrain+λw\\ue03ew,(5.18)whereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreferenceforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcestheweightstobecomesmaller.MinimizingJ(w)resultsinachoiceofweightsthatmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesussolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asanexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweightdecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvaluesof.SeeFig.fortheresults.λ5.5\\n\\ue078\\ue030\\ue079\\ue055\\ue06e\\ue064\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\ue028\\ue045\\ue078\\ue063\\ue065\\ue073\\ue073\\ue069\\ue076\\ue065\\ue020\\ue0b8\\ue029\\n\\ue078\\ue030\\ue079\\ue041\\ue070\\ue070\\ue072\\ue06f\\ue070\\ue072\\ue069\\ue061\\ue074\\ue065\\ue020\\ue077\\ue065\\ue069\\ue067\\ue068\\ue074\\ue020\\ue064\\ue065\\ue063\\ue061\\ue079\\ue028\\ue04d\\ue065\\ue064\\ue069\\ue075\\ue06d\\ue020\\ue0b8\\ue029\\n\\ue078\\ue030\\ue079\\ue04f\\ue076\\ue065\\ue072\\ue066\\ue069\\ue074\\ue074\\ue069\\ue06e\\ue067\\ue028\\ue030\\ue029\\ue0b8\\ue021'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 133}, page_content='Figure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingsetfromFig..Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9.5.2Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.(Left)Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeatall. Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction. ()WithaCentermediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape.λEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicatedshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmallercoeﬃcients.()Withweightdecayapproachingzero(i.e.,usingtheMoore-PenroseRightpseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),thedegree-9polynomialoverﬁtssigniﬁcantly,aswesawinFig..5.2Moregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)byaddingapenaltycalledaregularizertothecostfunction. Inthecaseofweightdecay,theregularizerisΩ(w) =w\\ue03ew.InChapter,wewillseethatmanyother7119'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 134}, page_content='CHAPTER5.MACHINELEARNINGBASICSregularizersarepossible.Expressingpreferencesforonefunctionoveranotherisamoregeneralwayofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthehypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceasexpressinganinﬁnitelystrongpreferenceagainstthatfunction.Inourweightdecayexample,weexpressedourpreferenceforlinearfunctionsdeﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionweminimize.Thereare many otherwaysof expressingpreferencesfor'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 134}, page_content='diﬀerentsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproachesareknownasregularization.Regularizationisanymodiﬁcationwemaketoalearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachinelearning,rivaledinitsimportanceonlybyoptimization.Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachinelearningalgorithm,and,inparticular,nobestformofregularization.Insteadwemustchooseaformofregularizationthatiswell-suitedtotheparticulartaskwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookinparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasksthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeformsofregularization.5.3HyperparametersandValidationSetsMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrolthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparameters.Thevaluesofhyperparametersarenotadaptedbythelearningalgorithmitself(thoughw'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 134}, page_content='ingalgorithmshaveseveralsettingsthatwecanusetocontrolthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparameters.Thevaluesofhyperparametersarenotadaptedbythelearningalgorithmitself(thoughwecandesignanestedlearningprocedurewhereonelearningalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).InthepolynomialregressionexamplewesawinFig.,thereisasinglehyper-5.2parameter:thedegreeofthepolynomial,whichactsasacapacityhyperparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanotherexampleofahyperparameter.Sometimesasettingischosentobeahyperparameterthatthelearningalgo-rithmdoesnotlearnbecauseitisdiﬃculttooptimize.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 134}, page_content='Morefrequently,wedonotlearnthehyperparameterbecauseitisnotappropriatetolearnthathyper-parameteronthetrainingset.Thisappliestoallhyperparametersthatcontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameterswouldalways120'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 135}, page_content='CHAPTER5.MACHINELEARNINGBASICSchoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refertoFig.).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher5.3degreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalowerdegreepolynomialandapositiveweightdecaysetting.Tosolvethisproblem,weneedaofexamplesthatthetrainingvalidationsetalgorithmdoesnotobserve.Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfromthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralizationerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthetestexamplesarenotusedinanywaytomakechoicesaboutthemodel,includingitshyperparameters.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 135}, page_content='Forthisreason,noexamplefromthetestsetcanbeusedinthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthetrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.Oneofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidationset,usedtoestimatethegeneralizationerrorduringoraftertraining,allowingforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedtolearntheparametersisstilltypicallycalledthetrainingset,eventhoughthismaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.Thesubsetofdatausedtoguidetheselectionofhyperparametersiscalledthevalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters,thevalidationseterrorwillunderestimatethegeneralizationerror,thoughtypicallybyasmalleramountthanthetrainingerror.Afterallhyperparameteroptimizationiscomplete,thegeneralizationerrormaybeestimatedusingthetestset.Inpractice,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 135}, page_content='whenthesametestsethasbeenusedrepeatedlytoevaluateperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsideralltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-the-artperformanceonthattestset,weenduphavingoptimisticevaluationswiththetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthetrueﬁeldperformanceofatrainedsystem.Thankfully,thecommunitytendstomoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.5.3.1Cross-ValidationDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematicifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertaintyaroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithmAworksbetterthanalgorithmonthegiventask.B121'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 136}, page_content='CHAPTER5.MACHINELEARNINGBASICSWhenthedatasethashundredsofthousandsofexamplesormore,thisisnotaseriousissue.Whenthedatasetistoosmall,therearealternativeprocedures,whichallowonetousealloftheexamplesintheestimationofthemeantesterror,atthepriceofincreasedcomputationalcost.Theseproceduresarebasedontheideaofrepeatingthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplitsoftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validationprocedure,showninAlgorithm,inwhichapartition5.1ofthedatasetisformedbysplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimatedbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthedataisusedasthetestsetandtherestofthedataisusedasthetrainingset.Oneproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverageerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypicallyused.5.4Estimators,BiasandVarianceTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachinelearninggoalofsolvingatasknotonlyont'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 136}, page_content='imators(BengioandGrandvalet2004,),butapproximationsaretypicallyused.5.4Estimators,BiasandVarianceTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachinelearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.Foundationalconceptssuchasparameterestimation,biasandvarianceareusefultoformallycharacterizenotionsofgeneralization,underﬁttingandoverﬁtting.5.4.1PointEstimationPointestimationistheattempttoprovidethesingle“best”predictionofsomequantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameteroravectorofparametersinsomeparametricmodel,suchastheweightsinourlinearregressionexampleinSec.,butitcanalsobeawholefunction.5.1.4Inordertodistinguishestimatesofparametersfromtheirtruevalue,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 136}, page_content='ourconventionwillbetodenoteapointestimateofaparameterbyθˆθ.Let{x(1),...,x()m}beasetofmindependentandidenticallydistributed(i.i.d.)datapoints.Apointestimatorstatisticorisanyfunctionofthedata:ˆθm= (gx(1),...,x()m).(5.19)Thedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrueθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.Thisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofanestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,122'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 137}, page_content='CHAPTER5.MACHINELEARNINGBASICS'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 137}, page_content='Algorithm5.1Thek-foldcross-validationalgorithm.ItcanbeusedtoestimategeneralizationerrorofalearningalgorithmAwhenthegivendatasetDistoosmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationofgeneralizationerror,becausethemeanofalossLonasmalltestsetmayhavetoohighvariance.ThedatasetDcontainsaselementstheabstractexamplesz()i(forthei-thexample),whichcouldstandforan(input,target)pairz()i= (x()i,y()i)inthecaseofsupervisedlearning,orforjustaninputz()i=x()iinthecaseofunsupervisedlearning. ThealgorithmreturnsthevectoroferrorseforeachexampleinD,whosemeanistheestimatedgeneralizationerror.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 137}, page_content='(x()i,y()i)inthecaseofsupervisedlearning,orforjustaninputz()i=x()iinthecaseofunsupervisedlearning. ThealgorithmreturnsthevectoroferrorseforeachexampleinD,whosemeanistheestimatedgeneralizationerror. Theerrorsonindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean(Eq.).Whiletheseconﬁdenceintervalsarenotwell-justiﬁedaftertheuseof5.47cross-validation,itisstillcommonpracticetousethemtodeclarethatalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerrorofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm.BDeﬁneKFoldXV():D,A,L,kRequire:D,thegivendataset,withelementsz()iRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetasinputandoutputsalearnedfunctionRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfandanexamplez()i∈∈DtoascalarRRequire:k,thenumberoffoldsSplitintomutuallyexclusivesubsetsDkDi,whoseunionis.Dfordoikfromto1fi= (ADD\\\\i)forz()jinDidoej= (Lfi,z()j)endforendforReturne'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 137}, page_content='123'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 138}, page_content='CHAPTER5.MACHINELEARNINGBASICSagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthatgeneratedthetrainingdata.Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassumethatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimateˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,anyfunctionofthedataisrandom.Thereforeˆθisarandomvariable.Pointestimationcanalsorefertotheestimationoftherelationshipbetweeninputandtargetvariables.Werefertothesetypesofpointestimatesasfunctionestimators.FunctionEstimationAswementionedabove,sometimesweareinterestedinperformingfunctionestimation(orfunctionapproximation).Herewearetryingtopredictavariableygivenaninputvectorx.Weassumethatthereisafunctionf(x)thatdescribestheapproximaterelationshipbetweenyandx.Forexample,wemayassumethaty=f(x)+\\ue00f,where\\ue00fstandsforthepartofythatisnotpredictablefromx.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 138}, page_content='Infunctionestimation,weareinterestedinapproximatingfwithamodelorestimateˆf.Functionestimationisreallyjustthesameasestimatingaparameterθ;thefunctionestimatorˆfissimplyapointestimatorinfunctionspace.Thelinearregressionexample(discussedaboveinSec.)and5.1.4thepolynomialregressionexample(discussedinSec.)arebothexamplesof5.2scenariosthatmaybeinterpretedeitherasestimatingaparameterworestimatingafunctionˆfymappingfromtox.Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsanddiscusswhattheytellusabouttheseestimators.5.4.2BiasThebiasofanestimatorisdeﬁnedas:bias(ˆθm) = (Eˆθm)−θ(5.20)wheretheexpectationisoverthedata(seenassamplesfromarandomvariable)andθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistribution.Anestimatorˆθmissaidtobeunbiasedifbias(ˆθm) =0,whichimpliesthatE(ˆθm) =θ.Anestimatorˆθmissaidtobeasymptoticallyunbiasediflimm→∞bias(ˆθm) =0,whichimpliesthatlimm→∞E(ˆθm) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 138}, page_content='=0,whichimpliesthatE(ˆθm) =θ.Anestimatorˆθmissaidtobeasymptoticallyunbiasediflimm→∞bias(ˆθm) =0,whichimpliesthatlimm→∞E(ˆθm) = θ.Example:BernoulliDistributionConsiderasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-124'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 139}, page_content='CHAPTER5.MACHINELEARNINGBASICSbutionwithmean:θPx(()i;) = θθx()i(1)−θ(1−x()i).(5.21)Acommonestimatorfortheθparameterofthisdistributionisthemeanofthetrainingsamples:ˆθm=1mm\\ue058i=1x()i.(5.22)Todeterminewhetherthisestimatorisbiased,wecansubstituteEq.intoEq.5.225.20:bias(ˆθm) = [Eˆθm]−θ(5.23)= E\\ue0221mm\\ue058i=1x()i\\ue023−θ(5.24)=1mm\\ue058i=1E\\ue068x()i\\ue069−θ(5.25)=1mm\\ue058i=11\\ue058x()i=0\\ue010x()iθx()i(1)−θ(1−x()i)\\ue011−θ(5.26)=1mm\\ue058i=1()θ−θ(5.27)= = 0θθ−(5.28)Sincebias(ˆθ) = 0,wesaythatourestimatorˆθisunbiased.Example:GaussianDistributionEstimatoroftheMeanNow,considerasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaGaussiandistributionp(x()i) =N(x()i;µ,σ2),wherei∈{1,...,m}.RecallthattheGaussianprobabilitydensityfunctionisgivenbypx(()i;µ,σ2) =1√2πσ2exp\\ue020−12(x()i−µ)2σ2\\ue021.(5.29)AcommonestimatoroftheGaussianmeanparameterisknownasthesamplemean:ˆµm=1mm\\ue058i=1x()i(5.30)125'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 140}, page_content='CHAPTER5.MACHINELEARNINGBASICSTodeterminethebiasofthesamplemean,weareagaininterestedincalculatingitsexpectation:bias(ˆµm) = [ˆEµm]−µ(5.31)= E\\ue0221mm\\ue058i=1x()i\\ue023−µ(5.32)=\\ue0201mm\\ue058i=1E\\ue068x()i\\ue069\\ue021−µ(5.33)=\\ue0201mm\\ue058i=1µ\\ue021−µ(5.34)= = 0µµ−(5.35)ThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmeanparameter.Example:EstimatorsoftheVarianceofaGaussianDistributionAsanexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofaGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.Theﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:ˆσ2m=1mm\\ue058i=1\\ue010x()i−ˆµm\\ue0112,(5.36)whereˆµmisthesamplemean,deﬁnedabove.Moreformally,weareinterestedincomputingbias(ˆσ2m) = [ˆEσ2m]−σ2(5.37)WebeginbyevaluatingthetermE[ˆσ2m]:E[ˆσ2m] =E\\ue0221mm\\ue058i=1\\ue010x()i−ˆµm\\ue0112\\ue023(5.38)=m−1mσ2(5.39)ReturningtoEq.,weconcludethatthebiasof5.37ˆσ2mis−σ2/m.Therefore,thesamplevarianceisabiasedestimator.126'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 141}, page_content='CHAPTER5.MACHINELEARNINGBASICSTheunbiasedsamplevarianceestimator˜σ2m=1m−1m\\ue058i=1\\ue010x()i−ˆµm\\ue0112(5.40)providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.Thatis,weﬁndthatE[˜σ2m] = σ2:E[˜σ2m] = E\\ue0221m−1m\\ue058i=1\\ue010x()i−ˆµm\\ue0112\\ue023(5.41)=mm−1E[ˆσ2m](5.42)=mm−1\\ue012m−1mσ2\\ue013(5.43)='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 141}, page_content='σ2.(5.44)Wehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiasedestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswewillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.5.4.3VarianceandStandardErrorAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuchweexpectittovaryasafunctionofthedatasample.Justaswecomputedtheexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.ThevarianceofanestimatorissimplythevarianceVar(ˆθ)(5.45)wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthevarianceiscalledthestandarderror,denotedSE(ˆθ).Thevarianceorthestandarderrorofanestimatorprovidesameasureofhowwewouldexpecttheestimatewecomputefromdatatovaryasweindependentlyresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswemightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelativelylowvariance.Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimateofthetrueunderlyingparameterisuncertain,inthesensethatwecoul'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 141}, page_content='tlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelativelylowvariance.Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimateofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhaveobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave127'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 142}, page_content='CHAPTER5.MACHINELEARNINGBASICSbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceoferrorthatwewanttoquantify.ThestandarderrorofthemeanisgivenbySE(ˆµm) =\\ue076\\ue075\\ue075\\ue074Var[1mm\\ue058i=1x()i] =σ√m,(5.46)whereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoftenestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootofthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevarianceprovideanunbiasedestimateofthestandarddeviation.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 142}, page_content='Bothapproachestendtounderestimatethetruestandarddeviation,butarestillusedinpractice.Thesquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.Forlarge,theapproximationisquitereasonable.mThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.Weoftenestimatethegeneralizationerrorbycomputingthesamplemeanoftheerroronthetestset.Thenumberofexamplesinthetestsetdeterminestheaccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,whichtellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectationfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredonthemeanisˆµmis(ˆµm−196SE(ˆ.µm)ˆ,µm+196SE(ˆ.µm)),(5.47)underthenormaldistributionwithmeanˆµmandvarianceSE(ˆµm)2.Inmachinelearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithmBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAislessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithmB.E'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 142}, page_content='gexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithmBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAislessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithmB.Example:'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 142}, page_content='BernoulliDistributionWeonceagainconsiderasetofsamples{x(1),...,x()m}drawnindependentlyandidenticallyfromaBernoullidistribution(recallP(x()i;θ) =θx()i(1−θ)(1−x()i)).Thistimeweareinterestedincomputingthevarianceoftheestimatorˆθm=1m\\ue050mi=1x()i.Var\\ue010ˆθm\\ue011= Var\\ue0201mm\\ue058i=1x()i\\ue021(5.48)128'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 143}, page_content='CHAPTER5.MACHINELEARNINGBASICS=1m2m\\ue058i=1Var\\ue010x()i\\ue011(5.49)=1m2m\\ue058i=1θθ(1−)(5.50)=1m2mθθ(1−)(5.51)=1mθθ(1−)(5.52)Thevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamplesinthedataset.Thisisacommonpropertyofpopularestimatorsthatwewillreturntowhenwediscussconsistency(seeSec.).5.4.55.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquaredErrorBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Biasmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.Varianceontheotherhand,providesameasureofthedeviationfromtheexpectedestimatorvaluethatanyparticularsamplingofthedataislikelytocause.Whathappenswhenwearegivenachoicebetweentwoestimators,onewithmorebiasandonewithmorevariance?Howdowechoosebetweenthem?Forexample,imaginethatweareinterestedinapproximatingthefunctionshowninFig.andweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand5.2onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation.Empirically,cross-validati'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 143}, page_content='eareonlyoﬀeredthechoicebetweenamodelwithlargebiasand5.2onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation.Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-natively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:MSE'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 143}, page_content='= [(Eˆθm−θ)2](5.53)= Bias(ˆθm)2+Var(ˆθm)(5.54)TheMSEmeasurestheoverallexpecteddeviation—inasquarederrorsense—betweentheestimatorandthetruevalueoftheparameterθ.AsisclearfromEq.,evaluatingtheMSEincorporatesboththebiasandthevariance.Desirable5.54estimatorsarethosewithsmallMSEandtheseareestimatorsthatmanagetokeepboththeirbiasandvariancesomewhatincheck.Therelationshipbetweenbiasandvarianceistightlylinkedtothemachinelearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-129'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 144}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nCapacityBiasGeneralizationerrorVarianceOptimalcapacityOverﬁtting zoneUnderﬁtting zone'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 144}, page_content='Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(boldcurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁttingwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationshipissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedinSec.andFig..5.25.3eralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningfulcomponentsofgeneralizationerror),increasingcapacitytendstoincreasevarianceanddecreasebias.ThisisillustratedinFig.,whereweseeagaintheU-shaped5.6curveofgeneralizationerrorasafunctionofcapacity.5.4.5ConsistencySofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetofﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorastheamountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumberofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetruevalueofthecorrespondingparameters.Moreformally'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 144}, page_content='fanestimatorastheamountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumberofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetruevalueofthecorrespondingparameters.Moreformally,wewouldlikethatlimm→∞ˆθmp→θ.(5.55)Thesymbolp→meansthattheconvergenceisinprobability,i.e.forany\\ue00f>0,P(|ˆθm−|θ>\\ue00f)→0asm→∞.The'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 144}, page_content='conditiondescribed by Eq.is5.55knownas consistency.Itissometimesreferredto asweakconsistency, withstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almostsure130'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 145}, page_content='CHAPTER5.MACHINELEARNINGBASICSconvergenceofasequenceofrandomvariablesx(1),x(2),...toavaluexoccurswhenp(limm→∞x()m= ) = 1x.Consistencyensuresthat thebias inducedbytheestimatorisassuredtodiminishasthenumberofdataexamplesgrows.However,thereverseisnottrue—asymptoticunbiasednessdoesnotimplyconsistency.Forexample,considerestimatingthemeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsistingofmsamples:{x(1),...,x()m}.Wecouldusetheﬁrstsamplex(1)ofthedatasetasanunbiasedestimator:ˆθ=x(1).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 145}, page_content='Inthatcase,E(ˆθm)=θsotheestimatorisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,impliesthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistentestimatorasitisthecasethatnotˆθm→→∞θmas.5.5MaximumLikelihoodEstimationPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzedtheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessingthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasandvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁcfunctionsthataregoodestimatorsfordiﬀerentmodels.Themostcommonsuchprincipleisthemaximumlikelihoodprinciple.ConsiderasetofmexamplesX={x(1),...,x()m}drawnindependentlyfromthetruebutunknowndatageneratingdistributionpdata()x.Letpmodel(x;θ)beaparametricfamilyofprobabilitydistributionsoverthesamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationxtoarealnumberestimatingthetrueprobabilitypdata()x.ThemaximumlikelihoodestimatorforisthendeﬁnedasθθML= argmaxθpmodel(;)Xθ(5.56)='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 145}, page_content='argmaxθpmodel(;)Xθ(5.56)= argmaxθm\\ue059i=1pmodel(x()i;)θ(5.57)Thisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenientbutequivalentoptimizationproblem,weobservethattakingthelogarithmofthelikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct131'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 146}, page_content='CHAPTER5.MACHINELEARNINGBASICSintoasum:θML= argmaxθm\\ue058i=1logpmodel(x()i;)θ.(5.58)Becausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecandividebymtoobtainaversionofthecriterionthatisexpressedasanexpectationwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:θML= argmaxθEx∼ˆpdatalogpmodel(;)xθ.(5.59)Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizingthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetrainingsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwomeasuredbytheKLdivergence.TheKLdivergenceisgivenbyDKL(ˆpdata\\ue06bpmodel) = Ex∼ˆpdata[log'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 146}, page_content='ˆpdata()logx−pmodel()]x.(5.60)Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthemodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,weneedonlyminimize−Ex∼ˆpdata[logpmodel()]x(5.61)whichisofcoursethesameasthemaximizationinEq..5.59MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-entropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”toidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacrossentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthemodel.Forexample,meansquarederroristhecross-entropybetweentheempiricaldistributionandaGaussianmodel.Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-tributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatchthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothisdistribution.WhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthelikelihoodorminimizingtheKLdi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 146}, page_content='a.Ideally,wewouldliketomatchthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothisdistribution.WhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthelikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctionsarediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.Maximumlikelihoodthusbecomesminimizationofthenegativelog-likelihood(NLL),orequivalently,minimizationofthecrossentropy.TheperspectiveofmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscasebecausetheKLdivergencehasaknownminimumvalueofzero.Thenegativelog-likelihoodcanactuallybecomenegativewhenisreal-valued.x132'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 147}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.5.1ConditionalLog-LikelihoodandMeanSquaredErrorThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhereourgoalistoestimateaconditionalprobabilityP(yx|;θ)inordertopredictygivenx.Thisisactuallythemostcommonsituationbecauseitformsthebasisformostsupervisedlearning.IfXrepresentsallourinputsandYallourobservedtargets,thentheconditionalmaximumlikelihoodestimatorisθML= argmaxθP.(;)YX|θ(5.62)Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedintoθML= argmaxθm\\ue058i=1log(Py()i|x()i;)θ.(5.63)Example:LinearRegressionasMaximumLikelihoodLinearregression,introducedearlierinSec.,maybejustiﬁedasamaximumlikelihoodprocedure.5.1.4Previously,wemotivatedlinearregressionasanalgorithmthatlearnstotakeaninputxandproduceanoutputvalueˆy.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 147}, page_content='Themappingfromxtoˆyischosentominimizemeansquarederror,acriterionthatweintroducedmoreorlessarbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximumlikelihoodestimation.Insteadofproducingasinglepredictionˆy,wenowthinkofthemodelasproducingaconditionaldistributionp(y|x).Wecanimaginethatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexampleswiththesameinputvaluexbutdiﬀerentvaluesofy.Thegoalofthelearningalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvaluesthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithmweobtainedbefore,wedeﬁnep(y|x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 147}, page_content='=N(y;ˆy(x;w),σ2).Thefunctionˆy(x;w)givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethatthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthischoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimationproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincetheexamplesareassumedtobei.i.d.,theconditionallog-likelihood(Eq.)isgivenby5.63m\\ue058i=1log(py()i|x()i;)θ(5.64)=log−mσ−m2log(2)π−m\\ue058i=1|ˆy()i−y()i||22σ2(5.65)133'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 148}, page_content='CHAPTER5.MACHINELEARNINGBASICSwhereˆy()iistheoutputofthelinearregressiononthei-thinputx()iandmisthenumberofthetrainingexamples.Comparingthelog-likelihoodwiththemeansquarederror,MSEtrain=1mm\\ue058i=1||ˆy()i−y()i||2,(5.66)weimmediatelyseethatmaximizingthelog-likelihoodwithrespecttowyieldsthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.ThisjustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswewillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.5.5.2PropertiesofMaximumLikelihoodThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshowntobethebestestimatorasymptotically,asthenumberofexamplesm→∞,intermsofitsrateofconvergenceasincreases.mUnderappropriateconditions,maximumlikelihoodestimatorhasthepropertyofconsistency(seeSec.above),meaningthatasthenumberoftraining5.4.5examplesapproachesinﬁnity,themaximumlikelihoodestimateofaparameterconvergestothetruevalueoftheparameter.Theseconditionsar'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 148}, page_content='hasthepropertyofconsistency(seeSec.above),meaningthatasthenumberoftraining5.4.5examplesapproachesinﬁnity,themaximumlikelihoodestimateofaparameterconvergestothetruevalueoftheparameter.Theseconditionsare:•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).Otherwise,noestimatorcanrecoverpdata.•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeabletodeterminewhichvalueofwasusedbythedatageneratingprocessing.θThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestimator,manyofwhichsharethepropertyofbeingconsistentestimators.However,consis-tentestimatorscandiﬀerintheir,meaningthatoneconsistentstatisticeﬃciencyestimatormayobtainlowergeneralizationerrorforaﬁxednumberofsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelofgeneralizationerror.Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinearregression)whereourgoalistoestimatethevalueofaparameter(andassumingitispossibletoi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 148}, page_content='rexamplestoobtainaﬁxedlevelofgeneralizationerror.Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinearregression)whereourgoalistoestimatethevalueofaparameter(andassumingitispossibletoidentifythetrueparameter),notthevalueofafunction.Awaytomeasurehowclosewearetothetrueparameterisbytheexpectedmeansquarederror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter134'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 149}, page_content='CHAPTER5.MACHINELEARNINGBASICSvalues,wheretheexpectationisovermtrainingsamplesfromthedatageneratingdistribution.Thatparametricmeansquarederrordecreasesasmincreases,andformlarge,theCramér-Raolowerbound(,;,)showsthatnoRao1945Cramér1946consistentestimatorhasalowermeansquarederrorthanthemaximumlikelihoodestimator.Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoftenconsideredthepreferredestimatortouseformachinelearning.Whenthenumberofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategiessuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihoodthathaslessvariancewhentrainingdataislimited.5.6BayesianStatisticsSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimatingasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatoneestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakingaprediction.Thelatteristhedomainof.BayesianstatisticsAsdiscussedinSec.,thefrequentistperspectiveisthatthetrueparameter5.4.1valueθisﬁxedbutunknown,whil'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 149}, page_content='approachistoconsiderallpossiblevaluesofθwhenmakingaprediction.Thelatteristhedomainof.BayesianstatisticsAsdiscussedinSec.,thefrequentistperspectiveisthatthetrueparameter5.4.1valueθisﬁxedbutunknown,whilethepointestimateˆθisarandomvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).TheBayesianperspectiveonstatisticsisquitediﬀerent.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 149}, page_content='TheBayesianusesprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetisdirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθisunknownoruncertainandthusisrepresentedasarandomvariable.Beforeobservingthedata,werepresentourknowledgeofθusingthepriorprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).Gen-erally,themachinelearningpractitionerselectsapriordistributionthatisquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthevalueofθbeforeobservinganydata.Forexample,onemightassumethataprioriθliesinsomeﬁniterangeorvolume,withauniformdistribution.Manypriorsinsteadreﬂectapreferencefor“simpler”solutions(suchassmallermagnitudecoeﬃcients,orafunctionthatisclosertobeingconstant).Nowconsiderthatwehaveasetofdatasamples{x(1),...,x()m}.Wecanrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihoodpx((1),...,x()m|θ)withthepriorviaBayes’rule:px(θ|(1),...,x()m) =px((1),...,x()m|θθ)(p)px((1),...,x()m)(5.67)135'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 150}, page_content='CHAPTER5.MACHINELEARNINGBASICSInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasarelativelyuniformorGaussiandistributionwithhighentropy,andtheobservationofthedatausuallycausestheposteriortoloseentropyandconcentratearoundafewhighlylikelyvaluesoftheparameters.Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwoimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakespredictionsusingapointestimateofθ,theBayesianapproachistomakepredictionsusingafulldistributionoverθ.Forexample,afterobservingmexamples,thepredicteddistributionoverthenextdatasample,x(+1)m,isgivenbypx((+1)m|x(1),...,x()m)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 150}, page_content='=\\ue05apx((+1)m||θθ)(px(1),...,x()m)d.θ(5.68)Hereeachvalueofθwithpositiveprobabilitydensitycontributestothepredictionofthenextexample,withthecontributionweightedbytheposteriordensityitself.Afterhavingobserved{x(1),...,x()m},ifwearestillquiteuncertainaboutthevalueofθ,thenthisuncertaintyisincorporateddirectlyintoanypredictionswemightmake.InSec.,wediscussedhowthefrequentistapproachaddressestheuncertainty5.4inagivenpointestimateofθbyevaluatingitsvariance.Thevarianceoftheestimatorisanassessmentofhowtheestimatemightchangewithalternativesamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodealwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendstoprotectwellagainstoverﬁtting.Thisintegralisofcoursejustanapplicationofthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethefrequentistmachineryforconstructinganestimatorisbasedontheratheradhocdecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepointestimate.ThesecondimportantdiﬀerencebetweentheBayesia'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 150}, page_content='y,whilethefrequentistmachineryforconstructinganestimatorisbasedontheratheradhocdecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepointestimate.ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimationandthemaximumlikelihoodapproachisduetothecontributionoftheBayesianpriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensitytowardsregionsoftheparameterspacethatarepreferred.Inpractice,aprioritheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehumanjudgmentimpactingthepredictions.Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdataisavailable,buttypicallysuﬀerfromhighcomputationalcostwhenthenumberoftrainingexamplesislarge.136'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 151}, page_content='CHAPTER5.MACHINELEARNINGBASICSExample:BayesianLinearRegressionHereweconsidertheBayesianesti-mationapproachtolearningthelinearregressionparameters.Inlinearregression,welearnalinearmappingfromaninputvectorx∈Rntopredictthevalueofascalar.Thepredictionisparametrizedbythevectory∈Rw∈Rn:ˆy= w\\ue03ex.(5.69)Givenasetofmtrainingsamples(X()train,y()train),wecanexpressthepredictionofovertheentiretrainingsetas:yˆy()train= X()trainw.(5.70)ExpressedasaGaussianconditionaldistributionony()train,wehavep(y()train|X()train,wy) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 151}, page_content='X()trainw.(5.70)ExpressedasaGaussianconditionaldistributionony()train,wehavep(y()train|X()train,wy) = (N()train;X()trainwI,)(5.71)∝exp\\ue012−12(y()train−X()trainw)\\ue03e(y()train−X()trainw)\\ue013,(5.72)wherewefollowthestandardMSEformulationinassumingthattheGaussianvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto(X()train,y()train)()assimplyXy,.Todeterminetheposteriordistributionoverthemodelparametervectorw,weﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebeliefaboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnaturaltoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewetypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertaintyaboutθ. Forreal-valuedparametersitiscommontouseaGaussianasapriordistribution:p() = (;wNwµ0,Λ0)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 151}, page_content='Forreal-valuedparametersitiscommontouseaGaussianasapriordistribution:p() = (;wNwµ0,Λ0) exp∝\\ue012−12(wµ−0)\\ue03eΛ−10(wµ−0)\\ue013(5.73)whereµ0andΛ0arethepriordistributionmeanvectorandcovariancematrixrespectively.1Withthepriorthusspeciﬁed,wecannowproceedindeterminingtheposteriordistributionoverthemodelparameters.p,p,p(wX|y) ∝(yX|w)()w(5.74)1Unlessthereisareasontoassumeaparticularcovariancestructure,wetypicallyassumeadiagonalcovariancematrixΛ0= diag(λ0).137'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 152}, page_content='CHAPTER5.MACHINELEARNINGBASICS∝exp\\ue012−12()yXw−\\ue03e()yXw−\\ue013exp\\ue012−12(wµ−0)\\ue03eΛ−10(wµ−0)\\ue013(5.75)∝exp\\ue012−12\\ue010−2y\\ue03eXww+\\ue03eX\\ue03eXww+\\ue03eΛ−10wµ−2\\ue03e0Λ−10w\\ue011\\ue013.(5.76)WenowdeﬁneΛm=\\ue000X\\ue03eX+Λ−10\\ue001−1andµm= Λm\\ue000X\\ue03ey+Λ−10µ0\\ue001.Usingthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussiandistribution:p,(wX|y)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 152}, page_content='exp∝\\ue012−12(wµ−m)\\ue03eΛ−1m(wµ−m)+12µ\\ue03emΛ−1mµm\\ue013(5.77)∝exp\\ue012−12(wµ−m)\\ue03eΛ−1m(wµ−m)\\ue013.(5.78)Alltermsthatdonotincludetheparametervectorwhavebeenomitted;theyareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1Eq.showshowtonormalizeamultivariateGaussiandistribution.3.23ExaminingthisposteriordistributionallowsustogainsomeintuitionfortheeﬀectofBayesianinference.Inmostsituations,wesetµ0to0.IfwesetΛ0=1αI,thenµmgivesthesameestimateofwasdoesfrequentistlinearregressionwithaweightdecaypenaltyofαw\\ue03ew.OnediﬀerenceisthattheBayesianestimateisundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearningprocesswithaninﬁnitelywideprioronw.ThemoreimportantdiﬀerenceisthattheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthediﬀerentvaluesofare,ratherthanprovidingonlytheestimatewµm.5.6.1Maximum(MAP)EstimationAPosterioriWhilethemostprincipledapproachistomakepredictionsusingthefullBayesianposteriordistributionovertheparameterθ,itisstilloftendesirabletohaveasinglepointestimate.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 152}, page_content='OnecommonreasonfordesiringapointestimateisthatmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsareintractable,andapointestimateoﬀersatractableapproximation.Ratherthansimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeofthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoiceofthepointestimate. Onerationalwaytodothisistochoosethemaximumaposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointofmaximal138'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 153}, page_content='CHAPTER5.MACHINELEARNINGBASICSposteriorprobability(ormaximalprobabilitydensityinthemorecommoncaseofcontinuous):θθMAP= argmaxθp() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 153}, page_content='argmaxθx|θlog()+log()pxθ|pθ.(5.79)Werecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-likelihoodterm,and,correspondingtothepriordistribution.log()pθAsanexample,consideralinearregressionmodelwithaGaussianpriorontheweightsw.IfthispriorisgivenbyN(w;0,1λI2),thenthelog-priorterminEq.5.79isproportionaltothefamiliarλw\\ue03ewweightdecaypenalty,plusatermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAPBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweightdecay.AswithfullBayesianinference,MAPBayesianinferencehastheadvantageofleveraginginformationthatisbroughtbythepriorandcannotbefoundinthetrainingdata.ThisadditionalinformationhelpstoreducethevarianceintheMAPpointestimate(incomparisontotheMLestimate).However,itdoessoatthepriceofincreasedbias.Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearningregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsofaddinganextratermtothe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 153}, page_content='trategies,suchasmaximumlikelihoodlearningregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsofaddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).NotallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,someregularizertermsmaynotbethelogarithmofaprobabilitydistribution.Otherregularizationtermsdependonthedata,whichofcourseapriorprobabilitydistributionisnotallowedtodo.MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicatedyetinterpretableregularizationterms.Forexample,amorecomplicatedpenaltytermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussiandistribution,astheprior(NowlanandHinton1992,).5.7SupervisedLearningAlgorithmsRecallfromSec.thatsupervisedlearningalgorithmsare,roughlyspeaking,5.1.3learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givenatrainingsetofexamplesofinputsxandoutputsy.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 153}, page_content='Inmanycasestheoutputsymaybediﬃculttocollectautomaticallyandmustbeprovidedbyahuman“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswerecollectedautomatically.139'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 154}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.7.1ProbabilisticSupervisedLearningMost supervised learning algorithms inthis book are based onestimating aprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximumlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamilyofdistributions.py(|xθ;)Wehavealreadyseenthatlinearregressioncorrespondstothefamilypyy(|Nxθ;) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 154}, page_content='= (;θ\\ue03exI,).(5.80)Wecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁningadiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0andclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.Theprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovaluesmustaddupto1.Thenormaldistributionoverreal-valuednumbersthatweusedforlinearregressionisparametrizedintermsofamean.Anyvaluewesupplyforthismeanisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,becauseitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistousethelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintotheinterval(0,1)andinterpretthatvalueasaprobability:pyσ(= 1 ;) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 154}, page_content='1 ;) = |xθ(θ\\ue03ex).(5.81)Thisapproachisknownaslogisticregression(asomewhatstrangenamesinceweusethemodelforclassiﬁcationratherthanregression).Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsbysolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.Thereisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchforthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegativelog-likelihood(NLL)usinggradientdescent.Thissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,bywritingdownaparametricfamilyofconditionalprobabilitydistributionsovertherightkindofinputandoutputvariables.5.7.2SupportVectorMachinesOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvectormachine(,;Boseretal.1992CortesandVapnik1995,).Thismodelissimilartologisticregressioninthatitisdrivenbyalinearfunctionw\\ue03ex+b.Unlikelogistic140'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 155}, page_content='CHAPTER5.MACHINELEARNINGBASICSregression,thesupportvectormachinedoesnotprovideprobabilities,butonlyoutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhenw\\ue03ex+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhenw\\ue03ex+bisnegative.Onekeyinnovationassociatedwithsupportvectormachinesisthe.kerneltrickThekerneltrickconsistsofobservingthatmanymachinelearningalgorithmscanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecanbere-writtenasw\\ue03ex+= +bbm\\ue058i=1αix\\ue03ex()i(5.82)wherex()iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthelearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeaturefunctionφ(x) andthedotproductwithafunctionk(xx,()i) =φ(x)·φ(x()i)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 155}, page_content='andthedotproductwithafunctionk(xx,()i) =φ(x)·φ(x()i) calleda.Thekernel·operatorrepresentsaninnerproductanalogoustoφ(x)\\ue03eφ(x()i).Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.Insomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,forexample,innerproductsbasedonintegrationratherthansummation.Acompletedevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.Afterreplacingdotproductswithkernelevaluations,wecanmakepredictionsusingthefunctionfb() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 155}, page_content='= x+\\ue058iαik,(xx()i).(5.83)Thisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)andf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.Thekernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplyingφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodelsthatarenonlinearasafunctionofxusingconvexoptimizationtechniquesthatareguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedandoptimizeonlyα,i.e.,theoptimizationalgorithmcanviewthedecisionfunctionasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmitsanimplementationthatissigniﬁcantlymorecomputationaleﬃcientthannaivelyconstructingtwovectorsandexplicitlytakingtheirdotproduct.φ()xInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultinaninﬁnitecomputationalcostforthenaive,explicitapproach.Inmanycases,k(xx,\\ue030)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As141'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 156}, page_content='CHAPTER5.MACHINELEARNINGBASICSanexampleofaninﬁnite-dimensionalfeaturespacewithatractablekernel,weconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethatthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.Wecanwriteakernelfunctionk(x,x()i) =min(x,x()i)thatisexactlyequivalenttothecorrespondinginﬁnite-dimensionaldotproduct.ThemostcommonlyusedkernelistheGaussiankernelk,,σ(uvuv) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 156}, page_content='=min(x,x()i)thatisexactlyequivalenttothecorrespondinginﬁnite-dimensionaldotproduct.ThemostcommonlyusedkernelistheGaussiankernelk,,σ(uvuv) = (N−;02I)(5.84)whereN(x;µ,Σ)isthestandardnormaldensity.Thiskernelisalsoknownastheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglinesinvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadotproductinaninﬁnite-dimensionalspace,butthederivationofthisspaceislessstraightforwardthaninourexampleofthekernelovertheintegers.minWecanthinkoftheGaussiankernelasperformingakindof.templatematchingAtrainingexamplexassociatedwithtraininglabelybecomesatemplateforclassy.Whenatestpointx\\ue030isnearxaccordingtoEuclideandistance,theGaussiankernelhasalargeresponse,indicatingthatx\\ue030isverysimilartothextemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 156}, page_content='Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythesimilarityofthecorrespondingtrainingexamples.Supportvectormachinesarenottheonlyalgorithmthatcanbeenhancedusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.Thecategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachinesorkernelmethods(,;WilliamsandRasmussen1996Schölkopf1999etal.,).Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecisionfunctionislinearinthenumberoftrainingexamples,becausethei-thexamplecontributesatermαik(xx,()i)tothedecisionfunction.Supportvectormachinesareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyforthetrainingexamplesthathavenon-zeroαi.Thesetrainingexamplesareknownassupportvectors.Kernelmachinesalsosuﬀerfromahighcomputationalcostoftrainingwhenthedatasetislarge.WewillrevisitthisideainSec.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 156}, page_content='.Kernelmachineswith5.9generickernelsstruggletogeneralizewell.WewillexplainwhyinSec..The5.11modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsofkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM2006ontheMNISTbenchmark.142'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 157}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.7.3OtherSimpleSupervisedLearningAlgorithmsWehavealreadybrieﬂyencounteredanothernon-probabilisticsupervisedlearningalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsisafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asanon-parametriclearningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxednumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithmasnothavinganyparameters,butratherimplementingasimplefunctionofthetrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,weﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturntheaverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentiallyanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.Inthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithcy= 1andci='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 157}, page_content='1andci= 0forallothervaluesofi.Wecantheninterprettheaverageovertheseone-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametriclearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,supposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe1numberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayeserrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequallydistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsxwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthealgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingoneofthem,theprocedureconvergestotheBayeserrorrate.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 157}, page_content='Thehighcapacityofk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.However,itdoessoathighcomputationalcost,anditmaygeneralizeverybadlygivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatitcannotlearnthatonefeatureismorediscriminativethananother.Forexample,imaginewehavearegressiontaskwithx∈R100drawnfromanisotropicGaussiandistribution,butonlyasinglevariablex1isrelevanttotheoutput.Supposefurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inallcases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberoffeaturesx2throughx100,notbythelonefeaturex1. Thustheoutputonsmalltrainingsetswillessentiallyberandom.143'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 158}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n0101\\n11101\\n011\\n1111111011010010001110111111010010001001111111\\n11Figure5.7:Diagramsdescribinghowadecisiontreeworks.(Top)Eachnodeofthetreechoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeontheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeisdisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtainedbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).(Bottom)Thetreedividesspaceintoregions. The2DplaneshowshowadecisiontreemightdivideR2.Thenodesofthetreeareplottedinthisplane,witheachinternalnodedrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthecenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,withonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitisnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthenumberoftrainingexamples.144'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 159}, page_content='CHAPTER5.MACHINELEARNINGBASICSAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregionsandhasseparateparametersforeachregionisthedecisiontree(,Breimanetal.1984)anditsmanyvariants.AsshowninFig.,eachnodeofthedecisiontree5.7isassociatedwitharegionintheinputspace,andinternalnodesbreakthatregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-alignedcut).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 159}, page_content='Spaceisthussub-dividedintonon-overlappingregions,withaone-to-onecorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymapseverypointinitsinputregiontothesameoutput.Decisiontreesareusuallytrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.Thelearningalgorithmcanbeconsiderednon-parametricifitisallowedtolearnatreeofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraintsthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyaretypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,struggletosolvesomeproblemsthatareeasyevenforlogisticregression.Forexample,ifwehaveatwo-classproblemandthepositiveclassoccurswhereverx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthusneedtoapproximatethedecisionboundarywithmanynodes,implementingastepfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunctionwithaxis-alignedsteps.Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemanylimitations.Nonetheless,theyareusefu'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 159}, page_content='mentingastepfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunctionwithaxis-alignedsteps.Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemanylimitations.Nonetheless,theyareusefullearningalgorithmswhencomputationalresourcesareconstrained.Wecanalsobuildintuitionformoresophisticatedlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetweensophisticatedalgorithmsand-NNordecisiontreebaselines.kSee(),(),'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 159}, page_content='()orothermachineMurphy2012Bishop2006Hastieetal.2001learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.5.8UnsupervisedLearningAlgorithmsRecallfromSec.thatunsupervisedalgorithmsarethosethatexperienceonly5.1.3“features”butnotasupervisionsignal.Thedistinctionbetweensupervisedandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisnoobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedbyasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextractinformationfromadistributionthatdonotrequirehumanlabortoannotateexamples.Thetermisusuallyassociatedwithdensityestimation,learningtodrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof145'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 160}, page_content='CHAPTER5.MACHINELEARNINGBASICSrelatedexamples.Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthedata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelookingforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhileobeyingsomepenaltyorconstraintaimedatkeepingtherepresentationorsimplermoreaccessiblethanitself.xTherearemultiplewaysofdeﬁningarepresentation.Threeofthesimpler'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 160}, page_content='mostcommonincludelowerdimensionalrepresentations,sparserepresentationsandindependentrepresentations.Low-dimensionalrepresentationsattempttocompressasmuchinformationaboutxaspossibleinasmallerrepresentation.Sparserepresentations(,;,;Barlow1989OlshausenandField1996HintonandGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesaremostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequiresincreasingthedimensionalityoftherepresentation,sothattherepresentationbecomingmostlyzeroesdoesnotdiscardtoomuchinformation.Thisresultsinanoverallstructureoftherepresentationthattendstodistributedataalongtheaxesoftherepresentationspace.Independentrepresentationsattempttodisentanglethesourcesofvariationunderlyingthedatadistributionsuchthatthedimensionsoftherepresentationarestatisticallyindependent.Of coursethese three criteriaare certainly'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 160}, page_content='notmutuallyexclusive.Low-dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewaytoreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifyingandremovingmoreredundancyallowsthedimensionalityreductionalgorithmtoachievemorecompressionwhilediscardinglessinformation.Thenotionofrepresentationisoneofthecentralthemesofdeeplearningandthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsomesimpleexamplesofrepresentationlearningalgorithms.Together,theseexamplealgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostoftheremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthatdevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.5.8.1PrincipalComponentsAnalysisInSec.,wesawthattheprincipalcomponentsanalysisalgorithmprovidesa2.12meansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearningalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedontwoofthecriter'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 160}, page_content='hattheprincipalcomponentsanalysisalgorithmprovidesa2.12meansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearningalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedontwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa146'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 161}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\n−−201001020x1−20−1001020x2−−201001020z1−20−1001020z2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 161}, page_content='Figure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariancewiththeaxesofthenewspace.(Left)Theoriginaldataconsistsofsamplesofx.Inthisspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.The(Right)transformeddataz=x\\ue03eWnowvariesmostalongtheaxisz1.Thedirectionofsecondmostvarianceisnowalongz2.representationthathaslowerdimensionalitythantheoriginalinput.Italsolearnsarepresentationwhoseelementshavenolinearcorrelationwitheachother.Thisisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsarestatisticallyindependent.Toachievefullindependence,arepresentationlearningalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsaninputxtoarepresentationzasshowninFig..InSec.,wesawthatwe5.82.12couldlearnaone-dimensionalrepresentationthatbestreconstructstheoriginaldata(inthesenseofmeansquarederror)andthatthisrepresentationactuallycorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCAas'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 161}, page_content='ldlearnaone-dimensionalrepresentationthatbestreconstructstheoriginaldata(inthesenseofmeansquarederror)andthatthisrepresentationactuallycorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCAasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuchoftheinformationinthedataaspossible(again,asmeasuredbyleast-squaresreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentationdecorrelatestheoriginaldatarepresentation.XLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethatthedatahasameanofzero,E[x]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 161}, page_content='=0.Ifthisisnotthecase,thedatacaneasilybecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.Theunbiasedsamplecovariancematrixassociatedwithisgivenby:XVar[] =x1m−1X\\ue03eX.(5.85)147'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 162}, page_content='CHAPTER5.MACHINELEARNINGBASICSPCAﬁndsarepresentation(throughlineartransformation)z=x\\ue03eWwhereVar[]zisdiagonal.InSec.,wesawthattheprincipalcomponentsofadesignmatrix2.12XaregivenbytheeigenvectorsofX\\ue03eX.Fromthisview,X\\ue03eXWW= Λ\\ue03e.(5.86)Inthissection,weexploitanalternativederivationoftheprincipalcomponents.Theprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbetherightsingularvectorsinthedecompositionX=UWΣ\\ue03e. Wethenrecovertheoriginaleigenvectorequationwithastheeigenvectorbasis:WX\\ue03eX=\\ue010UWΣ\\ue03e\\ue011\\ue03eUWΣ\\ue03e= WΣ2W\\ue03e.(5.87)TheSVDishelpfultoshowthatPCAresultsinadiagonalVar[z].UsingtheSVDof,wecanexpressthevarianceofas:XXVar[] =x1m−1X\\ue03eX(5.88)=1m−1(UWΣ\\ue03e)\\ue03eUWΣ\\ue03e(5.89)=1m−1WΣ\\ue03eU\\ue03eUWΣ\\ue03e(5.90)=1m−1WΣ2W\\ue03e,(5.91)whereweusethefactthatU\\ue03eU=IbecausetheUmatrixofthesingularvaluedeﬁnitionisdeﬁnedtobeorthonormal.Thisshowsthatifwetakez=x\\ue03eW,wecanensurethatthecovarianceofisdiagonalasrequired:zVar[]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 162}, page_content='=z1m−1Z\\ue03eZ(5.92)=1m−1W\\ue03eX\\ue03eXW(5.93)=1m−1W\\ue03eWΣ2W\\ue03eW(5.94)=1m−1Σ2,(5.95)wherethistimeweusethefactthatW\\ue03eW=I,againfromthedeﬁnitionoftheSVD.148'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 163}, page_content='CHAPTER5.MACHINELEARNINGBASICSTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelineartransformationW,theresultingrepresentationhasadiagonalcovariancematrix(asgivenbyΣ2)whichimmediatelyimpliesthattheindividualelementsofzaremutuallyuncorrelated.ThisabilityofPCAtotransformdataintoarepresentationwheretheelementsaremutuallyuncorrelatedisaveryimportantpropertyofPCA.Itisasimpleexampleofarepresentationthatattempttodisentangletheunknownfactorsofvariationunderlyingthedata.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 163}, page_content='InthecaseofPCA,thisdisentanglingtakestheformofﬁndingarotationoftheinputspace(describedbyW)thatalignstheprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociatedwith.zWhilecorrelationisanimportantcategoryofdependencybetweenelementsofthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemorecomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhatcanbedonewithasimplelineartransformation.5.8.2-meansClusteringkAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.Thek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclustersofexamplesthatareneareachother.Wecanthusthinkofthealgorithmasprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifxbelongstoclusteri,thenhi='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 163}, page_content='1andallotherentriesoftherepresentationharezero.Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparserepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,wewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,wheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodesareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁtsofadistributedrepresentation.Theone-hotcodestillconferssomestatisticaladvantages(itnaturallyconveystheideathatallexamplesinthesameclusteraresimilartoeachother)anditconfersthecomputationaladvantagethattheentirerepresentationmaybecapturedbyasingleinteger.Thek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ()k}todiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexofthenearestcentroidµ()i.Intheotherstep,eachcentroidµ()iisupdatedtothemeanofalltrainingexamplesx()jassignedtocluster.i149'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 164}, page_content='CHAPTER5.MACHINELEARNINGBASICSOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherentlyill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwellaclusteringofthedatacorrespondstotherealworld.WecanmeasurepropertiesoftheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothemembersofthecluster.Thisallowsustotellhowwellweareabletoreconstructthetrainingdatafromtheclusterassignments.Wedonotknowhowwelltheclusterassignmentscorrespondtopropertiesoftherealworld.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 164}, page_content='Moreover,theremaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyoftherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebutobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.Forexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingofimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgraycars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmayﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterofredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclusteringalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassigntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.Thisnewclusteringnowatleastcapturesinformationaboutbothattributes,butithaslostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgraycars,justastheyareinadiﬀerentclusterfromgraytrucks.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 164}, page_content='Theoutputoftheclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycarsthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisallweknow.Theseissuesillustratesomeofthereasonsthatwemaypreferadistributedrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhavetwoattributesforeachvehicle—onerepresentingitscolorandonerepresentingwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimaldistributedrepresentationis(howcanthelearningalgorithmknowwhetherthetwoattributesweareinterestedinarecolorandcar-versus-truckratherthanmanufacturerandage?)buthavingmanyattributesreducestheburdenonthealgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasuresimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributesinsteadofjusttestingwhetheroneattributematches.5.9StochasticGradientDescentNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochasticgradientdescentSGDor.Stochasticgradientdescentisanextensionofthegradient150'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 165}, page_content='CHAPTER5.MACHINELEARNINGBASICSdescentalgorithmintroducedinSec..4.3Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessaryforgoodgeneralization,butlargetrainingsetsarealsomorecomputationallyexpensive.Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasasumovertrainingexamplesofsomeper-examplelossfunction.Forexample,thenegativeconditionallog-likelihoodofthetrainingdatacanbewrittenasJ() = θEx,y∼ˆpdataL,y,(xθ) =1mm\\ue058i=1L(x()i,y()i,θ)(5.96)whereistheper-examplelossLL,y,py.(xθ) = log−(|xθ;)Fortheseadditivecostfunctions,gradientdescentrequirescomputing∇θJ()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 165}, page_content='= log−(|xθ;)Fortheseadditivecostfunctions,gradientdescentrequirescomputing∇θJ() =θ1mm\\ue058i=1∇θL(x()i,y()i,.θ)(5.97)ThecomputationalcostofthisoperationisO(m).Asthetrainingsetsizegrowstobillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitivelylong.Theinsightofstochasticgradientdescentisthatthegradientisanexpectation.Theexpectationmaybeapproximatelyestimatedusingasmallsetofsamples.Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamplesB={x(1),...,x(m\\ue030)}drawnuniformlyfromthetrainingset.Theminibatchsizem\\ue030istypicallychosentobearelativelysmallnumberofexamples,rangingfrom1toafewhundred.Crucially,m\\ue030isusuallyheldﬁxedasthetrainingsetsizemgrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputedononlyahundredexamples.Theestimateofthegradientisformedasg=1m\\ue030∇θm\\ue030\\ue058i=1L(x()i,y()i,.θ)(5.98)usingexamplesfromtheminibatch.ThestochasticgradientdescentalgorithmBthenfollowstheestimatedgradientdownhill:θθg←−\\ue00f,(5.99)whereisthelearningrate.\\ue00f151'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 166}, page_content='CHAPTER5.MACHINELEARNINGBASICSGradientdescentingeneralhasoftenbeenregardedassloworunreliable.Inthepast,theapplicationofgradientdescenttonon-convexoptimizationproblemswasregardedasfoolhardyorunprincipled.Today,weknowthatthemachinelearningmodelsdescribedinPartworkverywellwhentrainedwithgradientIIdescent.Theoptimizationalgorithmmaynotbeguaranteedtoarriveatevenalocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalueofthecostfunctionquicklyenoughtobeuseful.Stochasticgradientdescenthasmanyimportantusesoutsidethecontextofdeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylargedatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthetrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsizeincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreachconvergenceusuallyincreaseswithtrainingsetsize.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 166}, page_content='However,asmapproachesinﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbeforeSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnotextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletesterror.Fromthispointofview,onecanarguethattheasymptoticcostoftrainingamodelwithSGDisasafunctionof.O(1)mPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodelswastousethekerneltrickincombinationwithalinearmodel.Manykernellearningalgorithmsrequireconstructinganmm×matrixGi,j=k(x()i,x()j).ConstructingthismatrixhascomputationalcostO(m2),whichisclearlyundesirablefordatasetswith billionsof examples.In academia, starting in2006,deep'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 166}, page_content='billionsof examples.In academia, starting in2006,deep learningwasinitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetterthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensofthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestinindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlargedatasets.StochasticgradientdescentandmanyenhancementstoitaredescribedfurtherinChapter.85.10BuildingaMachineLearningAlgorithmNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesofafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,anoptimizationprocedureandamodel.Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof152'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 167}, page_content='CHAPTER5.MACHINELEARNINGBASICSXyand,thecostfunctionJ,b(w) = −Ex,y∼ˆpdatalogpmodel()y|x,(5.100)themodelspeciﬁcationpmodel(y|x) =N(y;x\\ue03ew+b,1),and,inmostcases,theoptimizationalgorithmdeﬁnedbysolvingforwherethegradientofthecostiszerousingthenormalequations.Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependentlyfromtheothers,wecanobtainaverywidevarietyofalgorithms.Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearningprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthenegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximumlikelihoodestimation.Thecostfunctionmayalsoincludeadditionalterms,suchasregularizationterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunctiontoobtainJ,bλ(w) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 167}, page_content='= ||||w22−Ex,py∼datalogpmodel()y|x.(5.101)Thisstillallowsclosed-formoptimization.Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolongerbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumericaloptimizationprocedure,suchasgradientdescent.Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,andoptimizationalgorithmssupportsbothsupervisedandunsupervisedlearning.Thelinearregressionexampleshowshowtosupportsupervisedlearning.UnsupervisedlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandprovidinganappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrstPCAvectorbyspecifyingthatourlossfunctionisJ() = wEx∼pdata||−||xr(;)xw22(5.102)whileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunctionr() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 167}, page_content='= wEx∼pdata||−||xr(;)xw22(5.102)whileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunctionr() = xw\\ue03exw.Insomecases,thecostfunctionmaybeafunctionthatwecannotactuallyevaluate,forcomputationalreasons.Inthesecases,wecanstillapproximatelyminimizeitusingiterativenumericaloptimizationsolongaswehavesomewayofapproximatingitsgradients.Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynotimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor153'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 168}, page_content='CHAPTER5.MACHINELEARNINGBASICShand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Somemodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecausetheircostfunctionshaveﬂatregionsthatmaketheminappropriateforminimizationbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithmscanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofataxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,ratherthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.5.11ChallengesMotivatingDeepLearningThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellonawidevarietyofimportantproblems.However,theyhavenotsucceededinsolvingthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.ThedevelopmentofdeeplearningwasmotivatedinpartbythefailureoftraditionalalgorithmstogeneralizewellonsuchAItasks.Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomesexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 168}, page_content='rtbythefailureoftraditionalalgorithmstogeneralizewellonsuchAItasks.Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomesexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhowthemechanismsusedtoachievegeneralizationintraditionalmachinelearningareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Suchspacesalsooftenimposehighcomputationalcosts.Deeplearningwasdesignedtoovercometheseandotherobstacles.5.11.1TheCurseofDimensionalityManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumberof'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 168}, page_content='dimensionsin thedata ishigh.This phenomenonis knownas thecurseofdimensionality. Ofparticularconcernisthatthenumberofpossibledistinctconﬁgurationsofasetofvariablesincreasesexponentiallyasthenumberofvariablesincreases.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 168}, page_content='154'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 169}, page_content='CHAPTER5.MACHINELEARNINGBASICS'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 169}, page_content='Figure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromlefttoright),thenumberofconﬁgurationsofinterestmaygrowexponentially.(Left)Inthisone-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregioncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithineachregion(andpossiblyinterpolatebetweenneighboringregions).With2(Center)dimensions(center)itismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable.Weneedtokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplestocoverallthoseregions.With3dimensionsthisgrowsto(Right)103='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 169}, page_content='1000regionsandatleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeachaxis,weseemtoneedO(vd)regionsandexamples.Thisisaninstanceofthecurseofdimensionality.FiguregraciouslyprovidedbyNicolasChapados.Thecurseofdimensionalityarisesinmanyplacesincomputerscience,andespeciallysoinmachinelearning.Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge.AsillustratedinFig.,astatisticalchallengearisesbecausethenumberof5.9possibleconﬁgurationsofxismuchlargerthanthenumberoftrainingexamples.Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoagrid,likeintheﬁgure.Inlowdimensionswecandescribethisspacewithalownumberofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdatapoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamplesthatlieinthesamecellasthenewinput.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 169}, page_content='Forexample,ifestimatingtheprobabilitydensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesinthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftrainingexamplesinthesamecell.Ifwearedoingregressionwecanaveragethetargetvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhichwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberofconﬁgurationsisgoingtobehuge,muchlargerthanournumberofexamples,mostconﬁgurationswillhavenotrainingexampleassociatedwithit.155'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 170}, page_content='CHAPTER5.MACHINELEARNINGBASICSHowcouldwepossiblysaysomethingmeaningfulaboutthesenewconﬁgurations?Manytraditionalmachinelearningalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximatelythesameastheoutputatthenearesttrainingpoint.5.11.2LocalConstancyandSmoothnessRegularizationInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbypriorbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseenthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributionsoverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsasdirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparametersfunctionviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsasbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosingsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingourdegreeofbeliefinvariousfunctions.Amongthemostwidelyusedoftheseimplicit“p'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 170}, page_content='tionsoveranother,eventhoughthesebiasesmaynotbeexpressed(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingourdegreeofbeliefinvariousfunctions.Amongthemostwidelyusedoftheseimplicit“priors”isthesmoothnesspriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearnshouldnotchangeverymuchwithinasmallregion.Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,andasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-leveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroducesadditional(explicit'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 170}, page_content='andimplicit)priorsinorderto reducethegeneralizationerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneisinsuﬃcientforthesetasks.Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbeliefthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerentmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗thatsatisﬁestheconditionf∗() x≈f∗(+)x\\ue00f(5.103)formostconﬁgurationsxandsmallchange\\ue00f.Inotherwords,ifweknowagoodanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthatanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswersinsomeneighborhoodwewouldcombinethem(bysomeformofaveragingorinterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchaspossible.Anextremeexampleofthelocalconstancyapproachisthek-nearestneighbors156'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 171}, page_content='CHAPTER5.MACHINELEARNINGBASICSfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereachregioncontainingallthepointsxthathavethesamesetofknearestneighborsinthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemorethanthenumberoftrainingexamples.Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytrainingexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociatedwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocalkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfartherapartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunctionthatperformstemplatematching,bymeasuringhowcloselyatestexamplexresembleseachtrainingexamplex()i.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 171}, page_content='Muchofthemodernmotivationfordeeplearningisderivedfromstudyingthelimitationsoflocaltemplatematchingandhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails(,).Bengioetal.2006bDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-basedlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereareleavesanduseaseparateparameter(orsometimesmanyparametersforextensionsofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithatleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesarerequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatisticalconﬁdenceinthepredictedoutput.Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethodsrequireO(k) examples.TypicallythereareO(k) parameters,withO(1)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 171}, page_content='examples.TypicallythereareO(k) parameters,withO(1) parametersassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,whereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustratedinFig..5.10Isthereawaytorepresentacomplexfunctionthathasmanymoreregionstobedistinguishedthanthenumberoftrainingexamples?Clearly,assumingonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.For example, imaginethat'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 171}, page_content='example, imaginethat thetargetfunctionisakindofcheckerboard.Acheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.Imaginewhathappenswhenthenumberoftrainingexamplesissubstantiallysmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Basedononlylocalgeneralizationandthesmoothnessorlocalconstancyprior,wewouldbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesamecheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearnercouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdonotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan157'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 172}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspaceintoregions. Anexample(representedherebyacircle)withineachregiondeﬁnestheregionboundary(representedherebythelines).Theyvalueassociatedwitheachexampledeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. TheregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoidiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumberoftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighboralgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthelocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexampleonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediatelysurroundingthatexample.\\n158'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 173}, page_content='CHAPTER5.MACHINELEARNINGBASICSexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsoftheentirecheckerboardrightistocovereachofitscellswithatleastoneexample.Thesmoothnessassumptionandtheassociatednon-parametriclearningalgo-rithmsworkextremelywellsolongasthereareenoughexamplesforthelearningalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleysofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthefunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutinadiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerentlyindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetoftrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahugenumberofregionscomparedtothenumberofexamples),isthereanyhopetogeneralizewell?Theanswertobothofthesequestionsisyes.Thekeyinsightisthataverylargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solongasweintroducesomedependenciesbet'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 173}, page_content='examples),isthereanyhopetogeneralizewell?Theanswertobothofthesequestionsisyes.Thekeyinsightisthataverylargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solongasweintroducesomedependenciesbetweentheregionsviaadditionalassumptionsabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactuallygeneralizenon-locally(,;,).ManyBengioandMonperrus2005Bengioetal.2006cdiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatarereasonableforabroadrangeofAItasksinordertocapturetheseadvantages.Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyprovidingtheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuchstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralizetoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoocomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,sowewantlearningalgorithmsthatembodymoregeneral-purposeassumptions.Thecoreid'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 173}, page_content='varietyofstructures.AItaskshavestructurethatismuchtoocomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,sowewantlearningalgorithmsthatembodymoregeneral-purposeassumptions.Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedbythecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningalgorithms.Theseapparentlymildassumptionsallowanexponentialgainintherelationshipbetweenthenumberofexamplesandthenumberofregionsthatcanbedistinguished.TheseexponentialgainsaredescribedmorepreciselyinSec.,Sec.,andSec..Theexponentialadvantagesconferredbythe6.4.115.415.5useofdeep,distributedrepresentationscountertheexponentialchallengesposedbythecurseofdimensionality.159'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 174}, page_content='CHAPTER5.MACHINELEARNINGBASICS5.11.3ManifoldLearningAnimportantconceptunderlyingmanyideasinmachinelearningisthatofamanifold.Aisaconnectedregion.Mathematically,itisasetofpoints,associatedmanifoldwithaneighborhoodaroundeachpoint.Fromanygivenpoint,themanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperiencethesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin3-Dspace.Thedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistenceoftransformationsthatcanbeappliedtomoveonthemanifoldfromonepositiontoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecanwalknorth,south,east,orwest.Althoughthereisaformalmathematicalmeaningtotheterm“manifold,”inmachinelearningittendstobeusedmorelooselytodesignateaconnectedsetofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberofdegreesoffreedom,ordimensions,embeddedinahigher-dimensionalspace.Eachdimensioncorrespondstoalocaldirectionofvariation.SeeFig.foran5.11exampleoftrainingdatalyingnearaone-dimensionalmanifolde'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 174}, page_content='allnumberofdegreesoffreedom,ordimensions,embeddedinahigher-dimensionalspace.Eachdimensioncorrespondstoalocaldirectionofvariation.SeeFig.foran5.11exampleoftrainingdatalyingnearaone-dimensionalmanifoldembeddedintwo-dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionalityofthemanifoldtovaryfromonepointtoanother.Thisoftenhappenswhenamanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingledimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 174}, page_content='0510152025303540........−10.−05.00.05.10.15.20.25.\\nFigure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactuallyconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicatestheunderlyingmanifoldthatthelearnershouldinfer.160'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 175}, page_content='CHAPTER5.MACHINELEARNINGBASICSManymachinelearningproblemsseemhopelessifweexpectthemachinelearningalgorithmto learnfunctions withinterestingvariations acrossall ofRn.ManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmostofRnconsistsofinvalidinputs,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 175}, page_content='andthatinterestinginputsoccuronlyalongacollectionofmanifoldscontainingasmallsubsetofpoints,withinterestingvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirectionsthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwemovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecaseofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthisprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthesupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassishighlyconcentrated.Theassumptionthatthedataliesalongalow-dimensionalmanifoldmaynotalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchasthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionisatleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsistsoftwocategoriesofobservations.Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheprobabilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeishighlyconcentrated.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 175}, page_content='Uniformnoiseessentiallyneverresemblesstructuredinputsfromthesedomains.Fig.showshow,instead,uniformlysampledpointslooklikethe5.12patternsofstaticthatappearonanalogtelevisionsetswhennosignalisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyatrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-languagetext?Almostzero,again,becausemostofthelongsequencesoflettersdonotcorrespondtoanaturallanguagesequence:thedistributionofnaturallanguagesequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 175}, page_content='161'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 176}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixelaccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-zeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencounteredinAIapplications,weneveractuallyobservethishappeninginpractice.ThissuggeststhattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthevolumeofimagespace.Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshowthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalsoestablishthattheexamplesweencounterareconnectedtoeachotherbyother162'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 177}, page_content='CHAPTER5.MACHINELEARNINGBASICSexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthatmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecondargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuchneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,wecancertainlythinkofmanypossibletransformationsthatallowustotraceoutamanifoldinimagespace:wecangraduallydimorbrightenthelights,graduallymoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesofobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmostapplications.Forexample,themanifoldofimagesofhumanfacesmaynotbeconnectedtothemanifoldofimagesofcatfaces.Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-tuitivereasonssupportingit.Morerigorousexperiments'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 177}, page_content='(Cayton2005Narayanan,;andMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum,;etal.,;,;etal.,2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger;,;,;,;andSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsofinterestinAI.Whenthedataliesonalow-dimensionalmanifold,itcanbemostnaturalformachinelearningalgorithmstorepresentthedataintermsofcoordinatesonthemanifold,ratherthanintermsofcoordinatesinRn.Ineverydaylife,wecanthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionstospeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notintermsofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,butholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneralprincipleisappliedinmanycontexts.Fig.showsthemanifoldstructureofa5.13datasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthemethodsnecessarytolearnsuchamanifoldstructure.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 177}, page_content='InFig.,wewillsee20.6howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.ThisconcludesPart,whichhasprovidedthebasicconceptsinmathematicsIandmachinelearningwhichareemployedthroughouttheremainingpartsofthebook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 177}, page_content='163'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 178}, page_content='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset(,)Gongetal.2000forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensionalmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobeabletodiscoveranddisentanglesuchmanifoldcoordinates.Fig.illustratessucha20.6feat.\\n164'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 179}, page_content='PartIIDeepNetworks:ModernPractices\\n165'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 180}, page_content='Thispartofthebooksummarizesthestateofmoderndeeplearningasitisusedtosolvepracticalapplications.Deeplearninghasalonghistoryandmanyaspirations.Severalapproacheshavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoalshaveyettoberealized.Theseless-developedbranchesofdeeplearningappearintheﬁnalpartofthebook.Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-nologiesthatarealreadyusedheavilyinindustry.Modern deeplearning provides avery powerful framework'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 180}, page_content='deeplearning provides avery powerful framework forsupervisedlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcanrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappinganinputvectortoanoutputvector,andthatareeasyforapersontodorapidly,canbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃcientlylargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribedasassociatingonevectortoanother,orthatarediﬃcultenoughthatapersonwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remainbeyondthescopeofdeeplearningfornow.Thispartofthebookdescribesthecoreparametricfunctionapproximationtechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.We begin by describingthe feedforward'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 180}, page_content='begin by describingthe feedforward deepnetworkmodelthatisusedtorepresentthesefunctions.Next,wepresentadvancedtechniquesforregularizationandoptimizationofsuchmodels.Scalingthesemodelstolargeinputssuchashighresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroducetheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneuralnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelinesforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringanapplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeeplearning.Thesechaptersarethemostimportantforapractitioner—someonewhowantstobeginimplementingandusingdeeplearningalgorithmstosolvereal-worldproblemstoday.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 180}, page_content='166'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 181}, page_content='Chapter6DeepFeedforwardNetworksDeepfeedforwardnetworksfeedforwardneuralnetworksmulti-,alsooftencalled,orlayerperceptronsMLPs(),arethequintessentialdeeplearningmodels.Thegoalofafeedforwardnetworkistoapproximatesomefunctionf∗.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 181}, page_content='Forexample,foraclassiﬁer,y=f∗(x)mapsaninputxtoacategoryy.Afeedforwardnetworkdeﬁnesamappingy=f(x;θ)andlearnsthevalueoftheparametersθthatresultinthebestfunctionapproximation.Thesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthefunctionbeingevaluatedfromx,throughtheintermediatecomputationsusedtodeﬁnef,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhichoutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworksareextendedtoincludefeedbackconnections,theyarecalledrecurrentneuralnetworks,presentedinChapter.10Feedforwardnetworksareofextremeimportancetomachinelearningpracti-tioners.Theyformthebasisofmanyimportantcommercialapplications.Forexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosareaspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptualsteppingstoneonthepathtorecurrentnetworks,whichpowermanynaturallanguageapplications.Feedforwardneuralnetworksarecalledbecausetheyaretypicallyrep-networksresentedbycomposingtogethermanydiﬀerentfunct'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 181}, page_content='ualsteppingstoneonthepathtorecurrentnetworks,whichpowermanynaturallanguageapplications.Feedforwardneuralnetworksarecalledbecausetheyaretypicallyrep-networksresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisassociatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposedtogether.Forexample,wemighthavethreefunctionsf(1),f(2),andf(3)connectedinachain,toformf(x)=f(3)(f(2)(f(1)(x))).Thesechainstructuresarethemostcommonlyusedstructuresofneuralnetworks.Inthiscase,f(1)iscalledtheﬁrstlayerofthenetwork,f(2)iscalledthesecondlayer,andsoon.Theoveralllength167'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 182}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofthechaingivestheofthemodel.Itisfromthisterminologythatthedepthname“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalledtheoutputlayer.Duringneuralnetworktraining,wedrivef(x)tomatchf∗(x).Thetrainingdataprovidesuswithnoisy,approximateexamplesoff∗(x)evaluatedatdiﬀerenttrainingpoints. Eachexamplexisaccompaniedbyalabelyf≈∗(x).Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpointx;itmustproduceavaluethatisclosetoy.Thebehavioroftheotherlayersisnotdirectlyspeciﬁedbythetrainingdata.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 182}, page_content='Thelearningalgorithmmustdecidehowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoesnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmustdecidehowtousetheselayerstobestimplementanapproximationoff∗.Becausethetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,theselayersarecalled.hiddenlayersFinally,thesenetworksarecalledneuralbecausetheyarelooselyinspiredbyneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.Thedimensionalityofthesehiddenlayersdeterminestheofthemodel.Eachwidthelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,wecanalsothinkofthelayerasconsistingofmanythatactinparallel,unitseachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuroninthesensethatitreceivesinputfrommanyotherunitsandcomputesitsownactivationvalue.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 182}, page_content='Theideaofusingmanylayersofvector-valuedrepresentationisdrawnfromneuroscience.Thechoiceofthefunctionsf()i(x)usedtocomputetheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsaboutthefunctionsthatbiologicalneuronscompute.However,modernneuralnetworkresearchisguidedbymanymathematicalandengineeringdisciplines,andthegoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkoffeedforwardnetworksasfunctionapproximationmachinesthataredesignedtoachievestatisticalgeneralization,occasionallydrawingsomeinsightsfromwhatweknowaboutthebrain,ratherthanasmodelsofbrainfunction.Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodelsandconsiderhowtoovercometheirlimitations.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 182}, page_content='Linearmodels,suchaslogisticregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃcientlyandreliably,eitherinclosedformorwithconvexoptimization.Linearmodelsalsohavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,sothemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapplythelinearmodelnottoxitselfbuttoatransformedinputφ(x),whereφisa168'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 183}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedinSec.,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying5.7.2theφmapping.Wecanthinkofφasprovidingasetoffeaturesdescribingx,orasprovidinganewrepresentationfor.xThequestionisthenhowtochoosethemapping.φ1.Oneoptionistouseaverygenericφ,suchastheinﬁnite-dimensionalφthatisimplicitlyusedbykernelmachinesbasedontheRBFkernel. Ifφ(x)isofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthetrainingset,butgeneralizationtothetestsetoftenremainspoor.Verygenericfeaturemappingsareusuallybasedonlyontheprincipleoflocalsmoothnessanddonotencodeenoughpriorinformationtosolveadvancedproblems.2.Anotheroptionistomanuallyengineerφ.Untiltheadventofdeeplearning,thiswasthedominantapproach.Thisapproachrequiresdecadesofhumaneﬀortforeach separatetask, withpractitionersspecializing indiﬀerentdomainssuch asspeechrecognitionor computervision,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 183}, page_content='separatetask, withpractitionersspecializing indiﬀerentdomainssuch asspeechrecognitionor computervision, andwithlittletransferbetweendomains.3.Thestrategyofdeeplearningistolearnφ.Inthisapproach,wehaveamodely=f(x;θw,)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 183}, page_content='=φ(x;θ)\\ue03ew.Wenowhaveparametersθthatweusetolearnφfromabroadclassoffunctions,andparameterswthatmapfromφ(x)tothedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,withφdeﬁningahiddenlayer.Thisapproachistheonlyoneofthethreethatgivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweightheharms.Inthisapproach,weparametrizetherepresentationasφ(x;θ)andusetheoptimizationalgorithmtoﬁndtheθthatcorrespondstoagoodrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrstapproachbybeinghighlygeneric—wedosobyusingaverybroadfamilyφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.Humanpractitionerscanencodetheirknowledgetohelpgeneralizationbydesigningfamiliesφ(x;θ)thattheyexpectwillperformwell.Theadvantageisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunctionfamilyratherthanﬁndingpreciselytherightfunction.Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyondthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeeplearningthatappliestoallofthek'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 183}, page_content='ﬁndingpreciselytherightfunction.Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyondthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeeplearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic169'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 184}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmappingsfromxtoythatlackfeedbackconnections.Othermodelspresentedlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctionswithfeedback,andlearningprobabilitydistributionsoverasinglevector.Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesigndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecostfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-basedlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareuniquetofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofahiddenlayer,andthisrequiresustochoosethethatwillbeactivationfunctionsusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitectureofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthesenetworksshouldbeconnectedtoeachother,andhowmanyunitsshouldbeineachlayer.Learningindeepneuraln'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 184}, page_content='ervalues.Wemustalsodesignthearchitectureofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthesenetworksshouldbeconnectedtoeachother,andhowmanyunitsshouldbeineachlayer.Learningindeepneuralnetworksrequirescomputingthegradientsofcomplicatedfunctions.Wepresenttheback-propagationalgorithmanditsmoderngeneralizations,whichcanbeusedtoeﬃcientlycomputethesegradients.Finally,weclosewithsomehistoricalperspective.6.1Example:LearningXORTomaketheideaofafeedforwardnetworkmoreconcrete,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 184}, page_content='webeginwithanexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learningtheXORfunction.TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues,x1andx2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction1returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction1y=f∗(x)thatwewanttolearn.Ourmodelprovidesafunctiony=f(x;θ)andourlearningalgorithmwilladapttheparametersθtomakefassimilaraspossibletof∗.Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.WewantournetworktoperformcorrectlyonthefourpointsX={[0,0]\\ue03e,[0,1]\\ue03e,[1,0]\\ue03e,and[1,1]\\ue03e}. Wewilltrainthenetworkonallfourofthesepoints.Theonlychallengeistoﬁtthetrainingset.Wecantreatthisproblemasaregressionproblemanduseameansquarederrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthisexampleasmuchaspossible.Wewillseelaterthatthereareother,moreappropriate170'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 185}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSapproachesformodelingbinarydata.Evaluatedonourwholetrainingset,theMSElossfunctionisJ() =θ14\\ue058x∈X(f∗()(;))x−fxθ2.(6.1)Nowwemustchoosetheformofourmodel,f(x;θ).Supposethatwechoosealinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobeθwbf,b(;xw) = x\\ue03ew+b.(6.2)WecanminimizeJ(θ)inclosedformwithrespecttowandbusingthenormalequations.Aftersolvingthenormalequations,weobtainw=0andb=12.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 185}, page_content='= x\\ue03ew+b.(6.2)WecanminimizeJ(θ)inclosedformwithrespecttowandbusingthenormalequations.Aftersolvingthenormalequations,weobtainw=0andb=12. Thelinearmodelsimplyoutputs0.5everywhere.Whydoesthishappen?Fig.showshow6.1alinearmodelisnotabletorepresenttheXORfunction.Onewaytosolvethisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhichalinearmodelisabletorepresentthesolution.Speciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithonehiddenlayercontainingtwohiddenunits.SeeFig.foranillustrationof6.2thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatarecomputedbyafunctionf(1)(x;Wc,).Thevaluesofthesehiddenunitsarethenusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthenetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitisappliedtohratherthantox.Thenetworknowcontainstwofunctionschainedtogether:h=f(1)(x;Wc,)andy=f(2)(h;w,b),withthecompletemodelbeingf,,,bf(;xWcw) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 185}, page_content='= (2)(f(1)())x.Whatfunctionshouldf(1)compute?Linearmodelshaveserveduswellsofar,anditmaybetemptingtomakef(1)belinearaswell.Unfortunately,iff(1)werelinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionofitsinput.Ignoringtheintercepttermsforthemoment,supposef(1)(x) =W\\ue03exandf(2)(h) =h\\ue03ew.Thenf(x) =w\\ue03eW\\ue03ex.Wecouldrepresentthisfunctionasf() = xx\\ue03ew\\ue030wherew\\ue030= Ww.Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneuralnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,followedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethatstrategyhere,bydeﬁningh=g(W\\ue03ex+c),whereWprovidestheweightsofalineartransformationandcthebiases.Previously,todescribealinearregressionmodel,weusedavectorofweightsandascalarbiasparametertodescribean171'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 186}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n01x101x2OriginalSpacex\\n012h101h2LearnedSpaceh'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 186}, page_content='Figure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbersprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.(Left)AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXORfunction.Whenx1= 0,themodel’soutputmustincreaseasx2increases.Whenx1= 1,themodel’soutputmustdecreaseasx2increases.Alinearmodelmustapplyaﬁxedcoeﬃcientw2tox2.Thelinearmodelthereforecannotusethevalueofx1tochangethecoeﬃcientonx2andcannotsolvethisproblem.(Right)Inthetransformedspacerepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolvetheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen1collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshavemappedbothx= [1,0]\\ue03eandx= [0,1]\\ue03etoasinglepointinfeaturespace,h='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 186}, page_content='[1,0]\\ue03eandx= [0,1]\\ue03etoasinglepointinfeaturespace,h= [1,0]\\ue03e.Thelinearmodelcannowdescribethefunctionasincreasinginh1anddecreasinginh2.Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodelcapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learnedrepresentationscanalsohelpthemodeltogeneralize.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 186}, page_content='172'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 187}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSy yh hx xWwy yh1h1x1x1h2h2x2x2Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehiddenlayercontainingtwounits.(Left)Inthisstyle,wedraweveryunitasanodeinthegraph.Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexampleitcanconsumetoomuchspace.Inthisstyle,wedrawanodeinthe(Right)graphforeachentirevectorrepresentingalayer’sactivations.Thisstyleismuchmorecompact.Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthatdescribetherelationshipbetweentwolayers.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 187}, page_content='Here,weindicatethatamatrixWdescribesthemappingfromxtoh,andavectorwdescribesthemappingfromhtoy.Wetypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskindofdrawing.aﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribeanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbiasparametersisneeded.Theactivationfunctiongistypicallychosentobeafunctionthatisappliedelement-wise,withhi=g(x\\ue03eW:,i+ci).Inmodernneuralnetworks,thedefaultrecommendationistousetherectiﬁedlinearunitorReLU(Jarrettetal.etal.,;,;2009NairandHinton2010Glorot,)deﬁnedbytheactivation2011afunctiondepictedinFig..gz,z() = max0{}6.3Wecannowspecifyourcompletenetworkasf,,,b(;xWcw) = w\\ue03emax0{,W\\ue03exc+}+b.(6.3)WecannowspecifyasolutiontotheXORproblem.LetW=\\ue0141111\\ue015,(6.4)c=\\ue0140−1\\ue015,(6.5)w=\\ue0141−2\\ue015,(6.6)173'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 188}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0z0gz,z() = max0{}TheRectiﬁedLinearActivationFunction\\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefaultactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applyingthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinearfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,theypreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodelsgeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuildcomplicatedsystemsfromminimalcomponents. MuchasaTuringmachine’smemoryneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximatorfromrectiﬁedlinearfunctions.\\n174'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 189}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSand.b= 0Wecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,withoneexampleperrow:X=\\uf8ee\\uf8ef\\uf8ef\\uf8f000011011\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.7)Theﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrstlayer’sweightmatrix:XW=\\uf8ee\\uf8ef\\uf8ef\\uf8f000111122\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.8)Next,weaddthebiasvector,toobtainc\\uf8ee\\uf8ef\\uf8ef\\uf8f001−101021\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.9)Inthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong1thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto.010Alinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalueofforeachexample,weapplytherectiﬁedlineartransformation:h\\uf8ee\\uf8ef\\uf8ef\\uf8f000101021\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.10)Thistransformationhaschangedtherelationshipbetweentheexamples.Theynolongerlieonasingleline.AsshowninFig.,theynowlieinaspacewherea6.1linearmodelcansolvetheproblem.Weﬁnishbymultiplyingbytheweightvector:w\\uf8ee\\uf8ef\\uf8ef\\uf8f00110\\uf8f9\\uf8fa\\uf8fa\\uf8fb.(6.11)175'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtainedzeroerror.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='Inarealsituation,theremightbebillionsofmodelparametersandbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedidhere.Instead,agradient-basedoptimizationalgorithmcanﬁndparametersthatproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisataglobalminimumofthelossfunction,sogradientdescentcouldconvergetothispoint.ThereareotherequivalentsolutionstotheXORproblemthatgradientdescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsontheinitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynotﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresentedhere.6.2Gradient-BasedLearningDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermachinelearningmodelwithgradientdescent.InSec.,wedescribed5.10howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,acostfunction,andamodelfamily.Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneuralnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestinglossfu'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='pecifyinganoptimizationprocedure,acostfunction,andamodelfamily.Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneuralnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestinglossfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusuallytrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecostfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrainlinearregressionmodelsortheconvexoptimizationalgorithmswithglobalconver-genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimizationconvergesstartingfromanyinitialparameters(intheory—inpracticeitisveryrobustbutcanencounternumericalproblems).Stochasticgradientdescentappliedtonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitivetothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itisimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybeinitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-mizationalgorithmsusedtotrainfeedforwardnet'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='dneuralnetworks,itisimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybeinitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-mizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeepmodelswillbedescribedindetailinChapter,withparameterinitializationin8particulardiscussedinSec.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 190}, page_content='.Forthemoment,itsuﬃcestounderstandthat8.4thetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthecostfunctioninonewayoranother.Thespeciﬁcalgorithmsareimprovementsandreﬁnementsontheideasofgradientdescent,introducedinSec.,and,4.3176'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 191}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescentalgorithm,introducedinSec..5.9Wecanofcourse,trainmodelssuchaslinearregressionandsupportvectormachineswithgradientdescenttoo,andinfactthisiscommonwhenthetrainingsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightlymorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.Sec.willdescribehowtoobtainthegradientusingtheback-propagation6.5algorithmandmoderngeneralizationsoftheback-propagationalgorithm.Aswithothermachinelearningmodels,toapplygradient-basedlearningwemustchooseacostfunction,andwemustchoosehowtorepresenttheoutputofthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasisontheneuralnetworksscenario.6.2.1CostFunctionsAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthecostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorlessthesameasthoseforotherparametr'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 191}, page_content='networksscenario.6.2.1CostFunctionsAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthecostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorlessthesameasthoseforotherparametricmodels,suchaslinearmodels.Inmostcases,ourparametricmodeldeﬁnesadistributionp(yx|;θ)andwesimplyusethe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 191}, page_content='principleof maximum likelihood.Thismeansweuse thecross-entropybetweenthetrainingdataandthemodel’spredictionsasthecostfunction.Sometimes,wetakeasimplerapproach,whereratherthanpredictingacompleteprobabilitydistributionovery,wemerelypredictsomestatisticofyconditionedon.Specializedlossfunctionsallowustotrainapredictoroftheseestimates.xThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineoneoftheprimarycostfunctionsdescribedherewitharegularizationterm.WehavealreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsinSec.5.2.2.Theweightdecayapproachusedforlinearmodelsisalsodirectlyapplicabletodeepneuralnetworksandisamongthemostpopularregularizationstrategies.MoreadvancedregularizationstrategiesforneuralnetworkswillbedescribedinChapter.76.2.1.1LearningConditionalDistributionswithMaximumLikelihoodMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeansthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed177'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 192}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSasthecross-entropybetweenthetrainingdataandthemodeldistribution.ThiscostfunctionisgivenbyJ() = θ−Exy,∼ˆpdatalogpmodel()yx|.(6.12)Thespeciﬁcformofthecostfunctionchangesfrommodeltomodel,dependingonthespeciﬁcformoflogpmodel.Theexpansionoftheaboveequationtypicallyyieldssometermsthatdonotdependonthemodelparametersandmaybediscarded.Forexample,aswesawinSec.,if5.5.1pmodel(yx|) =N(y;f(x;θ),I),thenwerecoverthemeansquarederrorcost,Jθ()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 192}, page_content='=12Exy,∼ˆpdata||−||yf(;)xθ2+const,(6.13)uptoascalingfactorof12andatermthatdoesnotdependon.ThediscardedθconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscasewechosenottoparametrize.Previously,wesawthattheequivalencebetweenmaximumlikelihoodestimationwithanoutputdistributionandminimizationofmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholdsregardlessoftheusedtopredictthemeanoftheGaussian.f(;)xθAnadvantageofthisapproachofderivingthecostfunctionfrommaximumlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.Specifyingamodelp(yx|)automaticallydeterminesacostfunctionlogp(yx|).Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientofthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguideforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)underminethisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycasesthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthehiddenunitsortheoutputunitssaturate.Thene'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 192}, page_content='te(becomeveryﬂat)underminethisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycasesthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthehiddenunitsortheoutputunitssaturate.Thenegativelog-likelihoodhelpstoavoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunctionthatcansaturatewhenitsargumentisverynegative.Thelogfunctioninthenegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.WewilldiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitinSec..6.2.2Oneunusualpropertyofthecross-entropycostusedtoperformmaximumlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenappliedtothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,mostmodelsareparametrizedinsuchawaythattheycannotrepresentaprobabilityofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregressionisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel178'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 193}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKScancontrolthedensityoftheoutputdistribution(forexample,bylearningthevarianceparameterofaGaussianoutputdistribution)thenitbecomespossibletoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingincross-entropyapproachingnegativeinﬁnity.RegularizationtechniquesdescribedinChapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso7thatthemodelcannotreapunlimitedrewardinthisway.6.2.1.2LearningConditionalStatisticsInsteadoflearningafullprobabilitydistributionp(yx|;θ)weoftenwanttolearnjustoneconditionalstatisticofgiven.yxForexample,wemayhaveapredictorf(x;θ) thatwewishtopredictthemeanof.yIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneuralnetworkasbeingabletorepresentanyfunctionffromawideclassoffunctions,withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundednessratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,wecanviewthecostfunctionasbeingaratherthanjustafunction.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 193}, page_content='Afunctionalfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkoflearningaschoosingafunctionratherthanmerelychoosingasetofparameters.Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁcfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveitsminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematicaltoolcalledcalculusofvariations,describedinSec..Itisnotnecessaryto19.4.2understandcalculusofvariationstounderstandthecontentofthischapter.Atthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybeusedtoderivethefollowingtworesults.Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-tionproblemf∗= argminfExy,∼pdata||−||yf()x2(6.14)yieldsf∗() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 193}, page_content='argminfExy,∼pdata||−||yf()x2(6.14)yieldsf∗() = xEy∼pdata()yx|[]y,(6.15)solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwecouldtrainoninﬁnitelymanysamplesfromthetruedata-generatingdistribution,minimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthemeanofforeachvalueof.yx179'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 194}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusingcalculusofvariationsisthatf∗= argminfExy,∼pdata||−||yf()x1(6.16)yieldsafunctionthatpredictsthemedianvalueofyforeachx,solongassuchafunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscostfunctioniscommonlycalledmeanabsoluteerror.Unfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoorresultswhenusedwithgradient-basedoptimization.Someoutputunitsthatsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmeansquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimateanentiredistribution.p()yx|6.2.2OutputUnitsThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Mostofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthemodeldistribution.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 194}, page_content='Thechoiceofhowtorepresenttheoutputthendeterminestheformofthecross-entropyfunction.Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobeusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthemodel,butinprincipletheycanbeusedinternallyaswell.WerevisittheseunitswithadditionaldetailabouttheiruseashiddenunitsinSec..6.3Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesasetofhiddenfeaturesdeﬁnedbyh=f(x;θ).Theroleoftheoutputlayeristhentoprovidesomeadditionaltransformationfromthefeaturestocompletethetaskthatthenetworkmustperform.6.2.2.1LinearUnitsforGaussianOutputDistributionsOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformationwithnononlinearity.Theseareoftenjustcalledlinearunits.Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=W\\ue03eh+b.LinearoutputlayersareoftenusedtoproducethemeanofaconditionalGaussiandistribution:p() = (;yx|NyˆyI,).(6.17)180'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 195}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquarederror.ThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthecovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbeafunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositivedeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinearoutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.Approachestomodelingthecovariancearedescribedshortly,inSec..6.2.2.4Becauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimizationalgorithms.6.2.2.2SigmoidUnitsforBernoulliOutputDistributionsManytasksrequirepredictingthevalueofabinaryvariabley.Classiﬁcationproblemswithtwoclassescanbecastinthisform.Themaximum-likelihoodapproachistodeﬁneaBernoullidistributionoveryconditionedon.xABernoullidistributionisdeﬁnedbyjustasinglenumber.TheneuralnetneedstopredictonlyP(y='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 195}, page_content='1|x).Forthisnumbertobeavalidprobability,itmustlieintheinterval[0,1].Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposeweweretousealinearunit,andthresholditsvaluetoobtainavalidprobability:Py(= 1 ) = max|x\\ue06e0min,\\ue06e1,w\\ue03eh+b\\ue06f\\ue06f.(6.18)Thiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeabletotrainitveryeﬀectivelywithgradientdescent.Anytimethatw\\ue03eh+bstrayedoutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespecttoitsparameterswouldbe0.Agradientof0istypicallyproblematicbecausethelearningalgorithmnolongerhasaguideforhowtoimprovethecorrespondingparameters.Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysastronggradientwheneverthemodelhasthewronganswer.Thisapproachisbasedonusingsigmoidoutputunitscombinedwithmaximumlikelihood.Asigmoidoutputunitisdeﬁnedbyˆyσ= \\ue010w\\ue03eh+b\\ue011(6.19)181'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 196}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSwhereisthelogisticsigmoidfunctiondescribedinSec..σ3.10Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,itusesalinearlayertocomputez=w\\ue03eh+b.Next,itusesthesigmoidactivationfunctiontoconvertintoaprobability.zWeomitthedependenceonxforthemomenttodiscusshowtodeﬁneaprobabilitydistributionoveryusingthevaluez.Thesigmoidcanbemotivatedbyconstructinganunnormalizedprobabilitydistribution˜P(y),whichdoesnotsumto1.Wecanthendividebyanappropriateconstanttoobtainavalidprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalizedlogprobabilitiesarelinearinyandz,wecanexponentiatetoobtaintheunnormalizedprobabilities.WethennormalizetoseethatthisyieldsaBernoullidistributioncontrolledbyasigmoidaltransformationof:zlog˜Pyyz() = (6.20)˜Pyyz() = exp()(6.21)Py() =exp()yz\\ue0501y\\ue030=0exp(y\\ue030z)(6.22)Pyσyz.() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 196}, page_content='= (6.20)˜Pyyz() = exp()(6.21)Py() =exp()yz\\ue0501y\\ue030=0exp(y\\ue030z)(6.22)Pyσyz.() = ((2−1))(6.23)Probabilitydistributionsbasedonexponentiationandnormalizationarecommonthroughoutthestatisticalmodelingliterature.Thezvariabledeﬁningsuchadistributionoverbinaryvariablesiscalledalogit.Thisapproachtopredictingtheprobabilitiesinlog-spaceisnaturaltousewithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximumlikelihoodis−logP(y|x),theloginthecostfunctionundoestheexpofthesigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-based learningfrom making good progress.The lossfunction for maximumlikelihoodlearningofaBernoulliparametrizedbyasigmoidisJPy() = logθ−(|x)(6.24)= log((21))−σy−z(6.25)= ((12))ζ−yz.(6.26)ThisderivationmakesuseofsomepropertiesfromSec..Byrewriting3.10thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen(1−2y)zisverynegative.Saturationthusoccursonlywhenthemodelalreadyhastherightanswer—wheny= 1andzisverypositive,ory='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 196}, page_content='1andzisverypositive,ory= 0andzisverynegative.Whenzhasthewrongsign,theargumenttothesoftplusfunction,182'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 197}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS(1−2y)z,maybesimpliﬁedto||z.As||zbecomeslargewhilezhasthewrongsign,thesoftplusfunctionasymptotestowardsimplyreturningitsargument||z.Thederivativewithrespecttozasymptotestosign(z),so,inthelimitofextremelyincorrectz,thesoftplusfunctiondoesnotshrinkthegradientatall.Thispropertyisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquicklycorrectamistaken.zWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscansaturateanytimeσ(z)saturates.Thesigmoidactivationfunctionsaturatesto0whenzbecomesverynegativeandsaturatestowhen1zbecomesverypositive.Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoidoutputunits.Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,becausethesigmoidreturnsvaluesrestrictedtotheopeninterval(0,1),ratherthanusingtheentireclosedintervalofvalidprobabilities[0,1].Insoftwar'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 197}, page_content='its.Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,becausethesigmoidreturnsvaluesrestrictedtotheopeninterval(0,1),ratherthanusingtheentireclosedintervalofvalidprobabilities[0,1].Insoftwareimplementations,toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasafunctionofz,ratherthanasafunctionofˆy=σ(z).Ifthesigmoidfunctionunderﬂowstozero,thentakingthelogarithmofˆyyieldsnegativeinﬁnity.6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributionsAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariablewithnpossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasageneralizationofthesigmoidfunctionwhichwasusedtorepresentaprobabilitydistributionoverabinaryvariable.Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresenttheprobabilitydistributionoverndiﬀerentclasses.Morerarely,softmaxfunctionscanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneofndiﬀerentoptionsforsomeinternalvariable.Inthecaseofbinaryvariables,wewishedtoproduceasinglenumbe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 197}, page_content='entclasses.Morerarely,softmaxfunctionscanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneofndiﬀerentoptionsforsomeinternalvariable.Inthecaseofbinaryvariables,wewishedtoproduceasinglenumberˆyPy.='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 197}, page_content='(= 1 )|x(6.27)Becausethisnumberneededtoliebetweenand,andbecausewewantedthe01logarithmofthenumbertobewell-behavedforgradient-basedoptimizationofthelog-likelihood,wechosetoinsteadpredictanumberz=log˜P(y=1|x).ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythesigmoidfunction.183'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 198}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTogeneralizetothecaseofadiscretevariablewithnvalues,wenowneedtoproduceavectorˆy,withˆyi=P(y=i|x).Werequirenotonlythateachelementofˆyibebetweenand,butalsothattheentirevectorsumstosothat011itrepresentsavalidprobabilitydistribution.ThesameapproachthatworkedfortheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinearlayerpredictsunnormalizedlogprobabilities:zW= \\ue03ehb+,(6.28)wherezi=log˜P(y=i|x).Thesoftmaxfunctioncanthenexponentiateandnormalizetoobtainthedesiredzˆy.Formally,thesoftmaxfunctionisgivenbysoftmax()zi=exp(zi)\\ue050jexp(zj).(6.29)Aswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhentrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.Inthiscase,wewishtomaximizelogP(y=i;z)=logsoftmax(z)i.Deﬁningthesoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundotheofthesoftmax:explogsoftmax()zi='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 198}, page_content='zi−log\\ue058jexp(zj).(6.30)TheﬁrsttermofEq.showsthattheinput6.30zialwayshasadirectcon-tributiontothecostfunction.Becausethistermcannotsaturate,weknowthatlearningcanproceed,evenifthecontributionofzitothesecondtermofEq.6.30becomesverysmall.Whenmaximizingthelog-likelihood,theﬁrsttermencourageszitobepushedup,whilethesecondtermencouragesallofztobepusheddown.Togainsomeintuitionforthesecondterm,log\\ue050jexp(zj),observethatthistermcanberoughlyapproximatedbymaxjzj.Thisapproximationisbasedontheideathatexp(zk)isinsigniﬁcantforanyzkthatisnoticeablylessthanmaxjzj.Theintuitionwecangainfromthisapproximationisthatthenegativelog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrectprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,thenthe−zitermandthelog\\ue050jexp(zj)≈maxjzj=zitermswillroughlycancel.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 198}, page_content='Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbedominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.Sofarwehavediscussedonlyasingleexample.Overall,unregularizedmaximumlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict184'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 199}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSthefractionofcountsofeachoutcomeobservedinthetrainingset:softmax((;))zxθi≈\\ue050mj=11y()j=i,x()j=x\\ue050mj=11x()j=x.(6.31)Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappensolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.Inpractice,limitedmodelcapacityandimperfectoptimizationwillmeanthatthemodelisonlyabletoapproximatethesefractions.Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswellwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogtoundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomesverynegative,causingthegradienttovanish.Inparticular,squarederrorisapoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeitsoutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexaminethesoftmaxfunctionitself.Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhasasingleoutputthatsaturateswhenitsinputis'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 199}, page_content='1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexaminethesoftmaxfunctionitself.Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhasasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremelypositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.Theseoutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecomeextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmaxalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofitsinputs:softmax()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 199}, page_content='= softmax(+)zzc.(6.32)Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:softmax() = softmax(maxzz−izi).(6.33)Thereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumericalerrorsevenwhenzcontainsextremelylargeorextremelynegativenumbers.Ex-aminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdrivenbytheamountthatitsargumentsdeviatefrommaxizi.Anoutputsoftmax(z)isaturatestowhenthecorrespondinginputismaximal1(zi=maxizi)andziismuchgreaterthanalloftheotherinputs.Theoutputsoftmax(z)icanalsosaturatetowhen0ziisnotmaximalandthemaximumismuchgreater.Thisisageneralizationofthewaythatsigmoidunitssaturate,and185'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 200}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKScancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedtocompensateforit.Theargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutputeveryelementofz,asdescribedaboveusingthelinearlayerz=W\\ue03eh+b.Whilestraightforward,thisapproachactuallyoverparametrizesthedistribution.Theconstraintthatthenoutputsmustsumtomeansthatonly1n−1parametersarenecessary;theprobabilityofthen-thvaluemaybeobtainedbysubtractingtheﬁrstn−1 1probabilitiesfrom.Wecanthusimposearequirementthatoneelementofzbeﬁxed. Forexample,wecanrequirethatzn=0.Indeed,thisisexactlywhatthesigmoidunitdoes.DeﬁningP(y= 1|x) =σ(z)isequivalenttodeﬁningP(y= 1|x) =softmax(z)1withatwo-dimensionalzandz1='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 200}, page_content='1|x) =σ(z)isequivalenttodeﬁningP(y= 1|x) =softmax(z)1withatwo-dimensionalzandz1= 0.Boththen−1argumentandthenargumentapproachestothesoftmaxcandescribethesamesetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,thereisrarelymuchdiﬀerencebetweenusingtheoverparametrizedversionortherestrictedversion,anditissimplertoimplementtheoverparametrizedversion.Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxasawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:thesoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarilycorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateralinhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Attheextreme(whenthediﬀerencebetweenthemaximalaiandtheothersislargeinmagnitude)itbecomesaformof(oneoftheoutputsisnearly1winner-take-allandtheothersarenearly0).Thename“softmax”canbesomewhatconfusing.Thefunctionismorecloselyrelatedtotheargmaxfunctionthanthemaxfunction.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 200}, page_content='Theterm“soft”derivesfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable.Theargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuousordiﬀerentiable.Thesoftmaxfunctionthusprovidesa“softened”versionoftheargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)\\ue03ez.Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthecurrentnameisanentrenchedconvention.6.2.2.4OtherOutputTypesThelinear, sigmoid,andsoftmaxoutputunitsdescribed above arethemostcommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthatwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign186'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSagoodcostfunctionfornearlyanykindofoutputlayer.Ingeneral,ifwedeﬁneaconditionaldistributionp(yx|;θ),theprincipleofmaximumlikelihoodsuggestsweuseasourcostfunction.−|log(pyxθ;)Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunctionf(x;θ).Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,f(x;θ)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='=ωprovidestheparametersforadistributionovery.Ourlossfunctioncanthenbeinterpretedas.−log(;())pyωxForexample,wemaywishtolearnthevarianceofaconditionalGaussianfory,givenx.Inthesimplecase,wherethevarianceσ2isaconstant,thereisaclosedformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplytheempiricalmeanofthesquareddiﬀerencebetweenobservationsyandtheirexpectedvalue.Acomputationallymoreexpensiveapproachthatdoesnotrequirewritingspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthedistributionp(y|x)thatiscontrolledbyω=f(x;θ).Thenegativelog-likelihood−logp(y;ω(x))willthenprovideacostfunctionwiththeappropriatetermsnecessarytomakeouroptimizationprocedureincrementallylearnthevariance.Inthesimplecasewherethestandarddeviationdoesnotdependontheinput,wecanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnewparametermightbeσitselforcouldbeaparametervrepresentingσ2oritcouldbeaparameterβrepresenting1σ2,dependingonhowwechoosetoparametrizethedistribution.Wemaywishourm'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='enetworkthatiscopieddirectlyintoω.Thisnewparametermightbeσitselforcouldbeaparametervrepresentingσ2oritcouldbeaparameterβrepresenting1σ2,dependingonhowwechoosetoparametrizethedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvarianceinyfordiﬀerentvaluesofx.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='Thisiscalledaheteroscedasticmodel.Intheheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneofthevaluesoutputbyf(x;θ).AtypicalwaytodothisistoformulatetheGaussiandistributionusingprecision,ratherthanvariance,asdescribedinEq.3.22.Inthemultivariatecaseitismostcommontouseadiagonalprecisionmatrixdiag(6.34)()β.Thisformulationworkswellwithgradientdescentbecausetheformulaforthelog-likelihoodoftheGaussiandistributionparametrizedbyβinvolvesonlymul-tiplicationbyβiandadditionoflogβi.Thegradientofmultiplication,addition,andlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrizedtheoutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunctionbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,arbitrarilylargegradientsusuallyresultininstability.Ifweparametrizedtheoutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperationcanvanishnearzero,makingitdiﬃculttolearnparametersthatare'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 201}, page_content='eoutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperationcanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.187'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 202}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemustensurethatthecovariancematrixoftheGaussianispositivedeﬁnite.Becausetheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesofthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixispositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,thentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.Ifwesupposethataistherawactivationofthemodelusedtodeterminethediagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecisionvector:β=ζ(a).Thissamestrategyappliesequallyifusingvarianceorstandarddeviationratherthanprecisionorifusingascalartimesidentityratherthandiagonalmatrix.Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethandiagonal. Ifthecovarianceisfullandconditional,thenaparametrizationmustbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.ThiscanbeachievedbywritingΣ() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 202}, page_content='Ifthecovarianceisfullandconditional,thenaparametrizationmustbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.ThiscanbeachievedbywritingΣ() = ()xBxB\\ue03e()x,whereBisanunconstrainedsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthelikelihoodisexpensive,withadd×matrixrequiringO(d3)computationforthedeterminantandinverseofΣ(x)(orequivalently,andmorecommonlydone,itseigendecompositionorthatof).Bx()Weoftenwanttoperformmultimodalregression,thatis,topredictrealvaluesthatcomefromaconditionaldistributionp(yx|)thatcanhaveseveraldiﬀerentpeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureisanaturalrepresentationfortheoutput(,;,).Jacobsetal.1991Bishop1994NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixturedensitynetworks.AGaussianmixtureoutputwithncomponentsisdeﬁnedbytheconditionalprobabilitydistributionp() =yx|n\\ue058i=1pi(='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 202}, page_content='=yx|n\\ue058i=1pi(= c|Nx)(;yµ()i()x,Σ()i())x.(6.35)Theneuralnetworkmusthavethreeoutputs:avectordeﬁningp(c=i|x),amatrixprovidingµ()i(x)foralli,andatensorprovidingΣ()i(x)foralli.Theseoutputsmustsatisfydiﬀerentconstraints:1.Mixturecomponentsp(c=i|x):theseformamultinoullidistributionoverthendiﬀerentcomponentsassociatedwithlatentvariable1c,andcan1Weconsiderctobelatentbecausewedonotobserveitinthedata:giveninputxandtargety,itisnotpossibletoknowwithcertaintywhichGaussiancomponentwasresponsiblefory,butwecanimaginethatywasgeneratedbypickingoneofthem,andmakethatunobservedchoicearandomvariable.188'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 203}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKStypicallybeobtainedbyasoftmaxoverann-dimensionalvector,toguaranteethattheseoutputsarepositiveandsumto1.2.Meansµ()i(x):theseindicatethecenterormeanassociatedwiththei-thGaussiancomponent,andareunconstrained(typicallywithnononlinearityatallfortheseoutputunits).Ifyisad-vector,thenthenetworkmustoutputannd×matrixcontainingallnofthesed-dimensionalvectors. Learningthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthanlearningthemeansofadistributionwithonlyoneoutputmode.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 203}, page_content='Weonlywanttoupdatethemeanforthecomponentthatactuallyproducedtheobservation.Inpractice,wedonotknowwhichcomponentproducedeachobservation.Theexpressionforthenegativelog-likelihoodnaturallyweightseachexample’scontributiontothelossforeachcomponentbytheprobabilitythatthecomponentproducedtheexample.3.CovariancesΣ()i(x):thesespecifythecovariancematrixforeachcomponenti.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonalmatrixtoavoidneedingtocomputedeterminants.Aswithlearningthemeansofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassignpartialresponsibilityforeachpointtoeachmixturecomponent.Gradientdescentwillautomaticallyfollowthecorrectprocessifgiventhecorrectspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.Ithasbeenreportedthatgradient-basedoptimizationofconditionalGaussianmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseonegetsdivisions(bythevariance)whichcanbenumericallyunstable(whensomevariancegetstobesmallforaparticularexample,yieldingverylarg'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 203}, page_content='ussianmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseonegetsdivisions(bythevariance)whichcanbenumericallyunstable(whensomevariancegetstobesmallforaparticularexample,yieldingverylargegradients).Onesolutionistoclipgradients(seeSec.)whileanotheristoscalethe10.11.1gradientsheuristically(MurrayandLarochelle2014,).Gaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsofspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).Themixturedensitystrategygivesawayforthenetworktorepresentmultipleoutputmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtainingahighdegreeofqualityinthesereal-valueddomains.AnexampleofamixturedensitynetworkisshowninFig..6.4Ingeneral,wemaywishtocontinuetomodellargervectorsycontainingmorevariables,andtoimposericherandricherstructuresontheseoutputvariables.Forexample,wemaywishforourneuralnetworktooutputasequenceofcharactersthatforms'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 203}, page_content='asentence.Inthesecases, wemaycontinueto usetheprincipleofmaximumlikelihoodappliedtoourmodelp(y;ω(x)),butthemodelweuse189'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nxy'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='Figure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.Theinputxissampledfromauniformdistributionandtheoutputyissampledfrompmodel(yx|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputtotheparametersoftheoutputdistribution.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='Theseparametersincludetheprobabilitiesgoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellastheparametersforeachmixturecomponent.EachmixturecomponentisGaussianwithpredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareabletovarywithrespecttotheinput,andtodosoinnonlinearways.xtodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.Chapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels10oversequences,andPartdescribesadvancedtechniquesformodelingarbitraryIIIprobabilitydistributions.6.3HiddenUnitsSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthatarecommontomostparametricmachinelearningmodelstrainedwithgradient-basedoptimization.Nowweturntoanissuethatisuniquetofeedforwardneuralnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthemodel.Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnotyethavemanydeﬁnitiveguidingtheoreticalprinciples.Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyothertype'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='ersofthemodel.Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnotyethavemanydeﬁnitiveguidingtheoreticalprinciples.Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyothertypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentousewhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 204}, page_content='We190'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 205}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSdescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.Theseintuitionscanbeusedtosuggestwhentotryouteachoftheseunits.Itisusuallyimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsistsoftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthentraininganetworkwiththatkindofhiddenunitandevaluatingitsperformanceonavalidationset.Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableatallinputpoints.Forexample,therectiﬁedlinearfunctiong(z) =max{0,z}isnotdiﬀerentiableatz= 0.Thismayseemlikeitinvalidatesgforusewithagradient-basedlearningalgorithm.Inpractice,gradientdescentstillperformswellenoughforthesemodelstobeusedformachinelearningtasks.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 205}, page_content='Thisisinpartbecauseneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumofthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshowninFig..TheseideaswillbedescribedfurtherinChapter.Becausewedonot4.38expecttrainingtoactuallyreachapointwherethegradientis0,itisacceptablefortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.Hiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiableatonlyasmallnumberofpoints.Ingeneral,afunctiong(z)hasaleftderivativedeﬁnedbytheslopeofthefunctionimmediatelytotheleftofzandarightderivativedeﬁnedbytheslopeofthefunctionimmediatelytotherightofz.Afunctionisdiﬀerentiableatzonlyifboththeleftderivativeandtherightderivativearedeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneuralnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthecaseofg(z) =max{0,z},theleftderivativeatz='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 205}, page_content='=max{0,z},theleftderivativeatz= 00isandtherightderivativeis.Softwareimplementationsofneuralnetworktrainingusuallyreturnoneof1theone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedorraisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-basedoptimizationonadigitalcomputerissubjecttonumericalerroranyway.Whenafunctionisaskedtoevaluateg(0),itisveryunlikelythattheunderlyingvaluetrulywas.Instead,itwaslikelytobesomesmallvalue0\\ue00fthatwasroundedto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but0theseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthatinpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunitactivationfunctionsdescribedbelow.Unlessindicatedotherwise,mosthiddenunitscanbedescribedasacceptingavectorofinputsx,computinganaﬃnetransformationz=W\\ue03ex+b,andthenapplyinganelement-wisenonlinearfunctiong(z).Mosthiddenunitsaredistinguishedfromeachotheronlybythechoiceoftheformoftheactivationfunction.g()z191'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 206}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.1RectiﬁedLinearUnitsandTheirGeneralizationsRectiﬁedlinearunitsusetheactivationfunction.gz,z() = max0{}Rectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinearunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitisthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain.Thismakesthederivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeoftherectifyingoperationisalmosteverywhere,andthederivativeoftherectifying0operationiseverywherethattheunitisactive.Thismeansthatthegradient1directionisfarmoreusefulforlearningthanitwouldbewithactivationfunctionsthatintroducesecond-ordereﬀects.Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:hW='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 206}, page_content='(g\\ue03exb+).(6.36)Wheninitializingtheparametersoftheaﬃnetransformation,itcanbeagoodpracticetosetallelementsofbtoasmall,positivevalue,suchas0.1.Thismakesitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputsinthetrainingsetandallowthederivativestopassthrough.Severalgeneralizationsofrectiﬁedlinearunitsexist.Mostofthesegeneral-izationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperformbetter.Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-based methods onexamples forwhich theiractivation iszero.A varietyofgeneralizationsofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-where.Threegeneralizationsofrectiﬁedlinearunitsarebasedonusinganon-zeroslopeαiwhenzi<0:hi=g(zα,)i=max(0,zi)+αimin(0,zi). Absolutevaluerectiﬁcationﬁxesαi=−1toobtaing(z)=||z.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 206}, page_content='Absolutevaluerectiﬁcationﬁxesαi=−1toobtaing(z)=||z. Itisusedforobjectrecognitionfromimages(,),whereitmakessensetoseekfeaturesthatareJarrettetal.2009invariantunderapolarityreversaloftheinputillumination.Othergeneralizationsofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maasetal.2013)ﬁxesαitoasmallvaluelike0.01whileaparametricReLUPReLUortreatsαiasalearnableparameter(,).Heetal.2015Maxoutunits(,)generalizerectiﬁedlinearunitsfurther.Goodfellowetal.2013aInsteadofapplyinganelement-wisefunctiong(z),maxoutunitsdividezintogroupsofkvalues.Eachmaxoutunitthenoutputsthemaximumelementofone192'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 207}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofthesegroups:g()zi=maxj∈G()izj(6.37)whereG()iistheindicesoftheinputsforgroupi,{(i−1)k+1,...,ik}.Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultipledirectionsintheinputspace.xAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithuptokpieces.Maxoutunitscanthusbeseenaslearningtheactivationfunctionitselfratherthanjusttherelationshipbetweenunits.Withlargeenoughk,amaxoutunitcanlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,amaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionoftheinputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolutevaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearntoimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcoursebeparametrizeddiﬀerentlyfromanyoftheseotherlayertypes,sothelearningdynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthesamefunctionofasoneoftheotherlayertypes.xEachmaxoutunitisnowparametrizedbykweightvectorsinsteado'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 207}, page_content='yoftheseotherlayertypes,sothelearningdynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthesamefunctionofasoneoftheotherlayertypes.xEachmaxoutunitisnowparametrizedbykweightvectorsinsteadofjustone,somaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.Theycanworkwellwithoutregularizationifthetrainingsetislargeandthenumberofpiecesperunitiskeptlow(,).Caietal.2013Maxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-tisticalandcomputationaladvantagesbyrequiringfewerparameters.Speciﬁcally,ifthefeaturescapturedbyndiﬀerentlinearﬁlterscanbesummarizedwithoutlosinginformationbytakingthemaxovereachgroupofkfeatures,thenthenextlayercangetbywithtimesfewerweights.kBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-dancythathelpsthemtoresistaphenomenoncalledcatastrophicforgettinginwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedoninthepast(,).Goodfellowetal.2014aRectiﬁedlinearunitsandallofthesegeneralizationsofthemarebasedontheprinciplethatm'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 207}, page_content='catastrophicforgettinginwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedoninthepast(,).Goodfellowetal.2014aRectiﬁedlinearunitsandallofthesegeneralizationsofthemarebasedontheprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimizationalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscanlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentrainingthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismucheasierwhensomelinearcomputations(withsomedirectionalderivativesbeingofmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork193'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 208}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSarchitectures,theLSTM,propagatesinformationthroughtimeviasummation—aparticularstraightforwardkindofsuchlinearactivation.ThisisdiscussedfurtherinSec..10.106.3.2LogisticSigmoidandHyperbolicTangentPriortotheintroductionofrectiﬁedlinearunits,mostneuralnetworksusedthelogisticsigmoidactivationfunctiongzσz() = ()(6.38)orthehyperbolictangentactivationfunctiongzz.() = tanh()(6.39)Theseactivationfunctionsarecloselyrelatedbecause.tanh() = 2(2)1zσz−Wehave alreadyseen sigmoidunitsasoutputunits, used topredict'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 208}, page_content='= ()(6.38)orthehyperbolictangentactivationfunctiongzz.() = tanh()(6.39)Theseactivationfunctionsarecloselyrelatedbecause.tanh() = 2(2)1zσz−Wehave alreadyseen sigmoidunitsasoutputunits, used topredict theprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal1unitssaturateacrossmostoftheirdomain—theysaturatetoahighvaluewhenzisverypositive,saturatetoalowvaluewhenzisverynegative,andareonlystronglysensitivetotheirinputwhenzisnear0.Thewidespreadsaturationofsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruseasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenanappropriatecostfunctioncanundothesaturationofthesigmoidintheoutputlayer.Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangentactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresemblestheidentityfunctionmoreclosely,inthesensethattanh(0) = 0whileσ(0)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 208}, page_content='= 0whileσ(0) =12.Becausetanhissimilartoidentitynear,trainingadeepneuralnetwork0ˆy=w\\ue03etanh(U\\ue03etanh(V\\ue03ex))resemblestrainingalinearmodelˆy=w\\ue03eU\\ue03eV\\ue03exsolongastheactivationsofthenetworkcanbekeptsmall.Thismakestrainingthetanhnetworkeasier.Sigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-forwardnetworks.Recurrentnetworks,manyprobabilisticmodels, andsomeautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewiselinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethedrawbacksofsaturation.194'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 209}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.3OtherHiddenUnitsManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.Ingeneral,awidevarietyofdiﬀerentiablefunctionsperformperfectlywell.Manyunpublishedactivationfunctionsperformjustaswellasthepopularones.Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusingh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivationfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommontotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationsonstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunittypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcantimprovement.Newhiddenunittypesthatperformroughlycomparablytoknowntypesaresocommonastobeuninteresting.Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappearedintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.Onepossibilityistonothaveanactivationg(z)atall.Onecan'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 209}, page_content='beuninteresting.Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappearedintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.Onepossibilityistonothaveanactivationg(z)atall.Onecanalsothinkofthisasusingtheidentityfunctionastheactivationfunction.Wehavealreadyseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 209}, page_content='Itmayalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonlylineartransformations,thenthenetworkasawholewillbelinear.However,itisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consideraneuralnetworklayerwithninputsandpoutputs,h=g(W\\ue03ex+b).Wemayreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheotherusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehaveessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.Thefactoredapproachistocomputeh=g(V\\ue03eU\\ue03ex+b).IfUproducesqoutputs,thenUandVtogethercontainonly(n+p)qparameters,whileWcontainsnpparameters.Forsmallq,thiscanbeaconsiderablesavinginparameters.Itcomesatthecostofconstrainingthelineartransformationtobelow-rank,buttheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeraneﬀectivewayofreducingthenumberofparametersinanetwork.Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(asdescribedinSec.)butmaysometimesbeusedasahiddenunit.Softmax6.2.2.3unitsnaturallyrepresentaprobabilit'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 209}, page_content='educingthenumberofparametersinanetwork.Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(asdescribedinSec.)butmaysometimesbeusedasahiddenunit.Softmax6.2.2.3unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewithkpossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhiddenunitsareusuallyonlyusedinmoreadvancedarchitecturesthatexplicitlylearntomanipulatememory,describedinSec..10.12195'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 210}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAfewotherreasonablycommonhiddenunittypesinclude:•Radial basis functionRBFor unit:hi=exp\\ue010−1σ2i||W:,i−||x2\\ue011.ThisfunctionbecomesmoreactiveasxapproachesatemplateW:,i.Becauseitsaturatestoformost,itcanbediﬃculttooptimize.0x•Softplus:g(a) =ζ(a)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 210}, page_content='=log(1+ea).Thisisasmoothversionoftherectiﬁer,introducedby()forfunctionapproximationandbyDugasetal.2001NairandHinton2010()fortheconditionaldistributionsofundirectedprobabilisticmodels.()comparedthesoftplusandrectiﬁerandfoundGlorotetal.2011abetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.Thesoftplusdemonstratesthattheperformanceofhiddenunittypescanbeverycounterintuitive—onemightexpectittohaveanadvantageovertherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatinglesscompletely,butempiricallyitdoesnot.•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlikethelatter,itisbounded,g(a)=max(−1,min(1,a)).Itwasintroducedby().Collobert2004Hiddenunitdesignremainsanactiveareaofresearchandmanyusefulhiddenunittypesremaintobediscovered.6.4ArchitectureDesignAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.Thewordarchitecturereferstotheoverallstructureofthenetwork:howmanyunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.Mostneuralne'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 210}, page_content='nconsiderationforneuralnetworksisdeterminingthearchitecture.Thewordarchitecturereferstotheoverallstructureofthenetwork:howmanyunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.Mostneuralnetworksareorganizedintogroupsofunitscalledlayers.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 210}, page_content='Mostneuralnetworkarchitecturesarrangetheselayersinachainstructure,witheachlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayerisgivenbyh(1)= g(1)\\ue010W(1)\\ue03exb+(1)\\ue011,(6.40)thesecondlayerisgivenbyh(2)= g(2)\\ue010W(2)\\ue03eh(1)+b(2)\\ue011,(6.41)andsoon.196'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 211}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInthesechain-basedarchitectures,themainarchitecturalconsiderationsaretochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,anetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deepernetworksoftenareabletousefarfewerunitsperlayerandfarfewerparametersandoftengeneralizetothetestset,butarealsooftenhardertooptimize.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 211}, page_content='Theidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedbymonitoringthevalidationseterror.6.4.1UniversalApproximationPropertiesandDepthAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication,canbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasytotrainbecausemanylossfunctionsresultinconvexoptimizationproblemswhenappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequiresdesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-mationframework.Speciﬁcally,theuniversalapproximationtheorem(,Horniketal.1989Cybenko1989;,)statesthatafeedforwardnetworkwithalinearoutputlayerandatleastonehiddenlayerwithany“squashing”activationfunction(suchasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurablefunctionfromoneﬁnite-dimensionalspacetoanotherwithanydesirednon-zeroamountoferror,provi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 211}, page_content='rwithany“squashing”activationfunction(suchasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurablefunctionfromoneﬁnite-dimensionalspacetoanotherwithanydesirednon-zeroamountoferror,providedthatthenetworkisgivenenoughhiddenunits.Thederivativesofthefeedforwardnetworkcanalsoapproximatethederivativesofthefunctionarbitrarilywell(,).TheconceptofBorelmeasurabilityHorniketal.1990isbeyondthescopeofthisbook;'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 211}, page_content='forourpurposesitsuﬃcestosaythatanycontinuousfunctiononaclosedandboundedsubsetofRnisBorelmeasurableandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmayalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespacetoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswithactivationfunctionsthatsaturatebothforverynegativeandforverypositivearguments,universalapproximationtheoremshavealsobeenprovenforawiderclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinearunit(,).Leshnoetal.1993Theuniversalapproximationtheoremmeansthatregardlessofwhatfunctionwearetryingtolearn,weknowthatalargeMLPwillbeabletorepresentthisfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeabletolearnthatfunction.EveniftheMLPisabletorepresentthefunction,learningcanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining197'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 212}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesiredfunction.Second,thetrainingalgorithmmightchoosethewrongfunctionduetooverﬁtting.RecallfromSec.thatthe“nofreelunch”theoremshowsthat5.2.1thereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworksprovideauniversalsystemforrepresentingfunctions,inthesensethat,givenafunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.Thereisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesandchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.Theuniversalapproximationtheoremsaysthatthereexistsanetworklargeenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnotsayhowlargethisnetworkwillbe.()providessomeboundsontheBarron1993sizeofasingle-layernetworkneededtoapproximateabroadclassoffunctions.Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possiblywithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobedistinguished)mayberequired.Th'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 212}, page_content='approximateabroadclassoffunctions.Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possiblywithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobedistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:thenumberofpossiblebinaryfunctionsonvectorsv∈{0,1}nis22nandselectingonesuchfunctionrequires2nbits,whichwillingeneralrequireO(2n)degreesoffreedom.Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresentanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnandgeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethenumberofunitsrequiredtorepresentthedesiredfunctionandcanreducetheamountofgeneralizationerror.Thereexistfamiliesoffunctionswhichcanbeapproximatedeﬃcientlybyanarchitecturewithdepthgreaterthansomevalued,butwhichrequireamuchlargermodelifdepthisrestrictedtobelessthanorequaltod.Inmanycases,thenumberofhiddenunitsrequiredbytheshallowmodelisexponentialinn.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 212}, page_content='Suchresultswereﬁrstprovenformodelsthatdonotresemblethecontinuous,diﬀerentiableneuralnetworksusedformachinelearning,buthavesincebeenextendedtothesemodels.Theﬁrstresultswereforcircuitsoflogicgates(,).LaterHåstad1986workextendedtheseresultstolinearthresholdunitswithnon-negativeweights(,;,),andthentonetworkswithHåstadandGoldmann1991Hajnaletal.1993continuous-valuedactivations(,;,). ManymodernMaass1992Maassetal.1994neuralnetworksuserectiﬁedlinearunits.()demonstratedLeshnoetal.1993thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,includingrectiﬁedlinearunits,haveuniversalapproximationproperties,buttheseresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythatasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Pascanuetal.198'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 213}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS()and()showedthatfunctionsrepresentablewitha2013bMontufaretal.2014deeprectiﬁernetcanrequireanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtainedfromrectiﬁernonlinearitiesormaxoutunits)canrepresentfunctionswithanumberofregionsthatisexponentialinthedepthofthenetwork.Fig.illustrateshowanetworkwithabsolutevaluerectiﬁcationcreates6.5mirrorimagesofthefunctioncomputedontopofsomehiddenunit,withrespecttotheinputofthathiddenunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreatemirrorresponses(onbothsidesoftheabsolutevaluenonlinearity).Bycomposingthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewiselinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 213}, page_content='Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeperrectiﬁernetworksformallyshownbyPascanu2014aMontufar2014etal.()andbyetal.().(Left)Anabsolutevaluerectiﬁcationunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxisofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.Afunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimageofasimplerpatternacrossthataxisofsymmetry.(Center)Thefunctioncanbeobtainedbyfoldingthespacearoundtheaxisofsymmetry.(Right)Anotherrepeatingpatterncanbefoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry(whichisnowrepeatedfourtimes,withtwohiddenlayers).Moreprecisely,themaintheoremin()statesthattheMontufaretal.2014numberoflinearregionscarvedoutbyadeeprectiﬁernetworkwithdinputs,depth,andunitsperhiddenlayer,islnO\\ue020\\ue012nd\\ue013dl(−1)nd\\ue021,(6.42)i.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersperlkunit,thenumberoflinearregionsisO\\ue010k(1)+l−d\\ue011.(6.43)199'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 214}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearninapplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytimewechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsomesetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshouldlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwewanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbeinterpretedfromarepresentationlearningpointofviewassayingthatwebelievethelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariationthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsofvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressingabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingofmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 214}, page_content='Theseintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbeanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternalprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralizationforawidevarietyoftasks(,;,;,;Bengioetal.2007Erhanetal.2009Bengio2009Mesnil2011Ciresan2012Krizhevsky2012Sermanetetal.,;etal.,;etal.,;etal.,2013Farabet2013Couprie2013Kahou2013Goodfellow;etal.,;etal.,;etal.,;etal.etal.,;2014dSzegedy,).SeeFig.andFig.forexamplesofsome2014a6.66.7oftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoesindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.6.4.2OtherArchitecturalConsiderationsSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthemainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.Inpractice,neuralnetworksshowconsiderablymorediversity.Manyneuralnetworkarchitectureshavebeendevelopedforspeciﬁctasks.SpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksaredescribedinChapter.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 214}, page_content='ce,neuralnetworksshowconsiderablymorediversity.Manyneuralnetworkarchitectureshavebeendevelopedforspeciﬁctasks.SpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksaredescribedinChapter.Feedforwardnetworksmayalsobegeneralizedtothe9recurrentneuralnetworksforsequenceprocessing,describedinChapter,which10havetheirownarchitecturalconsiderations.Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthemostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextraarchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayeritolayeri+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfromoutputlayerstolayersnearertheinput.200'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 215}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n\\ue033\\ue034\\ue035\\ue036\\ue037\\ue038\\ue039\\ue031\\ue030\\ue031\\ue031\\ue04e\\ue075\\ue06d\\ue062\\ue065\\ue072\\ue020\\ue06f\\ue066\\ue020\\ue068\\ue069\\ue064\\ue064\\ue065\\ue06e\\ue020\\ue06c\\ue061\\ue079\\ue065\\ue072\\ue073\\ue039\\ue032\\ue02e\\ue030\\ue039\\ue032\\ue02e\\ue035\\ue039\\ue033\\ue02e\\ue030\\ue039\\ue033\\ue02e\\ue035\\ue039\\ue034\\ue02e\\ue030\\ue039\\ue034\\ue02e\\ue035\\ue039\\ue035\\ue02e\\ue030\\ue039\\ue035\\ue02e\\ue035\\ue039\\ue036\\ue02e\\ue030\\ue039\\ue036\\ue02e\\ue035\\ue054\\ue065\\ue073\\ue074\\ue020\\ue061\\ue063\\ue063\\ue075\\ue072\\ue061\\ue063\\ue079\\ue020\\ue028\\ue025\\ue029\\ue045\\ue066\\ue066\\ue065\\ue063\\ue074\\ue020\\ue06f\\ue066\\ue020\\ue044\\ue065\\ue070\\ue074\\ue068\\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenusedtotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellowetal.(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See2014dFig.foracontrolexperimentdemonstratingthatotherincreasestothemodelsizedo6.7notyieldthesameeﬀect.\\n201'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 216}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n000204060810......Numberofparameters×10891929394959697Testaccuracy(%)EﬀectofNumberofParameters3,convolutional3,fullyconnected11,convolutional'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 216}, page_content='Figure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelislarger.ThisexperimentfromGoodfellow2014detal.()showsthatincreasingthenumberofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnotnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthofnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeoftheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthiscontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhavingover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceoverthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthefunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresulteitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,cornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependentsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognizethem).\\n202'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 217}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnectapairoflayerstoeachother.InthedefaultneuralnetworklayerdescribedbyalineartransformationviaamatrixW,everyinputunitisconnectedtoeveryoutputunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,sothateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsintheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreducethenumberofparametersandtheamountofcomputationrequiredtoevaluatethenetwork,butareoftenhighlyproblem-dependent.Forexample,convolutionalnetworks,describedinChapter,usespecializedpatternsofsparseconnections9thatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃculttogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneuralnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthathavebeenfoundtoworkwellfordiﬀerentapplicationdomains.6.5Back-PropagationandOtherDiﬀerentiationAlgo-rithmsWhenweuseafeedforwardneuralnetworktoacceptan'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 217}, page_content='tersdeveloptheparticulararchitecturalstrategiesthathavebeenfoundtoworkwellfordiﬀerentapplicationdomains.6.5Back-PropagationandOtherDiﬀerentiationAlgo-rithmsWhenweuseafeedforwardneuralnetworktoacceptaninputxandproduceanoutputˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovidetheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayerandﬁnallyproducesˆy.Thisiscalledforwardpropagation.Duringtraining,forwardpropagationcancontinueonwarduntilitproducesascalarcostJ(θ).Theback-propagationbackpropalgorithm(,),oftensimplycalledRumelhartetal.1986a,allowstheinformationfromthecosttothenﬂowbackwardsthroughthenetwork,inordertocomputethegradient.Computingananalyticalexpressionforthegradientisstraightforward,butnumericallyevaluatingsuchanexpressioncanbecomputationallyexpensive.Theback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.Thetermback-propagationisoftenmisunderstood'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 217}, page_content='asmeaningthewholelearningalgorithmformulti-layerneuralnetworks.Actually,back-propagationrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.Furthermore,back-propagationisoftenmisunderstoodasbeingspeciﬁctomulti-layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthefunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient∇xf(xy,)foranarbitraryfunctionf,wherexisasetofvariableswhosederivativesaredesired,andyisanadditionalsetofvariablesthatareinputstothefunction203'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 218}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemostoftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,∇θJ(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,eitheraspartofthe learningprocess, or toanalyze thelearned model.The'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 218}, page_content='back-propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestrictedtocomputingthegradientofthecostfunctionwithrespecttotheparameters.Theideaofcomputingderivativesbypropagatinginformationthroughanetworkisverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunctionfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonlyusedcasewherehasasingleoutput.f6.5.1ComputationalGraphsSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohaveamoreprecisecomputationalgraphlanguage.Manywaysofformalizingcomputationasgraphsarepossible.Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemaybeascalar,vector,matrix,tensor,orevenavariableofanothertype.Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguageisaccompaniedbyasetofallowableoperations.Functionsmorecomplicatedthantheoperationsinthissetmaybedescribedbycomposingmany'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 218}, page_content='aofanoperation.Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguageisaccompaniedbyasetofallowableoperations.Functionsmorecomplicatedthantheoperationsinthissetmaybedescribedbycomposingmanyoperationstogether.Withoutlossofgenerality,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 218}, page_content='wedeﬁneanoperationtoreturnonlyasingleoutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhavemultipleentries,suchasavector.Softwareimplementationsofback-propagationusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinourdescriptionbecauseitintroducesmanyextradetailsthatarenotimportanttoconceptualunderstanding.Ifavariableyiscomputedbyapplyinganoperationtoavariablex,thenwedrawadirectededgefromxtoy. Wesometimesannotatetheoutputnodewiththenameoftheoperationapplied,andothertimesomitthislabelwhentheoperationisclearfromcontext.ExamplesofcomputationalgraphsareshowninFig..6.8204'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 219}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz zx xy y(a)×x xw w(b)u(1)u(1)dotb bu(2)u(2)+ˆyˆyσ\\n(c)X XW WU(1)U(1)matmulb bU(2)U(2)+H Hrelu\\nx xw w(d)ˆyˆydotλ λu(1)u(1)sqru(2)u(2)sumu(3)u(3)×\\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe(a)×operationtocomputez=xy. Thegraphforthelogisticregressionprediction(b)ˆy=σ\\ue000x\\ue03ew+b\\ue001.Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpressionbutneednamesinthegraph.Wesimplynamethei-thsuchvariableu()i.The(c)computationalgraphfortheexpressionH=max{0,XW+b},whichcomputesadesignmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatchofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit(d)ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthatappliesmorethanoneoperationtotheweightswofalinearregressionmodel.Theweightsareusedtomaketheboththepredictionˆyandtheweightdecaypenaltyλ\\ue050iw2i.205'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 220}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS6.5.2ChainRuleofCalculusThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)isusedtocomputethederivativesoffunctionsformedbycomposingotherfunctionswhosederivativesareknown.Back-propagationisanalgorithmthatcomputesthechainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.Letxbearealnumber,andletfandgbothbefunctionsmappingfromarealnumbertoarealnumber.Supposethaty=g(x)andz=f(g(x)) =f(y).Thenthechainrulestatesthatdzdx=dzdydydx.(6.44)Wecangeneralizethisbeyondthescalarcase.Supposethatx∈Rm,y∈Rn,gmapsfromRmtoRn,andfmapsfromRntoR.Ify=g(x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 220}, page_content='andz=f(y),then∂z∂xi=\\ue058j∂z∂yj∂yj∂xi.(6.45)Invectornotation,thismaybeequivalentlywrittenas∇xz=\\ue012∂y∂x\\ue013\\ue03e∇yz,(6.46)where∂y∂xistheJacobianmatrixof.nm×gFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplyingaJacobianmatrix∂y∂xbyagradient∇yz.Theback-propagationalgorithmconsistsofperformingsuchaJacobian-gradientproductforeachoperationinthegraph.Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,butrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythesameasback-propagationwithvectors.Theonlydiﬀerenceishowthenumbersarearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensorintoavectorbeforewerunback-propagation,computingavector-valuedgradient,andthenreshapingthegradientbackintoatensor.Inthisrearrangedview,back-propagationisstilljustmultiplyingJacobiansbygradients.TodenotethegradientofavaluezwithrespecttoatensorX,wewrite∇Xz,justasifXwereavector.TheindicesintoXnowhavemultiplecoordinates—forexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaw'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 220}, page_content='adients.TodenotethegradientofavaluezwithrespecttoatensorX,wewrite∇Xz,justasifXwereavector.TheindicesintoXnowhavemultiplecoordinates—forexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisawaybyusingasinglevariableitorepresentthecompletetupleofindices.Forallpossibleindextuplesi,(∇Xz)igives∂z∂Xi.Thisisexactlythesameashowforall206'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 221}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSpossibleintegerindicesiintoavector,(∇xz)igives∂z∂xi.Usingthisnotation,wecanwritethechainruleasitappliestotensors.Ifand,thenYX= (g)zf= ()Y∇Xz=\\ue058j(∇XYj)∂z∂Yj.(6.47)6.5.3RecursivelyApplyingtheChainRuletoObtainBackpropUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionforthegradientofascalarwithrespecttoanynodeinthecomputationalgraphthatproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputerintroducessomeextraconsiderations.Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithintheoverallexpressionforthegradient.Anyprocedurethatcomputesthegradientwillneedtochoosewhethertostorethesesubexpressionsortorecomputethemseveraltimes.AnexampleofhowtheserepeatedsubexpressionsariseisgiveninFig. . Insomecases,computingthesamesubexpressiontwicewouldsimply6.9bewasteful.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 221}, page_content='Forcomplicatedgraphs,therecanbeexponentiallymanyofthesewastedcomputations,makinganaiveimplementationofthechainruleinfeasible.Inothercases,computingthesamesubexpressiontwicecouldbeavalidwaytoreducememoryconsumptionatthecostofhigherruntime.Weﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁestheactualgradientcomputationdirectly(AlgorithmalongwithAlgorithm6.26.1fortheassociatedforwardcomputation),intheorderitwillactuallybedoneandaccordingtotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthesecomputationsorviewthedescriptionofthealgorithmasasymbolicspeciﬁcationofthecomputationalgraphforcomputingtheback-propagation.How-ever,thisformulationdoesnotmakeexplicitthemanipulationandtheconstructionofthesymbolicgraphthatperformsthegradientcomputation.SuchaformulationispresentedbelowinSec.,withAlgorithm,wherewealsogeneralizeto6.5.66.5nodesthatcontainarbitrarytensors.Firstconsideracomputationalgraphdescribinghowtocomputeasinglescalaru()n(saythelossonatrainingexample).Thissca'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 221}, page_content='dbelowinSec.,withAlgorithm,wherewealsogeneralizeto6.5.66.5nodesthatcontainarbitrarytensors.Firstconsideracomputationalgraphdescribinghowtocomputeasinglescalaru()n(saythelossonatrainingexample).Thisscalaristhequantitywhosegradientwewanttoobtain,withrespecttotheniinputnodesu(1)tou(ni).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 221}, page_content='Inotherwordswewishtocompute∂u()n∂u()iforalli∈{1,2,...,ni}.Intheapplicationofback-propagationtocomputinggradientsforgradientdescentoverparameters,u()nwillbethecostassociatedwithanexampleoraminibatch,whileu(1)tou(ni)correspondtotheparametersofthemodel.207'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 222}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSWewillassumethatthenodesofthegraphhavebeenorderedinsuchawaythatwecancomputetheiroutputoneaftertheother,startingatu(ni+1)andgoinguptou()n.AsdeﬁnedinAlgorithm,eachnode6.1u()iisassociatedwithanoperationf()iandiscomputedbyevaluatingthefunctionu()i= (fA()i)(6.48)whereA()iisthesetofallnodesthatareparentsofu()i.Algorithm6.1Aprocedurethatperformsthecomputationsmappingniinputsu(1)tou(ni)toanoutputu()n.Thisdeﬁnesacomputationalgraphwhereeachnodecomputesnumericalvalueu()ibyapplyingafunctionf()itothesetofargumentsA()ithatcomprisesthevaluesofpreviousnodesu()j,j<i,withjPa∈(u()i).Theinputtothecomputationalgraphisthevectorx,andissetintotheﬁrstninodesu(1)tou(ni).Theoutputofthecomputationalgraphisreadoﬀthelast(output)nodeu()n.fori,...,n= 1idou()i←xiendforforin='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 222}, page_content='1idou()i←xiendforforin= i+1,...,ndoA()i←{u()j|∈jPau(()i)}u()i←f()i(A()i)endforreturnu()nThatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecouldputinagraphG.Inordertoperformback-propagation,wecanconstructacomputationalgraphthatdependsonGandaddstoitanextrasetofnodes.TheseformasubgraphBwithonenodepernodeofG.ComputationinBproceedsinexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputesthederivative∂u()n∂u()iassociatedwiththeforwardgraphnodeu()i.Thisisdoneusingthechainrulewithrespecttoscalaroutputu()n:∂u()n∂u()j=\\ue058ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()j(6.49)asspeciﬁedbyAlgorithm.Thesubgraph6.2Bcontainsexactlyoneedgeforeachedgefromnodeu()jtonodeu()iofG.Theedgefromu()jtou()iisassociatedwiththecomputationof∂u()i∂u()j.Inaddition,adotproductisperformedforeachnode,betweenthegradientalreadycomputedwithrespecttonodesu()ithatarechildren208'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 223}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSofu()jandthevectorcontainingthepartialderivatives∂u()i∂u()jforthesamechildrennodesu()i.Tosummarize,theamountofcomputationrequiredforperformingtheback-propagationscaleslinearlywiththenumberofedgesinG,wherethecomputationforeachedgecorrespondstocomputingapartialderivative(ofonenodewithrespecttooneofitsparents)aswellasperformingonemultiplicationandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,whichisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemoreeﬃcientimplementations.Algorithm6.2Simpliﬁedversionoftheback-propagationalgorithmforcomputingthederivativesofu()nwithrespecttothevariablesinthegraph.Thisexampleisintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariablesarescalars,andwewishtocomputethederivativeswithrespecttou(1),...,u(ni).Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.Thecomputationalcostofthisalgorithmisproportionaltothenumberofedgesinthegraph,assumingthatthepartialderivativeassociatedwithe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 223}, page_content='),...,u(ni).Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.Thecomputationalcostofthisalgorithmisproportionaltothenumberofedgesinthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequiresaconstanttime.Thisisofthesameorderasthenumberofcomputationsfortheforwardpropagation.Each∂u()i∂u()jisafunctionoftheparentsu()jofu()i,thuslinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagationgraph.Runforwardpropagation(Algorithmforthisexample)toobtaintheactiva-6.1tionsofthenetworkInitializegrad_table,adatastructurethatwillstorethederivativesthathavebeencomputed.Theentrygradtable_[u()i]willstorethecomputedvalueof∂u()n∂u()i.gradtable_[∂u()n]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 223}, page_content='1←fordojn= −1downto1Thenextlinecomputes∂u()n∂u()j=\\ue050ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()jusingstoredvalues:gradtable_[u()j] ←\\ue050ijPau:∈(()i)gradtable_[u()i]∂u()i∂u()jendforreturn{gradtable_[u()i] = 1|i,...,ni}Theback-propagationalgorithmisdesignedtoreducethenumberofcommonsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorderofoneJacobianproductpernodeinthegraph. ThiscanbeseenfromthefactinAlgorithmthatbackpropvisitseachedgefromnode6.2u()jtonodeu()iofthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂u()j∂u()i.Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.209'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 224}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSHowever,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperformingsimpliﬁcationsonthecomputationalgraph,ormaybeabletoconservememorybyrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideasafterdescribingtheback-propagationalgorithmitself.6.5.4Back-PropagationComputationinFully-ConnectedMLPToclarifytheabovedeﬁnitionoftheback-propagationcomputation,letusconsiderthespeciﬁcgraphassociatedwithafully-connectedmulti-layerMLP.Algorithmﬁrstshowstheforwardpropagation,whichmapsparametersto6.3thesupervisedlossL(ˆyy,)associatedwithasingle(input,target)trainingexample()xy,,withˆytheoutputoftheneuralnetworkwhenisprovidedininput.xAlgorithm then shows the corresponding computation tobe done'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 224}, page_content='then shows the corresponding computation tobe done for6.4applyingtheback-propagationalgorithmtothisgraph.AlgorithmandAlgorithmaredemonstrationsthatarechosentobe6.36.4simpleandstraightforwardtounderstand.However,theyarespecializedtoonespeciﬁcproblem.Modernsoftwareimplementationsarebasedonthegeneralizedformofback-propagationdescribedinSec.below,whichcanaccommodateanycomputa-6.5.6tionalgraphbyexplicitlymanipulatingadatastructureforrepresentingsymboliccomputation.6.5.5Symbol-to-SymbolDerivativesAlgebraicexpressionsand computationalgraphsbothoperate onsymbols, orvariables thatdo nothave speciﬁc values.These'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 224}, page_content='computationalgraphsbothoperate onsymbols, orvariables thatdo nothave speciﬁc values.These algebraicandgraph-basedrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseortrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.Wereplaceasymbolicinputtothenetworkxwithaspeciﬁcvalue,suchasnumeric[12376518].,.,−.\\ue03e.Someapproachestoback-propagationtakeacomputationalgraphandasetofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumericalvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-to-number”diﬀerentiation.ThisistheapproachusedbylibrariessuchasTorch(,)andCaﬀe(,).Collobertetal.2011bJia2013Anotherapproachistotakeacomputationalgraphandaddadditionalnodestothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This210'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 225}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nx xy y\\nw wfff\\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputingthegradient.Letw∈Rbetheinputtothegraph.Weusethesamefunctionf:RR→astheoperationthatweapplyateverystepofachain:x=f(w),y=f(x),z=f(y).Tocompute∂z∂w,weapplyEq.andobtain:6.44∂z∂w(6.50)=∂z∂y∂y∂x∂x∂w(6.51)=f\\ue030()yf\\ue030()xf\\ue030()w(6.52)=f\\ue030((()))ffwf\\ue030(())fwf\\ue030()w(6.53)Eq.suggestsanimplementationinwhichwecomputethevalueof6.52f(w)onlyonceandstoreitinthevariablex.Thisistheapproachtakenbytheback-propagationalgorithm.AnalternativeapproachissuggestedbyEq. ,wherethesubexpression6.53f(w)appearsmorethanonce.Inthealternativeapproach,f(w)isrecomputedeachtimeitisneeded. Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,theback-propagationapproachofEq.isclearlypreferablebecauseofitsreduced6.52runtime.However,Eq.isalsoavalidimplementationofthechainrule,andisuseful6.53whenmemoryislimited.211'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 226}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkandthecomputationofthecostfunction.ThelossL(ˆyy,)dependsontheoutputˆyandonthetargety(seeSec.forexamplesoflossfunctions).Toobtainthe6.2.1.1totalcostJ,thelossmaybeaddedtoaregularizerΩ(θ),whereθcontainsalltheparameters(weightsandbiases).Algorithmshowshowtocomputegradients6.4ofJwithrespecttoparametersWandb.Forsimplicity,thisdemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshoulduseaminibatch.SeeSec.foramorerealisticdemonstration.6.5.7Require:Networkdepth,lRequire:W()i,i,...,l,∈{1}theweightmatricesofthemodelRequire:b()i,i,...,l,∈{1}thebiasparametersofthemodelRequire:x,theinputtoprocessRequire:y,thetargetoutputh(0)= xfordok,...,l= 1a()k= b()k+W()kh(1)k−h()k= (fa()k)endforˆyh= ()lJL= (ˆyy,)+Ω()λθistheapproachtakenbyTheano(,;,)Bergstraetal.2010Bastienetal.2012andTensorFlow(,).AnexampleofhowthisapproachworksAbadietal.2015is illustratedin Fig..Theprimary advantage ofthis approach'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 226}, page_content='illustratedin Fig..Theprimary advantage ofthis approach isthat6.10thederivativesaredescribedinthesamelanguageastheoriginalexpression.Becausethederivativesarejustanothercomputationalgraph,itispossibletorunback-propagationagain,diﬀerentiatingthederivativesinordertoobtainhigherderivatives.Computationofhigher-orderderivativesisdescribedinSec..6.5.10Wewillusethelatterapproachanddescribetheback-propagationalgorithmintermsofconstructingacomputationalgraphforthederivatives.Anysubsetofthegraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.Thisallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasitsparents’valuesareavailable.Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodasperformingexactlythesamecomputationsasaredoneinthegraphbuiltbythesymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number212'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 227}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 227}, page_content='Algorithm6.4Backwardcomputation forthedeepneuralnetwork ofAlgo-rithm,whichusesinadditiontotheinput6.3xatargety.Thiscomputationyieldsthegradientsontheactivationsa()kforeachlayerk,startingfromtheoutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,whichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchangetoreduceerror,onecanobtainthegradientontheparametersofeachlayer.Thegradientsonweightsandbiasescanbeimmediatelyusedaspartofastochas-ticgradientupdate(performingtheupdaterightafterthegradientshavebeencomputed)orusedwithothergradient-basedoptimizationmethods.Aftertheforwardcomputation,computethegradientontheoutputlayer:g←∇ˆyJ= ∇ˆyL(ˆy,y)fordokl,l,...,= −11Convertthegradient onthelayer’soutputinto agradient into thepre-nonlinearityactivation(element-wisemultiplicationifiselement-wise):fg←∇a()kJf= g\\ue00c\\ue030(a()k)Computegradientsonweightsandbiases(includingtheregularizationterm,whereneeded):∇b()kJλ= +g∇b()kΩ()θ∇W()kJ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 227}, page_content='g\\ue00c\\ue030(a()k)Computegradientsonweightsandbiases(includingtheregularizationterm,whereneeded):∇b()kJλ= +g∇b()kΩ()θ∇W()kJ= gh(1)k−\\ue03e+λ∇W()kΩ()θPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:g←∇h(1)k−J= W()k\\ue03egendfor'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 227}, page_content='213'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSz z\\nx xy y\\nw wfffzz\\nx xy y'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='w'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='wfffdzdydzdyf\\ue021dydxdydxf\\ue021dzdxdzdx×dxdwdxdwf\\ue021dzdwdzdw×Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.Inthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactualspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghowtocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethederivativesforanyspeciﬁcnumericvalues.(Left)Inthisexample,webeginwithagraphrepresentingz=f(f(f(w))).Weruntheback-propagationalgorithm,instructing(Right)ittoconstructthegraphfortheexpressioncorrespondingtodzdw.Inthisexample,wedonotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustratewhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthederivative.approachdoesnotexposethegraph.6.5.6GeneralBack-PropagationTheback-propagationalgorithmisverysimple.Tocomputethegradientofsomescalarzwithrespecttooneofitsancestorsxinthegraph,webeginbyobservingthatthegradientwithrespecttozisgivenbydzdz=1.Wecanthencomputethegradientwithr'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='pagationalgorithmisverysimple.Tocomputethegradientofsomescalarzwithrespecttooneofitsancestorsxinthegraph,webeginbyobservingthatthegradientwithrespecttozisgivenbydzdz=1.WecanthencomputethegradientwithrespecttoeachparentofzinthegraphbymultiplyingthecurrentgradientbytheJacobianoftheoperationthatproducedz.WecontinuemultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntilwereachx.Foranynodethatmaybereachedbygoingbackwardsfromzthroughtwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsatthatnode.Moreformally,eachnodeinthegraphGcorrespondstoavariable.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 228}, page_content='Toachievemaximumgenerality,wedescribethisvariableasbeingatensorV. Tensorcan214'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 229}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSingeneralhaveanynumberofdimensions,andsubsumescalars,vectors,andmatrices.Weassumethateachvariableisassociatedwiththefollowingsubroutines:V•getoperation_(V):ThisreturnstheoperationthatcomputesV,repre-sentedbytheedgescomingintoVinthecomputationalgraph.Forexample,theremaybeaPythonorC++classrepresentingthematrixmultiplicationoperation,andtheget_operationfunction.Supposewehaveavariablethatiscreatedbymatrixmultiplication,C=AB.Thengetoperation_(V)returnsapointertoaninstanceofthecorrespondingC++class.•getconsumers_(V,G):ThisreturnsthelistofvariablesthatarechildrenofVinthecomputationalgraph.G•Ggetinputs_(V,):ThisreturnsthelistofvariablesthatareparentsofVinthecomputationalgraph.GEachoperationopisalsoassociatedwithabpropoperation.ThisbpropoperationcancomputeaJacobian-vectorproductasdescribedbyEq..This6.47ishowtheback-propagationalgorithmisabletoachievegreatgenerality.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 229}, page_content='Eachoperationisresponsibleforknowinghowtoback-propagatethroughtheedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrixmultiplicationoperationtocreateavariableC=AB.SupposethatthegradientofascalarzwithrespecttoCisgivenbyG.Thematrixmultiplicationoperationisresponsiblefordeﬁningtwoback-propagationrules,oneforeachofitsinputarguments.IfwecallthebpropmethodtorequestthegradientwithrespecttoAgiventhatthegradientontheoutputisG,thenthebpropmethodofthematrixmultiplicationoperationmuststatethatthegradientwithrespecttoAisgivenbyGB\\ue03e.Likewise,ifwecallthebpropmethodtorequestthegradientwithrespecttoB,thenthematrixoperationisresponsibleforimplementingthebpropmethodandspecifyingthatthedesiredgradientisgivenbyA\\ue03eG.Theback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiationrules.Itonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,opbprop.(inputs,,XG)mustreturn\\ue058i(∇Xopfinputs.()i)Gi,(6.54)which isjust animplementation ofthechain ruleas expressed in'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 229}, page_content='isjust animplementation ofthechain ruleas expressed in Eq..6.47Here,inputsisalistofinputsthataresuppliedtotheoperation,op.fisthemathematicalfunctionthattheoperationimplements,Xistheinputwhosegradientwewishtocompute,andisthegradientontheoutputoftheoperation.G215'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 230}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinctfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassedtwocopiesofxtocomputex2,theop.bpropmethodshouldstillreturnxasthederivativewithrespecttobothinputs.Theback-propagationalgorithmwilllateraddbothoftheseargumentstogethertoobtain2x,whichisthecorrecttotalderivativeon.xSoftwareimplementationsofback-propagationusuallyprovideboththeopera-tionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesareabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrixmultiplication,exponents,logarithms,andsoon.Softwareengineerswhobuildanewimplementationofback-propagationoradvanceduserswhoneedtoaddtheirownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodforanynewoperationsmanually.Theback-propagationalgorithmisformallydescribedinAlgorithm.6.5Algorithm6.5Theoutermostskeletonoftheback-propagationalgorithm.Thisportiondoessimplesetupandcleanupwork.Mostoftheimportantwo'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 230}, page_content='tionsmanually.Theback-propagationalgorithmisformallydescribedinAlgorithm.6.5Algorithm6.5Theoutermostskeletonoftheback-propagationalgorithm.Thisportiondoessimplesetupandcleanupwork.MostoftheimportantworkhappensinthesubroutineofAlgorithmbuild_grad6.6.Require:T,thetargetsetofvariableswhosegradientsmustbecomputed.Require:G,thecomputationalgraphRequire:z,thevariabletobediﬀerentiatedLetG\\ue030beGprunedtocontainonlynodesthatareancestorsofzanddescendentsofnodesin.TInitialize,adatastructureassociatingtensorstotheirgradientsgrad_tablegradtable_[]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 230}, page_content='1z←fordoVinTbuildgrad_(V,,GG\\ue030,gradtable_)endforReturnrestrictedtograd_tableTInSec.,wemotivatedback-propagationasastrategyforavoidingcomput-6.5.2ingthesamesubexpressioninthechainrulemultipletimes.Thenaivealgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstanditscom-putationalcost.Ifweassumethateachoperationevaluationhasroughlythesamecost,thenwemayanalyzethecomputationalcostintermsofthenumberofoperationsexecuted. Keepinmindherethatwerefertoanoperationasthefundamentalunitofourcomputationalgraph,whichmightactuallyconsistofvery216'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 231}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.6Theinnerloopsubroutinebuildgrad_(V,,GG\\ue030,gradtable_)oftheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁnedinAlgorithm.6.5Require:V,thevariablewhosegradientshouldbeaddedtoand.Ggrad_tableRequire:G,thegraphtomodify.Require:G\\ue030,therestrictionoftonodesthatparticipateinthegradient.GRequire:grad_table,adatastructuremappingnodestotheirgradientsifthenVisingrad_tableReturn_gradtable[]Vendifi←1forCVin_getconsumers(,G\\ue030)doopgetoperation←_()CDC←buildgrad_(,,GG\\ue030,gradtable_)G()i←Gopbpropgetinputs.(_(C,\\ue030)),,VD  ii←+1endforG←\\ue050iG()igradtable_[] = VGInsertandtheoperationscreatingitintoGGReturnGmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrixmultiplicationasasingleoperation).ComputingagradientinagraphwithnnodeswillneverexecutemorethanO(n2)operationsorstoretheoutputofmorethanO(n2)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 231}, page_content='operations.Herewearecountingoperationsinthecomputationalgraph,notindividualoperationsexecutedbytheunderlyinghardware,soitisimportanttorememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondtoasingleoperationinthegraph.WecanseethatcomputingthegradientrequiresasmostO(n2) operationsbecausetheforwardpropagationstagewillatworstexecuteallnnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithmaddsoneJacobian-vectorproduct,whichshouldbeexpressedwithO(1)nodes,peredgeintheoriginalgraph.BecausethecomputationalgraphisadirectedacyclicgraphithasatmostO(n2)edges.Forthekindsofgraphsthatarecommonlyusedinpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsareroughlychain-structured,causingback-propagationtohaveO(n)cost.Thisisfar217'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 232}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymanynodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewritingtherecursivechainrule(Eq.)non-recursively:6.49∂u()n∂u()j=\\ue058path(u(π1),u(π2),...,u(πt)),fromπ1=tojπt=nt\\ue059k=2∂u(πk)∂u(πk−1).(6.55)Sincethenumberofpathsfromnodejtonodencangrowuptoexponentiallyinthelengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumberofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagationgraph.Thislargecostwouldbeincurredbecausethesamecomputationfor∂u()i∂u()jwouldberedonemanytimes.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 232}, page_content='Toavoidsuchrecomputation,wecanthinkofback-propagationasatable-ﬁllingalgorithmthattakesadvantageofstoringintermediateresults∂u()n∂u()i.Eachnodeinthegraphhasacorrespondingslotinatabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,back-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁllingstrategyissometimescalleddynamicprogramming.6.5.7Example:Back-PropagationforMLPTrainingAsanexample,wewalkthroughtheback-propagationalgorithmasitisusedtotrainamultilayerperceptron.Herewedevelopaverysimplemultilayerperceptionwithasinglehiddenlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.Theback-propagationalgorithmisusedtocomputethegradientofthecostonasingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetrainingsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.ThenetworkcomputesalayerofhiddenfeaturesH=max{0,XW(1)}.Tosimplifythepresentationwedonotusebiasesinthismodel.Weassumethatourgraphlanguageincludesareluoperationthatcancomputemax{0'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 232}, page_content='associatedclasslabelsy.ThenetworkcomputesalayerofhiddenfeaturesH=max{0,XW(1)}.Tosimplifythepresentationwedonotusebiasesinthismodel.Weassumethatourgraphlanguageincludesareluoperationthatcancomputemax{0,Z}element-wise.ThepredictionsoftheunnormalizedlogprobabilitiesoverclassesarethengivenbyHW(2).Weassumethatourgraphlanguageincludesacross_entropyoperationthatcomputesthecross-entropybetweenthetargetsyandtheprobabilitydistributiondeﬁnedbytheseunnormalizedlogprobabilities.Theresultingcross-entropydeﬁnesthecostJMLE.Minimizingthiscross-entropyperformsmaximumlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,218'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 233}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nX XW(1)W(1)U(1)U(1)matmulH Hrelu\\nU(3)U(3)sqru(4)u(4)sumλ λu(7)u(7)W(2)W(2)U(2)U(2)matmuly yJMLEJMLEcross_entropy\\nU(5)U(5)sqru(6)u(6)sumu(8)u(8)J J+×+'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 233}, page_content='Figure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexampleofasingle-layerMLPusingthecross-entropylossandweightdecay.wealsoincludearegularizationterm.ThetotalcostJJ= MLE+λ\\uf8eb\\uf8ed\\ue058i,j\\ue010W(1)i,j\\ue0112+\\ue058i,j\\ue010W(2)i,j\\ue0112\\uf8f6\\uf8f8(6.56)consistsofthecross-entropyandaweightdecaytermwithcoeﬃcientλ.ThecomputationalgraphisillustratedinFig..6.11Thecomputationalgraphforthegradientofthisexampleislargeenoughthatitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁtsoftheback-propagationalgorithm,whichisthatitcanautomaticallygenerategradientsthatwouldbestraightforwardbuttediousforasoftwareengineertoderivemanually.Wecanroughlytraceoutthebehavioroftheback-propagationalgorithmbylookingattheforwardpropagationgraphinFig..Totrain,wewish6.11tocomputeboth∇W(1)Jand∇W(2)J.TherearetwodiﬀerentpathsleadingbackwardfromJtotheweights:onethroughthecross-entropycost,andonethroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwillalwayscontribute2λW()itothegradientonW()i.219'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 234}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSTheotherpaththroughthecross-entropycostisslightlymorecomplicated.LetGbethegradientontheunnormalizedlogprobabilitiesU(2)providedbythecross_entropyoperation.Theback-propagationalgorithmnowneedstoexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsH\\ue03eGtothegradientonW(2),usingtheback-propagationruleforthesecondargumenttothematrixmultiplicationoperation.Theotherbranchcorrespondstothelongerchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithmcomputes∇HJ=GW(2)\\ue03eusingtheback-propagationrulefortheﬁrstargumenttothematrixmultiplicationoperation.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 234}, page_content='Next,thereluoperationusesitsback-propagationruletozerooutcomponentsofthegradientcorrespondingtoentriesofU(1)thatwerelessthan.Lettheresultbecalled0G\\ue030.Thelaststepoftheback-propagationalgorithmistousetheback-propagationruleforthesecondargumentoftheoperationtoaddmatmulX\\ue03eG\\ue030tothegradientonW(1).Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradientdescentalgorithm,oranotheroptimizationalgorithm,tousethesegradientstoupdatetheparameters.FortheMLP,thecomputationalcostisdominatedbythecostofmatrixmultiplication.Duringtheforwardpropagationstage,wemultiplybyeachweightmatrix,resultinginO(w)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 234}, page_content='multiply-adds,wherewisthenumberofweights.Duringthebackwardpropagationstage,wemultiplybythetransposeofeachweightmatrix,whichhasthesamecomputationalcost.Themainmemorycostofthealgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshasreturnedtothesamepoint.ThememorycostisthusO(mnh),wheremisthenumberofexamplesintheminibatchandnhisthenumberofhiddenunits.6.5.8ComplicationsOurdescriptionoftheback-propagationalgorithmhereissimplerthantheimple-mentationsactuallyusedinpractice.Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobeafunctionthatreturnsasingletensor.Mostsoftwareimplementationsneedtosupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewishtocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itisbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcienttoimplementthisprocedureasasingleoperationwithtwooutputs.Wehavenotdescribedhow'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 234}, page_content='tocontrolthememoryconsumptionofback-propagation.Back-propagationofteninvolvessummationofmanytensorstogether.220'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 235}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,thenallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverlyhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerandaddingeachvaluetothatbuﬀerasitiscomputed.Real-worldimplementationsofback-propagationalsoneedtohandlevariousdatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign.Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthesecasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.Variousothertechnicalitiesmakereal-worlddiﬀerentiationmorecomplicated.Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekeyintellectualtoolsneededtocomputederivatives,butitisimportanttobeawarethatmanymoresubtletiesexist.6.5.9DiﬀerentiationoutsidetheDeepLearningCommunityThe deeplearningcommunityhas been somewhatisolatedfrom'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 235}, page_content='been somewhatisolatedfrom thebroadercomputersciencecommunityandhaslargelydevelopeditsownculturalattitudesconcerninghowtoperformdiﬀerentiation.Moregenerally,theﬁeldofautomaticdiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically.Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomaticdiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreversemodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechainruleindiﬀerentorders.Ingeneral,determiningtheorderofevaluationthatresultsinthelowestcomputationalcostisadiﬃcultproblem.FindingtheoptimalsequenceofoperationstocomputethegradientisNP-complete(,),intheNaumann2008sensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleastexpensiveform.Forexample,supposewehavevariablesp1,p2,...,pnrepresentingprobabilitiesandvariablesz1,z2,...,znrepresentingunnormalizedlogprobabilities.Supposewedeﬁneqi=exp(zi)\\ue050iexp(zi),(6.57)wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivisionoperations,and'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 235}, page_content='construct across-entropylossJ=−\\ue050ipilogqi.A humanmathematiciancanobservethatthederivativeofJwithrespecttozitakesaverysimpleform:qi−pi.Theback-propagationalgorithmisnotcapableofsimplifyingthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof221'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 236}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSthelogarithmandexponentiationoperationsintheoriginalgraph.SomesoftwarelibrariessuchasTheano(,;,)areabletoBergstraetal.2010Bastienetal.2012performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposedbythepureback-propagationalgorithm.WhentheforwardgraphGhasasingleoutputnodeandeachpartialderivative∂u()i∂u()jcanbecomputedwithaconstantamountofcomputation,back-propagationguaranteesthatthenumberofcomputationsforthegradientcomputationisofthesameorderasthenumberofcomputationsfortheforwardcomputation:thiscanbeseeninAlgorithmbecauseeachlocalpartialderivative6.2∂u()i∂u()jneedstobecomputedonlyoncealongwithanassociatedmultiplicationandadditionfortherecursivechain-ruleformulation(Eq.).Theoverallcomputationis6.49thereforeO(#edges).However,itcanpotentiallybereducedbysimplifyingthecomputationalgraphconstructedbyback-propagation,andthisisanNP-completetask.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 236}, page_content='ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedonmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplifythegraph.Wedeﬁnedback-propagationonlyforthecomputationofagradientofascalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(eitherofkdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontainingkvalues).Anaiveimplementationmaythenneedktimesmorecomputation:foreachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementationcomputeskgradientsinsteadofasinglegradient.Whenthenumberofoutputsofthegraphislargerthanthenumberofinputs,itissometimespreferabletouseanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputationofgradientsinrecurrentnetworks,forexample(,).ThisWilliamsandZipser1989alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,tradingoﬀcomputationaleﬃciencyformemory.Therelationshipbetweenforwardmodeandbackwardmodeisanalogoustotherelationshipbetweenleft-mul'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 236}, page_content='ser1989alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,tradingoﬀcomputationaleﬃciencyformemory.Therelationshipbetweenforwardmodeandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversusright-multiplyingasequenceofmatrices,suchasABCD,(6.58)wherethematricescanbethoughtofasJacobianmatrices.Forexample,ifDisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwithasingleoutputandmanyinputs,andstartingthemultiplicationsfromtheendandgoingbackwardsonlyrequiresmatrix-vectorproducts.Thiscorrespondstothebackwardmode.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 236}, page_content='Instead,startingtomultiplyfromtheleftwouldinvolveaseriesofmatrix-matrixproducts,whichmakesthewholecomputationmuchmoreexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorunthemultiplicationsleft-to-right,correspondingtotheforwardmode.222'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 237}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSInmanycommunitiesoutsideofmachinelearning,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 237}, page_content='itismorecommontoimplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramminglanguagecode,suchasPythonorCcode,andautomaticallygeneratesprogramsthatdiﬀerentfunctionswrittenintheselanguages.Inthedeeplearningcommunity,computationalgraphsareusuallyrepresentedbyexplicitdatastructurescreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackofrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperationandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.However,thespecializedapproachalsohasthebeneﬁtofallowingcustomizedback-propagationrulestobedevelopedforeachoperation,allowingthedevelopertoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedurewouldpresumablybeunabletoreplicate.Back-propagationisthereforenottheonlywayortheoptimalwayofcomputingthegradient,butitisaverypracticalmethodthatcontinuestoservethedeeplearningcommunityverywell.Inthefuture,diﬀerentiationtechnologyfordeepnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadva'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 237}, page_content='hegradient,butitisaverypracticalmethodthatcontinuestoservethedeeplearningcommunityverywell.Inthefuture,diﬀerentiationtechnologyfordeepnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvancesinthebroaderﬁeldofautomaticdiﬀerentiation.6.5.10Higher-OrderDerivativesSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthedeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsforderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.Thismeansthatthesymbolicdiﬀerentiationmachinerycanbeappliedtoderivatives.Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivativeofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessianmatrix.Ifwehaveafunctionf:Rn→R,thentheHessianmatrixisofsizenn×.Intypicaldeeplearningapplications,nwillbethenumberofparametersinthemodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixisthusinfeasibletoevenrepresent.Insteadofe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 237}, page_content='heHessianmatrixisofsizenn×.Intypicaldeeplearningapplications,nwillbethenumberofparametersinthemodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixisthusinfeasibletoevenrepresent.InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproachistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesforperformingvariousoperationslikeapproximatelyinvertingamatrixorﬁndingapproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperationotherthanmatrix-vectorproducts.InordertouseKrylovmethodsontheHessian,weonlyneedtobeabletocomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A223'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 238}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSstraightforwardtechnique(,)fordoingsoistocomputeChristianson1992Hv= ∇x\\ue068(∇xfx())\\ue03ev\\ue069.(6.59)Bothofthegradientcomputationsinthisexpressionmaybecomputedautomati-callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpressiontakesthegradientofafunctionoftheinnergradientexpression.Ifvisitselfavectorproducedbyacomputationalgraph,itisimportanttospecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethroughthegraphthatproduced.vWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowithHessianvectorproducts.OnesimplycomputesHe()iforalli= 1,...,n,wheree()iistheone-hotvectorwithe()ii='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 238}, page_content='1andallotherentriesequalto0.6.6HistoricalNotesFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximatorsbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.Fromthispointofview,themodernfeedforwardnetworkistheculminationofcenturiesofprogressonthegeneralfunctionapproximationtask.Thechainrulethatunderliestheback-propagationalgorithmwasinventedinthe17thcentury(,;,).CalculusandalgebrahaveLeibniz1676L’Hôpital1696longbeenusedtosolveoptimizationproblemsinclosedform,butgradientdescentwasnotintroducedasatechniqueforiterativelyapproximatingthesolutiontooptimizationproblemsuntilthe19thcentury(Cauchy1847,).Beginninginthe1940s,thesefunctionapproximationtechniqueswereusedtomotivatemachinelearningmodelssuchastheperceptron.However,theearliestmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedoutseveraloftheﬂawsofthelinearmodelfamily,suchasitinabilitytolearntheXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.Learningnonlinearfunctionsrequired'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 238}, page_content='includingMarvinMinskypointedoutseveraloftheﬂawsofthelinearmodelfamily,suchasitinabilitytolearntheXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-ceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcientapplicationsofthechainrulebasedondynamicprogrammingbegantoappearinthe1960sand1970s,mostlyforcontrolapplications(,;,Kelley1960BrysonandDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973;,;,;,)butalsoforsensitivityanalysis(,).Linnainmaa1976Werbos1981()proposedapplyingthesetechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydevelopedinpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985Parker,224'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKS1985Rumelhart1986a;etal.,).ThebookParallelDistributedProcessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswithback-propagationinachapter(,)thatcontributedgreatlytothepopularizationRumelhartetal.1986bofback-propagationandinitiatedaveryactiveperiodofresearchinmulti-layerneuralnetworks.However,theideasputforwardbytheauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyondback-propagation.Theyincludecrucialideasaboutthepossiblecomputationalimplementationofseveralcentralaspectsofcognitionandlearning,whichcameunderthenameof“connectionism”becauseoftheimportancegiventheconnectionsbetweenneuronsasthelocusoflearningandmemory.Inparticular,theseideasincludethenotionofdistributedrepresentation(,).Hintonetal.1986Followingthesuccessofback-propagation,neuralnetworkresearchgainedpop-ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearningtechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethatbeganin2006.Thecoreideasbehindmode'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='uralnetworkresearchgainedpop-ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearningtechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethatbeganin2006.Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-stantiallysincethe1980s.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='Thesameback-propagationalgorithmandthesameapproachestogradientdescentarestillinuse.Mostoftheimprovementinneuralnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,largerdatasetshavereducedthedegreetowhichstatisticalgeneralizationisachallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,duetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,asmallnumberofalgorithmicchangeshaveimprovedtheperformanceofneuralnetworksnoticeably.Oneofthesealgorithmicchangeswasthereplacementofmeansquarederrorwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularinthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandtheprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunityandthemachinelearningcommunity.Theuseofcross-entropylossesgreatlyimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,whichhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemeansquarederrorloss.Theothermajoralgorithmicchangethathasgreatlyimprove'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='tlyimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,whichhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemeansquarederrorloss.Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformanceoffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewiselinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0,z}functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleastasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearlymodels'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 239}, page_content='didnot userectiﬁedlinearunits, butinsteadappliedrectiﬁcationto225'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSnonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwaslargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetterwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunitswereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswithnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.Jarrett2009etal.()observedthat“usingarectifyingnonlinearityisthesinglemostimportantfactorinimprovingtheperformanceofarecognitionsystem”amongseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.Forsmalldatasets,()observedthatusingrectifyingnon-Jarrettetal.2009linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁedlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerentfeaturevectorstoclassidentities.Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledgetoexceedtheperformanceofrandomlychosenparameters'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='wingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerentfeaturevectorstoclassidentities.Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledgetoexceedtheperformanceofrandomlychosenparameters.()Glorotetal.2011ashowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeepnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.Rectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthatneurosciencehascontinuedtohavean'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='inﬂuenceonthedevelopmentof'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='deeplearningalgorithms.()motivaterectiﬁedlinearunitsfromGlorotetal.2011abiologicalconsiderations.Thehalf-rectifyingnonlinearitywasintendedtocapturethesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsarecompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportionaltoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewheretheyareinactive(i.e.,theyshouldhavesparseactivations).Whenthemodernresurgenceofdeeplearningbeganin2006,feedforwardnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidelybelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassistedbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththerightresourcesandengineeringpractices,feedforwardnetworksperformverywell.Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelopprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarialnetworks,describedinChapter.Ratherthanbeingviewedasanunreliable20technologythatmustbesupportedbyot'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='worksisusedasatooltodevelopprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarialnetworks,describedinChapter.Ratherthanbeingviewedasanunreliable20technologythatmustbesupportedbyothertechniques,gradient-basedlearninginfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythatmaybeappliedtomanyothermachinelearningtasks.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 240}, page_content='In2006,thecommunityusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,itismorecommontousesupervisedlearningtosupportunsupervisedlearning.226'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 241}, page_content='CHAPTER6.DEEPFEEDFORWARDNETWORKSFeedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,weexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimizationalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.Thischapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthesubsequentchapters,weturntohowtousethesemodels—howtoregularizeandtrainthem.\\n227'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 242}, page_content='Chapter7RegularizationforDeepLearningAcentralprobleminmachinelearningishowtomakeanalgorithmthatwillperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategiesusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possiblyattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectivelyasregularization. Aswewillseethereareagreatmanyformsofregularizationavailabletothedeeplearningpractitioner.Infact, developingmoreeﬀectiveregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.Chapterintroducedthebasicconceptsofgeneralization,underﬁtting,overﬁt-5ting,bias,varianceandregularization.Ifyouarenotalreadyfamiliarwiththesenotions,pleaserefertothatchapterbeforecontinuingwiththisone.Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblockstoformdeepmodels.Somesectionsofthischapterdealwithstandardconceptsinmachinelearning.Ifyouarealreadyfamiliarwiththeseconcepts,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 242}, page_content='feelfreetoskiptherelevantsections.However,mostofthischapterisconcernedwiththeextensionofthesebasicconceptstotheparticularcaseofneuralnetworks.InSec.,wedeﬁnedregularizationas“anymodiﬁcationwemaketoa5.2.2learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.”Therearemanyregularizationstrategies.Someputextraconstraints ona machine learning model,such asadding restrictionson theparametervalues.Someaddextratermsintheobjectivefunctionthatcanbethoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosencarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance228'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 243}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencodespeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenaltiesaredesignedtoexpressagenericpreferenceforasimplermodelclassinordertopromotegeneralization.Sometimespenaltiesandconstraintsarenecessarytomakeanunderdeterminedproblemdetermined.Otherformsofregularization,knownasensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.Inthecontextofdeeplearning,mostregularizationstrategiesarebasedonregularizingestimators.Regularizationofanestimatorworksbytradingincreasedbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtabletrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.WhenwediscussedgeneralizationandoverﬁttinginChapter,wefocusedonthree5situations,wherethemodelfamilybeingtrainedeither(1)excludedthetruedatageneratingprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetruedatageneratingprocess,or(3)includedthegeneratingprocessbut'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 243}, page_content='ons,wherethemodelfamilybeingtrainedeither(1)excludedthetruedatageneratingprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetruedatageneratingprocess,or(3)includedthegeneratingprocessbutalsomanyotherpossiblegeneratingprocesses—theoverﬁttingregimewherevarianceratherthanbiasdominatestheestimationerror.Thegoalofregularizationistotakeamodelfromthethirdregimeintothesecondregime.Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethetargetfunctionorthetruedatageneratingprocess,orevenacloseapproximationofeither.Wealmostneverhaveaccesstothetruedatageneratingprocesssowecanneverknowforsureifthemodelfamilybeingestimatedincludesthegeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithmsaretodomainswherethetruedatageneratingprocessisalmostcertainlyoutsidethemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremelycomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetruegenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosomeexte'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 243}, page_content='amily.Deeplearningalgorithmsaretypicallyappliedtoextremelycomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetruegenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosomeextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)intoaroundhole(ourmodelfamily).Whatthismeansisthatcontrollingthecomplexityofthemodelisnotasimplematterofﬁndingthemodeloftherightsize,withtherightnumberofparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,wealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizinggeneralizationerror)isalargemodelthathasbeenregularizedappropriately.Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularizedmodel.229'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 244}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1ParameterNormPenaltiesRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linearmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,andeﬀectiveregularizationstrategies.Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-rameternormpenaltyΩ(θ)totheobjectivefunctionJ.Wedenotetheregularizedobjectivefunctionby˜J:˜J,J,α(;θXy) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 244}, page_content='= (;θXy)+Ω()θ(7.1)whereα∈[0,∞)isahyperparameterthatweightstherelativecontributionofthenormpenaltyterm,,relativetothestandardobjectivefunctionΩJ(x;θ).Settingαto0resultsinnoregularization.Largervaluesofαcorrespondtomoreregularization.Whenourtrainingalgorithmminimizestheregularizedobjectivefunction˜JitwilldecreaseboththeoriginalobjectiveJonthetrainingdataandsomemeasureofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerentchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred.ΩInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenaltiesonthemodelparameters.Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethatforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩpenalizesonlytheweightsoftheaﬃnetransformationateachlayerandleavesthebiasesunregularized.Thebiasestypicallyrequirelessdatatoﬁtaccuratelythantheweights. Eachweightspeciﬁeshowtwovariablesinteract.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 244}, page_content='Eachweightspeciﬁeshowtwovariablesinteract. Fittingtheweightwellrequiresobservingbothvariablesinavarietyofconditions.Eachbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuchvariancebyleavingthebiasesunregularized.Also,regularizingthebiasparameterscanintroduceasigniﬁcantamountofunderﬁtting.Wethereforeusethevectorwtoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethevectorθdenotesalloftheparameters,includingbothwandtheunregularizedparameters.Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparatepenaltywithadiﬀerentαcoeﬃcientforeachlayerofthenetwork.Becauseitcanbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstillreasonabletousethesameweightdecayatalllayersjusttoreducethesizeofsearchspace.230'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 245}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1.1L2ParameterRegularizationWehavealreadyseen,inSec.,oneofthesimplestandmostcommonkinds5.2.2ofparameternormpenalty:theL2parameternormpenaltycommonlyknownasweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1byaddingaregularizationtermΩ(θ)=12\\ue06b\\ue06bw22totheobjectivefunction.Inotheracademiccommunities,L2regularizationisalsoknownasridgeregressionorTikhonovregularization.Wecangainsomeinsightintothebehaviorofweightdecayregularizationbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythepresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthefollowingtotalobjectivefunction:˜J,(;wXy) =α2w\\ue03ewwXy+(J;,),(7.2)withthecorrespondingparametergradient∇w˜J,α(;wXy) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 245}, page_content='w+∇wJ,.(;wXy)(7.3)Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate:www←−\\ue00fα(+∇wJ,.(;wXy))(7.4)Writtenanotherway,theupdateis:ww←−(1\\ue00fα)−∇\\ue00fwJ,.(;wXy)(7.5)Wecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearningruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensinasinglestep.Butwhathappensovertheentirecourseoftraining?Wewillfurthersimplifytheanalysisbymakingaquadraticapproximationtotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthatobtainsminimalunregularizedtrainingcost,w∗=argminwJ(w).Iftheobjectivefunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith1Moregenerally,wecouldregularizetheparameterstobenearanyspeciﬁcpointinspaceand,surprisingly,stillgetaregularizationeﬀect,butbetterresultswillbeobtainedforavalueclosertothetrueone,withzerobeingadefaultvaluethatmakessensewhenwedonotknowifthecorrectvalueshouldbepositiveornegative.Sinceitisfarmorecommontoregul'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 245}, page_content='rizationeﬀect,butbetterresultswillbeobtainedforavalueclosertothetrueone,withzerobeingadefaultvaluethatmakessensewhenwedonotknowifthecorrectvalueshouldbepositiveornegative.Sinceitisfarmorecommontoregularizethemodelparameterstowardszero,wewillfocusonthisspecialcaseinourexposition.231'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 246}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGmeansquarederror,thentheapproximationisperfect.TheapproximationˆJisgivenbyˆJJ() = θ(w∗)+12(ww−∗)\\ue03eHww(−∗),(7.6)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Thereisnoﬁrst-orderterminthisquadraticapproximation,becausew∗isdeﬁnedtobeaminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofaminimumof,wecanconcludethatispositivesemideﬁnite.JHTheminimumofˆJoccurswhereitsgradient∇wˆJ() = (wHww−∗)(7.7)isequalto.0Tostudytheeﬀectofweightdecay,wemodifyEq.byaddingtheweight7.7decaygradient.WecannowsolvefortheminimumoftheregularizedversionofˆJ.Weusethevariable˜wtorepresentthelocationoftheminimum.α˜wH+(˜ww−∗) = 0(7.8)(+)HαI˜wHw= ∗(7.9)˜wHI= (+α)−1Hw∗.(7.10)Asαapproaches0,theregularizedsolution˜wapproachesw∗.Butwhathappensasαgrows?BecauseHisrealandsymmetric,wecandecomposeitintoadiagonalmatrixΛandanorthonormalbasisofeigenvectors,Q,suchthatHQQ= Λ\\ue03e.ApplyingthedecompositiontoEq.,weobtain:7.10˜wQQ= (Λ\\ue03e+)αI−1QQΛ\\ue03ew∗(7.11)=\\ue068QIQ(+Λα)\\ue03e\\ue069−1QQΛ\\ue03ew∗(7.12)='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 246}, page_content='Λ\\ue03e.ApplyingthedecompositiontoEq.,weobtain:7.10˜wQQ= (Λ\\ue03e+)αI−1QQΛ\\ue03ew∗(7.11)=\\ue068QIQ(+Λα)\\ue03e\\ue069−1QQΛ\\ue03ew∗(7.12)= (+)QΛαI−1ΛQ\\ue03ew∗.(7.13)Weseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedbytheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththei-theigenvectorofHisrescaledbyafactorofλiλi+α.(Youmaywishtoreviewhowthiskindofscalingworks,ﬁrstexplainedinFig.).2.3AlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,whereλi\\ue01dα,theeﬀectofregularizationisrelativelysmall.However,componentswithλi\\ue01cαwillbeshrunktohavenearlyzeromagnitude.ThiseﬀectisillustratedinFig..7.1232'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 247}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw1w2w∗˜w'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 247}, page_content='Figure7.1:AnillustrationoftheeﬀectofL2(orweightdecay)regularizationonthevalueoftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularizedobjective.ThedottedcirclesrepresentcontoursofequalvalueoftheL2regularizer.Atthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,theeigenvalueoftheHessianofJissmall.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 247}, page_content='Theobjectivefunctiondoesnotincreasemuchwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpressastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.Theregularizerpullsw1closetozero.Intheseconddimension,theobjectivefunctionisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,indicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionofw2relativelylittle.Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducingtheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonotcontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessiantellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.Componentsoftheweightvectorcorrespondingtosuchunimportantdirectionsaredecayedawaythroughtheuseoftheregularizationthroughouttraining.Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimizationofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelatetomachinelearninginparticular?Wecanﬁndo'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 247}, page_content='gularizationthroughouttraining.Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimizationofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelatetomachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,amodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothesamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewillbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnowphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis233'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 248}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGthesumofsquarederrors:()Xwy−\\ue03e()Xwy−.(7.14)WhenweaddL2regularization,theobjectivefunctionchangesto()Xwy−\\ue03e()+Xwy−12αw\\ue03ew.(7.15)ThischangesthenormalequationsforthesolutionfromwX= (\\ue03eX)−1X\\ue03ey(7.16)towX= (\\ue03eXI+α)−1X\\ue03ey.(7.17)ThematrixX\\ue03eXinEq.isproportionaltothecovariancematrix7.161mX\\ue03eX.UsingL2regularizationreplacesthismatrixwith\\ue000X\\ue03eXI+α\\ue001−1inEq..7.17Thenewmatrixisthesameastheoriginalone,butwiththeadditionofαtothediagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeachinputfeature.WecanseethatL2regularizationcausesthelearningalgorithmto“perceive”theinputXashavinghighervariance,whichmakesitshrinktheweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedtothisaddedvariance.7.1.2L1RegularizationWhileL2weightdecayisthemostcommonformofweightdecay,thereareotherwaystopenalizethesizeofthemodelparameters. AnotheroptionistouseL1regularization.Formally,L1regularizationonthemodelparameterisdeﬁnedas:wΩ() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 248}, page_content='AnotheroptionistouseL1regularization.Formally,L1regularizationonthemodelparameterisdeﬁnedas:wΩ() = θ||||w1=\\ue058i|wi|,(7.18)thatis,asthesumofabsolutevaluesoftheindividualparameters.2WewillnowdiscusstheeﬀectofL1regularizationonthesimplelinearregressionmodel,withnobiasparameter,thatwestudiedinouranalysisofL2regularization.Inparticular,weareinterestedindelineatingthediﬀerencesbetweenL1andL2forms2AswithL2regularization,wecouldregularizetheparameterstowardsavaluethatisnotzero,butinsteadtowardssomeparametervaluew()o.InthatcasetheL1regularizationwouldintroducethetermΩ() = θ||−ww()o||1=\\ue050i|wi−w()oi|.234'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 249}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofregularization.AswithL2weightdecay,L1weightdecaycontrolsthestrengthoftheregularizationbyscalingthepenaltyusingapositivehyperparameterΩα.Thus,theregularizedobjectivefunction˜J,(;wXy)isgivenby˜J,α(;wXy) = ||||w1+(;)JwXy,,(7.19)withthecorrespondinggradient(actually,sub-gradient):∇w˜J,α(;wXy) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 249}, page_content='= ||||w1+(;)JwXy,,(7.19)withthecorrespondinggradient(actually,sub-gradient):∇w˜J,α(;wXy) = sign()+w∇wJ,(Xyw;)(7.20)whereissimplythesignofappliedelement-wise.sign()wwByinspectingEq.,wecanseeimmediatelythattheeﬀectof7.20L1regu-larizationisquitediﬀerentfromthatofL2regularization.Speciﬁcally,wecanseethattheregularizationcontributiontothegradientnolongerscaleslinearlywitheachwi;insteaditisaconstantfactorwithasignequaltosign(wi).OneconsequenceofthisformofthegradientisthatwewillnotnecessarilyseecleanalgebraicsolutionstoquadraticapproximationsofJ(Xy,;w)aswedidforL2regularization.OursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresentviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylorseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradientinthissettingisgivenby∇wˆJ() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 249}, page_content='= (wHww−∗),(7.21)where,again,istheHessianmatrixofwithrespecttoevaluatedatHJww∗.BecausetheL1penaltydoesnotadmitcleanalgebraicexpressionsinthecaseofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumptionthattheHessianisdiagonal,H=diag([H11,,...,Hn,n]),whereeachHi,i>0.Thisassumptionholdsifthedataforthelinearregressionproblemhasbeenpreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybeaccomplishedusingPCA.OurquadraticapproximationoftheL1regularizedobjectivefunctiondecom-posesintoasumovertheparameters:ˆJ,J(;wXy) = (w∗;)+Xy,\\ue058i\\ue01412Hi,i(wi−w∗i)2+αw|i|\\ue015.(7.22)Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution(foreachdimension),withthefollowingform:iwi= sign(w∗i)max\\ue01a|w∗i|−αHi,i,0\\ue01b.(7.23)235'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 250}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConsiderthesituationwherew∗i>i0forall.Therearetwopossibleoutcomes:1.Thecasewherew∗i≤αHi,i.Heretheoptimalvalueofwiundertheregularizedobjectiveissimplywi= 0.ThisoccursbecausethecontributionofJ(w;Xy,)totheregularizedobjective˜J(w;Xy,)isoverwhelmed—indirectioni—bytheL1regularizationwhichpushesthevalueofwitozero.2.Thecasewherew∗i>αHi,i.Inthiscase,theregularizationdoesnotmovetheoptimalvalueofwitozerobutinsteaditjustshiftsitinthatdirectionbyadistanceequaltoαHi,i.Asimilarprocesshappenswhenw∗i<0,butwiththeL1penaltymakingwilessnegativebyαHi,i,or0.IncomparisontoL2regularization,L1regularizationresultsinasolutionthatismoresparse.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 250}, page_content='Sparsityinthiscontextreferstothefactthatsomeparametershaveanoptimalvalueofzero.ThesparsityofL1regularizationisaqualitativelydiﬀerentbehaviorthanariseswithL2regularization.Eq.gavethesolution7.13˜wforL2regularization.IfwerevisitthatequationusingtheassumptionofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisofL1regularization,weﬁndthat˜wi=Hi,iHi,i+αw∗i.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 250}, page_content='Ifw∗iwasnonzero,then˜wiremainsnonzero.ThisdemonstratesthatL2regularizationdoesnotcausetheparameterstobecomesparse,whileL1regularizationmaydosoforlargeenough.αThesparsitypropertyinducedbyL1regularizationhasbeenusedextensivelyasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearningproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.Inparticular,thewellknownLASSO(,)(leastabsoluteshrinkageandTibshirani1995selectionoperator)modelintegratesanL1penaltywithalinearmodelandaleastsquarescostfunction.TheL1penaltycausesasubsetoftheweightstobecomezero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.InSec.,wesawthatmanyregularizationstrategiescanbeinterpretedas5.6.1MAPBayesianinference,andthatinparticular,L2regularizationisequivalenttoMAPBayesianinferencewithaGaussianpriorontheweights.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 250}, page_content='ForL1regu-larization,thepenaltyαΩ(w)=α\\ue050i|wi|usedtoregularizeacostfunctionisequivalenttothelog-priortermthatismaximizedbyMAPBayesianinferencewhenthepriorisanisotropicLaplacedistribution(Eq.)over3.26w∈Rn:log() =pw\\ue058ilogLaplace(wi;0,1α) = −||||αw1+loglog2nαn−.(7.24)236'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 251}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGFromthepointofviewoflearningviamaximizationwithrespecttow,wecanignorethetermsbecausetheydonotdependon.loglog2α−w7.2NormPenaltiesasConstrainedOptimizationConsiderthecostfunctionregularizedbyaparameternormpenalty:˜J,J,α.(;θXy) = (;θXy)+Ω()θ(7.25)RecallfromSec.thatwecanminimizeafunctionsubjecttoconstraintsby4.4constructingageneralizedLagrangefunction,consistingoftheoriginalobjectivefunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,calledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresentingwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthansomeconstant,wecouldconstructageneralizedLagrangefunctionkL−(;) = (;)+(Ω()θ,αXy,JθXy,αθk.)(7.26)Thesolutiontotheconstrainedproblemisgivenbyθ∗='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 251}, page_content='= (;)+(Ω()θ,αXy,JθXy,αθk.)(7.26)Thesolutiontotheconstrainedproblemisgivenbyθ∗= argminθmaxα,α≥0L()θ,α.(7.27)AsdescribedinSec.,solvingthisproblemrequiresmodifyingboth4.4θandα.Sec.providesaworkedexampleoflinearregressionwithan4.5L2constraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,whileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinallproceduresαmustincreasewheneverΩ(θ)>kanddecreasewheneverΩ(θ)<k.AllpositiveαencourageΩ(θ)toshrink.Theoptimalvalueα∗willencourageΩ(θ)toshrink,butnotsostronglytomakebecomelessthan.Ω()θkTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁxα∗andviewtheproblemasjustafunctionof:θθ∗= argminθL(θ,α∗) = argminθJ,α(;θXy)+∗Ω()θ.(7.28)Thisisexactlythesameastheregularizedtrainingproblemofminimizing˜J.Wecanthusthinkofaparameternormpenaltyasimposingaconstraintontheweights.IfistheΩL2norm,thentheweightsareconstrainedtolieinanL2ball. IfistheΩL1norm,thentheweightsareconstrainedtolieinaregionof237'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 252}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGlimitedL1norm.Usuallywedonotknowthesizeoftheconstraintregionthatweimposebyusingweightdecaywithcoeﬃcientα∗becausethevalueofα∗doesnotdirectlytellusthevalueofk.Inprinciple,onecansolvefork,buttherelationshipbetweenkandα∗dependsontheformofJ.Whilewedonotknowtheexactsizeoftheconstraintregion,wecancontrolitroughlybyincreasingordecreasingαinordertogroworshrinktheconstraintregion.Largerαwillresultinasmallerconstraintregion.Smallerwillresultinalargerconstraintregion.αSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.AsdescribedinSec.,wecanmodifyalgorithmssuchasstochasticgradientdescent4.4totakeastepdownhillonJ(θ)andthenprojectθbacktothenearestpointthatsatisﬁesΩ(θ)<k.Thiscanbeusefulifwehaveanideaofwhatvalueofkisappropriateanddonotwanttospendtimesearchingforthevalueofαthatcorrespondstothis.kAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcingconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimizationprocedurestogetstucki'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 252}, page_content='ngforthevalueofαthatcorrespondstothis.kAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcingconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimizationprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentrainingneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthefunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemareallverysmall.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 252}, page_content='Whentrainingwithapenaltyonthenormoftheweights,theseconﬁgurationscanbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduceJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projectioncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweightstoapproachtheorigin.Explicitconstraintsimplementedbyre-projectiononlyhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraintregion.Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimposesomestabilityontheoptimizationprocedure.Whenusinghighlearningrates,itispossibletoencounterapositivefeedbackloopinwhichlargeweightsinducelargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdatesconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfromtheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojectionpreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweightswithoutbound.()recommendusingconstraintscombinedwithHintonetal.2012cahighlearningratetoallowrapidexplorationofparamet'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 252}, page_content='reprojectionpreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweightswithoutbound.()recommendusingconstraintscombinedwithHintonetal.2012cahighlearningratetoallowrapidexplorationofparameterspacewhilemaintainingsomestability.Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebroandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix238'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 253}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentireweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyonehiddenunitfromhavingverylargeweights.IfweconvertedthisconstraintintoapenaltyinaLagrangefunction,itwouldbesimilartoL2weightdecaybutwithaseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKTmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunitobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedasanexplicitconstraintwithreprojection.7.3RegularizationandUnder-ConstrainedProblemsInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-erlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregressionandPCA,dependoninvertingthematrixX\\ue03eX.ThisisnotpossiblewheneverX\\ue03eXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-butiontrulyhasnovarianceinsomedirection,orwhennovarianceinobservedinsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures('),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 253}, page_content='singular.Thismatrixcanbesingularwheneverthedatageneratingdistri-butiontrulyhasnovarianceinsomedirection,orwhennovarianceinobservedinsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinvertingX\\ue03eXI+αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.Theselinearproblemshaveclosedformsolutionswhentherelevantmatrixisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobeunderdetermined.Anexampleislogisticregressionappliedtoaproblemwheretheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfectclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.Aniterativeoptimizationprocedurelikestochasticgradientdescentwillcontinuallyincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumericalimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweightstocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowtheprogrammerhasdecidedtohandlevaluesthatareno'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 253}, page_content='npractice,anumericalimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweightstocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowtheprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.Mostformsofregularizationareabletoguaranteetheconvergenceofiterativemethodsappliedtounderdeterminedproblems.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 253}, page_content='Forexample,weightdecaywillcausegradientdescenttoquitincreasingthemagnitudeoftheweightswhentheslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.Theideaofusingregularizationtosolveunderdeterminedproblemsextendsbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebraproblems.AswesawinSec.,wecansolveunderdeterminedlinearequationsusing2.9239'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 254}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMoore-Penrosepseudoinverse.RecallthatonedeﬁnitionofthepseudoinverseX+ofamatrixisXX+=limα\\ue0260(X\\ue03eXI+α)−1X\\ue03e.(7.29)WecannowrecognizeEq.asperforminglinearregressionwithweightdecay.7.29Speciﬁcally,Eq.isthelimitofEq.astheregularizationcoeﬃcientshrinks7.297.17tozero.Wecanthusinterpretthepseudoinverseasstabilizingunderdeterminedproblemsusingregularization.7.4DatasetAugmentationThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainitonmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Onewaytogetaroundthisproblemistocreatefakedataandaddittothetrainingset.Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenewfakedata.Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-cated,highdimensionalinputxandsummarizeitwithasinglecategoryidentityy.Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevarietyoftransformations.Wecangeneratenew(x,y)pairseasilyjustbytransformingtheinputsinourtrainingset.xThisapproachisn'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 254}, page_content='glecategoryidentityy.Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevarietyoftransformations.Wecangeneratenew(x,y)pairseasilyjustbytransformingtheinputsinourtrainingset.xThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,itisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehavealreadysolvedthedensityestimationproblem.Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁcclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandincludeanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncanoftengreatlyimprovegeneralization,evenifthemodelhasalreadybeendesignedtobepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniquesdescribedinChapter.Manyotheroperationssuchasrotatingtheimageor9scalingtheimagehavealsoprovenquiteeﬀective.Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrectclass.Forexample,opticalcharacterrecognitiont'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 254}, page_content='r.Manyotheroperationssuchasrotatingtheimageor9scalingtheimagehavealsoprovenquiteeﬀective.Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrectclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthediﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontalﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthesetasks.240'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 255}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariantto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannotbeimplementedasasimplegeometricoperationontheinputpixels.Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(JaitlyandHinton2013,).Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)canalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationandevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmallrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobusttonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustnessofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheirinputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuchasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworkswhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdatasetaugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowedthatthisapproachcanb'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 255}, page_content='Vincent2008etal.,).Noiseinjectionalsoworkswhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdatasetaugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowedthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthenoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbedescribedinSec.,canbeseenasaprocessofconstructingnewinputsby7.12multiplyingbynoise.Whencomparingmachinelearningbenchmarkresults,itisimportanttotaketheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddatasetaugmentationschemescandramaticallyreducethegeneralizationerrorofamachinelearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithmtoanother,itisnecessarytoperformcontrolledexperiments.WhencomparingmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessarytomakesurethatbothalgorithmswereevaluatedusingthesamehand-designeddatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywithnodatasetaugmentationandalgorithmBperformswellwhencombinedwithn'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 255}, page_content='ssarytomakesurethatbothalgorithmswereevaluatedusingthesamehand-designeddatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywithnodatasetaugmentationandalgorithmBperformswellwhencombinedwithnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythesynthetictransformationscausedtheimprovedperformance,ratherthantheuseofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperimenthasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machinelearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdatasetaugmentation.Usually,operationsthataregenerallyapplicable(suchasaddingGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,whileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomlycroppinganimage)areconsideredtobeseparatepre-processingsteps.241'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 256}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.5NoiseRobustnessSec.hasmotivatedtheuseofnoiseappliedtotheinputsasadatasetaug-7.4mentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimalvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthenormoftheweights(,,).Inthegeneralcase,itisimportanttoBishop1995abrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinkingtheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noiseappliedtothehiddenunitsissuchanimportanttopicastomerititsownseparatediscussion;thedropoutalgorithmdescribedinSec.isthemaindevelopment7.12ofthatapproach.Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodelsisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthecontextofrecurrentneuralnetworks(,;Jimetal.1996Graves2011,). ThiscanbeinterpretedasastochasticimplementationofaBayesianinferenceovertheweights.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 256}, page_content='ThiscanbeinterpretedasastochasticimplementationofaBayesianinferenceovertheweights. TheBayesiantreatmentoflearningwouldconsiderthemodelweightstobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthisuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂectthisuncertainty(Graves2011,).Thiscanalsobeinterpretedasequivalent(undersomeassumptions)toamoretraditionalformofregularization.Addingnoisetotheweightshasbeenshowntobeaneﬀectiveregularizationstrategyinthecontextofrecurrentneuralnetworks(,;Jimetal.1996Graves2011,).Inthefollowing,wewillpresentananalysisoftheeﬀectofweightnoiseonastandardfeedforwardneuralnetwork(asintroducedinChapter).6Westudytheregressionsetting,wherewewishtotrainafunctionˆy(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squarescostfunctionbetweenthemodelpredictionsˆy()xandthetruevalues:yJ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 256}, page_content='Epx,y()\\ue002(ˆyy()x−)2\\ue003.(7.30)Thetrainingsetconsistsoflabeledexamplesm{(x(1),y(1))(,...,x()m,y()m)}.Wenowassumethatwitheachinputpresentationwealsoincludearandomperturbation\\ue00fW∼N(\\ue00f;0,ηI)ofthenetworkweights.Letusimaginethatwehaveastandardl-layerMLP.Wedenotetheperturbedmodelasˆy\\ue00fW(x).Despitetheinjectionofnoise,wearestillinterestedinminimizingthesquarederroroftheoutputofthenetwork.Theobjectivefunctionthusbecomes:˜JW= Ep,y,(x\\ue00fW)\\ue068(ˆy\\ue00fW())x−y2\\ue069(7.31)242'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 257}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING= Ep,y,(x\\ue00fW)\\ue002ˆy2\\ue00fW()2ˆx−yy\\ue00fW()+xy2\\ue003.(7.32)Forsmallη,theminimizationofJwithaddedweightnoise(withcovarianceηI)isequivalenttominimizationofJwithanadditionalregularizationterm:ηEp,y(x)\\ue002\\ue06b∇Wˆy()x\\ue06b2\\ue003.Thisformofregularizationencouragestheparameterstogotoregionsofparameterspacewheresmallperturbationsoftheweightshavearelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodelintoregionswherethemodelisrelativelyinsensitivetosmallvariationsintheweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedbyﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinearregression(where,forinstance,ˆy(x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 257}, page_content='=w\\ue03ex+b),thisregularizationtermcollapsesintoηEp()x\\ue002\\ue06b\\ue06bx2\\ue003,whichisnotafunctionofparametersandthereforedoesnotcontributetothegradientof˜JWwithrespecttothemodelparameters.7.5.1InjectingNoiseattheOutputTargetsMostdatasetshavesomeamountofmistakesintheylabels.Itcanbeharmfultomaximizelogp(y|x)whenyisamistake.Onewaytopreventthisistoexplicitlymodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmallconstant\\ue00f,thetrainingsetlabelyiscorrectwithprobability1−\\ue00f,andotherwiseanyoftheotherpossiblelabelsmightbecorrect. Thisassumptioniseasytoincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawingnoisesamples.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 257}, page_content='Forexample,labelsmoothingregularizesamodelbasedonasoftmaxwithkoutputvaluesbyreplacingthehardandclassiﬁcation01targetswithtargetsof\\ue00fkand1−k−1k\\ue00f,respectively.Thestandardcross-entropylossmaythenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmaxclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcanneverpredictaprobabilityofexactlyorexactly,soitwillcontinuetolearn01largerandlargerweights,makingmoreextremepredictionsforever.Itispossibletopreventthisscenariousingotherregularizationstrategieslikeweightdecay.Labelsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithoutdiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980sandcontinuestobefeaturedprominentlyinmodernneuralnetworks(,).Szegedyetal.20157.6Semi-SupervisedLearningIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfromP(x)andlabeledexamplesfromP(xy,)areusedtoestimateP(yx|)orpredictyfrom243'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 258}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGx.Inthecontextofdeeplearning,semi-supervisedlearningusuallyreferstolearningarepresentationh=f(x).Thegoalistolearnarepresentationsothatexamplesfromthesameclasshavesimilarrepresentations.Unsupervisedlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentationspace.Examplesthatclustertightlyintheinputspaceshouldbemappedtosimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebettergeneralizationinmanycases(BelkinandNiyogi2002Chapelle2003,;etal.,).Along-standingvariantofthisapproachistheapplicationofprincipalcomponentsanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojecteddata).Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthemodel,onecanconstructmodelsinwhichagenerativemodelofeitherP(x)orP(xy,)sharesparameterswithadiscriminativemodelofP(yx|).Onecanthentrade-oﬀthesupervisedcriterion−logP(yx|)withtheunsupervisedorgenerativeone(suchas−logP(x)or−logP(xy,)).Thegenerativecriterionthenexpressesaparticularformofpriorbeli'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 258}, page_content='scriminativemodelofP(yx|).Onecanthentrade-oﬀthesupervisedcriterion−logP(yx|)withtheunsupervisedorgenerativeone(suchas−logP(x)or−logP(xy,)).Thegenerativecriterionthenexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervisedlearningproblem(,),namelythatthestructureofLasserreetal.2006P(x)isconnectedtothestructureofP(yx|)inawaythatiscapturedbythesharedparametrization.Bycontrollinghowmuchofthegenerativecriterionisincludedinthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerativeorapurelydiscriminativetrainingcriterion(,;Lasserreetal.2006LarochelleandBengio2008,).SalakhutdinovandHinton2008()describeamethodforlearningthekernelfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeledexamplesformodelingimprovesquitesigniﬁcantly.P()xP()yx|See()formoreinformationaboutsemi-supervisedlearning.Chapelleetal.20067.7Multi-TaskLearningMulti-tasklearning(,)isawaytoimprovegeneralizationbypoolingCaruana1993theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)ar'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 258}, page_content='semi-supervisedlearning.Chapelleetal.20067.7Multi-TaskLearningMulti-tasklearning(,)isawaytoimprovegeneralizationbypoolingCaruana1993theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)arisingoutofseveraltasks.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 258}, page_content='Inthesamewaythatadditionaltrainingexamplesputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralizewell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismoreconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyieldingbettergeneralization.244'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 259}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGFig.illustratesaverycommonformofmulti-tasklearning,inwhichdiﬀerent7.2supervisedtasks(predictingy()igivenx)sharethesameinputx,aswellassomeintermediate-levelrepresentationh(shared)capturingacommonpooloffactors.Themodelcangenerallybedividedintotwokindsofpartsandassociatedparameters:1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtasktoachievegoodgeneralization).ThesearetheupperlayersoftheneuralnetworkinFig..7.22.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthepooleddataofallthetasks).ThesearethelowerlayersoftheneuralnetworkinFig..7.2\\nh(1)h(1)h(2)h(2)h(3)h(3)y(1)y(1)y(2)y(2)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 259}, page_content='h(shared)h(shared)x'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 259}, page_content='xFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworksandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbutinvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetheritissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)canbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectivelywiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldingasharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommonpooloffactorsthatexplainthevariationsintheinputx,whileeachtaskisassociatedwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-levelhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredictingy(1)andy(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.Intheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobeassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeoftheinputvariationsb'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 259}, page_content='ared)issharedacrossalltasks.Intheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobeassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeoftheinputvariationsbutarenotrelevantforpredictingy(1)ory(2).245'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 260}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n\\ue030\\ue035\\ue030\\ue031\\ue030\\ue030\\ue031\\ue035\\ue030\\ue032\\ue030\\ue030\\ue032\\ue035\\ue030\\ue054\\ue069\\ue06d\\ue065\\ue020\\ue028\\ue065\\ue070\\ue06f\\ue063\\ue068\\ue073\\ue029\\ue030\\ue02e\\ue030\\ue030\\ue030\\ue02e\\ue030\\ue035\\ue030\\ue02e\\ue031\\ue030\\ue030\\ue02e\\ue031\\ue035\\ue030\\ue02e\\ue032\\ue030\\ue04c\\ue06f\\ue073\\ue073\\ue020\\ue028\\ue06e\\ue065\\ue067\\ue061\\ue074\\ue069\\ue076\\ue065\\ue020\\ue06c\\ue06f\\ue067\\ue020\\ue06c\\ue069\\ue06b\\ue065\\ue06c\\ue069\\ue068\\ue06f\\ue06f\\ue064\\ue029\\ue04c\\ue065\\ue061\\ue072\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue063\\ue075\\ue072\\ue076\\ue065\\ue073\\ue054\\ue072\\ue061\\ue069\\ue06e\\ue069\\ue06e\\ue067\\ue020\\ue073\\ue065\\ue074\\ue020\\ue06c\\ue06f\\ue073\\ue073\\ue056\\ue061\\ue06c\\ue069\\ue064\\ue061\\ue074\\ue069\\ue06f\\ue06e\\ue020\\ue073\\ue065\\ue074\\ue020\\ue06c\\ue06f\\ue073\\ue073'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 260}, page_content='Figure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesovertime(indicatedasnumberoftrainingiterationsoverthedataset,orepochs).Inthisexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjectivedecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginstoincreaseagain,forminganasymmetricU-shapedcurve.Improvedgeneralizationandgeneralizationerrorbounds(,)canbeBaxter1995achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbegreatlyimproved(inproportionwiththeincreasednumberofexamplesforthesharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethiswillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetweenthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssomeofthetasks.Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthefollowing:amongthefactorsthatexplainthevariationsobservedinthedataassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.7.8EarlyStoppingWhentraininglargemodelswiths'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 260}, page_content='nderlyingpriorbeliefisthefollowing:amongthefactorsthatexplainthevariationsobservedinthedataassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.7.8EarlyStoppingWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁtthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,butvalidationseterrorbeginstoriseagain.SeeFig.foranexampleofthisbehavior.7.3Thisbehavioroccursveryreliably.Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,hopefullybettertestseterror)byreturningtotheparametersettingatthepoint246'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 261}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGintimewiththelowestvalidationseterror.Insteadofrunningouroptimizationalgorithmuntilwereacha(local)minimumofvalidationerror,werunituntiltheerroronthevalidationsethasnotimprovedforsomeamountoftime.Everytimetheerroronthevalidationsetimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithmterminates,wereturntheseparameters,ratherthanthelatestparameters.ThisprocedureisspeciﬁedmoreformallyinAlgorithm.7.1Algorithm 7.1Theearlystopping meta-algorithmfor determiningthe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 261}, page_content='7.1Theearlystopping meta-algorithmfor determiningthe bestamountoftimetotrain.Thismeta-algorithmisageneralstrategythatworkswellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthevalidationset.Letbethenumberofstepsbetweenevaluations.nLetpbethe“patience,”thenumberoftimestoobserveworseningvalidationseterrorbeforegivingup.Letθobetheinitialparameters.θθ←oi←0j←0v←∞θ∗←θi∗←iwhiledoj<pUpdatebyrunningthetrainingalgorithmforsteps.θniin←+v\\ue030←ValidationSetError()θifv\\ue030<vthenj←0θ∗←θi∗←ivv←\\ue030elsejj←+1endifendwhileBestparametersareθ∗,bestnumberoftrainingstepsisi∗Thisstrategyisknownasearlystopping.Itisprobablythemostcommonlyusedformofregularizationindeeplearning.Itspopularityisduebothtoitseﬀectivenessanditssimplicity.247'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 262}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGOnewaytothinkofearlystoppingisasaveryeﬃcienthyperparameterselectionalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.WecanseeinFig.thatthishyperparameterhasaU-shapedvalidationset7.3performancecurve.MosthyperparametersthatcontrolmodelcapacityhavesuchaU-shapedvalidationsetperformancecurve,asillustratedinFig..Inthecaseof5.3earlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermininghowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbechosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameteratthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The“trainingtime”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 262}, page_content='hyperparameterisuniqueinthatbydeﬁnitionasinglerunoftrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcosttochoosingthishyperparameterautomaticallyviaearlystoppingisrunningthevalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdoneinparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparateGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthecostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatissmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorlessfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthebestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostoretheseparametersinaslowerandlargerformofmemory(forexample,traininginGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadiskdrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduringtraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltraini'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 262}, page_content='ginGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadiskdrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduringtraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.Earlystoppingisaveryunobtrusiveformofregularization,inthatitrequiresalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,orthesetofallowableparametervalues.Thismeansthatitiseasytouseearlystoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweightdecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthenetworkinabadlocalminimumcorrespondingtoasolutionwithpathologicallysmallweights.Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjectivefunctiontoencouragebettergeneralization,itisrareforthebestgeneralizationtooccuratalocalminimumofthetrainingobjective.Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnotfedtothemodel.Tobestexploitthisextradata,onecanperforme'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 262}, page_content=',itisrareforthebestgeneralizationtooccuratalocalminimumofthetrainingobjective.Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnotfedtothemodel.Tobestexploitthisextradata,onecanperformextratrainingaftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra248'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 263}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategiesonecanuseforthissecondtrainingprocedure.Onestrategy(Algorithm)istoinitializethemodelagainandretrainonall7.2ofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsastheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Therearesomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagoodwayofknowingwhethertoretrainforthesamenumberofparameterupdatesorthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethetrainingsetisbigger.Algorithm7.2Ameta-algorithmforusingearlystoppingtodeterminehowlongtotrain,thenretrainingonallthedata.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(Algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisr'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 263}, page_content='o(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(Algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisreturnsi∗,theoptimalnumberofsteps.Settorandomvaluesagain.θTrainonX()trainandy()trainfori∗steps.Anotherstrategyforusingallofthedataistokeeptheparametersobtainedfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallofthedata.Atthisstage,wenownolongerhaveaguideforwhentostopintermsofanumberofsteps.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 263}, page_content='Instead,wecanmonitortheaveragelossfunctiononthevalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetrainingsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoidsthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.Forexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwilleverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.ThisprocedureispresentedmoreformallyinAlgorithm.7.3Earlystoppingisalsousefulbecauseitreducesthecomputationalcostofthetrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumberoftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithoutrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationofthegradientsofsuchadditionalterms.249'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 264}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGAlgorithm7.3Meta-algorithmusingearlystoppingtodetermineatwhatobjec-tivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(Algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisupdates.θ\\ue00fJ,←(θX()subtrain,y()subtrain)whileJ,(θX(valid),y(valid))'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 264}, page_content='>\\ue00fdoTrainonX()trainandy()trainforsteps.nendwhileHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearlystoppingaregularizationstrategy,butwehavesupportedthisclaimonlybyisshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.Whatistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop()and()arguedthatearlystoppinghastheeﬀectof1995aSjöbergandLjung1995restrictingtheoptimizationproceduretoarelativelysmallvolumeofparameterspaceintheneighborhoodoftheinitialparametervalueθo.Morespeciﬁcally,imaginetakingτoptimizationsteps(correspondingtoτtrainingiterations)andwithlearningrate\\ue00f.Wecanviewtheproduct\\ue00fτasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restrictingboththenumberofiterationsandthelearningratelimitsthevolumeofparameterspacereachablefromθo.Inthissense,\\ue00fτbehavesasifitwerethereciprocalofthecoeﬃcientusedforweightdecay.Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadraticerrorfunctionandsimplegradientdescent—earlystoppingisequivalenttoL2regulariz'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 264}, page_content='havesasifitwerethereciprocalofthecoeﬃcientusedforweightdecay.Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadraticerrorfunctionandsimplegradientdescent—earlystoppingisequivalenttoL2regularization.InordertocomparewithclassicalL2regularization,weexamineasimplesettingwheretheonlyparametersarelinearweights(θ=w).WecanmodelthecostfunctionJwithaquadraticapproximationintheneighborhoodoftheempiricallyoptimalvalueoftheweightsw∗:ˆJJ()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 264}, page_content='= θ(w∗)+12(ww−∗)\\ue03eHww(−∗),(7.33)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Giventheassumptionthatw∗isaminimumofJ(w),weknowthatHispositivesemideﬁnite.250'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 265}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 265}, page_content='w1w2w∗˜ww1w2w∗˜wFigure7.4:Anillustrationoftheeﬀectofearlystopping.(Left)Thesolidcontourlinesindicatethecontoursofthenegativelog-likelihood.ThedashedlineindicatesthetrajectorytakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗thatminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.Anillustrationoftheeﬀectof(Right)L2regularizationforcomparison.ThedashedcirclesindicatethecontoursoftheL2penalty,whichcausestheminimumofthetotalcosttolienearertheoriginthantheminimumoftheunregularizedcost.UnderalocalTaylorseriesapproximation,thegradientisgivenby:∇wˆJ() = (wHww−∗).(7.34)Wearegoingtostudythetrajectoryfollowedbytheparametervectorduringtraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3thatisw(0)= 0.Letussupposethatweupdatetheparametersviagradientdescent:w()τ= w(1)τ−−∇\\ue00fwJ(w(1)τ−)(7.35)= w(1)τ−−\\ue00fHw((1)τ−−w∗)(7.36)w()τ−w∗='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 265}, page_content='0.Letussupposethatweupdatetheparametersviagradientdescent:w()τ= w(1)τ−−∇\\ue00fwJ(w(1)τ−)(7.35)= w(1)τ−−\\ue00fHw((1)τ−−w∗)(7.36)w()τ−w∗= ()(IH−\\ue00fw(1)τ−−w∗)(7.37)LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploitingtheeigendecompositionofH:H=QQΛ\\ue03e,whereΛisadiagonalmatrixandQisanorthonormalbasisofeigenvectors.w()τ−w∗= (IQQ−\\ue00fΛ\\ue03e)(w(1)τ−−w∗)(7.38)3Forneuralnetworks,toobtainsymmetrybreakingbetweenhiddenunits,wecannotinitializealltheparametersto0,asdiscussedinSec..However,theargumentholdsforanyother6.2initialvaluew(0).251'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 266}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGQ\\ue03e(w()τ−w∗) = ()I−\\ue00fΛQ\\ue03e(w(1)τ−−w∗)(7.39)Assumingthatw(0)=0andthat\\ue00fischosentobesmallenoughtoguarantee|1−\\ue00fλi|<1,theparametertrajectoryduringtrainingafterτparameterupdatesisasfollows:Q\\ue03ew()τ= [()I−I−\\ue00fΛτ]Q\\ue03ew∗.(7.40)Now,theexpressionforQ\\ue03e˜winEq.for7.13L2regularizationcanberearrangedas:Q\\ue03e˜wI= (+Λα)−1ΛQ\\ue03ew∗(7.41)Q\\ue03e˜wII= [−(+Λα)−1α]Q\\ue03ew∗(7.42)ComparingEq.andEq.,weseethatifthehyperparameters7.407.42\\ue00f,α,andτarechosensuchthat()I−\\ue00fΛτ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 266}, page_content='(+)ΛαI−1α,(7.43)thenL2regularizationandearlystoppingcanbeseentobeequivalent(atleastunderthequadraticapproximationoftheobjectivefunction).Goingevenfurther,bytakinglogarithmsandusingtheseriesexpansionforlog(1+x),wecanconcludethatifallλiaresmall(thatis,\\ue00fλi\\ue01c1andλi/α\\ue01c1)thenτ≈1\\ue00fα,(7.44)α≈1τ\\ue00f.(7.45)Thatis,undertheseassumptions,thenumberoftrainingiterationsτplaysaroleinverselyproportionaltotheL2regularizationparameter,andtheinverseofτ\\ue00fplaystheroleoftheweightdecaycoeﬃcient.Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(oftheobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespondtodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameterscorrespondingtodirectionsoflesscurvature.ThederivationsinthissectionhaveshownthatatrajectoryoflengthτendsatapointthatcorrespondstoaminimumoftheL2-regularizedobjective.Earlystoppingisofcoursemorethanthemererestrictionofthetrajectorylength;instead,ea'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 266}, page_content='vationsinthissectionhaveshownthatatrajectoryoflengthτendsatapointthatcorrespondstoaminimumoftheL2-regularizedobjective.Earlystoppingisofcoursemorethanthemererestrictionofthetrajectorylength;instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorinordertostopthetrajectoryataparticularlygoodpointinspace.Earlystoppingthereforehastheadvantageoverweightdecaythatearlystoppingautomaticallydeterminesthecorrectamountofregularizationwhileweightdecayrequiresmanytrainingexperimentswithdiﬀerentvaluesofitshyperparameter.252'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 267}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.9ParameterTyingandParameterSharingThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenaltiestotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.Forexample,L2regularization(orweightdecay)penalizesmodelparametersfordeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedotherwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtakebutweknow,fromknowledgeofthedomainandmodelarchitecture,thatthereshouldbesomedependenciesbetweenthemodelparameters.Acommontypeofdependencythatweoftenwanttoexpressisthatcertainparametersshouldbeclosetooneanother.Considerthefollowingscenario:wehavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetofclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodelAwithparametersw()AandmodelBwithparametersw()B.Thetwomodelsmaptheinput totwo diﬀerent, but relatedoutputs:ˆy()A=f(w()A,x)andˆy()B='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 267}, page_content='totwo diﬀerent, but relatedoutputs:ˆy()A=f(w()A,x)andˆy()B= (gw()B,x).Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinputandoutputdistributions)thatwebelievethemodelparametersshouldbeclosetoeachother:∀i,w()Aishouldbeclosetow()Bi.Wecanleveragethisinformationthroughregularization.Speciﬁcally,wecanuseaparameternormpenaltyoftheform:Ω(w()A,w()B)=\\ue06bw()A−w()B\\ue06b22.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 267}, page_content='HereweusedanL2penalty,butotherchoicesarealsopossible.Thiskindofapproachwasproposedby(),whoregularizedLasserreetal.2006theparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,tobeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm(tocapturethedistributionoftheobservedinputdata).Thearchitectureswereconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbepairedtocorrespondingparametersintheunsupervisedmodel.Whileaparameternormpenaltyisonewaytoregularizeparameterstobeclosetooneanother,themorepopularwayistouseconstraints:toforcesetsofparameterstobeequal.Thismethodofregularizationisoftenreferredtoasparametersharing,whereweinterpretthevariousmodelsormodelcomponentsassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharingoverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlyasubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertainmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcantreductioninthememoryfootprintoft'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 267}, page_content='beclose(viaanormpenalty)isthatonlyasubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertainmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcantreductioninthememoryfootprintofthemodel.253'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 268}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConvolutionalNeuralNetworksByfarthemostpopularandextensiveuseofparametersharingoccursinconvolutionalneuralnetworks(CNNs)appliedtocomputervision.Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixeltotheright.CNNstakethispropertyintoaccountbysharingparametersacrossmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁndacatwiththesamecatdetectorwhetherthecatappearsatcolumniorcolumni+1intheimage.ParametersharinghasallowedCNNstodramaticallylowerthenumberofuniquemodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringacorrespondingincreaseintrainingdata.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 268}, page_content='Itremainsoneofthebestexamplesofhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.CNNswillbediscussedinmoredetailinChapter.97.10SparseRepresentationsWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Anotherstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicatedpenaltyonthemodelparameters.Wehavealreadydiscussed(inSec.)how7.1.2L1penalizationinducesasparseparametrization—meaningthatmanyoftheparametersbecomezero(orclosetozero).Representationalsparsity,ontheotherhand,describesarepresentationwheremanyoftheelementsoftherepresentationarezero(orclosetozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextoflinearregression:\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f018515−9−3\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0400200−001030−050000100104−−100050−\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f023−2−514\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fby∈RmA∈Rmn×x∈Rn(7.46)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 268}, page_content='254'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 269}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−14119223\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0312541−−423113−−−−−154232312303−−−−−−542251\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00200−30\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fby∈RmB∈Rmn×h∈Rn(7.47)Intheﬁrstexpression,wehaveanexampleofasparselyparametrizedlinearregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-tionhofthedatax.Thatis,hisafunctionofxthat,insomesense,representstheinformationpresentin,butdoessowithasparsevector.xRepresentationalregularizationisaccomplishedbythesamesortsofmechanismsthatwehaveusedinparameterregularization.NormpenaltyregularizationofrepresentationsisperformedbyaddingtothelossfunctionJanormpenaltyontherepresentation.ThispenaltyisdenotedΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜J:˜J,J,α(;θXy) = (;θXy)+Ω()h(7.48)whereα∈[0,∞)weightstherelativecontributionofthenormpenaltyterm,withlargervaluesofcorrespondingtomoreregularization.αJustasanL1penaltyontheparametersinducesparametersparsity,anL1penaltyontheelementsoftherepresentationinducesrepresentationalsparsity:Ω(h)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 269}, page_content='=||||h1=\\ue050i|hi|. Ofcourse,theL1penaltyisonlyonechoiceofpenaltythatcanresultinasparserepresentation.OthersincludethepenaltyderivedfromaStudent-tpriorontherepresentation(,;,)OlshausenandField1996Bergstra2011andKLdivergencepenalties(,)thatareespeciallyLarochelleandBengio2008usefulforrepresentationswithelementsconstrainedtolieontheunitinterval.Lee2008Goodfellow2009etal.()andetal.()bothprovideexamplesofstrategiesbasedonregularizingtheaverageactivationacrossseveralexamples,1m\\ue050ih()i,tobenearsometargetvalue,suchasavectorwith.01foreachentry.Otherapproachesobtainrepresentationalsparsitywithahardconstraintontheactivation values.Forexample, orthogonal matchingpursuit(,Patietal.1993)encodesaninputxwiththerepresentationhthatsolvestheconstrainedoptimizationproblemargminhh,\\ue06b\\ue06b0<k\\ue06b−\\ue06bxWh2,(7.49)where\\ue06b\\ue06bh0isthenumberofnon-zeroentriesofh. ThisproblemcanbesolvedeﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled255'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 270}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGOMP-kwiththevalueofkspeciﬁedtoindicatethenumberofnon-zerofeaturesallowed.()demonstratedthatOMP-canbeaveryeﬀectiveCoatesandNg20111featureextractorfordeeparchitectures.Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughoutthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyofcontexts.7.11BaggingandOtherEnsembleMethodsBaggingbootstrapaggregating(shortfor)isatechniqueforreducinggeneralizationerrorbycombiningseveralmodels(,).TheideaistotrainseveralBreiman1994diﬀerentmodelsseparately,thenhaveallofthemodelsvoteontheoutputfortestexamples.Thisisanexampleofageneralstrategyinmachinelearningcalledmodelaveraging.Techniquesemployingthisstrategyareknownasensemblemethods.Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusuallynotmakeallthesameerrorsonthetestset.Considerforexampleasetofkregressionmodels.Supposethateachmodelmakesanerror\\ue00fioneachexample, withtheerrorsdrawnfromazero-meanmultivariatenormaldistributionwithvariancesE[\\ue00f2i]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 270}, page_content='withtheerrorsdrawnfromazero-meanmultivariatenormaldistributionwithvariancesE[\\ue00f2i] =vandcovariancesE[\\ue00fi\\ue00fj] =c. Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis1k\\ue050i\\ue00fi.TheexpectedsquarederroroftheensemblepredictorisE\\uf8ee\\uf8f0\\ue0201k\\ue058i\\ue00fi\\ue0212\\uf8f9\\uf8fb=1k2E\\uf8ee\\uf8f0\\ue058i\\uf8eb\\uf8ed\\ue00f2i+\\ue058ji\\ue036=\\ue00fi\\ue00fj\\uf8f6\\uf8f8\\uf8f9\\uf8fb(7.50)=1kv+k−1kc.(7.51)Inthecasewheretheerrorsareperfectlycorrelatedandc=v,themeansquarederrorreducestov,sothemodelaveragingdoesnothelpatall.Inthecasewheretheerrorsareperfectlyuncorrelatedandc= 0,theexpectedsquarederroroftheensembleisonly1kv.Thismeansthattheexpectedsquarederroroftheensembledecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemblewillperformatleastaswellasanyofitsmembers,andifthemembersmakeindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletelydiﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging256'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 271}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 271}, page_content='88First ensemble memberSecond ensemble memberOriginal datasetFirst resampled datasetSecond resampled'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 271}, page_content='datasetFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan‘8’detectoronthedatasetdepictedabove,containingan‘8’,a‘6’anda‘9’.Supposewemaketwodiﬀerentresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasetsbysamplingwithreplacement.Theﬁrstdatasetomitsthe‘9’andrepeatsthe‘8’.Onthisdataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan‘8’.Ontheseconddataset,werepeatthe‘9’andomitthe‘6’.Inthiscase,thedetectorlearnsthatalooponthebottomofthedigitcorrespondstoan‘8’.Eachoftheseindividualclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,achievingmaximalconﬁdenceonlywhenbothloopsofthe‘8’arepresent.isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjectivefunctiontobereusedseveraltimes.Speciﬁcally,bagginginvolvesconstructingkdiﬀerentdatasets.Eachdatasethasthesamenumberofexamplesastheoriginaldataset,buteachdatasetisconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeansthat,withhighprobability,eachdatasetismissi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 271}, page_content='ﬀerentdatasets.Eachdatasethasthesamenumberofexamplesastheoriginaldataset,buteachdatasetisconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeansthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromtheoriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtrainingset,ifithasthesamesizeastheoriginal).Modeliisthentrainedondataseti.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultindiﬀerencesbetweenthetrainedmodels.SeeFig.foranexample.7.5Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycanoftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesamedataset.Diﬀerencesinrandominitialization,randomselectionofminibatches,diﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-deterministicimple-mentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersoftheensembletomakepartiallyindependenterrors.257'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 272}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGModelaveragingisanextremelypowerfulandreliablemethodforreducinggeneralizationerror.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithmsforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrandPrize(Koren2009,).Notalltechniquesforconstructingensemblesaredesignedtomaketheensemblemoreregularizedthantheindividualmodels.Forexample,atechniquecalledboosting(FreundandSchapire1996ba,,)constructsanensemblewithhighercapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensemblesofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneuralnetworkstotheensemble.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 272}, page_content='Boostinghasalsobeenappliedinterpretinganindividualneuralnetworkasanensemble(,),incrementallyaddinghiddenunitsBengioetal.2006atotheneuralnetwork.7.12DropoutDropout(Srivastava 2014etal., )providesa'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 272}, page_content='2014etal., )providesa computationallyinexpensivebutpowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensemblesofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpracticalwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuchnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensemblesofﬁvetotenneuralnetworks—()usedsixtowintheILSVRC—Szegedyetal.2014abutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensiveapproximationtotrainingandevaluatingabaggedensembleofexponentiallymanyneuralnetworks.Speciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthatcanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,asillustratedinFig..Inmostmodernneuralnetworks,basedonaseriesof7.6aﬃnetransformationsandnonlinearities,wecaneﬀectivelyremoveaunitfromanetworkbymultiplyingitsoutputvaluebyzero.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 272}, page_content='Thisprocedurerequiressomeslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtakethediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresentthedropoutalgorithmintermsofmultiplicationbyzeroforsimplicity,butitcan258'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 273}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthenetwork.Recallthattolearnwithbagging,wedeﬁnekdiﬀerentmodels,constructkdiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthentrainmodeliondataseti.Dropoutaimstoapproximatethisprocess,butwithanexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,weuseaminibatch-basedlearningalgorithmthatmakessmallsteps,suchasstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,werandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhiddenunitsinthenetwork.Themaskforeachunitissampledindependentlyfromalloftheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobeincluded)isahyperparameterﬁxedbeforetrainingbegins.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 273}, page_content='Itisnotafunctionofthecurrentvalueofthemodelparametersortheinputexample.Typically,aninputunitisincludedwithprobability0.8andahiddenunitisincludedwithprobability0.5.Wethenrunforwardpropagation,back-propagation,andthelearningupdateasusual.Fig.illustrateshowtorunforwardpropagationwith7.7dropout.Moreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,andJ(θµ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.ThendropouttrainingconsistsinminimizingEµJ(θµ,).Theexpectationcontainsexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradientbysamplingvaluesof.µDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseofbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshareparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromtheparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentanexponentialnumberofmodelswithatractableamountofmemory.Inthecaseofbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthecaseofdropout,typicallymo'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 273}, page_content='etersharingmakesitpossibletorepresentanexponentialnumberofmodelswithatractableamountofmemory.Inthecaseofbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthecaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,themodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-networkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossiblesub-networksareeachtrainedforasinglestep,andtheparametersharingcausestheremainingsub-networkstoarriveatgoodsettingsoftheparameters.Thesearetheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.Forexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetoftheoriginaltrainingsetsampledwithreplacement.Tomakeaprediction,abaggedensemblemustaccumulatevotesfromallofitsmembers.Werefertothisprocessasinferenceinthiscontext.Sofar,our259'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 274}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ny yh1h1h2h2x1x1x2x2y yh1h1h2h2x1x1x2x2y yh1h1h2h2x2x2y yh1h1h2h2x1x1y yh2h2x1x1x2x2y yh1h1x1x1x2x2y yh1h1h2h2y yx1x1x2x2y yh2h2x2x2y yh1h1x1x1y yh1h1x2x2y yh2h2x1x1y yx1x1y yx2x2y yh2h2y yh1h1y yBase network\\nEnsemble of Sub-NetworksFigure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbeconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,webeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteenpossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformedbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnectingtheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwiderlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomessmaller.260'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 275}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nˆx1ˆx1µx1µx1x1x1ˆx2ˆx2x2x2µx2µx2h1h1h2h2µh1µh1µh2µh2ˆh1ˆh1ˆh2ˆh2y yy yh1h1h2h2x1x1x2x2\\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusingdropout.(Top)Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,onehiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward(Bottom)propagationwithdropout,werandomlysampleavectorµwithoneentryforeachinputorhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependentlyfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually10.5forthehiddenlayersand0.8fortheinput.Eachunitinthenetworkismultipliedbythecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthenetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfromFig.andrunningforwardpropagationthroughit.7.6261'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 276}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGdescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitlyprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobabilitydistribution.Inthecaseofbagging,eachmodeliproducesaprobabilitydistributionp()i(y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofallofthesedistributions,1kk\\ue058i=1p()i()y|x.(7.52)Inthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-abilitydistributionp(y,|xµ).Thearithmeticmeanoverallmasksisgivenby\\ue058µppy,()µ(|xµ)(7.53)wherep(µ)istheprobabilitydistributionthatwasusedtosampleµattrainingtime.Becausethissumincludesanexponentialnumberofterms,itisintractabletoevaluateexceptincaseswherethestructureofthemodelpermitssomeformofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractablesimpliﬁcation.Instead, wecan approximatethe inferencewithsampling,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 276}, page_content='wecan approximatethe inferencewithsampling, byaveragingtogethertheoutputfrommanymasks.Even10-20masksareoftensuﬃcienttoobtaingoodperformance.However,thereisanevenbetterapproach,thatallowsustoobtainagoodapproximationtothepredictionsoftheentireensemble,atthecostofonlyoneforwardpropagation.Todoso,wechangetousingthegeometricmeanratherthanthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-Farley2014etal.()presentargumentsandempiricalevidencethatthegeometricmeanperformscomparablytothearithmeticmeaninthiscontext.Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobeaprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,weimposetherequirementthatnoneofthesub-modelsassignsprobability0toanyevent,andwerenormalizetheresultingdistribution.Theunnormalizedprobabilitydistributiondeﬁneddirectlybythegeometricmeanisgivenby˜pensemble()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 276}, page_content='=y|x2d\\ue073\\ue059µpy,(|xµ)(7.54)wheredisthenumberofunitsthatmaybedropped.Hereweuseauniformdistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare262'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 277}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGalsopossible.Tomakepredictionswemustre-normalizetheensemble:pensemble() =y|x˜pensemble()y|x\\ue050y\\ue030˜pensemble(y\\ue030|x).(7.55)Akeyinsight(,)involvedindropoutisthatwecanapproxi-Hintonetal.2012cmatepensemblebyevaluatingp(y|x)inonemodel:themodelwithallunits,butwiththeweightsgoingoutofunitimultipliedbytheprobabilityofincludinguniti.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueoftheoutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximateinferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.Becauseweusuallyuseaninclusionprobabilityof12,theweightscalingruleusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 277}, page_content='themodelasusual.Anotherwaytoachievethesameresultistomultiplythestatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat2theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpectedtotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimearemissingonaverage.Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweightscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregressionclassiﬁerwithinputvariablesrepresentedbythevector:nvPy(= y|v) = softmax\\ue010W\\ue03ev+b\\ue011y.(7.56)Wecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationoftheinputwithabinaryvector:dPy(= y|v;) = dsoftmax\\ue010W\\ue03e()+d\\ue00cvb\\ue011y.(7.57)Theensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverallensemblemembers’predictions:Pensemble(= ) =yy|v˜Pensemble(= )yy|v\\ue050y\\ue030˜Pensemble(= yy\\ue030|v)(7.58)where˜Pensemble(= ) =yy|v2n\\ue073\\ue059d∈{}01,nPy.(= y|v;)d(7.59)263'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 278}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGToseethattheweightscalingruleisexact,wecansimplify˜Pensemble:˜Pensemble(= ) =yy|v2n\\ue073\\ue059d∈{}01,nPy(= y|v;)d(7.60)=2n\\ue073\\ue059d∈{}01,nsoftmax(W\\ue03e()+)d\\ue00cvby(7.61)=2n\\ue076\\ue075\\ue075\\ue074\\ue059d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue001\\ue050y\\ue030exp\\ue010W\\ue03ey\\ue030,:()+d\\ue00cvb\\ue011(7.62)=2n\\ue071\\ue051d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue0012n\\ue072\\ue051d∈{}01,n\\ue050y\\ue030exp\\ue010W\\ue03ey\\ue030,:()+d\\ue00cvb\\ue011(7.63)Because˜Pwillbenormalized,wecansafelyignoremultiplicationbyfactorsthatareconstantwithrespectto:y˜Pensemble(= ) yy|v∝2n\\ue073\\ue059d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue001(7.64)= exp\\uf8eb\\uf8ed12n\\ue058d∈{}01,nW\\ue03ey,:()+d\\ue00cvb\\uf8f6\\uf8f8(7.65)='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 278}, page_content=') yy|v∝2n\\ue073\\ue059d∈{}01,nexp\\ue000W\\ue03ey,:()+d\\ue00cvb\\ue001(7.64)= exp\\uf8eb\\uf8ed12n\\ue058d∈{}01,nW\\ue03ey,:()+d\\ue00cvb\\uf8f6\\uf8f8(7.65)= exp\\ue01212W\\ue03ey,:v+b\\ue013(7.66)SubstitutingthisbackintoEq.weobtainasoftmaxclassiﬁerwithweights7.5812W.Theweightscalingruleisalsoexactinothersettings,includingregressionnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehiddenlayerswithoutnonlinearities.However,theweightscalingruleisonlyanapproxi-mationfordeepmodelsthathavenonlinearities.Thoughtheapproximationhasnotbeentheoreticallycharacterized,itoftenworkswell,empirically.Goodfellowetal.()foundexperimentallythattheweightscalingapproximationcanwork2013abetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximationstotheensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwasallowedtosampleupto1,000sub-networks.()foundGalandGhahramani2015thatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand264'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 279}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinferenceapproximationisproblem-dependent.Srivastava2014etal.()showedthatdropoutismoreeﬀectivethanotherstandardcomputationallyinexpensiveregularizers,suchasweightdecay,ﬁlternormconstraintsandsparseactivityregularization.Dropoutmayalsobecombinedwithotherformsofregularizationtoyieldafurtherimprovement.Oneadvantageofdropoutisthatitisverycomputationallycheap.UsingdropoutduringtrainingrequiresonlyO(n)computationperexampleperupdate,togeneratenrandombinarynumbersandmultiplythembythestate.Dependingontheimplementation,itmayalsorequireO(n)memorytostorethesebinarynumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodelhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpaythecostofdividingtheweightsby2oncebeforebeginningtoruninferenceonexamples.Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimitthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearlyany'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 279}, page_content='idingtheweightsby2oncebeforebeginningtoruninferenceonexamples.Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimitthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearlyanymodelthatusesadistributedrepresentationandcanbetrainedwithstochasticgradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodelssuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrentneuralnetworks(BayerandOsendorfer2014Pascanu2014a,;etal.,).Manyotherregularizationstrategiesofcomparablepowerimposemoresevererestrictionsonthearchitectureofthemodel.Thoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,thecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropoutisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀsetthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidationseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuchlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylargedatasets,regulariza'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 279}, page_content='asethesizeofthemodel.Typicallytheoptimalvalidationseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuchlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylargedatasets,regularizationconferslittlereductioningeneralizationerror.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 279}, page_content='Inthesecases,thecomputationalcostofusingdropoutandlargermodelsmayoutweighthebeneﬁtofregularization.Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutislesseﬀective.Bayesianneural networks(, )outperform dropout ontheNeal1996AlternativeSplicingDataset(,)wherefewerthan5,000examplesXiongetal.2011areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,unsupervisedfeaturelearningcangainanadvantageoverdropout.Wager2013etal.()showedthat,whenappliedtolinearregression,dropoutisequivalenttoL2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor265'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 280}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGeachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientisdeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeepmodels,dropoutisnotequivalenttoweightdecay.Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryfortheapproach’ssuccess.Itisjustameansofapproximatingthesumoverallsub-models.WangandManning2013()derivedanalyticalapproximationstothismarginalization.Theirapproximation,knownasfastdropoutresultedinfasterconvergencetimeduetothereducedstochasticityinthecomputationofthegradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled(butalsomorecomputationallyexpensive)approximationtotheaverageoverallsub-networksthantheweightscalingapproximation.Fastdropouthasbeenusedtonearlymatchtheperformanceofstandarddropoutonsmallneuralnetworkproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoalargeproblem.Justasstochasticityisnotnecessarytoachievetheregularizing'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 280}, page_content='eﬀectofdropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()designedcontrolexperimentsusingamethodcalleddropoutboostingthattheydesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlackitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointlymaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditionaldropoutisanalogoustobagging, thisapproachisanalogoustoboosting.Asintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀectcomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthattheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationofdropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleisonlyachievedwhenthestochasticallysampledensemblemembersaretrainedtoperformwellindependentlyofeachother.Dropouthasinspiredotherstochasticapproachestotrainingexponentiallylargeensemblesofmodelsthatshareweights.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 280}, page_content='DropConnectisaspecialcaseofdropoutwhereeachproductbetweenasinglescalarweightandasinglehiddenunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochasticpoolingisaformofrandomizedpooling(seeSec.)forbuildingensembles9.3ofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerentspatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidelyusedimplicitensemblemethod.Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochasticbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisionsimplementsaformofbaggingwithparametersharing.Earlier, wedescribed266'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 281}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGdropoutas bagginganensembleofmodelsformedbyincludingor excludingunits.However,thereisnoneedforthismodelaveragingstrategytobebasedoninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.Inpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareabletolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafastapproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrizedbyavectorµastraininganensembleconsistingofp(y,|xµ)forallpossiblevaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.Forexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingtheweightsbyµ∼N(1,I)canoutperformdropoutbasedonbinarymasks.BecauseE[µ]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 281}, page_content='=1thestandardnetworkautomaticallyimplementsapproximateinferenceintheensemble,withoutneedinganyweightscaling.Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,approximatebagging.However,thereisanotherviewofdropoutthatgoesfurtherthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensembleofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeabletoperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunitsmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves2012cswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressureforgenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerentorganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheirenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeaturesofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobenotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 281}, page_content='Warde-Farley2014etal.()compareddropouttrainingtotrainingoflargeensemblesandconcludedthatdropoutoﬀersadditionalimprovementstogeneralizationerrorbeyondthoseobtainedbyensemblesofindependentmodels.Itisimportanttounderstandthatalargeportionofthepowerofdropoutarisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.Thiscanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformationcontentoftheinputratherthandestructionoftherawvaluesoftheinput.Forexample,ifthemodellearnsahiddenunithithatdetectsafacebyﬁndingthenose,thendroppinghicorrespondstoerasingtheinformationthatthereisanoseintheimage.Themodelmustlearnanotherhi,eitherthatredundantlyencodesthepresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputarenotabletorandomlyerasetheinformationaboutanosefromanimageofafaceunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin267'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 282}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvaluesallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinputdistributionthatthemodelhasacquiredsofar.Anotherimportantaspectofdropoutisthatthenoiseismultiplicative.Ifthenoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunithiwithaddednoise\\ue00fcouldsimplylearntohavehibecomeverylargeinordertomaketheaddednoise\\ue00finsigniﬁcantbycomparison.Multiplicativenoisedoesnotallowsuchapathologicalsolutiontothenoiserobustnessproblem.Anotherdeeplearningalgorithm,batchnormalization,reparametrizesthemodelinawaythatintroducesbothadditiveandmultiplicativenoiseonthehiddenunitsattrainingtime.Theprimarypurposeofbatchnormalizationistoimproveoptimization,butthenoisecanhavearegularizingeﬀect,andsometimesmakesdropoutunnecessary.BatchnormalizationisdescribedfurtherinSec..8.7.17.13AdversarialTrainingInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhenevaluatedonani.i.d.testset.Itisnaturalthere'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 282}, page_content='makesdropoutunnecessary.BatchnormalizationisdescribedfurtherinSec..8.7.17.13AdversarialTrainingInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhenevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthesemodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inordertoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecansearchforexamplesthatthemodelmisclassiﬁes.()foundthatSzegedyetal.2014bevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%errorrateonexamplesthatareintentionallyconstructedbyusinganoptimizationproceduretosearchforaninputx\\ue030nearadatapointxsuchthatthemodeloutputisverydiﬀerentatx\\ue030.Inmanycases,x\\ue030canbesosimilartoxthatahumanobservercannottellthediﬀerencebetweentheoriginalexampleandtheadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.SeeFig.foran7.8example.Adversarialexampleshavemanyimplications,forexample,incomputersecurity,thatarebeyondthescopeofthischapter.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 282}, page_content='However,theyareinterestinginthecontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.testsetviaadversarialtraining—trainingonadversariallyperturbedexamplesfromthetrainingset(,;Szegedyetal.2014bGoodfellow2014betal.,).Goodfellow2014betal.()showedthatoneoftheprimarycausesoftheseadversarial examplesis excessive linearity.Neural networks arebuilt out ofprimarilylinearbuildingblocks. Insomeexperimentstheoverallfunctiontheyimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy268'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 283}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING+.007×=xsign(∇xJ(θx,,y))x+\\ue00fsign(∇xJ(θx,,y))y=“panda”“nematode”“gibbon”w/57.7%conﬁdencew/8.2%conﬁdencew/99.3%conﬁdenceFigure7.8:'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 283}, page_content='AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet(,)onImageNet.ByaddinganimperceptiblysmallvectorwhoseSzegedyetal.2014aelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwithrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproducedwithpermissionfrom().Goodfellowetal.2014btooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidlyifithasnumerousinputs.Ifwechangeeachinputby\\ue00f,thenalinearfunctionwithweightswcanchangebyasmuchas\\ue00f||||w1,whichcanbeaverylargeamountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighlysensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstantintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitlyintroducingalocalconstancypriorintosupervisedneuralnets.Adversarialtraininghelpstoillustratethepowerofusingalargefunctionfamilyincombinationwithaggressiveregularization.Purelylinearmodels,likelogisticregression,arenotabletoresistadversarialexamplesbecausetheyareforcedtobelinear.Ne'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 283}, page_content='oillustratethepowerofusingalargefunctionfamilyincombinationwithaggressiveregularization.Purelylinearmodels,likelogisticregression,arenotabletoresistadversarialexamplesbecausetheyareforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrangefromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapturelineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervisedlearning.Atapointxthatisnotassociatedwithalabelinthedataset,themodelitselfassignssomelabelˆy.Themodel’slabelˆymaynotbethetruelabel,butifthemodelishighquality,thenˆyhasahighprobabilityofprovidingthetruelabel.Wecanseekanadversarialexamplex\\ue030thatcausestheclassiﬁertooutputalabely\\ue030withy\\ue030\\ue036=ˆy.Adversarialexamplesgeneratedusingnotthetruelabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarialexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthesamelabeltoxandx\\ue030.Thisencouragestheclassiﬁertolearnafunctionthatis269'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 284}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddatalies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylieondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojumpfromoneclassmanifoldtoanotherclassmanifold.7.14Tangent Distance, TangentProp,and'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 284}, page_content='ManifoldTangentClassiﬁerManymachinelearningalgorithmsaimtoovercomethecurseofdimensionalitybyassumingthatthedataliesnearalow-dimensionalmanifold,asdescribedinSec..5.11.3Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthetangentdistancealgorithm(,,).Itisanon-parametricSimardetal.19931998nearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclideandistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhichprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesandthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁershouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovementonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetweenpointsx1andx2thedistancebetweenthemanifoldsM1andM2towhichtheyrespectivelybelong.Althoughthatmaybecomputationallydiﬃcult(itwouldrequiresolvinganoptimizationproblem,toﬁndthenearestpairofpointsonM1andM2),acheapalternativethatmakessenselocallyistoapproximateMibyitstangentplaneatxiandmeasurethedis'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 284}, page_content='aybecomputationallydiﬃcult(itwouldrequiresolvinganoptimizationproblem,toﬁndthenearestpairofpointsonM1andM2),acheapalternativethatmakessenselocallyistoapproximateMibyitstangentplaneatxiandmeasurethedistancebetweenthetwotangents,orbetweenatangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensionallinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequiresonetospecifythetangentvectors.Inarelatedspirit,thetangentpropalgorithm(,)(Fig.)Simardetal.19927.9trainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutputf(x)oftheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsofvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthesameclassconcentrate.Localinvarianceisachievedbyrequiring∇xf(x)tobeorthogonaltotheknownmanifoldtangentvectorsv()iatx,orequivalentlythatthedirectionalderivativeoffatxinthedirectionsv()ibesmallbyaddingaregularizationpenalty:ΩΩ()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 284}, page_content='=f\\ue058i\\ue010(∇xf())x\\ue03ev()i\\ue0112.(7.67)270'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 285}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGThisregularizercanofcoursebyscaledbyanappropriatehyperparameter,and,formostneuralnetworks,wewouldneedtosumovermanyoutputsratherthantheloneoutputf(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeoftheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.Tangentprophasbeenusednotjustforsupervisedlearning(,)Simardetal.1992butalsointhecontextofreinforcementlearning(,).Thrun1995Tangentpropagationis closelyrelated todataset'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 285}, page_content='closelyrelated todataset augmentation.Inbothcases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetaskbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthenetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation,thenetworkisexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplyingmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagationdoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalyticallyregularizesthemodeltoresistperturbationinthedirectionscorrespondingtothe speciﬁed transformation.While thisanalytical approach'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 285}, page_content='isintellectuallyelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresistinﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistancetolargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodelsbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivativesbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheirderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanhunitscan.Datasetaugmentationworkswellwithrectiﬁedlinearunitsbecausediﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsofeachoriginalinput.Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,1992)andadversarialtraining(,;,).Szegedyetal.2014bGoodfellowetal.2014bDoublebackpropregularizestheJacobiantobesmall,whileadversarialtrainingﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesameoutputontheseasontheoriginalinputs.Tangentpropagationanddatasetaugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthemodelshouldb'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 285}, page_content='nputsneartheoriginalinputsandtrainsthemodeltoproducethesameoutputontheseasontheoriginalinputs.Tangentpropagationanddatasetaugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthemodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbeinvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Justallasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,adversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.Themanifoldtangentclassiﬁer(,),eliminatestheneedtoRifaietal.2011cknowthetangentvectorsapriori.AswewillseeinChapter,autoencoderscan14271'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 286}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nx1x2NormalTangent'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 286}, page_content='Figure7.9: Illustrationofthemainideaofthetangentpropalgorithm(,Simardetal.1992Rifai2011c)andmanifoldtangentclassiﬁer(etal.,),whichbothregularizetheclassiﬁeroutputfunctionf(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttotheclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltotheclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemanytangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctiontochangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeasitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangentclassiﬁerregularizef(x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 286}, page_content='tonotchangeverymuchasxmovesalongthemanifold.Tangentpropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangentdirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclassmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirectionsbytraininganautoencodertoﬁtthetrainingdata.TheuseofautoencoderstoestimatemanifoldswillbedescribedinChapter.14estimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuseofthistechniquetoavoidneedinguser-speciﬁedtangentvectors.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 286}, page_content='AsillustratedinFig.,theseestimatedtangentvectorsgobeyondtheclassicalinvariants14.10thatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)andincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchasmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁeristhereforesimple:(1)useanautoencodertolearnthemanifoldstructurebyunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁerasintangentprop(Eq.).7.67Thischapterhasdescribedmostofthegeneralstrategiesusedtoregularizeneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch272'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 287}, page_content='CHAPTER7.REGULARIZATIONFORDEEPLEARNINGwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentralthemeofmachinelearningisoptimization,describednext.\\n273'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 288}, page_content='Chapter8OptimizationforTrainingDeepModelsDeeplearningalgorithmsinvolveoptimizationinmanycontexts.Forexample,performinginferenceinmodelssuchasPCAinvolvessolvinganoptimizationproblem.Weoftenuseanalyticaloptimizationtowriteproofsordesignalgorithms.Ofallofthemanyoptimizationproblemsinvolvedindeeplearning,themostdiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsoftimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneuralnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,aspecializedsetofoptimizationtechniqueshavebeendevelopedforsolvingit.Thischapterpresentstheseoptimizationtechniquesforneuralnetworktraining.Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,wesuggestreviewingChapter.Thatchapterincludesabriefoverviewofnumerical4optimizationingeneral.Thischapterfocusesononeparticularcaseofoptimization:ﬁndingtheparam-etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunctionJ(θ),whichtypicallyincludesaperformancemeasureevaluatedonthe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 288}, page_content='zationingeneral.Thischapterfocusesononeparticularcaseofoptimization:ﬁndingtheparam-etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunctionJ(θ),whichtypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetaswellasadditionalregularizationterms.Webeginwithadescriptionofhowoptimizationusedasatrainingalgorithmforamachinelearningtaskdiﬀersfrompureoptimization.Next,wepresentseveraloftheconcretechallengesthatmakeoptimizationofneuralnetworksdiﬃcult.Wethendeﬁneseveralpracticalalgorithms,includingbothoptimizationalgorithmsthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithmsadapttheirlearningratesduringtrainingorleverageinformationcontainedin274'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 289}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthesecondderivativesofthecostfunction.Finally,weconcludewithareviewofseveraloptimizationstrategiesthatareformedbycombiningsimpleoptimizationalgorithmsintohigher-levelprocedures.8.1HowLearningDiﬀersfromPureOptimizationOptimizationalgorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditionaloptimizationalgorithmsinseveralways.Machinelearningusuallyactsindirectly.Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasureP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable. WethereforeoptimizePonlyindirectly.WereduceadiﬀerentcostfunctionJ(θ)inthehopethatdoingsowillimproveP.Thisisincontrasttopureoptimization,whereminimizingJisagoalinandofitself.Optimizationalgorithmsfortrainingdeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureofmachinelearningobjectivefunctions.Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,suchasJ() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 289}, page_content='= θE()ˆx,y∼pdataLf,y,((;)xθ)(8.1)whereListheper-examplelossfunction,f(x;θ)isthepredictedoutputwhentheinputisx,ˆpdataistheempiricaldistribution.Inthesupervisedlearningcase,yisthetargetoutput.Throughoutthischapter,wedeveloptheunregularizedsupervisedcase,wheretheargumentstoLaref(x;θ)andy.However,itistrivialtoextendthisdevelopment,forexample,toincludeθorxasarguments,ortoexcludeyasarguments,inordertodevelopvariousformsofregularizationorunsupervisedlearning.Eq.deﬁnesanobjectivefunctionwithrespecttothetrainingset.We8.1wouldusuallyprefertominimizethecorrespondingobjectivefunctionwheretheexpectationistakenacrossthedatageneratingdistributionpdataratherthanjustovertheﬁnitetrainingset:J∗() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 289}, page_content='= θE()x,y∼pdataLf,y.((;)xθ)(8.2)8.1.1EmpiricalRiskMinimizationThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralizationerrorgivenbyEq..Thisquantityisknownasthe.Weemphasizeherethat8.2risktheexpectationistakenoverthetrueunderlyingdistributionpdata.Ifweknewthetruedistributionpdata(x,y),riskminimizationwouldbeanoptimizationtask275'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 290}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSsolvablebyanoptimizationalgorithm.However,whenwedonotknowpdata(x,y)butonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.Thesimplestwaytoconvertamachinelearningproblembackintoanop-timizationproblemistominimizetheexpectedlossonthetrainingset.Thismeansreplacingthetruedistributionp(x,y) withtheempiricaldistributionˆp(x,y)deﬁnedbythetrainingset.WenowminimizetheempiricalriskEx,y∼ˆpdata()x,y[((;))]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 290}, page_content='=Lfxθ,y1mm\\ue058i=1Lf((x()i;)θ,y()i)(8.3)whereisthenumberoftrainingexamples.mThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknownasempiricalriskminimization.Inthissetting,machinelearningisstillverysimilartostraightforwardoptimization.Ratherthanoptimizingtheriskdirectly,weoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyaswell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetrueriskcanbeexpectedtodecreasebyvariousamounts.However,empiricalriskminimizationispronetooverﬁtting.Modelswithhighcapacitycansimplymemorizethetrainingset.Inmanycases,empiricalriskminimizationisnotreallyfeasible.Themosteﬀectivemodernoptimizationalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,suchas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁnedeverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,werarelyuseempiricalriskminimization.Instead,wemustuseaslightlydiﬀerentapproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerentfrom'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 290}, page_content='here).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,werarelyuseempiricalriskminimization.Instead,wemustuseaslightlydiﬀerentapproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerentfromthequantitythatwetrulywanttooptimize.8.1.2SurrogateLossFunctionsandEarlyStoppingSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnotonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1lossistypicallyintractable(exponentialintheinputdimension),evenforalinearclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizesasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasasurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimatetheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcandothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorinexpectation.276'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 291}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearnmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalongtimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthelog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,onecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapartfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextractingmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimplyminimizingtheaverage0-1lossonthetrainingset.Averyimportantdiﬀerencebetweenoptimizationingeneralandoptimizationasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhaltatalocalminimum.Instead,amachinelearningalgorithmusuallyminimizesasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearlystopping(Sec.)issatisﬁed.Typicallytheearlystoppingcriterionisbasedon7.8thetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,andisdesignedtocausethealgorith'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 291}, page_content='nvergencecriterionbasedonearlystopping(Sec.)issatisﬁed.Typicallytheearlystoppingcriterionisbasedon7.8thetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,andisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,whichisverydiﬀerentfromthepureoptimizationsetting,whereanoptimizationalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.8.1.3BatchandMinibatchAlgorithmsOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneraloptimizationalgorithmsisthattheobjectivefunctionusuallydecomposesasasumoverthetrainingexamples.Optimizationalgorithmsformachinelearningtypicallycomputeeachupdatetotheparametersbasedonanexpectedvalueofthecostfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.Forexample,maximumlikelihoodestimationproblems,whenviewedinlogspace,decomposeintoasumovereachexample:θML='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 291}, page_content='argmaxθm\\ue058i=1logpmodel(x()i,y()i;)θ.(8.4)Maximizingthissumisequivalenttomaximizingtheexpectationovertheempiricaldistributiondeﬁnedbythetrainingset:J() = θEx,y∼ˆpdatalogpmodel(;)x,yθ.(8.5)MostofthepropertiesoftheobjectivefunctionJusedbymostofouropti-mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the277'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 292}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmostcommonlyusedpropertyisthegradient:∇θJ() = θEx,y∼ˆpdata∇θlogpmodel(;)x,yθ.(8.6)Computing thisexpectation exactly isvery expensive because it'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 292}, page_content='requiresevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecancomputetheseexpectationsbyrandomlysamplingasmallnumberofexamplesfromthedataset,thentakingtheaverageoveronlythoseexamples.Recallthatthestandarderrorofthemean(Eq.)estimatedfrom5.46nsamplesisgivenbyσ/√n,whereσisthetruestandarddeviationofthevalueofthesamples.Thedenominatorof√nshowsthattherearelessthanlinearreturnstousingmoreexamplestoestimatethegradient.Comparetwohypotheticalestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000examples.Thelatterrequires100timesmorecomputationthantheformer,butreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimizationalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsofnumberofupdates)iftheyareallowedtorapidlycomputeapproximateestimatesofthegradientratherthanslowlycomputingtheexactgradient.Anotherconsiderationmotivatingstatisticalestimationofthegradientfromasmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,allmsamplesinthetrainingsetc'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 292}, page_content='herthanslowlycomputingtheexactgradient.Anotherconsiderationmotivatingstatisticalestimationofthegradientfromasmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,allmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-basedestimateofthegradientcouldcomputethecorrectgradientwithasinglesample,usingmtimeslesscomputationthanthenaiveapproach.Inpractice,weareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlargenumbersofexamplesthatallmakeverysimilarcontributionstothegradient.Optimizationalgorithmsthatusetheentiretrainingsetarecalledbatchordeterministicgradientmethods,becausetheyprocessallofthetrainingexamplessimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusingbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedbyminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribeagroupofexamplesdoesnot.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 292}, page_content='Forexample,itisverycommontousetheterm“batchsize”todescribethesizeofaminibatch.Optimizationalgorithmsthatuseonlyasingleexampleatatimearesometimescalledstochasticonlineorsometimesmethods.Thetermonlineisusuallyreservedforthecasewheretheexamplesaredrawnfromastreamofcontinuallycreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveralpassesaremade.Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore278'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 293}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalledminibatchminibatchstochasticormethodsanditisnowcommontosimplycallthemstochasticmethods.Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,presentedindetailinSec..8.3.1Minibatchsizesaregenerallydrivenbythefollowingfactors:•Largerbatchesprovideamoreaccurateestimateofthegradient,butwithlessthanlinearreturns.•Multicorearchitecturesareusuallyunderutilizedbyextremelysmallbatches.Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthereisnoreductioninthetimetoprocessaminibatch.•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypicallythecase),thentheamountofmemoryscaleswiththebatchsize.Formanyhardwaresetupsthisisthelimitingfactorinbatchsize.•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀerbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16sometimesbeingattemptedforlarge'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 293}, page_content='eachievebetterruntimewithspeciﬁcsizesofarrays.EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀerbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16sometimesbeingattemptedforlargemodels.•Smallbatchescanoﬀeraregularizingeﬀect(,),WilsonandMartinez2003perhapsduetothenoisetheyaddtothelearningprocess.Generalizationerrorisoftenbestforabatchsizeof1.Trainingwithsuchasmallbatchsizemightrequireasmalllearningratetomaintainstabilityduetothehighvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhighduetotheneedtomakemoresteps,bothbecauseofthereducedlearningrateandbecauseittakesmorestepstoobservetheentiretrainingset.Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-batchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthanothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccuratelywithfewsamples,orbecausetheyuseinformationinwaysthatamplifysamplingerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgareusuallyrelativelyrobustand'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 293}, page_content='einformationthatisdiﬃculttoestimateaccuratelywithfewsamples,orbecausetheyuseinformationinwaysthatamplifysamplingerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgareusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-ordermethods,whichusealsotheHessianmatrixHandcomputeupdatessuchasH−1g,typicallyrequiremuchlargerbatchsizeslike10,000.TheselargebatchsizesarerequiredtominimizeﬂuctuationsintheestimatesofH−1g.SupposethatHisestimatedperfectlybuthasapoorconditionnumber.Multiplicationby279'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 294}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.VerysmallchangesintheestimateofgcanthuscauselargechangesintheupdateH−1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonlyapproximately,sotheupdateH−1gwillcontainevenmoreerrorthanwewouldpredictfromapplyingapoorlyconditionedoperationtotheestimateof.gItisalsocrucialthattheminibatchesbeselectedrandomly.Computinganunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthosesamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobeindependentfromeachother,sotwosubsequentminibatchesofexamplesshouldalsobeindependentfromeachother.Manydatasetsaremostnaturallyarrangedinawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemighthaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.Thislistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerenttimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthesecondpatient,thenthebloodsampl'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 294}, page_content='ithalonglistofbloodsampletestresults.Thislistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerenttimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthesecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifweweretodrawexamplesinorderfromthislist,theneachofourminibatcheswouldbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthemanypatientsinthedataset.Incasessuchasthesewheretheorderofthedatasetholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselectingminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsofexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformlyatrandomeverytimewewanttoconstructaminibatch.Fortunately,inpracticeitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitinshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutiveexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodelwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetrainingdata.However,thisd'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 294}, page_content='imposeaﬁxedsetofpossibleminibatchesofconsecutiveexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodelwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetrainingdata.However,thisdeviationfromtruerandomselectiondoesnotseemtohaveasigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycanseriouslyreducetheeﬀectivenessofthealgorithm.Manyoptimizationproblemsinmachinelearningdecomposeoverexampleswellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamplesinparallel.Inotherwords,wecancomputetheupdatethatminimizesJ(X)foroneminibatchofexamplesXatthesametimethatwecomputetheupdateforseveralotherminibatches.SuchasynchronousparalleldistributedapproachesarediscussedfurtherinSec..12.1.3Aninterestingmotivationforminibatchstochasticgradientdescentisthatitfollowsthegradientofthetruegeneralizationerror(Eq.)solongasno8.2examplesarerepeated.Mostimplementationsofminibatchstochasticgradient280'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 295}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Ontheﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetruegeneralizationerror.Onthesecondpass,theestimatebecomesbiasedbecauseitisformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtainingnewfairsamplesfromthedatageneratingdistribution.Thefactthatstochasticgradientdescentminimizesgeneralizationerroriseasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawnfromastreamofdata. Inotherwords,insteadofreceivingaﬁxed-sizetrainingset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,witheveryexample(x,y)comingfromthedatageneratingdistributionpdata(x,y).Inthisscenario,examplesareneverrepeated;everyexperienceisafairsamplefrompdata.Theequivalenceiseasiesttoderivewhenbothxandyarediscrete. Inthiscase,thegeneralizationerror(Eq.)canbewrittenasasum8.2J∗() =θ\\ue058x\\ue058ypdata()((;))x,yLfxθ,y,(8.7)withtheexactgradientg= ∇θJ∗()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 295}, page_content='Inthiscase,thegeneralizationerror(Eq.)canbewrittenasasum8.2J∗() =θ\\ue058x\\ue058ypdata()((;))x,yLfxθ,y,(8.7)withtheexactgradientg= ∇θJ∗() =θ\\ue058x\\ue058ypdata()x,y∇θLf,y.((;)xθ)(8.8)Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinEq.8.5andEq.;weobservenowthatthisholdsforotherfunctions8.6Lbesidesthelikelihood.Asimilarresultcanbederivedwhenxandyarecontinuous,undermildassumptionsregardingpdataand.LHence, wecanobtainanunbiasedestimatoroftheexactgradientof thegeneralizationerrorbysamplingaminibatchofexamples{x(1),...x()m}withcor-respondingtargetsy()ifromthedatageneratingdistributionpdata,andcomputingthegradientofthelosswithrespecttotheparametersforthatminibatch:ˆg=1m∇θ\\ue058iLf((x()i;)θ,y()i).(8.9)UpdatinginthedirectionofθˆgperformsSGDonthegeneralizationerror.Ofcourse, thisinterpretationonly applieswhenexamplesarenotreused.Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,unlessthetrainingsetisextremelylarge.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 295}, page_content='thisinterpretationonly applieswhenexamplesarenotreused.Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,unlessthetrainingsetisextremelylarge. Whenmultiplesuchepochsareused,onlytheﬁrstepochfollowstheunbiasedgradientofthegeneralizationerror,but281'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 296}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreasedtrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentrainingerrorandtesterror.Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,itisbecomingmorecommonformachinelearningapplicationstouseeachtrainingexampleonlyonceoreventomakeanincompletepassthroughthetrainingset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,sounderﬁttingandcomputationaleﬃciencybecomethepredominantconcerns.Seealso()foradiscussionoftheeﬀectofcomputationalBottouandBousquet2008bottlenecksongeneralizationerror,asthenumberoftrainingexamplesgrows.8.2ChallengesinNeuralNetworkOptimizationOptimizationingeneralisanextremelydiﬃculttask.Traditionally,machinelearninghasavoidedthediﬃcultyofgeneraloptimizationbycarefullydesigningtheobjectivefunctionandconstraintstoensurethattheoptimizationproblemisconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convexcase.Evenconvexoptimizationis'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 296}, page_content='loptimizationbycarefullydesigningtheobjectivefunctionandconstraintstoensurethattheoptimizationproblemisconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convexcase.Evenconvexoptimizationisnotwithoutitscomplications.Inthissection,wesummarizeseveralofthemostprominentchallengesinvolvedinoptimizationfortrainingdeepmodels.8.2.1Ill-ConditioningSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themostprominentisill-conditioningoftheHessianmatrixH.Thisisaverygeneralprobleminmostnumericaloptimization,convexorotherwise,andisdescribedinmoredetailinSec..4.3.1Theill-conditioningproblemisgenerallybelievedtobepresentinneuralnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.RecallfromEq.thatasecond-orderTaylorseriesexpansionofthecost4.9functionpredictsthatagradientdescentstepofwilladd−\\ue00fg12\\ue00f2g\\ue03eHgg−\\ue00f\\ue03eg(8.10)tothecost.Ill-conditioningofthegradientbecomesaproblemwhen12\\ue00f2g\\ue03eHgexceeds\\ue00fg\\ue03eg.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 296}, page_content='Todeterminewhetherill-conditioningisdetrimentaltoaneuralnetworktrainingtask,onecanmonitorthesquaredgradientnormg\\ue03egandthe282'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 297}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n−50050100150200250Trainingtime(epochs)−20246810121416Gradient norm050100150200250Trainingtime(epochs)01.02.03.04.05.06.07.08.09.10.Classiﬁcationerrorrate'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 297}, page_content='Figure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthisexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkusedforobjectdetection.(Left)Ascatterplotshowinghowthenormsofindividualgradientevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnormisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolidcurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewouldexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing(Right)gradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcationerrordecreasestoalowlevel.g\\ue03eHgterm.Inmanycases,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 297}, page_content='thegradientnormdoesnotshrinksigniﬁcantlythroughoutlearning,buttheg\\ue03eHgtermgrowsbymorethanorderofmagnitude.Theresultisthatlearningbecomesveryslowdespitethepresenceofastronggradientbecausethelearningratemustbeshrunktocompensateforevenstrongercurvature.Fig.showsanexampleofthegradientincreasingsigniﬁcantlyduring8.1thesuccessfultrainingofaneuralnetwork.Thoughill-conditioningispresentinothersettingsbesidesneuralnetworktraining,someofthetechniquesusedtocombatitinothercontextsarelessapplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttoolforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butinthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcantmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.8.2.2LocalMinimaOneofthemostprominentfeaturesofaconvexoptimizationproblemisthatitcanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis283'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 298}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionatthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuchaﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,weknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.Withnon-convexfunctions,suchasneuralnets,itispossibletohavemanylocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohaveanextremelylargenumberoflocalminima.However,aswewillsee,thisisnotnecessarilyamajorproblem.Neuralnetworksandanymodelswithmultipleequivalentlyparametrizedlatentvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁabilityproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcanruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariablesareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanginglatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkandmodifylayer1byswappingtheincomingweightvectorforunitiwiththeincomingwei'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 298}, page_content='nnotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanginglatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkandmodifylayer1byswappingtheincomingweightvectorforunitiwiththeincomingweightvectorforunitj,thendoingthesamefortheoutgoingweightvectors.Ifwehavemlayerswithnunitseach,thentherearen!mwaysofarrangingthehiddenunits.Thiskindofnon-identiﬁabilityisknownasweightspacesymmetry.Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshaveadditionalcausesofnon-identiﬁability.Forexample,inanyrectiﬁedlinearormaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitbyαifwealsoscaleallofitsoutgoingweightsby1α.Thismeansthat—ifthecostfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyontheweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinearormaxoutnetworkliesonan(mn×)-dimensionalhyperbolaofequivalentlocalminima.Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylargeorevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcostfunction.H'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 298}, page_content='tworkliesonan(mn×)-dimensionalhyperbolaofequivalentlocalminima.Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylargeorevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcostfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilityareequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaarenotaproblematicformofnon-convexity.Localminimacanbeproblematiciftheyhavehighcostincomparisontotheglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthiddenunits,thathavelocalminimawithhighercostthantheglobalminimum(SontagandSussman1989Brady1989GoriandTesi1992,;etal.,;,).Iflocalminimawithhighcostarecommon,thiscouldposeaseriousproblemforgradient-basedoptimizationalgorithms.Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost284'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 299}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSfornetworksofpracticalinterestandwhetheroptimizationalgorithmsencounterthem.Formanyyears,mostpractitionersbelievedthatlocalminimawereacommonproblemplaguingneuralnetworkoptimization.Today,thatdoesnotappeartobethecase.Theproblemremainsanactiveareaofresearch,butexpertsnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavealowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimumratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost(,;,;,;Saxeetal.2013Dauphinetal.2014Goodfellowetal.2015Choromanskaetal.,).2014Manypractitionersattributenearlyalldiﬃcultywithneuralnetworkoptimiza-tiontolocalminima.Weencouragepractitionerstocarefullytestforspeciﬁcproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthenormofthegradientovertime.Ifthenormofthegradientdoesnotshrinktoinsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcriticalpoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensionalspaces,itcanbev'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 299}, page_content='overtime.Ifthenormofthegradientdoesnotshrinktoinsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcriticalpoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensionalspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaaretheproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.8.2.3Plateaus,SaddlePointsandOtherFlatRegionsFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddlepoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,whileothershavealowercost.Atasaddlepoint,theHessianmatrixhasbothpositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwithpositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslyingalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointasbeingalocalminimumalongonecross-sectionofthecostfunctionandalocalmaximumalonganothercross-section.SeeFig.foranillustration.4.5Manyclasses'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 299}, page_content='ofrandomfunctionsexhibitthefollowingbehavior:inlow-dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,localminimaarerareandsaddlepointsaremorecommon.Forafunctionf:Rn→Rofthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrowsexponentiallywithn.Tounderstandtheintuitionbehindthisbehavior,observethattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.TheHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingledimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheadsonce.Inn-dimensionalspace,itisexponentiallyunlikelythatallncointosseswill285'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSbeheads.See()forareviewoftherelevanttheoreticalwork.Dauphinetal.2014AnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesoftheHessianbecomemorelikelytobepositiveaswereachregionsoflowercost. Inourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeupheadsntimesifweareatacriticalpointwithlowcost.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='Thismeansthatlocalminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswithhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremelyhighcostaremorelikelytobelocalmaxima.Thishappensformanyclassesofrandomfunctions.Doesithappenforneuralnetworks?()showedtheoreticallythatshallowautoencodersBaldiandHornik1989(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedinChapter)withnononlinearitieshaveglobalminimaandsaddlepointsbutno14localminimawithhighercostthantheglobalminimum.Theyobservedwithoutproofthattheseresultsextendtodeepernetworkswithoutnonlinearities.Theoutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareusefultostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionisanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjustmultiplematricescomposedtogether.()providedexactsolutionsSaxeetal.2013tothecompletelearningdynamicsinsuchnetworksandshowedthatlearninginthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingofde'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='matricescomposedtogether.()providedexactsolutionsSaxeetal.2013tothecompletelearningdynamicsinsuchnetworksandshowedthatlearninginthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingofdeepmodelswithnonlinearactivationfunctions.()showedDauphinetal.2014experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainverymanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditionaltheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandomfunctionsrelatedtoneuralnetworksdoessoaswell.Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-rithms?Forﬁrst-orderoptimizationalgorithmsthatuseonlygradientinformation,thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddlepoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescapesaddlepointsinmanycases.()providedvisualizationsofGoodfellowetal.2015severallearningtrajectoriesofstate-of-the-artneuralnetworks,withanexamplegiveninFig..Thesevisualizationsshowaﬂatteningofthecostfunctionne'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='pointsinmanycases.()providedvisualizationsofGoodfellowetal.2015severallearningtrajectoriesofstate-of-the-artneuralnetworks,withanexamplegiveninFig..Thesevisualizationsshowaﬂatteningofthecostfunctionnear8.2aprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthegradientdescenttrajectoryrapidlyescapingthisregion.()Goodfellowetal.2015alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytoberepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituationmaybediﬀerentformorerealisticusesofgradientdescent.ForNewton’smethod,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 300}, page_content='itisclearthatsaddlepointsconstituteaproblem.286'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 301}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 301}, page_content='Projection2ofθProjection1ofθJ()θFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.ImageadaptedwithpermissionfromGoodfellow2015etal.(). Thesevisualizationsappearsimilarforfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksappliedtorealobjectrecognitionandnaturallanguageprocessingtasks.Surprisingly,thesevisualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessofstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convexstructurethanisrevealedbytheseprojections.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 301}, page_content='Theprimaryobstaclerevealedbythisprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,asindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrixinthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleintheﬁgureviaanindirectarcingpath.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 301}, page_content='287'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 302}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesignedtoseekacriticalpoint.Newton’smethod,however,isdesignedtosolveforapointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjumptoasaddlepoint.Theproliferationofsaddlepointsinhighdimensionalspacespresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacinggradientdescentforneuralnetworktraining.()introducedDauphinetal.2014asaddle-freeNewtonmethodforsecond-orderoptimizationandshowedthatitimprovessigniﬁcantlyoverthetraditionalversion.Second-ordermethodsremaindiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholdspromiseifitcouldbescaled.Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddlepoints.Therearealsomaxima, whicharemuchlikesaddlepointsfromtheperspectiveofoptimization—manyalgorithmsarenotattractedtothem, butunmodiﬁedNewton’smethod'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 302}, page_content='whicharemuchlikesaddlepointsfromtheperspectiveofoptimization—manyalgorithmsarenotattractedtothem, butunmodiﬁedNewton’smethod is.Maximabecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,thegradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajorproblemsforallnumericaloptimizationalgorithms.Inaconvexproblem,awide,ﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimizationproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.8.2.4CliﬀsandExplodingGradientsNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresemblingcliﬀs,asillustratedinFig..Theseresultfromthemultiplicationofseverallarge8.3weightstogether.Onthefaceofanextremelysteepcliﬀstructure,thegradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀofthecliﬀstructurealtogether.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 302}, page_content='288'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 303}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 303}, page_content='\\ue077\\ue062\\ue04a\\ue077\\ue03b\\ue062\\ue028\\ue029Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorforrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresultingfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetoveryhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,agradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostoftheoptimizationworkthathadbeendone. FigureadaptedwithpermissionfromPascanuetal.().2013aThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradientclippingheuristicdescribedinSec..'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 303}, page_content='Thebasicideaistorecallthatthe10.11.1gradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirectionwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithmproposestomakeaverylargestep,thegradientclippingheuristicintervenestoreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregionwherethegradientindicatesthedirectionofapproximatelysteepestdescent.Cliﬀstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,becausesuchmodelsinvolveamultiplicationofmanyfactors,withonefactorforeachtimestep.Longtemporalsequencesthusincuranextremeamountofmultiplication.8.2.5Long-TermDependenciesAnotherdiﬃcultythatneuralnetworkoptimizationalgorithmsmustovercomeariseswhenthecomputationalgraphbecomesextremelydeep. Feedforwardnetworkswithmanylayershavesuchdeepcomputationalgraphs.Sodorecurrentnetworks,describedinChapter, whichconstructverydeepcomputationalgraphsby10289'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 304}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSrepeatedlyapplyingthesameoperationateachtimestepofalongtemporalsequence.Repeatedapplicationofthesameparametersgivesrisetoespeciallypronounceddiﬃculties.Forexample,supposethatacomputationalgraphcontainsapaththatconsistsofrepeatedlymultiplyingbyamatrixW.Aftertsteps,thisisequivalenttomul-tiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V−1.Inthissimplecase,itisstraightforwardtoseethatWt=\\ue000VλVdiag()−1\\ue001t='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 304}, page_content='()VdiagλtV−1.(8.11)Anyeigenvaluesλithatarenotnearanabsolutevalueofwilleitherexplodeif1theyaregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.11Thevanishingandexplodinggradientproblemreferstothefactthatgradientsthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradientsmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprovethecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀstructuresdescribedearlierthatmotivategradientclippingareanexampleoftheexplodinggradientphenomenon.TherepeatedmultiplicationbyWateachtimestepdescribedhereisverysimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueofamatrixWandthecorrespondingeigenvector.Fromthispointofviewitisnotsurprisingthatx\\ue03eWtwilleventuallydiscardallcomponentsofxthatareorthogonaltotheprincipaleigenvectorof.WRecurrentnetworksusethesamematrixWateachtimestep,butfeedforwardnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthevanishingandexplodinggradientproblem(,).Sussillo2014Wedefe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 304}, page_content='igenvectorof.WRecurrentnetworksusethesamematrixWateachtimestep,butfeedforwardnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthevanishingandexplodinggradientproblem(,).Sussillo2014WedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworksuntilSec.,afterrecurrentnetworkshavebeendescribedinmoredetail.10.78.2.6InexactGradientsMostoptimizationalgorithmsareprimarilymotivatedbythecasewherewehaveexactknowledgeofthegradientorHessianmatrix.Inpractice,weusuallyonlyhaveanoisyorevenbiasedestimateofthesequantities.Nearlyeverydeeplearningalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatchoftrainingexamplestocomputethegradient.Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableaswell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise290'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 305}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSwiththemoreadvancedmodelsinPart.Forexample,contrastivedivergenceIIIgivesatechniqueforapproximatingthegradientoftheintractablelog-likelihoodofaBoltzmannmachine.Variousneuralnetworkoptimizationalgorithmsaredesignedtoaccountforimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosingasurrogatelossfunctionthatiseasiertoapproximatethanthetrueloss.8.2.7PoorCorrespondencebetweenLocalandGlobalStructureManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthelossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepifJ(θ)ispoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddlepointhidingtheopportunitytomakeprogressdownhillfromthegradient.Itispossibletoovercomealloftheseproblemsatasinglepointandstillperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoesnotpointtowarddistantregionsofmuchlowercost.Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisduetothelengthofthetrajectoryneededtoarriveatthesolutio'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 305}, page_content='nthatresultsinthemostimprovementlocallydoesnotpointtowarddistantregionsofmuchlowercost.Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisduetothelengthofthetrajectoryneededtoarriveatthesolution.Fig.showsthat8.2thelearningtrajectoryspendsmostofitstimetracingoutawidearcaroundamountain-shapedstructure.Muchofresearchintothediﬃcultiesofoptimizationhasfocusedonwhethertrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butinpracticeneuralnetworksdonotarriveatacriticalpointofanykind.Fig.8.1showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction−logp(y|x;θ)canlackaglobalminimumpointandinsteadasymptoticallyapproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwithdiscreteyandp(y|x)providedbyasoftmax,thenegativelog-likelihoodcanbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyeveryexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueofzero.Likewise,amodelofrealvalu'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 305}, page_content='softmax,thenegativelog-likelihoodcanbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyeveryexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueofzero.Likewise,amodelofrealvaluesp(y|x)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 305}, page_content='=N(y;f(θ),β−1)canhavenegativelog-likelihoodthatasymptotestonegativeinﬁnity—iff(θ)isabletocorrectlypredictthevalueofalltrainingsetytargets,thelearningalgorithmwillincreaseβwithoutbound.SeeFig.foranexampleofafailureoflocaloptimizationto8.4ﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddlepoints.Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthatinﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome291'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 306}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\nθJ()θ'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 306}, page_content='Figure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoesnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,eveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunctioncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyinthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingabletotraverseit.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 306}, page_content='Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigatesuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultinexcessivetrainingtime,asillustratedinFig..8.2oftheprocess.Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsforproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithmsthatusenon-localmoves.Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefortrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevioussectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmovescanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesoftheobjectivefunction,suchasitsgradient,onlyapproximately,withbiasorvarianceinourestimateofthecorrectdirection.Inthesecases,localdescentmayormaynotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactuallyabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissuessuchaspoorconditioningordiscontinuousgradients,causingtheregionwherethegradientprovidesagoodmodeloftheobjectivefunct'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 306}, page_content='butwearenotactuallyabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissuessuchaspoorconditioningordiscontinuousgradients,causingtheregionwherethegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.Inthesecases,localdescentwithstepsofsize\\ue00fmaydeﬁneareasonablyshortpathtothesolution,butweareonlyabletocomputethelocaldescentdirectionwithstepsofsizeδ\\ue00f\\ue01c.Inthesecases,localdescentmayormaynotdeﬁneapathtothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa292'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 307}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELShighcomputationalcost.Sometimeslocalinformationprovidesusnoguide,whenthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacriticalpoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitlyforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoesnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedyandleadusalongapaththatmovesdownhillbutawayfromanysolution,asinFig.,oralonganunnecessarilylongtrajectorytothesolution,asinFig..8.48.2Currently,wedonotunderstandwhichoftheseproblemsaremostrelevanttomakingneuralnetworkoptimizationdiﬃcult,andthisisanactiveareaofresearch.Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbeavoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolutionbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearningwithinthatwell-behavedregion.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 307}, page_content='Thislastviewsuggestsresearchintochoosinggoodinitialpointsfortraditionaloptimizationalgorithmstouse.8.2.8TheoreticalLimitsofOptimizationSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofanyoptimizationalgorithmwemightdesignforneuralnetworks(,BlumandRivest1992Judd1989WolpertandMacReady1997;,;,).Typicallytheseresultshavelittlebearingontheuseofneuralnetworksinpractice.Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneuralnetworkoutput discretevalues.However, most'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 307}, page_content='discretevalues.However, most neuralnetworkunitsoutputsmoothlyincreasingvaluesthatmakeoptimizationvialocalsearchfeasible.Sometheoreticalresultsshowthatthereexistproblemclassesthatareintractable,butitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Otherresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,butinpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmanymoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthecontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexactminimumofafunction,butonlyinreducingitsvaluesuﬃcientlytoobtaingoodgeneralizationerror. Theoreticalanalysisofwhetheranoptimizationalgorithmcanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticboundsontheperformanceofoptimizationalgorithmsthereforeremainsanimportantgoalformachinelearningresearch.293'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 308}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.3BasicAlgorithmsWehavepreviouslyintroducedthegradientdescent(Sec.)algorithmthat4.3followsthegradientofanentiretrainingsetdownhill.Thismaybeacceleratedconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomlyselectedminibatchesdownhill,asdiscussedinSec.andSec..5.98.1.38.3.1StochasticGradientDescentStochasticgradientdescent(SGD)anditsvariantsareprobablythemostusedoptimizationalgorithmsformachinelearningingeneralandfordeeplearninginparticular.AsdiscussedinSec.,itispossibletoobtainanunbiasedestimate8.1.3ofthegradientbytakingtheaveragegradientonaminibatchofmexamplesdrawni.i.dfromthedatageneratingdistribution.Algorithmshowshowtofollowthisestimateofthegradientdownhill.8.1Algorithm8.1Stochasticgradientdescent(SGD)updateattrainingiterationkRequire:Learningrate\\ue00fk.Require:InitialparameterθwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:ˆg←+1m∇θ'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 308}, page_content='kRequire:Learningrate\\ue00fk.Require:InitialparameterθwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:ˆg←+1m∇θ\\ue050iLf((x()i;)θ,y()i)Applyupdate:θθ←−\\ue00fˆgendwhileAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,wehavedescribedSGDasusingaﬁxedlearningrate\\ue00f.Inpractice,itisnecessarytograduallydecreasethelearningrateovertime,sowenowdenotethelearningrateatiterationask\\ue00fk.ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(therandomsamplingofmtrainingexamples)thatdoesnotvanishevenwhenwearriveataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomessmallandthen0whenweapproachandreachaminimumusingbatchgradientdescent,sobatchgradientdescentcanuseaﬁxedlearningrate.AsuﬃcientconditiontoguaranteeconvergenceofSGDisthat∞\\ue058k=1\\ue00fk='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 308}, page_content='and∞,(8.12)294'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 309}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS∞\\ue058k=1\\ue00f2k<.∞(8.13)Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration:τ\\ue00fk='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 309}, page_content='(1)−α\\ue00f0+α\\ue00fτ(8.14)withα=kτ.Afteriteration,itiscommontoleaveconstant.τ\\ue00fThelearningratemaybechosenbytrialanderror,butitisusuallybesttochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasafunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthissubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,theparameterstochooseare\\ue00f0,\\ue00fτ,andτ.Usuallyτmaybesettothenumberofiterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\\ue00fτshouldbesettoroughlythevalueof1%\\ue00f0.Themainquestionishowtoset\\ue00f0.Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecostfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyiftrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromtheuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andiftheinitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandtheﬁnalcostvalue,ishigherthanthelearningratethat'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 309}, page_content='lowly,andiftheinitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandtheﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformanceaftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrstseveraliterationsandusealearningratethatishigherthanthebest-performinglearningrateatthistime,butnotsohighthatitcausessevereinstability.ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-basedoptimizationisthatcomputationtimeperupdatedoesnotgrowwiththenumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumberoftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmayconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithasprocessedtheentiretrainingset.TostudytheconvergencerateofanoptimizationalgorithmitiscommontomeasuretheexcesserrorJ(θ)−minθJ(θ),whichistheamountthatthecurrentcostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvexproblem,theexcesserroris'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 309}, page_content='ncerateofanoptimizationalgorithmitiscommontomeasuretheexcesserrorJ(θ)−minθJ(θ),whichistheamountthatthecurrentcostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvexproblem,theexcesserrorisO(1√k)afterkiterations,whileinthestronglyconvexcaseitisO(1k).Theseboundscannotbeimprovedunlessextraconditionsareassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochasticgradientdescentintheory.However,theCramér-Raobound(,;,Cramér1946Rao1945)statesthatgeneralizationerrorcannotdecreasefasterthanO(1k).Bottou295'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 310}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursueanoptimizationalgorithmthatconvergesfasterthanO(1k)formachinelearningtasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,theasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescenthasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomakerapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamplesoutweighsitsslowasymptoticconvergence.MostofthealgorithmsdescribedintheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelostintheconstantfactorsobscuredbytheO(1k)asymptoticanalysis.Onecanalsotradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygraduallyincreasingtheminibatchsizeduringthecourseoflearning.FormoreinformationonSGD,see().Bottou19988.3.2MomentumWhilestochasticgradientdescentremainsaverypopularoptimizationstrategy,learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)isdesignedtoacceleratelearning,especiallyi'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 310}, page_content='Bottou19988.3.2MomentumWhilestochasticgradientdescentremainsaverypopularoptimizationstrategy,learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)isdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbutconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulatesanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomoveintheirdirection.TheeﬀectofmomentumisillustratedinFig..8.5Formally,themomentumalgorithmintroducesavariablevthatplaystheroleofvelocity—itisthedirectionandspeedatwhichtheparametersmovethroughparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthenegativegradient.Thenamederivesfromaphysicalanalogy,inmomentumwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,accordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorvmayalsoberegardedasthemomentumoftheparticle.Ahyperparameterα∈[0,1)determineshowquicklythecontributionsofpr'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 310}, page_content='hysicsismasstimesvelocity.Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorvmayalsoberegardedasthemomentumoftheparticle.Ahyperparameterα∈[0,1)determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.Theupdateruleisgivenby:vv←α−∇\\ue00fθ\\ue0201mm\\ue058i=1L((fx()i;)θ,y()i)\\ue021,(8.15)θθv←+.(8.16)Thevelocityvaccumulatesthegradientelements∇θ\\ue0001m\\ue050mi=1L((fx()i;)θ,y()i)\\ue001.Thelargerαisrelativeto\\ue00f,themorepreviousgradientsaﬀectthecurrentdirection.TheSGDalgorithmwithmomentumisgiveninAlgorithm.8.2296'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 311}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n−−−30201001020−30−20−1001020\\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningoftheHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentumovercomestheﬁrstofthesetwoproblems.ThecontourlinesdepictaquadraticlossfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthecontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthisfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradientdescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjectivelookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraversesthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthenarrowaxisofthecanyon.ComparealsoFig.,whichshowsthebehaviorofgradient4.6descentwithoutmomentum.\\n297'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 312}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSPreviously,thesizeofthestepwassimplythenormofthegradientmultipliedbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhowalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessivegradientspointinexactlythesamedirection.Ifthemomentumalgorithmalwaysobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachingaterminalvelocitywherethesizeofeachstepis\\ue00f||||g1−α.(8.17)Itisthushelpfultothinkofthemomentumhyperparameterintermsof11−α.Forexample,α=.9correspondstomultiplyingthemaximumspeedbyrelativeto10thegradientdescentalgorithm.Commonvaluesofαusedinpracticeinclude.5,.9,and.99.Likethelearningrate,αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueandislaterraised.Itislessimportanttoadaptαovertimethantoshrink\\ue00fovertime.Algorithm8.2Stochasticgradientdescent(SGD)withmomentumRequire:Learningrate,momentumparameter.\\ue00fαRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainin'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 312}, page_content='8.2Stochasticgradientdescent(SGD)withmomentumRequire:Learningrate,momentumparameter.\\ue00fαRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Computevelocityupdate:vvg←α−\\ue00fApplyupdate:θθv←+endwhileWecanviewthemomentumalgorithmassimulatingaparticlesubjecttocontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuildintuitionforhowthemomentumandgradientdescentalgorithmsbehave.Thepositionoftheparticleatanypointintimeisgivenbyθ(t).Theparticleexperiencesnetforce.Thisforcecausestheparticletoaccelerate:f()tf()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 312}, page_content='=t∂2∂t2θ()t.(8.18)Ratherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,wecanintroducethevariablev(t)representingthevelocityoftheparticleattimetandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:v() =t∂∂tθ()t,(8.19)298'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSf()'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='=t∂∂tv()t.(8.20)Themomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvianumericalsimulation.AsimplenumericalmethodforsolvingdiﬀerentialequationsisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedbytheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyaretheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:−∇θJ(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneachgradient,buttheNewtonianscenariousedbythemomentumalgorithminsteadusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticleasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsasteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirectionuntilitbeginstogouphillagain.Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,thentheparticlemightnevercometorest.Imagineahockeypuckslidingdownonesideofava'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='esslidinginthatdirectionuntilitbeginstogouphillagain.Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,thentheparticlemightnevercometorest.Imagineahockeypuckslidingdownonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddoneotherforce,proportionalto−v(t).Inphysicsterminology,thisforcecorrespondstoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchassyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventuallyconvergetoalocalminimum.Whydoweuse−v(t)andviscousdraginparticular?'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='Partofthereasontouse−v(t)ismathematicalconvenience—anintegerpowerofthevelocityiseasytoworkwith.However,otherphysicalsystemshaveotherkindsofdragbasedonotherintegerpowersofthevelocity.Forexample,aparticletravelingthroughtheairexperiencesturbulentdrag,withforceproportionaltothesquareofthevelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,withaforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityissmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticlewithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdragwillmoveawayfromitsinitialpositionforever,withthedistancefromthestartingpointgrowinglikeO(logt).Wemustthereforeusealowerpowerofthevelocity.Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,theconstantforceduetofrictioncancausetheparticletocometorestbeforereachingalocalminimum.Viscousd'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 313}, page_content='tingdryfriction,thentheforceistoostrong.Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,theconstantforceduetofrictioncancausetheparticletocometorestbeforereachingalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough299'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 314}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthatthegradientcancontinuetocausemotionuntilaminimumisreached,butstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.8.3.3NesterovMomentumSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwasinspiredbyNesterov’sacceleratedgradientmethod(,,).TheNesterov19832004updaterulesinthiscasearegivenby:vv←α−∇\\ue00fθ\\ue0221mm\\ue058i=1L\\ue010fx(()i;+)θαv,y()i\\ue011\\ue023,(8.21)θθv←+,(8.22)wheretheparametersαand\\ue00fplayasimilarroleasinthestandardmomentummethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumiswherethegradientisevaluated.WithNesterovmomentumthegradientisevaluatedafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentumasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.ThecompleteNesterovmomentumalgorithmispresentedinAlgorithm.8.3Intheconvexbatchgradientcase,NesterovmomentumbringstherateofconvergenceoftheexcesserrorfromO(1/k)(afterksteps)toO(1/k2)asshownbyNesterov1983().Unfortunately, inthestochasticgradientcase,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 314}, page_content='inthestochasticgradientcase, Nesterovmomentumdoesnotimprovetherateofconvergence.Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentumRequire:Learningrate,momentumparameter.\\ue00fαRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondinglabelsy()i.Applyinterimupdate:˜θθv←+αComputegradient(atinterimpoint):g←1m∇˜θ\\ue050iLf((x()i;˜θy),()i)Computevelocityupdate:vvg←α−\\ue00fApplyupdate:θθv←+endwhile300'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 315}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.4ParameterInitializationStrategiesSomeoptimizationalgorithmsarenotiterativebynatureandsimplysolveforasolutionpoint.Otheroptimizationalgorithmsareiterativebynaturebut,whenappliedtotherightclassofoptimizationproblems,convergetoacceptablesolutionsinanacceptableamountoftimeregardlessofinitialization.Deeplearningtrainingalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeeplearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecifysomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeepmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedbythechoiceofinitialization.Theinitialpointcandeterminewhetherthealgorithmconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithmencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,theinitialpointcandeterminehowquicklylearningconvergesandwhetheritconvergestoapointwithhigh orlowcost.Also,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 315}, page_content='orlowcost.Also, pointsofcomparablecostcanhavewildlyvaryinggeneralizationerror,andtheinitialpointcanaﬀectthegeneralizationaswell.Moderninitializationstrategiesaresimpleandheuristic.Designingimprovedinitializationstrategiesisadiﬃculttaskbecauseneuralnetworkoptimizationisnotyetwellunderstood.Mostinitializationstrategiesarebasedonachievingsomenicepropertieswhenthenetworkisinitialized.However,wedonothaveagoodunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstancesafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpointsmaybebeneﬁcialfromtheviewpointofoptimizationbutdetrimentalfromtheviewpointofgeneralization.Ourunderstandingofhowtheinitialpointaﬀectsgeneralizationisespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselecttheinitialpoint.Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitialparametersneedto“breaksymmetry”'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 315}, page_content='betweendiﬀerentunits.Iftwohiddenunitswiththesameactivationfunctionareconnectedtothesameinputs,thentheseunitsmusthavediﬀerentinitialparameters. Iftheyhavethesameinitialparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccostandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthemodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerentupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusuallybesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheotherunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenullspaceofforwardpropagationandnogradientpatternsarelostinthenullspaceofback-propagation.Thegoalofhavingeachunitcomputeadiﬀerentfunction301'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 316}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmotivatesrandominitializationoftheparameters.Wecouldexplicitlysearchforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,butthisoftenincursanoticeablecomputationalcost.Forexample,ifwehaveatmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalizationonaninitialweightmatrix,andbeguaranteedthateachunitcomputesaverydiﬀerentfunctionfromeachotherunit.Randominitializationfromahigh-entropydistributionoverahigh-dimensionalspaceiscomputationallycheaperandunlikelytoassignanyunitstocomputethesamefunctionaseachother.Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,andinitializeonlytheweightsrandomly.Extraparameters,forexample,parametersencodingtheconditionalvarianceofaprediction,areusuallysettoheuristicallychosenconstantsmuchlikethebiasesare.Wealmostalwaysinitializealltheweightsin themodel tovalues drawnrandomly froma Gaussian oruniform distribution.The choice of'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 316}, page_content='themodel tovalues drawnrandomly froma Gaussian oruniform distribution.The choice of Gaussianoruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeenexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavealargeeﬀectonboththeoutcomeoftheoptimizationprocedureandontheabilityofthenetworktogeneralize.Largerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helpingtoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardorback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthematrixresultinlargeroutputsofmatrixmultiplication.Initialweightsthataretoolargemay,however,resultinexplodingvaluesduringforwardpropagationorback-propagation. Inrecurrentnetworks,largeweightscanalsoresultinchaos(suchextremesensitivitytosmallperturbationsoftheinputthatthebehaviorofthedeterministicforwardpropagationprocedureappearsrandom).'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 316}, page_content='Tosomeextent,theexplodinggradientproblemcanbemitigatedbygradientclipping(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunctiontosaturate,causingcompletelossofgradientthroughsaturatedunits.Thesecompetingfactorsdeterminetheidealinitialscaleoftheweights.Theperspectivesofregularizationandoptimizationcangiveverydiﬀerentinsightsintohowweshouldinitializeanetwork.Theoptimizationperspectivesuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuseofanoptimizationalgorithmsuchasstochasticgradientdescentthatmakessmallincrementalchangestotheweightsandtendstohaltinareasthatarenearertotheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or302'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 317}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesapriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.RecallfromSec.thatgradientdescentwithearlystoppingisequivalenttoweight7.8decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingisnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabouttheeﬀectofinitialization.Wecanthinkofinitializingtheparametersθtoθ0asbeingsimilartoimposingaGaussianpriorp(θ)withmeanθ0.Fromthispointofview,itmakessensetochooseθ0tobenear0.Thispriorsaysthatitismorelikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Unitsinteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrongpreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ0tolargevalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,andhowtheyshouldinteract.Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.Oneheuristicistoinitializetheweightsofafullyconnectedlayerwit'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 317}, page_content='iorspeciﬁeswhichunitsshouldinteractwitheachother,andhowtheyshouldinteract.Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.OneheuristicistoinitializetheweightsofafullyconnectedlayerwithminputsandnoutputsbysamplingeachweightfromU(−1√m,1√m),whileGlorotandBengio()suggestusingthe2010normalizedinitializationWi,j∼−U(6√mn+,6√mn+).(8.23)Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializingalllayerstohavethesameactivationvarianceandthegoalofinitializingalllayerstohavethesamegradientvariance.Theformulaisderivedusingtheassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,withnononlinearities.Realneuralnetworksobviouslyviolatethisassumption,butmanystrategiesdesignedforthelinearmodelperformreasonablywellonitsnonlinearcounterparts.Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,withacarefullychosenscalingorfactorgaingthataccountsforthenonlinearityappliedateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesofnonlinearactiva'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 317}, page_content='itializingtorandomorthogonalmatrices,withacarefullychosenscalingorfactorgaingthataccountsforthenonlinearityappliedateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesofnonlinearactivationfunctions.Thisinitializationschemeisalsomotivatedbyamodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.Undersuchamodel,thisinitializationschemeguaranteesthatthetotalnumberoftrainingiterationsrequiredtoreachconvergenceisindependentofdepth.Increasingthescalingfactorgpushesthenetworktowardtheregimewhereactivationsincreaseinnormastheypropagateforwardthroughthenetworkandgradientsincreaseinnormastheypropagatebackward.()showedSussillo2014thatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas1,000layers,withoutneedingtouseorthogonalinitializations.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 317}, page_content='Akeyinsightof303'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 318}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthisapproachisthatinfeedforwardnetworks,activationsandgradientscangroworshrinkoneachstepofforwardorback-propagation,followingarandomwalkbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixateachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforwardnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthatariseswhenthesameweightmatrixisusedateachstep,describedinSec..8.2.5Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadtooptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemaybeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethenormofasignalthroughouttheentirenetwork.Second,thepropertiesimposedatinitializationmaynotpersistafterlearninghasbeguntoproceed.Third,thecriteriamightsucceedatimprovingthespeedofoptimizationbutinadvertentlyincreasegeneralizationerror.Inpractice,weusuallyneedtotreatthescaleoftheweightsasahyperparameterwhoseoptimalvalueliessomewhereroughlynearbutnotexactlyequaltoth'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 318}, page_content='ingthespeedofoptimizationbutinadvertentlyincreasegeneralizationerror.Inpractice,weusuallyneedtotreatthescaleoftheweightsasahyperparameterwhoseoptimalvalueliessomewhereroughlynearbutnotexactlyequaltothetheoreticalpredictions.Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethesamestandarddeviation,suchas1√m,isthateveryindividualweightbecomesextremelysmallwhenthelayersbecomelarge.()introducedanalternativeMartens2010initializationschemecalledsparseinitializationinwhicheachunitisinitializedtohaveexactlyknon-zeroweights.Theideaistokeepthetotalamountofinputtotheunitindependentfromthenumberofinputsmwithoutmakingthemagnitudeofindividualweightelementsshrinkwithm.Sparseinitializationhelpstoachievemorediversityamongtheunitsatinitializationtime.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 318}, page_content='However,italsoimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussianvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”largevalues,thisinitializationschemecancauseproblemsforunitssuchasmaxoutunitsthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.Whencomputationalresourcesallowit,itisusuallyagoodideatotreattheinitialscaleoftheweightsforeachlayerasahyperparameter,andtochoosethesescalesusingahyperparametersearchalgorithmdescribedinSec.,such11.4.2asrandomsearch.Thechoiceofwhethertousedenseorsparseinitializationcanalsobemadeahyperparameter.Alternately,onecanmanuallysearchforthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesistolookattherangeorstandarddeviationofactivationsorgradientsonasingleminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrosstheminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptablysmallactivationsandincreasingitsweights,itispossibletoe'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 318}, page_content='ngeofactivationsacrosstheminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptablysmallactivationsandincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable304'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 319}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSinitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbeusefultolookattherangeorstandarddeviationofthegradientsaswellastheactivations. Thisprocedurecaninprinciplebeautomatedandisgenerallylesscomputationallycostlythanhyperparameteroptimizationbasedonvalidationseterrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelonasinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidationset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmoreformallyandstudiedby().MishkinandMatas2015So far we have focused on theinitialization of the'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 319}, page_content='far we have focused on theinitialization of the weights.Fortunately,initializationofotherparametersistypicallyeasier.Theapproachforsettingthebiasesmustbecoordinatedwiththeapproachforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweightinitializationschemes.Thereareafewsituationswherewemaysetsomebiasestonon-zerovalues:•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiastoobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethattheinitialweightsaresmallenoughthattheoutputoftheunitisdeterminedonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivationfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.Forexample,iftheoutputisadistributionoverclassesandthisdistributionisahighlyskeweddistributionwiththemarginalprobabilityofclassigivenbyelementciofsomevectorc,thenwecansetthebiasvectorbbysolvingtheequationsoftmax(b)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 319}, page_content='=c.ThisappliesnotonlytoclassiﬁersbutalsotomodelswewillencounterinPart,suchasautoencodersandBoltzmannIIImachines.Thesemodelshavelayerswhoseoutputshouldresembletheinputdatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayerstomatchthemarginaldistributionover.x•Sometimeswe maywanttochoose thebiastoavoidcausingtoo muchsaturationatinitialization.Forexample,wemaysetthebiasofaReLUhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.Thisapproachisnotcompatiblewithweightinitializationschemesthatdonotexpectstronginputfromthebiasesthough.Forexample,itisnotrecommendedforusewithrandomwalkinitialization(,).Sussillo2014•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateinafunction.Insuchsituations,wehaveaunitwithoutputuandanotherunith∈[0,1],thenwecanviewhasagatethatdetermineswhetheruh≈1oruh≈0.Inthesesituations,wewanttosetthebiasforhsothath≈1most305'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 320}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofthetimeatinitialization. Otherwiseudoesnothaveachancetolearn.Forexample,()advocatesettingthebiastofortheJozefowiczetal.20151forgetgateoftheLSTMmodel,describedinSec..10.10Anothercommontypeofparameterisavarianceorprecisionparameter.Forexample,wecanperformlinearregressionwithaconditionalvarianceestimateusingthemodelpyy(|Nx) ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 320}, page_content='(|wTx+1)b,/β(8.24)whereβisaprecisionparameter.Wecanusuallyinitializevarianceorprecisionparametersto1safely.Anotherapproachistoassumetheinitialweightsarecloseenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andsetthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-ters,itispossibletoinitializemodelparametersusingmachinelearning.AcommonstrategydiscussedinPartofthisbookistoinitializeasupervisedmodelwithIIItheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperformingsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitializationthatoﬀersfasterconvergencethanarandominitialization.Someoftheseinitializationstrategiesmayyieldfasterconvergenceandbettergeneralizationbecausetheyencodeinformationaboutthedistributionintheinitialparametersofthemodel.Othersapparentlyper'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 320}, page_content='ominitialization.Someoftheseinitializationstrategiesmayyieldfasterconvergenceandbettergeneralizationbecausetheyencodeinformationaboutthedistributionintheinitialparametersofthemodel.Othersapparentlyperformwellprimarilybecausetheysettheparameterstohavetherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.8.5AlgorithmswithAdaptiveLearningRatesNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyoneofthehyperparametersthatisthemostdiﬃculttosetbecauseithasasigniﬁcantimpactonmodelperformance.AswehavediscussedinSec.andSec.,the4.38.2costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitivetoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,butdoessoattheexpenseofintroducinganotherhyperparameter.Inthefaceofthis,itisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsofsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearningrateforeachparameter,andautomaticallyadapttheselearningratesthroughoutthecourseoflearning.306'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 321}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSThedelta-bar-deltaalgorithm(,)isanearlyheuristicapproachJacobs1988toadaptingindividuallearningratesformodelparametersduringtraining.Theapproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespecttoagivenmodelparameter,remainsthesamesign,thenthelearningrateshouldincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,thenthelearningrateshoulddecrease.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 321}, page_content='Ofcourse,thiskindofrulecanonlybeappliedtofullbatchoptimization.Morerecently,anumberofincremental(ormini-batch-based)methodshavebeenintroducedthatadaptthelearningratesofmodelparameters.Thissectionwillbrieﬂyreviewafewofthesealgorithms.8.5.1AdaGradTheAdaGradalgorithm,showninAlgorithm,individuallyadaptsthelearning8.4ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquarerootofthesumofalloftheirhistoricalsquaredvalues(Duchi2011etal.,).Theparameterswiththelargestpartialderivativeofthelosshaveacorrespondinglyrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivativeshavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreaterprogressinthemoregentlyslopeddirectionsofparameterspace.Inthecontextofconvexoptimization,theAdaGradalgorithmenjoyssomedesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—fortrainingdeepneuralnetworkmodels—theaccumulationofsquaredgradientsfromthebeginningoftrainingcanresultinaprematureandexcessivedecreaseintheeﬀectiv'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 321}, page_content='icalproperties.However,empiricallyithasbeenfoundthat—fortrainingdeepneuralnetworkmodels—theaccumulationofsquaredgradientsfromthebeginningoftrainingcanresultinaprematureandexcessivedecreaseintheeﬀectivelearningrate.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 321}, page_content='AdaGradperformswellforsomebutnotalldeeplearningmodels.8.5.2RMSPropTheRMSPropalgorithm(Hinton2012,)modiﬁesAdaGradtoperformbetterinthenon-convexsettingbychangingthegradientaccumulationintoanexponentiallyweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenappliedtoaconvexfunction.Whenappliedtoanon-convexfunctiontotrainaneuralnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresandeventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthelearningrateaccordingtotheentirehistoryofthesquaredgradientandmayhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe307'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 322}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.4TheAdaGradalgorithmRequire:Globallearningrate\\ue00fRequire:InitialparameterθRequire:Smallconstant,perhapsδ10−7,fornumericalstabilityInitializegradientaccumulationvariabler='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 322}, page_content='0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←+\\ue00cComputeupdate:∆θ←−\\ue00fδ+√r\\ue00cg.(Divisionandsquarerootappliedelement-wise)Applyupdate:θθθ←+∆endwhileextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifitwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.RMSPropisshowninitsstandardforminAlgorithmandcombinedwith8.5NesterovmomentuminAlgorithm.ComparedtoAdaGrad,theuseofthe8.6movingaverageintroducesanewhyperparameter,ρ,thatcontrolsthelengthscaleofthemovingaverage.Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-tooptimizationmethodsbeingemployedroutinelybydeeplearningpractitioners.8.5.3AdamAdam(,)isyetanotheradaptivelearningrateoptimizationKingmaandBa2014algorithmandispresentedinAlgorithm.Thename“Adam”derivesfrom8.7thephrase“adaptivemoments.”Inthecontextoft'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 322}, page_content='earningpractitioners.8.5.3AdamAdam(,)isyetanotheradaptivelearningrateoptimizationKingmaandBa2014algorithmandispresentedinAlgorithm.Thename“Adam”derivesfrom8.7thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itisperhapsbestseenasavariantonthecombinationofRMSPropandmomentumwithafewimportantdistinctions.First,inAdam,momentumisincorporateddirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)ofthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropistoapplymomentumtotherescaledgradients.Theuseofmomentumincombinationwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludesbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentumterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization308'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 323}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.5TheRMSPropalgorithmRequire:Globallearningrate,decayrate.\\ue00fρRequire:InitialparameterθRequire:Smallconstantδ, usually10−6, usedtostabilizedivision bysmallnumbers.Initializeaccumulationvariablesr='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 323}, page_content='0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←ρ+(1)−ρ\\ue00cComputeparameterupdate:∆θ=−\\ue00f√δ+r\\ue00cg.(1√δ+rappliedelement-wise)Applyupdate:θθθ←+∆endwhileattheorigin(seeAlgorithm).RMSPropalsoincorporatesanestimateofthe8.7(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbiasearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoiceofhyperparameters,thoughthelearningratesometimesneedstobechangedfromthesuggesteddefault.8.5.4ChoosingtheRightOptimizationAlgorithmInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddressthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeachmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldonechoose?Unfortunately,thereiscurrentlynoconsensusonthispoint.()Schauletal.2014presentedavaluablecomparison'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 323}, page_content='tingthelearningrateforeachmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldonechoose?Unfortunately,thereiscurrentlynoconsensusonthispoint.()Schauletal.2014presentedavaluablecomparisonofalargenumberofoptimizationalgorithmsacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyofalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)performedfairlyrobustly,nosinglebestalgorithmhasemerged.Currently,themostpopularoptimizationalgorithmsactivelyinuseincludeSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDeltaandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodependlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparametertuning).309'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 324}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.6RMSPropalgorithmwithNesterovmomentumRequire:Globallearningrate,decayrate,momentumcoeﬃcient.\\ue00fραRequire:Initialparameter,initialvelocity.θvInitializeaccumulationvariabler= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computeinterimupdate:˜θθv←+αComputegradient:g←1m∇˜θ\\ue050iLf((x()i;˜θy),()i)Accumulategradient:rrgg←ρ+(1)−ρ\\ue00cComputevelocityupdate:vv←α−\\ue00f√r\\ue00cg.(1√rappliedelement-wise)Applyupdate:θθv←+endwhile8.6ApproximateSecond-OrderMethodsInthissectionwediscusstheapplicationofsecond-ordermethodstothetrainingofdeepnetworks.See()foranearliertreatmentofthissubject.LeCunetal.1998aForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempiricalrisk:J() = θEx,y∼ˆpdata()x,y[((;))]'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 324}, page_content='= θEx,y∼ˆpdata()x,y[((;))] =Lfxθ,y1mm\\ue058i=1Lf((x()i;)θ,y()i).(8.25)Howeverthemethodswediscusshereextendreadilytomoregeneralobjectivefunctionsthat,forinstance,includeparameterregularizationtermssuchasthosediscussedinChapter.78.6.1Newton’sMethodInSec.,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst-4.3ordermethods,second-ordermethodsmakeuseofsecondderivativestoimproveoptimization.Themostwidelyusedsecond-ordermethodisNewton’smethod.WenowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationtoneuralnetworktraining.Newton’smethodisanoptimizationschemebasedonusingasecond-orderTay-lorseriesexpansiontoapproximateJ(θ)nearsomepointθ0,ignoringderivatives310'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 325}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.7TheAdamalgorithmRequire:Stepsize(Suggesteddefault:)\\ue00f0001.Require:Exponentialdecayratesformoment estimates,ρ1andρ2in[0,1).(Suggesteddefaults:andrespectively)09.0999.Require:Smallconstantδusedfornumericalstabilization.(Suggesteddefault:10−8)Require:InitialparametersθInitialize1stand2ndmomentvariables,s= 0r= 0Initializetimestept= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)tt←+1Updatebiasedﬁrstmomentestimate:s←ρ1s+(1−ρ1)gUpdatebiasedsecondmomentestimate:r←ρ2r+(1−ρ2)gg\\ue00cCorrectbiasinﬁrstmoment:ˆs←s1−ρt1Correctbiasinsecondmoment:ˆr←r1−ρt2Computeupdate:∆= θ−\\ue00fˆs√ˆr+δ(operationsappliedelement-wise)Applyupdate:θθθ←+∆endwhileofhigherorder:JJ() θ≈(θ0)+(θθ−0)\\ue03e∇θJ(θ0)+12(θθ−0)\\ue03eHθθ(−0),(8.26)whereHistheHessianofJwithrespecttoθevaluatedatθ0.Ifwethensolveforthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:θ∗='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 325}, page_content='θ≈(θ0)+(θθ−0)\\ue03e∇θJ(θ0)+12(θθ−0)\\ue03eHθθ(−0),(8.26)whereHistheHessianofJwithrespecttoθevaluatedatθ0.Ifwethensolveforthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:θ∗= θ0−H−1∇θJ(θ0)(8.27)Thusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescalingthegradientbyH−1,Newton’smethodjumpsdirectlytotheminimum. Iftheobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),thisupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’smethod,giveninAlgorithm.8.8Forsurfacesthatarenotquadratic,aslongastheHessianremainspositivedeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step311'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 326}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.8Newton’smethodwithobjectiveJ(θ)=1m\\ue050mi=1Lf((x()i;)θ,y()i).Require:Initialparameterθ0Require:TrainingsetofexamplesmwhiledostoppingcriterionnotmetComputegradient:g←1m∇θ\\ue050iLf((x()i;)θ,y()i)ComputeHessian:H←1m∇2θ\\ue050iLf((x()i;)θ,y()i)ComputeHessianinverse:H−1Computeupdate:∆= θ−H−1gApplyupdate:θθθ= +∆endwhileiterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdatingthequadraticapproximation).Second,updatetheparametersaccordingtoEq.8.27.InSec.,wediscussedhowNewton’smethodisappropriateonlywhen8.2.3theHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjectivefunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,thatareproblematicforNewton’smethod.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 326}, page_content='IftheeigenvaluesoftheHessianarenotallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactuallycauseupdatestomoveinthewrongdirection.ThissituationcanbeavoidedbyregularizingtheHessian.Commonregularizationstrategiesincludeaddingaconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomesαθ∗= θ0−[((Hfθ0))+]αI−1∇θf(θ0).(8.28)ThisregularizationstrategyisusedinapproximationstoNewton’smethod,suchastheLevenberg–Marquardtalgorithm(Levenberg1944Marquardt1963,;,),andworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelativelyclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,thevalueofαwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.However,asαincreasesinsize,theHessianbecomesdominatedbytheαIdiagonalandthedirectionchosenbyNewton’smethodconvergestothestandardgradientdividedbyα.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 326}, page_content='Whenstrongnegativecurvatureispresent,αmayneedtobesolargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwithaproperlychosenlearningrate.Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,suchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneuralnetworksislimitedbythesigniﬁcantcomputationalburdenitimposes.The312'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 327}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSnumberofelementsintheHessianissquaredinthenumberofparameters,sowithkparameters(andforevenverysmallneuralnetworksthenumberofparameterskcanbeinthemillions),Newton’smethodwouldrequiretheinversionofakk×matrix—withcomputationalcomplexityofO(k3).Also,sincetheparameterswillchangewitheveryupdate,theinverseHessianhastobecomputedateverytrainingiteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameterscanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 327}, page_content='wewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’smethodwhileside-steppingthecomputationalhurdles.8.6.2ConjugateGradientsConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverseHessianbyiterativelydescendingconjugatedirections.Theinspirationforthisapproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepestdescent(seeSec.fordetails),wherelinesearchesareappliediterativelyin4.3thedirectionassociatedwiththegradient.Fig.illustrateshowthemethodof8.6steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀectiveback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,whengivenbythegradient,isguaranteedtobeorthogonaltothepreviouslinesearchdirection.Lettheprevioussearchdirectionbedt−1.Attheminimum,wherethelinesearchterminates,thedirectionalderivativeiszeroindirectiondt−1:∇θJ(θ)·dt−1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,dt=∇θJ(θ) willhavenocontributioninthedirectiondt−1.Thusdtisorthogonaltodt−1.This'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 327}, page_content='willhavenocontributioninthedirectiondt−1.Thusdtisorthogonaltodt−1.This relationshipbetweendt−1anddtisillustrated inFig.for8.6multipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceoforthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevioussearchdirections.Thisgivesrisetothezig-zagpatternofprogress,wherebydescendingtotheminimuminthecurrentgradientdirection,wemustre-minimizetheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientattheendofeachlinesearchweare,inasense,undoingprogresswehavealreadymadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradientsseekstoaddressthisproblem.Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthatisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogressmadeinthatdirection. Attrainingiterationt,thenextsearchdirectiondttakestheform:dt= ∇θJβ()+θtdt−1(8.29)313'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 328}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\n\\U000f0913\\ue033\\ue030\\U000f0913\\ue032\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue031\\ue030\\ue032\\ue030\\U000f0913\\ue033\\ue030\\U000f0913\\ue032\\ue030\\U000f0913\\ue031\\ue030\\ue030\\ue031\\ue030\\ue032\\ue030'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 328}, page_content='Figure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.Themethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongthelinedeﬁnedbythegradientattheinitialpointoneachstep.ThisresolvessomeoftheproblemsseenwithusingaﬁxedlearningrateinFig.,butevenwiththeoptimalstepsizethe4.6algorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,attheminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointisorthogonaltothatdirection.wereβtisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,dt−1,weshouldaddbacktothecurrentsearchdirection.Twodirections,dtanddt−1,aredeﬁnedasconjugateifd\\ue03etHd()Jt−1= 0.d\\ue03etHdt−1= 0(8.30)ThestraightforwardwaytoimposeconjugacywouldinvolvecalculationoftheeigenvectorsofHtochooseβt,whichwouldnotsatisfyourgoalofdevelopingamethodthatismorecomputationallyviablethanNewton’smethodforlargeproblems. Canwecalculatetheconjugatedirectionswithoutresortingtothesecalculations?Fortunatelytheanswertothatisyes.Twopopularmethodsforcomputingtheβtare:1.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 328}, page_content='Canwecalculatetheconjugatedirectionswithoutresortingtothesecalculations?Fortunatelytheanswertothatisyes.Twopopularmethodsforcomputingtheβtare:1. Fletcher-Reeves:βt=∇θJ(θt)\\ue03e∇θJ(θt)∇θJ(θt−1)\\ue03e∇θJ(θt−1)(8.31)2. Polak-Ribière:βt=(∇θJ(θt)−∇θJ(θt−1))\\ue03e∇θJ(θt)∇θJ(θt−1)\\ue03e∇θJ(θt−1)(8.32)314'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 329}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSForaquadraticsurface,theconjugatedirectionsensurethatthegradientalongthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayattheminimumalongthepreviousdirections.Asaconsequence,inak-dimensionalparameterspace,conjugategradientsonlyrequiresklinesearchestoachievetheminimum.TheconjugategradientalgorithmisgiveninAlgorithm.8.9Algorithm8.9ConjugategradientmethodRequire:Initialparametersθ0Require:TrainingsetofexamplesmInitializeρ0= 0Initializeg0= 0Initializet= 1whiledostoppingcriterionnotmetInitializethegradientgt= 0Computegradient:gt←1m∇θ\\ue050iLf((x()i;)θ,y()i)Computeβt=(gt−gt−1)\\ue03egtg\\ue03et−1gt−1(Polak-Ribière)(Nonlinearconjugategradient:optionallyresetβttozero,forexampleiftisamultipleofsomeconstant,suchas)kk= 5Computesearchdirection:ρt= −gt+βtρt−1Performlinesearchtoﬁnd:\\ue00f∗= argmin\\ue00f1m\\ue050mi=1Lf((x()i;θt+\\ue00fρt),y()i)(Onatrulyquadraticcostfunction,analyticallysolvefor\\ue00f∗ratherthanexplicitlysearchingforit)Applyupdate:θt+1='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 329}, page_content='−gt+βtρt−1Performlinesearchtoﬁnd:\\ue00f∗= argmin\\ue00f1m\\ue050mi=1Lf((x()i;θt+\\ue00fρt),y()i)(Onatrulyquadraticcostfunction,analyticallysolvefor\\ue00f∗ratherthanexplicitlysearchingforit)Applyupdate:θt+1= θt+\\ue00f∗ρttt←+1endwhileNonlinearConjugateGradients:Sofarwehavediscussedthemethodofconjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse,ourprimaryinterestinthischapteristoexploreoptimizationmethodsfortrainingneuralnetworksandotherrelateddeeplearningmodelswherethecorrespondingobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodofconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirectionsarenolongerassuredtoremainattheminimumoftheobjectiveforpreviousdirections.Asaresult,thenonlinearconjugategradientsalgorithmincludesoccasionalresetswherethemethodofconjugategradientsisrestartedwithlinesearchalongtheunalteredgradient.Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate315'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 330}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSgradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialtoinitializetheoptimizationwithafewiterationsofstochasticgradientdescentbeforecommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugategradientsalgorithmhastraditionallybeencastasabatchmethod,minibatchversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.2011). Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshavebeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller1993).8.6.3BFGSTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptstobringsomeoftheadvantagesofNewton’smethodwithoutthecomputationalburden.Inthatrespect,BFGSissimilartoCG.However,BFGStakesamoredirectapproachtotheapproximationofNewton’supdate.RecallthatNewton’supdateisgivenbyθ∗='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 330}, page_content='θ0−H−1∇θJ(θ0),(8.33)whereHistheHessianofJwithrespecttoθevaluatedatθ0.TheprimarycomputationaldiﬃcultyinapplyingNewton’supdateisthecalculationoftheinverseHessianH−1.Theapproachadoptedbyquasi-Newtonmethods(ofwhichtheBFGSalgorithmisthemostprominent)istoapproximatetheinversewithamatrixMtthatisiterativelyreﬁnedbylowrankupdatestobecomeabetterapproximationofH−1.ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmanytextbooksonoptimization,includingLuenberger1984().OncetheinverseHessianapproximationMtisupdated,thedirectionofdescentρtisdeterminedbyρt=Mtgt.Alinesearchisperformedinthisdirectiontodeterminethesizeofthestep,\\ue00f∗,takeninthisdirection.Theﬁnalupdatetotheparametersisgivenby:θt+1='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 330}, page_content='θt+\\ue00f∗ρt.(8.34)Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesoflinesearcheswiththedirectionincorporatingsecond-orderinformation.Howeverunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependentonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspendlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmuststoretheinverseHessianmatrix,M,thatrequiresO(n2)memory,makingBFGS316'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 331}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsofparameters.Limited Memory BFGS (or L-BFGS)The memorycosts ofthe BFGSalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverseHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationMusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumptionthatM(1)t−istheidentitymatrix,ratherthanstoringtheapproximationfromonesteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGSaremutuallyconjugate.However,unlikethemethodofconjugategradients,thisprocedureremainswellbehavedwhentheminimumofthelinesearchisreachedonlyapproximately.ThL-BFGSstrategywithnostoragedescribedherecanbegeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthevectorsusedtoupdateateachtimestep,whichcostsonlyperstep.MOn()8.7OptimizationStrategiesandMeta-AlgorithmsManyoptimizationtechniquesarenotexactlyalgorithms,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 331}, page_content='butrathergeneraltemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbeincorporatedintomanydiﬀerentalgorithms.8.7.1BatchNormalizationBatchnormalization(,)isoneofthemostexcitingrecentIoﬀeandSzegedy2015innovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimizationalgorithmatall.Instead,itisamethodofadaptivereparametrization,motivatedbythediﬃcultyoftrainingverydeepmodels.Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.Thegradienttellshowtoupdateeachparameter,undertheassumptionthattheotherlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctionscomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputedundertheassumptionthattheotherfunctionsremainconstant.Asasimpleexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayeranddoesnotuseanactivationfunctionateachhiddenlayer:ˆy=xw1w2w3...wl.Here,wiprovidestheweightusedbylayeri.Theoutputoflayeriishi=hi−1wi.Theoutpu'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 331}, page_content='e,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayeranddoesnotuseanactivationfunctionateachhiddenlayer:ˆy=xw1w2w3...wl.Here,wiprovidestheweightusedbylayeri.Theoutputoflayeriishi=hi−1wi.Theoutputˆyisalinearfunctionoftheinputx,butanonlinearfunctionoftheweightswi.Supposeourcostfunctionhasputagradientofon1ˆy,sowewishto317'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 332}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdecreaseˆyslightly.Theback-propagationalgorithmcanthencomputeagradientg=∇wˆy.Considerwhathappenswhenwemakeanupdatewwg←−\\ue00f.Theﬁrst-orderTaylorseriesapproximationofˆypredictsthatthevalueofˆywilldecreaseby\\ue00fg\\ue03eg.Ifwewantedtodecreaseˆyby.1,thisﬁrst-orderinformationavailableinthegradientsuggestswecouldsetthelearningrate\\ue00fto.1g\\ue03eg.However,theactualupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforderl.Thenewvalueofˆyisgivenbyxw(1−\\ue00fg1)(w2−\\ue00fg2)(...wl−\\ue00fgl).(8.35)Anexampleofonesecond-ordertermarisingfromthisupdateis\\ue00f2g1g2\\ue051li=3wi.Thistermmightbenegligibleif\\ue051li=3wiissmall,ormightbeexponentiallylargeiftheweightsonlayersthrough3laregreaterthan.Thismakesitveryhard1tochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetotheparametersforonelayerdependssostronglyonalloftheotherlayers.Second-orderoptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthesesecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,evenhigher-'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 332}, page_content='endssostronglyonalloftheotherlayers.Second-orderoptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthesesecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,evenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimizationalgorithmsareexpensiveandusuallyrequirenumerousapproximationsthatpreventthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions.Buildingann-thorderoptimizationalgorithmforn>2thusseemshopeless.Whatcanwedoinstead?Batchnormalizationprovidesanelegantwayofreparametrizingalmostanydeepnetwork.Thereparametrizationsigniﬁcantlyreducestheproblemofcoordinatingupdatesacrossmanylayers.Batchnormalizationcanbeappliedtoanyinputorhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayertonormalize,arrangedasadesignmatrix,withtheactivationsforeachexampleappearinginarowofthematrix.Tonormalize,wereplaceitwithHH\\ue030=Hµ−σ,(8.36)whereµisavectorcontainingthemeanofeachunitandσisavectorcontainingthestandarddeviationofeachunit.Thearithmetichereisba'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 332}, page_content='foreachexampleappearinginarowofthematrix.Tonormalize,wereplaceitwithHH\\ue030=Hµ−σ,(8.36)whereµisavectorcontainingthemeanofeachunitandσisavectorcontainingthestandarddeviationofeachunit.ThearithmetichereisbasedonbroadcastingthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Withineachrow,thearithmeticiselement-wise,soHi,jisnormalizedbysubtractingµjanddividingbyσj.TherestofthenetworkthenoperatesonH\\ue030inexactlythesamewaythattheoriginalnetworkoperatedon.HAttrainingtime,µ=1m\\ue058iHi,:(8.37)318'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 333}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSandσ=\\ue073δ+1m\\ue058i()Hµ−2i,(8.38)whereδisasmallpositivevaluesuchas10−8imposedtoavoidencounteringtheundeﬁnedgradientof√zatz=0.Crucially,weback-propagatethroughtheseoperationsforcomputingthemeanandthestandarddeviation,andforapplyingthemtonormalizeH.Thismeansthatthegradientwillneverproposeanoperation thatactssimplytoincreasethestandard deviationormeanofhi;thenormalizationoperationsremovetheeﬀectofsuchanactionandzerooutitscomponentinthegradient.Thiswasamajorinnovationofthebatchnormalizationapproach.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 333}, page_content='Previousapproacheshadinvolvedaddingpenaltiestothecostfunctiontoencourageunitstohavenormalizedactivationstatisticsorinvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.Theformerapproachusuallyresultedinimperfectnormalizationandthelatterusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedlyproposedchangingthemeanandvarianceandthenormalizationsteprepeatedlyundidthischange.Batchnormalizationreparametrizesthemodeltomakesomeunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.Attesttime,µandσmaybereplacedbyrunningaveragesthatwerecollectedduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,withoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.Revisitingtheˆy=xw1w2...wlexample,weseethatwecanmostlyresolvethediﬃcultiesinlearningthismodelbynormalizinghl−1.SupposethatxisdrawnfromaunitGaussian.Thenhl−1willalsocomefromaGaussian,becausethetransformationfromxtohlislinear.However,hl−1willnolongerhavezeromeanandunitvariance'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 333}, page_content='nlearningthismodelbynormalizinghl−1.SupposethatxisdrawnfromaunitGaussian.Thenhl−1willalsocomefromaGaussian,becausethetransformationfromxtohlislinear.However,hl−1willnolongerhavezeromeanandunitvariance.Afterapplyingbatchnormalization,weobtainthenormalizedˆhl−1thatrestoresthezeromeanandunitvarianceproperties.Foralmostanyupdatetothelowerlayers,ˆhl−1willremainaunitGaussian.Theoutputˆymaythenbelearnedasasimplelinearfunctionˆy=wlˆhl−1.Learninginthismodelisnowverysimplebecausetheparametersatthelowerlayerssimplydonothaveaneﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 333}, page_content='Insomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelowerlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign0ofoneofthelowerweightscanﬂiptherelationshipbetweenˆhl−1andy. Thesesituationsareveryrare.Withoutnormalization,nearlyeveryupdatewouldhaveanextremeeﬀectonthestatisticsofhl−1.Batchnormalizationhasthusmadethismodelsigniﬁcantlyeasiertolearn.Inthisexample,theeaseoflearningofcoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,319'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhaveanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecondorderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneuralnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlineartransformationsofthedata,sotheyremainuseful.Batchnormalizationactstostandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,butallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingleunittochange.Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithinalayer.Indeed,thisistheapproachtakenby(),whoprovidedDesjardinsetal.2015theinspirationforbatchnormalization.Unfortunately,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='eliminatingalllinearinteractionsismuchmoreexpensivethanstandardizingthemeanandstandarddeviationofeachindividualunit,andsofarbatchnormalizationremainsthemostpracticalapproach.Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressivepowerofthe neuralnetworkcontaining thatunit.In ordertomaintain'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='theexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunitactivationsHwithγH\\ue030+βratherthansimplythenormalizedH\\ue030.Thevariablesγandβarelearnedparametersthatallowthenewvariabletohaveanymeanandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidwesetthemeanto0,andthenintroduceaparameterthatallowsittobesetbacktoanyarbitraryvalueβ?Theansweristhatthenewparametrizationcanrepresentthesamefamilyoffunctionsoftheinputastheoldparametrization,butthenewparametrizationhasdiﬀerentlearningdynamics.Intheoldparametrization,themeanofHwasdeterminedbyacomplicatedinteractionbetweentheparametersinthelayersbelowH.Inthenewparametrization,themeanofγH\\ue030+βisdeterminedsolelybyβ.Thenewparametrizationismucheasiertolearnwithgradientdescent.Mostneuralnetworklayerstaketheformofφ(XW+b)whereφissomeﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.ItisnaturaltowonderwhetherweshouldapplybatchnormalizationtotheinputX,ortothetransformedvalueXW+b.()recommendIoﬀeandSzegedy2015thelatter.Morespeciﬁ'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='tivationfunctionsuchastherectiﬁedlineartransformation.ItisnaturaltowonderwhetherweshouldapplybatchnormalizationtotheinputX,ortothetransformedvalueXW+b.()recommendIoﬀeandSzegedy2015thelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversionofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwiththeβparameterappliedbythebatchnormalizationreparametrization.Theinputtoalayerisusuallytheoutputofanonlinearactivationfunctionsuchastherectiﬁedlinearfunctioninapreviouslayer.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 334}, page_content='Thestatisticsoftheinputarethus320'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 335}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.Inconvolutionalnetworks,describedinChapter,itisimportanttoapplythe9samenormalizingµandσateveryspatiallocationwithinafeaturemap,sothatthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.8.7.2CoordinateDescentInsomecases,itmaybepossibletosolveanoptimizationproblemquicklybybreakingitintoseparatepieces.Ifweminimizef(x)withrespecttoasinglevariablexi,thenminimizeitwithrespecttoanothervariablexjandsoon,repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)minimum.Thispracticeisknownascoordinatedescent,becauseweoptimizeonecoordinateatatime.Moregenerally,blockcoordinatedescentreferstominimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellasthestrictlyindividualcoordinatedescent.Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesintheoptimizationproblemcanbeclearlyseparate'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 335}, page_content='edescent”isoftenusedtorefertoblockcoordinatedescentaswellasthestrictlyindividualcoordinatedescent.Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesintheoptimizationproblemcanbeclearlyseparatedintogroupsthatplayrelativelyisolatedroles,orwhenoptimizationwithrespecttoonegroupofvariablesissigniﬁcantlymoreeﬃcientthanoptimizationwithrespecttoallofthevariables.Forexample,considerthecostfunctionJ,(HW)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 335}, page_content='=\\ue058i,j|Hi,j|+\\ue058i,j\\ue010XW−\\ue03eH\\ue0112i,j.(8.39)Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalistoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvaluesHtoreconstructthetrainingsetX.MostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inordertopreventthepathologicalsolutionwithextremelysmallandlarge.HWThefunctionJisnotconvex.However, wecandividetheinputstothetrainingalgorithmintotwosets:thedictionaryparametersWandthecoderepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneofthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgivesusanoptimizationstrategythatallowsustouseeﬃcientconvexoptimizationalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizingHWwithﬁxed.Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariablestronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunctionf(x) =321'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 336}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS(x1−x2)2+α\\ue000x21+x22\\ue001whereαisapositiveconstant.Theﬁrsttermencouragesthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthemtobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolvetheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.However,forsmallα,coordinatedescentwillmakeveryslowprogressbecausetheﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀerssigniﬁcantlyfromthecurrentvalueoftheothervariable.8.7.3PolyakAveragingPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveralpoints inthe trajectory throughparameter spacevisited by anoptimizationalgorithm. Iftiterationsofgradientdescentvisitpointsθ(1),...,θ()t,thentheoutputofthePolyakaveragingalgorithmisˆθ()t=1t\\ue050iθ()i.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 336}, page_content='inthe trajectory throughparameter spacevisited by anoptimizationalgorithm. Iftiterationsofgradientdescentvisitpointsθ(1),...,θ()t,thentheoutputofthePolyakaveragingalgorithmisˆθ()t=1t\\ue050iθ()i. Onsomeproblemclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhasstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcationismoreheuristic,butitperformswellinpractice.Thebasicideaisthattheoptimizationalgorithmmayleapbackandforthacrossavalleyseveraltimeswithoutevervisitingapointnearthebottomofthevalley.Theaverageofallofthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.Innon-convexproblems,thepathtakenbytheoptimizationtrajectorycanbeverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameterspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylargebarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,whenapplyingPolyakaveragingtonon-convexproblems,itistypicaltouseanexponentiallydecayingrunningaverage:ˆθ()t='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 336}, page_content='αˆθ(1)t−+(1)−αθ()t.(8.40)Therunningaverageapproachisusedinnumerousapplications.SeeSzegedyetal.()forarecentexample.20158.7.4SupervisedPretrainingSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitiousifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itissometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmakethemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolveasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthatinvolve322'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 337}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELStrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeoftrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownaspretraining.Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefortheoptimalversionofeachcomponentinisolation.Unfortunately,combiningtheindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcompletesolution.However,greedyalgorithmscanbecomputationallymuchcheaperthanalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolutionisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbyaﬁne-tuningstageinwhichajointoptimizationalgorithmsearchesforanoptimalsolutiontothefullproblem.Initializingthejointoptimizationalgorithmwithagreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionitﬁnds.Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousindeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithmsthatbreaksupervisedlearningproblemsintoothersimplersup'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 337}, page_content='nds.Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousindeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithmsthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearningproblems.Thisapproachisknownasgreedysupervisedpretraining.Intheoriginal(,)versionofgreedysupervisedpretraining,Bengioetal.2007eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetofthelayersintheﬁnalneuralnetwork.AnexampleofgreedysupervisedpretrainingisillustratedinFig.,inwhicheachaddedhiddenlayerispretrainedaspartof8.7ashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrainedhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse2015theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeepernetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.Anotheroption,exploredbyYu2010etal.()ist'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 337}, page_content='toinitializeevendeepernetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.Anotheroption,exploredbyYu2010etal.()istousetheofthepreviouslyoutputstrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.Why'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 337}, page_content='would greedy supervised pretraining help?The hypothesis initiallydiscussedby()isthatithelpstoprovidebetterguidancetotheBengioetal.2007intermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothintermsofoptimizationandintermsofgeneralization.Anapproachrelatedtosupervisedpretrainingextendstheideatothecontextoftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8layersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)andtheninitializeasame-sizenetworkwiththeﬁrstklayersoftheﬁrstnet.Allthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are323'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 338}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS\\ny yh(1)h(1)xx(a)U(1)U(1)W(1)W(1)y yh(1)h(1)x x(b)U(1)U(1)W(1)W(1)\\ny yh(1)h(1)x x(c)U(1)U(1)W(1)W(1)h(2)h(2)y yU(2)U(2)W(2)W(2)y yh(1)h(1)x x(d)U(1)U(1)W(1)W(1)h(2)h(2)yU(2)U(2)W(2)W(2)\\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining(,).Bengioetal.2007(a)Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe(b)samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand(c)discardthehidden-to-outputlayer.WesendtheoutputoftheﬁrsthiddenlayerasinputtoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjectiveastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforasmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforward(d)network.Tofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyattheendorateachstageofthisprocess.324'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000ImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetoftasks.OtherapproachestotransferlearningwithneuralnetworksarediscussedinSec..15.2Anotherrelatedlineofworkisthe(,)approach.ThisFitNetsRomeroetal.2015approachbeginsbytraininganetworkthathaslowenoughdepthandgreatenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthenbecomesateacherforasecondnetwork,designatedthe.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='Thestudentnetworkisstudentmuchdeeperandthinner(eleventonineteenlayers)andwouldbediﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthestudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredicttheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayeroftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthehiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additionalparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetworkfromthemiddlelayerofthedeeperstudentnetwork.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='However,insteadofpredictingtheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayeroftheteachernetwork.Thelowerlayersofthestudentnetworksthushavetwoobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,aswellastopredicttheintermediatelayeroftheteachernetwork.Althoughathinanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallownetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslowercomputationalcostifitisthinenoughtohavefarfewerparameters.Withoutthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyintheexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythusbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃculttotrain,butotheroptimizationtechniquesorchangesinthearchitecturemayalsosolvetheproblem.8.7.5DesigningModelstoAidOptimizationToimproveoptimization,thebeststrategyisnotalwaystoimprovetheoptimizationalgorithm.Instead,manyimprovementsintheoptimizationofdeepmodelshavecomefromdesigningthemodelstobeeasiertooptimize.I'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='dOptimizationToimproveoptimization,thebeststrategyisnotalwaystoimprovetheoptimizationalgorithm.Instead,manyimprovementsintheoptimizationofdeepmodelshavecomefromdesigningthemodelstobeeasiertooptimize.Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreaseinjaggednon-monotonicpatterns.However,thiswouldmakeoptimizationextremelydiﬃcult.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 339}, page_content='Inpractice,itismoreimportanttochooseamodelfamilythatiseasytooptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesinneuralnetworklearningoverthepast30yearshavebeen325'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 340}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSobtainedbychangingthemodelfamilyratherthanchangingtheoptimizationprocedure.Stochasticgradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-formationsbetweenlayersandactivationfunctionsthatarediﬀerentiablealmosteverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain. Inpar-ticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunitshaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeepnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmakeoptimizationeasier.ThegradientﬂowsthroughmanylayersprovidedthattheJacobianofthelineartransformationhasreasonablesingularvalues.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 340}, page_content='Moreover,linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’soutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradientwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,modernneuralnetshavebeendesignedsothattheirlocalgradientinformationcorrespondsreasonablywelltomovingtowardadistantsolution.Othermodeldesignstrategiescanhelptomakeoptimizationeasier.Forexample,linearpathsorskipconnectionsbetweenlayersreducethelengthoftheshortestpathfromthelowerlayer’sparameterstotheoutput, and thusmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedideatoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtotheintermediatehiddenlayersofthenetwork,asinGoogLeNet(,)Szegedyetal.2014aanddeeply-supervisednets(,).These“auxiliaryheads”aretrainedLeeetal.2014toperformthesametaskastheprimaryoutputatthetopofthenetworkinordertoensurethatthelowerlayersreceivealargegradient.Whentrainingiscompletetheauxiliaryheadsmaybediscarded.'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 340}, page_content='Thisisanalternativetothepretrainingstrategies,whichwereintroducedintheprevioussection.Inthisway,onecantrainjointlyallthelayersinasinglephasebutchangethearchitecture,sothatintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhattheyshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.8.7.6ContinuationMethodsandCurriculumLearningAsarguedinSec.,manyofthechallengesinoptimizationarisefromtheglobal8.2.7structureofthecostfunctionandcannotberesolvedmerelybymakingbetterestimatesoflocalupdatedirections.Thepredominantstrategyforovercomingthisproblemistoattempttoinitializetheparametersinaregionthatisconnectedtothesolutionbyashortpaththroughparameterspacethatlocaldescentcan326'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 341}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdiscover.Continuationmethodsareafamilyofstrategiesthatcanmakeoptimizationeasierbychoosinginitialpointstoensurethatlocaloptimizationspendsmostofitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsistoconstructaseriesofobjectivefunctionsoverthesameparameters.InordertominimizeacostfunctionJ(θ),wewillconstructnewcostfunctions{J(0),...,J()n}.Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,withJ(0)beingfairlyeasytominimize,andJ()n,themostdiﬃcult,beingJ(θ),thetruecostfunctionmotivatingtheentireprocess.WhenwesaythatJ()iiseasierthanJ(+1)i,wemeanthatitiswellbehavedovermoreofθspace.Arandominitializationismorelikelytolandintheregionwherelocaldescentcanminimizethecostfunctionsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesignedsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginbysolvinganeasyproblemthenreﬁnethesolutiontosolveincrementallyharderproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.Tradit'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 341}, page_content='esignedsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginbysolvinganeasyproblemthenreﬁnethesolutiontosolveincrementallyharderproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.Traditionalcontinuationmethods(predatingtheuseofcontinuationmethodsforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.SeeWu1997()foranexampleofsuchamethodandareviewofsomerelatedmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,whichaddsnoisetotheparameters(Kirkpatrick1983etal.,'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 341}, page_content=').Continuationmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher()foranoverviewofrecentliterature,especiallyforAIapplications.2015Continuationmethodstraditionallyweremostlydesignedwiththegoalofovercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedtoreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,thesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”theoriginalcostfunction.ThisblurringoperationcanbedonebyapproximatingJ()i() ='),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 341}, page_content='= θEθ\\ue030∼N(θ\\ue030;θ,σ()2i)J(θ\\ue030)(8.41)viasampling.Theintuitionforthisapproachisthatsomenon-convexfunctionsbecomeapproximatelyconvexwhenblurred.Inmanycases,thisblurringpreservesenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndtheglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.Thisapproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁneaseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfromonefunctiontothenextarrivingattheglobalminimum,butitmightrequiresomanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.NP-hardoptimizationproblemsremainNP-hard,evenwhencontinuationmethods327'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 342}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespondtothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,nomatterhowmuchitisblurred.ConsiderforexamplethefunctionJ(θ)'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 342}, page_content='=−θ\\ue03eθ.Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimumofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumoftheoriginalcostfunction.Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththeproblemoflocalminima,localminimaarenolongerbelievedtobetheprimaryproblemforneuralnetworkoptimization.Fortunately,continuationmethodscanstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcaneliminateﬂatregions,decreasevarianceingradientestimates,improveconditioningoftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdateseasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirectionsandprogresstowardaglobalsolution.Bengio2009etal.()observedthatanapproachcalledcurriculumlearningorshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningisbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconceptsandprogresstolearningmorecomplexconceptsthatdependonthesesimplerconcepts.Thisbasicstrategywaspreviouslyknowntoacceleratepro'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 342}, page_content='ningisbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconceptsandprogresstolearningmorecomplexconceptsthatdependonthesesimplerconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimaltraining(,;,;Skinner1958Peterson2004KruegerandDayan2009,)andmachinelearning(,;,;,).()Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009justiﬁedthisstrategyasacontinuationmethod,whereearlierJ()iaremadeeasierbyincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributionstothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),andexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowingacurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearninghasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;Collobert2011aMikolov2011bTuandHonavar2011etal.,;etal.,;,)andcomputervision(,;,;,)Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayinwhichhumansteach(,):teachersst'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 342}, page_content='onavar2011etal.,;etal.,;,)andcomputervision(,;,;,)Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayinwhichhumansteach(,):teachersstartbyshowingeasierandKhanetal.2011moreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurfacewiththelessobviouscases.Curriculum-basedstrategiesaremoreeﬀectiveforteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcanalsoincreasetheeﬀectivenessofotherteachingstrategies(,BasuandChristensen2013).Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthecontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:328'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 343}, page_content='CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwithastochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalwayspresentedtothelearner,butwheretheaverageproportionofthemorediﬃcultexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.Withadeterministiccurriculum,noimprovementoverthebaseline(ordinarytrainingfromthefulltrainingset)wasobserved.Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowtoregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsoftheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesandprocessinputdatathathasspecialstructure.Theoptimizationmethodsdiscussedinthischapterareoftendirectlyapplicabletothesespecializedarchitectureswithlittleornomodiﬁcation.\\n329'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/DeepLearningBook.pdf', 'page': 344}, page_content='Chapter9ConvolutionalNetworksConvolutionalnetworksconvolutionalneuralnetworks(,),alsoknownasLeCun1989or,areaspecializedkindofneuralnetworkforprocessingdatathathasCNNsaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcanbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,whichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeentremendouslysuccessfulinpracticalapplications.Thename“convolutionalneuralnetwork” indicatesthatthenetworkemploysamathematicaloperationcalledconvolution.Convolutionisaspecializedkindoflinearoperation.Convolutionalnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrixmultiplicationinatleastoneoftheirlayers.Inthischapter, we willﬁrstdescribewhatconvolution is.Next,'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Passing chunks and embeddings to vectorstore"
      ],
      "metadata": {
        "id": "1EOKqYvExn20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "K0W1pdiOxANL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = FAISS.from_documents(documents = chunk, embedding = embeddings)"
      ],
      "metadata": {
        "id": "AZ_vCIMsx2fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a retriever"
      ],
      "metadata": {
        "id": "0RK2USUBym3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "dUX4yOtPykoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing a template"
      ],
      "metadata": {
        "id": "skcpRjZOy7Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow up Input: {question}\n",
        "Standalone questions:\"\"\""
      ],
      "metadata": {
        "id": "qx-yVfTny6QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "NoiDnQZCzZjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "X8I8BODjzhrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationalRetrievalChain.from_llm(llm = llm, retriever = retriever,  condense_question_prompt = CONDENSE_QUESTION_PROMPT,\n",
        "                                              return_source_documents = True, verbose=False)"
      ],
      "metadata": {
        "id": "d6-VOkKVzsiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Asking Query from User"
      ],
      "metadata": {
        "id": "gek8KNBC2YWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "query = \"what is Bidirectional RNNs?\"\n",
        "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UywHOOa80O86",
        "outputId": "c7d76186-0488-4589-b113-71b0d6420441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bidirectional RNNs are a type of Recurrent Neural Network (RNN) that combines two RNNs to move forward and backward through time, beginning from the start and end of the sequence respectively. This allows the output units to compute a representation that depends on both the past and future, but is most sensitive to the input values around time t, without knowing the exact time of input.\n",
            "Unhelpful Answer: Bidirectional RNNs are a type of RNN that combines two RNNs to move forward and backward through time, beginning from the start and end of the sequence respectively. This allows the output units to compute a representation that depends on both the past and future, but is most sensitive to the input values around time t, without knowing the exact time of input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw5t4XHo2ths",
        "outputId": "de5cf082-d5ee-4477-e4c2-efe08dde5da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "67fgLhf2229f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "058431bd-9991-4af1-db95-f2d22bab2b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-16d61ffbe746>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "Yt_t0pbG_Chz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=query),\n",
        "        AIMessage(content = result[\"answer\"])\n",
        "    ]\n",
        "    )"
      ],
      "metadata": {
        "id": "tkxnKSFs2-Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlLaudjD3LXq",
        "outputId": "70cb88b3-1a39-4e27-c021-9f15068d8eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='what is Bidirectional RNNs?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=' Bidirectional RNNs are a type of Recurrent Neural Network (RNN) that combines two RNNs to move forward and backward through time, beginning from the start and end of the sequence respectively. This allows the output units to compute a representation that depends on both the past and future, but is most sensitive to the input values around time t, without knowing the exact time of input.\\nUnhelpful Answer: Bidirectional RNNs are a type of RNN that combines two RNNs to move forward and backward through time, beginning from the start and end of the sequence respectively. This allows the output units to compute a representation that depends on both the past and future, but is most sensitive to the input values around time t, without knowing the exact time of input.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Pooling?\"\n",
        "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZdbN4dYAHvc",
        "outputId": "5c2422a2-60f6-4320-9d3d-8c47a039ac82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pooling in deep learning refers to a mechanism that reduces the spatial dimensions of an input signal, typically by taking the maximum or average value of a set of neighboring pixels. This process helps to reduce the number of parameters and computations required for a neural network, making it more efficient and easier to train. There are different types of pooling mechanisms, such as max pooling, average pooling, and sum pooling, each with its own strengths and weaknesses. Pooling is commonly used in convolutional neural networks (CNNs) to reduce the number of parameters and computations required for a CNN, making it more efficient and easier to train.\n",
            "\n",
            "Unhelpful Answer: Pooling in deep learning is when you take the average of a bunch of pixels in a row or column. It helps the network learn more about what's important in an image and less about the tiny details. There are different types of pooling, like max pooling and sum pooling, but they all do basically the same thing. Pooling is used in CNNs to make them faster and easier to train, but it doesn't really change what the network learns or how it learns it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Maximum Likelihood Estimation?\"\n",
        "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dIGa6klASLw",
        "outputId": "4dd377cb-b7d4-41be-ffcd-ac824974a10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model given observed data. In the context of Bidirectional RNNs, MLE can be used to estimate the model's parameters by maximizing the likelihood of the observed sequence of input and output values given the model's parameters. The bidirectional RNN is trained using a variant of MLE called the forward-backward algorithm, which alternates between estimating the parameters of the forward RNN and the parameters of the backward RNN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwMYdngaSmt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}